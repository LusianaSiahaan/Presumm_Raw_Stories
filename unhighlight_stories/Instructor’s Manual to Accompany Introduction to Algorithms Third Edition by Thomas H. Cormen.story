Instructor‚Äôs Manual by Thomas H. Cormen to Accompany Introduction to Algorithms Third Edition by Thomas H. Cormen Charles E. Leiserson Ronald L. Rivest Clifford Stein  The MIT Press Cambridge, Massachusetts London, England   Instructor‚Äôs Manual to Accompany Introduction to Algorithms, Third Edition by Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein  Published by the MIT Press. Copyright c 2009 by The Massachusetts Institute of Technology. All rights reserved.  No part of this publication may be reproduced or distributed in any form or by any means, or stored in a database or retrieval system, without the prior written consent of The MIT Press, including, but not limited to, network or other electronic storage or transmission, or broadcast for distance learning.   Contents  Chapter 5: Probabilistic Analysis and Randomized Algorithms  R-1  Revision History Preface P-1 Chapter 2: Getting Started  Lecture Notes Solutions  2-17  2-1  Chapter 3: Growth of Functions  Chapter 4: Divide-and-Conquer  Lecture Notes Solutions 3-7  3-1  Lecture Notes Solutions  4-17  4-1  Lecture Notes Solutions 5-9  5-1  Chapter 6: Heapsort 6-1  Lecture Notes Solutions  6-10  Chapter 7: Quicksort 7-1  Lecture Notes Solutions 7-9  Chapter 8: Sorting in Linear Time  Lecture Notes Solutions  8-10  8-1  Chapter 9: Medians and Order Statistics  Lecture Notes Solutions  9-10  9-1  Chapter 11: Hash Tables  Lecture Notes Solutions  11-16  11-1  Chapter 12: Binary Search Trees  Lecture Notes Solutions  12-15  12-1  Chapter 13: Red-Black Trees  Lecture Notes Solutions  13-13  13-1  Chapter 14: Augmenting Data Structures  Lecture Notes Solutions  14-9  14-1   iv  Contents  Chapter 15: Dynamic Programming  Lecture Notes Solutions  15-21  15-1  Chapter 16: Greedy Algorithms  Lecture Notes Solutions  16-9  16-1  Chapter 17: Amortized Analysis  Lecture Notes Solutions  17-14  17-1  Chapter 21: Data Structures for Disjoint Sets  Chapter 22: Elementary Graph Algorithms  Lecture Notes Solutions  21-6  21-1  Lecture Notes Solutions  22-13  22-1  Lecture Notes Solutions  23-8  23-1  Lecture Notes Solutions  24-13  24-1  Chapter 23: Minimum Spanning Trees  Chapter 24: Single-Source Shortest Paths  Chapter 25: All-Pairs Shortest Paths  Lecture Notes Solutions  25-9  25-1  Chapter 26: Maximum Flow  Lecture Notes Solutions  26-12  26-1  Chapter 27: Multithreaded Algorithms  Solutions I-1  Index  27-1   Revision History  Revisions are listed by date rather than being numbered.                  22 February 2014. Corrected an error in the solution to Exercise 4.3-7, courtesy of Dan Suthers. Corrected an error in the solution to Exercise 23.1-6, courtesy of Rachel Ginzberg. Updated the Preface. 3 January 2012. Added solutions to Chapter 27. Added an alternative solution to Exercise 2.3-7, courtesy of Viktor Korsun and Crystal Peng. Corrected a minor error in the Chapter 15 notes in the recurrence for T .n  for the recursive CUT-ROD procedure. Updated the solution to Problem 24-3. Corrected an error in the proof about the Edmonds-Karp algorithm performing O.VE  Ô¨Çow augmentations. The bodies of all pseudocode procedures are indented slightly. 28 January 2011. Corrected an error in the solution to Problem 2-4 c , and removed unnecessary code in the solution to Problem 2-4 d . Added a missing parameter to recursive calls of REC-MAT-MULT on page 4-7. Changed the pseudocode for HEAP-EXTRACT-MAX on page 6-8 and MAX-HEAP-INSERT on page 6-9 to assume that the parameter n is passed by reference. 7 May 2010. Changed the solutions to Exercises 22.2-3 and 22.3-4 because these exercises changed. 17 February 2010. Corrected a minor error in the solution to Exercise 4.3-7. 16 December 2009. Added an alternative solution to Exercise 6.3-3, courtesy of Eyal Mashiach. 7 December 2009. Added solutions to Exercises 16.3-1, 26.1-1, 26.1-3, 26.1-7, 26.2-1, 26.2-8, 26.2-9, 26.2-12, 26.2-13, and 26.4-1 and to Problem 26-3. Cor- rected spelling in the solution to Exercise 16.2-4. Several corrections to the solution to Exercise 16.4-3, courtesy of Zhixiang Zhu. Minor changes to the solutions to Exercises 24.3-3 and 24.4-7 and Problem 24-1. 7 August 2009. Initial release.    Preface  This document is an instructor‚Äôs manual to accompany Introduction to Algorithms, Third Edition, by Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. It is intended for use in a course on algorithms. You might also Ô¨Ånd some of the material herein to be useful for a CS 2-style course in data structures. Unlike the instructor‚Äôs manual for the Ô¨Årst edition of the text‚Äîwhich was organized around the undergraduate algorithms course taught by Charles Leiserson at MIT in Spring 1991‚Äîbut like the instructor‚Äôs manual for the second edition, we have chosen to organize the manual for the third edition according to chapters of the text. That is, for most chapters we have provided a set of lecture notes and a set of exercise and problem solutions pertaining to the chapter. This organization allows you to decide how to best use the material in the manual in your own course. We have not included lecture notes and solutions for every chapter, nor have we included solutions for every exercise and problem within the chapters that we have selected. We felt that Chapter 1 is too nontechnical to include here, and Chap- ter 10 consists of background material that often falls outside algorithms and data- structures courses. We have also omitted the chapters that are not covered in the courses that we teach: Chapters 18‚Äì20 and 27‚Äì35  though we do include some solutions for Chapter 27 , as well as Appendices A‚ÄìD; future editions of this man- ual may include some of these chapters. There are two reasons that we have not included solutions to all exercises and problems in the selected chapters. First, writing up all these solutions would take a long time, and we felt it more important to release this manual in as timely a fashion as possible. Second, if we were to include all solutions, this manual would be much longer than the text itself. We have numbered the pages in this manual using the format CC-PP, where CC is a chapter number of the text and PP is the page number within that chapter‚Äôs lecture notes and solutions. The PP numbers restart from 1 at the beginning of each chapter‚Äôs lecture notes. We chose this form of page numbering so that if we add or change solutions to exercises and problems, the only pages whose numbering is affected are those for the solutions for that chapter. Moreover, if we add material for currently uncovered chapters, the numbers of the existing pages will remain unchanged.  The lecture notes  The lecture notes are based on three sources:   P-2  Preface   Some are from the Ô¨Årst-edition manual; they correspond to Charles Leiserson‚Äôs  lectures in MIT‚Äôs undergraduate algorithms course, 6.046.   Some are from Tom Cormen‚Äôs lectures in Dartmouth College‚Äôs undergraduate  algorithms course, CS 25.   Some are written just for this manual.  You will Ô¨Ånd that the lecture notes are more informal than the text, as is appro- priate for a lecture situation. In some places, we have simpliÔ¨Åed the material for lecture presentation or even omitted certain considerations. Some sections of the text‚Äîusually starred‚Äîare omitted from the lecture notes.  We have included lec- ture notes for one starred section: 12.4, on randomly built binary search trees, which we covered in an optional CS 25 lecture.  In several places in the lecture notes, we have included ‚Äúasides‚Äù to the instruc- tor. The asides are typeset in a slanted font and are enclosed in square brack- ets. [Hereisanaside.] Some of the asides suggest leaving certain material on the board, since you will be coming back to it later. If you are projecting a presenta- tion rather than writing on a blackboard or whiteboard, you might want to replicate slides containing this material so that you can easily reprise them later in the lec- ture. We have chosen not to indicate how long it takes to cover material, as the time nec- essary to cover a topic depends on the instructor, the students, the class schedule, and other variables. There are two differences in how we write pseudocode in the lecture notes and the text:   Lines are not numbered in the lecture notes. We Ô¨Ånd them inconvenient to  number when writing pseudocode on the board.   We avoid using the length attribute of an array.  Instead, we pass the array length as a parameter to the procedure. This change makes the pseudocode more concise, as well as matching better with the description of what it does.  We have also minimized the use of shading in Ô¨Ågures within lecture notes, since drawing a Ô¨Ågure with shading on a blackboard or whiteboard is difÔ¨Åcult.  The solutions  The solutions are based on the same sources as the lecture notes. They are written a bit more formally than the lecture notes, though a bit less formally than the text. We do not number lines of pseudocode, but we do use the length attribute  on the assumption that you will want your students to write pseudocode as it appears in the text . As of the third edition, we have publicly posted a few solutions on the book‚Äôs web- site. These solutions also appear in this manual, with the notation ‚ÄúThis solution is also posted publicly‚Äù after the exercise or problem number. The set of pub- licly posted solutions might increase over time, and so we encourage you to check whether a particular solution is posted on the website before you assign an exercise or problem to your students.   Preface  P-3  The index lists all the exercises and problems for which this manual provides solu- tions, along with the number of the page on which each solution starts. Asides appear in a handful of places throughout the solutions. Also, we are less reluctant to use shading in Ô¨Ågures within solutions, since these Ô¨Ågures are more likely to be reproduced than to be drawn on a board.  Source Ô¨Åles  For several reasons, we are unable to publish or transmit source Ô¨Åles for this man- ual. We apologize for this inconvenience. You can use the clrscode3e package for LATEX 2" to typeset pseudocode in the same way that we do. You can Ô¨Ånd this package at http:  www.cs.dartmouth.edu thc  clrscode . That site also includes documentation. Make sure to use the clrscode3e package, not the clrscode package; clrscode is for the second edition of the book.  Reporting errors and suggestions  Undoubtedly, instructors will Ô¨Ånd errors in this manual. Please report errors by sending email to clrs-manual-bugs@mitpress.mit.edu. If you have a suggestion for an improvement to this manual, please feel free to submit it via email to clrs-manual-suggestions@mitpress.mit.edu. As usual, if you Ô¨Ånd an error in the text itself, please verify that it has not already been posted on the errata web page before you submit it. You can use the MIT Press web site for the text, http:  mitpress.mit.edu algorithms , to locate the errata web page and to submit an error report. We thank you in advance for your assistance in correcting errors in both this manual and the text.  How we produced this manual  Like the third edition of Introduction to Algorithms, this manual was produced in LATEX 2". We used the Times font with mathematics typeset using the MathTime Pro 2 fonts. As in all three editions of the textbook, we compiled the index using Windex, a C program that we wrote. We drew the illustrations using MacDraw Pro,1 with some of the mathematical expressions in illustrations laid in with the psfrag package for LATEX 2". We created the PDF Ô¨Åles for this manual on a MacBook Pro running OS 10.5.  Acknowledgments  This manual borrows heavily from the manuals for the Ô¨Årst two editions. Julie Sussman, P.P.A., wrote the Ô¨Årst-edition manual. Julie did such a superb job on the  1See our plea in the preface for the third edition to Apple, asking that they update MacDraw Pro for OS X.   P-4  Preface  Ô¨Årst-edition manual, Ô¨Ånding numerous errors in the Ô¨Årst-edition text in the process, that we were thrilled to have her serve as technical copyeditor for both the second and third editions of the book. Charles Leiserson also put in large amounts of time working with Julie on the Ô¨Årst-edition manual. The manual for the second edition was written by Tom Cormen, Clara Lee, and Erica Lin. Clara and Erica were undergraduate computer science majors at Dart- mouth at the time, and they did a superb job. The other three Introduction to Algorithms authors‚ÄîCharles Leiserson, Ron Rivest, and Cliff Stein‚Äîprovided helpful comments and suggestions for solutions to exercises and problems. Some of the solutions are modiÔ¨Åcations of those written over the years by teaching assistants for algorithms courses at MIT and Dartmouth. At this point, we do not know which TAs wrote which solutions, and so we simply thank them collectively. Several of the solutions to new exercises and problems in the third edition were written by Sharath Gururaj of Columbia University; we thank Sharath for his Ô¨Åne work. The solutions for Chapter 27 were written by Priya Natarajan. We also thank the MIT Press and our editors‚ÄîAda Brunstein, Jim DeWolf, and Marie Lee‚Äî for moral and Ô¨Ånancial support. Tim Tregubov and Wayne Cripps provided computer support at Dartmouth.  THOMAS H. CORMEN Hanover, New Hampshire August 2009   Lecture Notes for Chapter 2: Getting Started  Chapter 2 overview  Goals  Start using frameworks for describing and analyzing algorithms.  Examine two algorithms for sorting: insertion sort and merge sort.  See how to describe algorithms in pseudocode.  Begin using asymptotic notation to express running-time analysis.  Learn the technique of ‚Äúdivide and conquer‚Äù in the context of merge sort.  Insertion sort  The sorting problem  Input: A sequence of n numbers ha1; a2; : : : ; ani. Output: A permutation  reordering  ha01; a02; : : : ; a0ni of the input sequence such  that a01  a02    a0n.  The sequences are typically stored in arrays. We also refer to the numbers as keys. Along with each key may be additional information, known as satellite data. [You might want to clarify that ‚Äúsatellite data‚Äùdoesnotnecessarilycomefromasatellite.] We will see several ways to solve the sorting problem. Each way will be expressed as an algorithm: a well-deÔ¨Åned computational procedure that takes some value, or set of values, as input and produces some value, or set of values, as output.  Expressing algorithms  We express algorithms in whatever way is the clearest and most concise. English is sometimes the best way. When issues of control need to be made perfectly clear, we often use pseudocode.   2-2  Lecture Notes for Chapter 2: Getting Started   Pseudocode is similar to C, C++, Pascal, and Java. If you know any of these  languages, you should be able to understand pseudocode.   Pseudocode is designed for expressing algorithms to humans. Software en- gineering issues of data abstraction, modularity, and error handling are often ignored.   We sometimes embed English statements into pseudocode. Therefore, unlike for ‚Äúreal‚Äù programming languages, we cannot create a compiler that translates pseudocode to machine code.  Insertion sort  A good algorithm for sorting a small number of elements. It works the way you might sort a hand of playing cards:   Start with an empty left hand and the cards face down on the table.  Then remove one card at a time from the table, and insert it into the correct   To Ô¨Ånd the correct position for a card, compare it with each of the cards already  position in the left hand.  in the hand, from right to left.   At all times, the cards held in the left hand are sorted, and these cards were  originally the top cards of the pile on the table.  Pseudocode We use a procedure INSERTION-SORT.   Takes as parameters an array A≈í1 : : n¬ç and the length n of the array.  As in Pascal, we use ‚Äú: :‚Äù to denote a range within an array.  [We usually use 1-origin indexing, as we do here. There are a few places in later chapters where we use 0-origin indexing instead. If you are translating pseudocode to C, C++, or Java, which use 0-origin indexing, you need to be carefultogettheindicesright. Oneoptionistoadjustallindexcalculations in the C, C++, or Java code to compensate. An easier option is, when using an array A≈í1 : : n¬ç,toallocatethearraytobeoneentrylonger‚ÄîA≈í0 : : n¬ç‚Äîandjust don‚Äôtusetheentryatindex 0.]   [In the lecture notes, we indicate array lengths by parameters rather than by usingthelength attributethatisusedinthebook. Thatsavesusalineofpseu- docodeeachtime. Thesolutionscontinuetousethelength attribute.]   The array A is sorted in place: the numbers are rearranged within the array,  with at most a constant number outside the array at any time.   Lecture Notes for Chapter 2: Getting Started  2-3  INSERTION-SORT.A; n  cost for j D 2 to n c1 key D A≈íj ¬ç c2    Insert A≈íj ¬ç into the sorted sequence A≈í1 : : j  cid:0  1¬ç. 0 i D j  cid:0  1 c4 while i > 0 and A≈íi ¬ç > key  A≈íi C 1¬ç D A≈íi ¬ç i D i  cid:0  1 A≈íi C 1¬ç D key  times n n  cid:0  1 n  cid:0  1 n  cid:0  1 c5 Pn c6 Pn c7 Pn n  cid:0  1  c8  jD2 tj jD2.tj  cid:0  1  jD2.tj  cid:0  1   [Leavethisontheboard, butshowonly thepseudocode fornow. We‚Äôllputinthe ‚Äúcost‚Äùand‚Äútimes‚Äùcolumnslater.]  Example  j 2 2  2 4  1 5  1 2  3 4  3 5  4 6  4 6  5 1  j 5 1  6 3  6 3  j 3 4  3 4  1 2  1 1  2 5  2 2  4 6  4 5  5 1  5 6  6 3  j 6 3  1 2  1 1  2 4  2 2  3 5  3 3  j 4 6  4 4  5 1  5 5  6 3  6 6  [ReadthisÔ¨Ågurerowbyrow. Eachpartshowswhathappensforaparticularitera- tionwiththevalueof j indicated. j indexesthe‚Äúcurrentcard‚Äùbeinginsertedinto thehand. ElementstotheleftofA≈íj ¬ç thataregreaterthan A≈íj ¬ç moveoneposition totheright, and A≈íj ¬ç movesintotheevacuated position. Theheavy vertical lines separatethepartofthearrayinwhichaniterationworks‚ÄîA≈í1 : : j ¬ç‚Äîfromthepart ofthearraythatisunaffectedbythisiteration‚ÄîA≈íj C 1 : : n¬ç. Thelastpartofthe Ô¨ÅgureshowstheÔ¨Ånalsortedarray.]  Correctness  We often use a loop invariant to help us understand why an algorithm gives the correct answer. Here‚Äôs the loop invariant for INSERTION-SORT:  Loop invariant: At the start of each iteration of the ‚Äúouter‚Äù for loop‚Äîthe loop indexed by j ‚Äîthe subarray A≈í1 : : j  cid:0  1¬ç consists of the elements orig- inally in A≈í1 : : j  cid:0  1¬ç but in sorted order.  To use a loop invariant to prove correctness, we must show three things about it:  Initialization: It is true prior to the Ô¨Årst iteration of the loop. Maintenance: If it is true before an iteration of the loop, it remains true before the  next iteration.  Termination: When the loop terminates, the invariant‚Äîusually along with the reason that the loop terminated‚Äîgives us a useful property that helps show that the algorithm is correct.   2-4  Lecture Notes for Chapter 2: Getting Started  Using loop invariants is like mathematical induction:  To prove that a property holds, you prove a base case and an inductive step.  Showing that the invariant holds before the Ô¨Årst iteration is like the base case.  Showing that the invariant holds from iteration to iteration is like the inductive  step.   The termination part differs from the usual use of mathematical induction, in which the inductive step is used inÔ¨Ånitely. We stop the ‚Äúinduction‚Äù when the loop terminates.   We can show the three parts in any order.  For insertion sort Initialization: Just before the Ô¨Årst iteration, j D 2. The subarray A≈í1 : : j  cid:0  1¬ç is the single element A≈í1¬ç, which is the element originally in A≈í1¬ç, and it is trivially sorted.  Maintenance: To be precise, we would need to state and prove a loop invariant for the ‚Äúinner‚Äù while loop. Rather than getting bogged down in another loop invariant, we instead note that the body of the inner while loop works by moving A≈íj  cid:0  1¬ç, A≈íj  cid:0  2¬ç, A≈íj  cid:0  3¬ç, and so on, by one position to the right until the proper position for key  which has the value that started out in A≈íj ¬ç  is found. At that point, the value of key is placed into this position.  Termination: The outer for loop ends when j > n, which occurs when j D nC1. Therefore, j  cid:0  1 D n. Plugging n in for j  cid:0  1 in the loop invariant, the subarray A≈í1 : : n¬ç consists of the elements originally in A≈í1 : : n¬ç but in sorted order. In other words, the entire array is sorted.  Pseudocode conventions  [Coveringmost,butnotall,here. Seebookpages20‚Äì22forallconventions.]    Indentation indicates block structure. Saves space and writing time.   Looping constructs are like in C, C++, Pascal, and Java. We assume that the loop variable in a for loop is still deÔ¨Åned when the loop exits  unlike in Pascal .      indicates that the remainder of the line is a comment.  Variables are local, unless otherwise speciÔ¨Åed.  We often use objects, which have attributes. For an attribute attr of object x, we write x:attr.  This notation matches x:attr in Java and is equivalent to x->attr in C++.  Attributes can cascade, so that if x:y is an object and this object has attribute attr, then x:y:attr indicates this object‚Äôs attribute. That is, x:y:attr is implicitly parenthesized as .x:y :attr.   Objects are treated as references, like in Java. If x and y denote objects, then the assignment y D x makes x and y reference the same object. It does not cause attributes of one object to be copied to another.  Parameters are passed by value, as in Java and C  and the default mechanism in Pascal and C++ . When an object is passed by value, it is actually a reference  or pointer  that is passed; changes to the reference itself are not seen by the caller, but changes to the object‚Äôs attributes are.   Lecture Notes for Chapter 2: Getting Started  2-5   The boolean operators ‚Äúand‚Äù and ‚Äúor‚Äù are short-circuiting: if after evaluating the left-hand operand, we know the result of the expression, then we don‚Äôt evaluate the right-hand operand.  If x is FALSE in ‚Äúx and y‚Äù then we don‚Äôt evaluate y. If x is TRUE in ‚Äúx or y‚Äù then we don‚Äôt evaluate y.   Analyzing algorithms  We want to predict the resources that the algorithm requires. Usually, running time. In order to predict resource requirements, we need a computational model.  Random-access machine  RAM  model        Instructions are executed one after another. No concurrent operations. It‚Äôs too tedious to deÔ¨Åne each of the instructions and their associated time costs. Instead, we recognize that we‚Äôll use instructions commonly found in real com- puters:   Arithmetic: add, subtract, multiply, divide, remainder, Ô¨Çoor, ceiling . Also,  shift left shift right  good for multiplying dividing by 2k .   Data movement: load, store, copy.  Control: conditional unconditional branch, subroutine call and return.  Each of these instructions takes a constant amount of time.  The RAM model uses integer and Ô¨Çoating-point types.   We don‚Äôt worry about precision, although it is crucial in certain numerical ap-  plications.   There is a limit on the word size: when working with inputs of size n, assume that integers are represented by c lg n bits for some constant c  1.  lg n is a very frequently used shorthand for log2 n.   c  1   we can hold the value of n   we can index the individual elements.  c is a constant   the word size cannot grow arbitrarily.  How do we analyze an algorithm‚Äôs running time?  The time taken by an algorithm depends on the input.   Sorting 1000 numbers takes longer than sorting 3 numbers.  A given sorting algorithm may even take differing amounts of time on two  inputs of the same size.   For example, we‚Äôll see that insertion sort takes less time to sort n elements when  they are already sorted than when they are in reverse sorted order.   2-6  Lecture Notes for Chapter 2: Getting Started  Input size Depends on the problem being studied.  Usually, the number of items in the input. Like the size n of the array being  sorted.   But could be something else. If multiplying two integers, could be the total  number of bits in the two integers.   Could be described by more than one number. For example, graph algorithm running times are usually expressed in terms of the number of vertices and the number of edges in the input graph.  Running time On a particular input, it is the number of primitive operations  steps  executed.  Want to deÔ¨Åne steps to be machine-independent.  Figure that each line of pseudocode requires a constant amount of time.  One line may take a different amount of time than another, but each execution  of line i takes the same amount of time ci .   This is assuming that the line consists only of primitive operations.      If the line is a subroutine call, then the actual call takes constant time, but the execution of the subroutine being called might not. If the line speciÔ¨Åes operations other than primitive ones, then it might take more than constant time. Example: ‚Äúsort the points by x-coordinate.‚Äù  Analysis of insertion sort  [Now add statement costs and number of times executed to INSERTION-SORT pseudocode.]   Assume that the ith line takes time ci , which is a constant.  Since the third line  is a comment, it takes no time.    For j D 2; 3; : : : ; n, let tj be the number of times that the while loop test is  Note that when a for or while loop exits in the usual way‚Äîdue to the test in the  executed for that value of j .  loop header‚Äîthe test is executed one time more than the loop body.  .cost of statement   .number of times statement is executed  :  The running time of the algorithm is  Xall statements Let T .n  D running time of INSERTION-SORT. XjD2 T .n  D c1n C c2.n  cid:0  1  C c4.n  cid:0  1  C c5  n  C c7  .tj  cid:0  1  C c8.n  cid:0  1  :  n  XjD2  tj C c6  n  XjD2 .tj  cid:0  1   The running time depends on the values of tj . These vary according to the input.   Lecture Notes for Chapter 2: Getting Started  2-7  Best case The array is already sorted.  Always Ô¨Ånd that A≈íi ¬ç  key upon the Ô¨Årst time the while loop test is run  when i D j  cid:0  1 .  All tj are 1.  Running time is  T .n  D c1n C c2.n  cid:0  1  C c4.n  cid:0  1  C c5.n  cid:0  1  C c8.n  cid:0  1   D .c1 C c2 C c4 C c5 C c8 n  cid:0  .c2 C c4 C c5 C c8  :   Can express T .n  as anC b for constants a and b  that depend on the statement  costs ci    T .n  is a linear function of n.  Worst case The array is in reverse sorted order.  Always Ô¨Ånd that A≈íi ¬ç > key in while loop test.  Have to compare key with all elements to the left of the j th position   compare  Since the while loop exits because i reaches 0, there‚Äôs one additional test after  .tj  cid:0  1  D  n  XjD2 .j  cid:0  1 .  j is known as an arithmetic series, and equation  A.1  shows that it equals      n  n  n  n  j and  tj D  XjD2  with j  cid:0  1 elements. the j  cid:0  1 tests   tj D j . XjD2 XjD2 XjD1 n.n C 1  XjD2  j D  n XjD1  2  .  n   Since  j!  cid:0  1, it equals  n.n C 1   2   cid:0  1.  [The parentheses around the summation are not strictly necessary. They are there for clarity, but it might be a good idea to remind the students that the meaningoftheexpressionwouldbethesameevenwithouttheparentheses.]   Letting k D j  cid:0  1, we see that  Running time is  n  XjD2 .j  cid:0  1  D  n cid:0 1  XkD1  n.n  cid:0  1   .  2  k D  2   cid:0  1 T .n  D c1n C c2.n  cid:0  1  C c4.n  cid:0  1  C c5 n.n C 1   C c8.n  cid:0  1  c5 c6 2  cid:0  2  cid:0    C c7 n.n  cid:0  1  2 n2 Cc1 C c2 C c4 C  C c6 n.n  cid:0  1  2 c6 D  c5 2 C 2 C  cid:0  .c2 C c4 C c5 C c8  :  2 C c8 n  Can express T .n  as an2 C bn C c for constants a; b; c  that again depend on  c7  c7  statement costs    T .n  is a quadratic function of n.  2   2-8  Lecture Notes for Chapter 2: Getting Started  Worst-case and average-case analysis  We usually concentrate on Ô¨Ånding the worst-case running time: the longest run- ning time for any input of size n.  Reasons  The worst-case running time gives a guaranteed upper bound on the running  time for any input.   For some algorithms, the worst case occurs often. For example, when search- ing, the worst case often occurs when the item being searched for is not present, and searches for absent items may be frequent.   Why not analyze the average case? Because it‚Äôs often about as bad as the worst  case. Example: Suppose that we randomly choose n numbers as the input to inser- tion sort. On average, the key in A≈íj ¬ç is less than half the elements in A≈í1 : : j  cid:0  1¬ç and it‚Äôs greater than the other half.   On average, the while loop has to look halfway through the sorted subarray A≈í1 : : j  cid:0  1¬ç to decide where to drop key.   tj  j=2. Although the average-case running time is approximately half of the worst-case running time, it‚Äôs still a quadratic function of n.  Order of growth  Another abstraction to ease analysis and focus on the important features. Look only at the leading term of the formula for running time.   Drop lower-order terms.    Ignore the constant coefÔ¨Åcient in the leading term.  Example: For insertion sort, we already abstracted away the actual statement costs to conclude that the worst-case running time is an2 C bn C c. Drop lower-order terms   an2. Ignore constant coefÔ¨Åcient   n2. But we cannot say that the worst-case running time T .n  equals n2. It grows like n2. But it doesn‚Äôt equal n2. We say that the running time is ‚Äö.n2  to capture the notion that the order of growth is n2. We usually consider one algorithm to be more efÔ¨Åcient than another if its worst- case running time has a smaller order of growth.   Lecture Notes for Chapter 2: Getting Started  2-9  Designing algorithms  There are many ways to design algorithms. For example, insertion sort is incremental: having sorted A≈í1 : : j  cid:0  1¬ç, place A≈íj ¬ç correctly, so that A≈í1 : : j ¬ç is sorted.  Divide and conquer  Another common approach.  same problem.  Divide the problem into a number of subproblems that are smaller instances of the  Conquer the subproblems by solving them recursively.  Base case: If the subproblems are small enough, just solve them by brute force. [Itwouldbeagood ideatomakesurethat yourstudents arecomfortable with recursion. Iftheyarenot,thentheywillhaveahardtimeunderstanding divide andconquer.]  Combine the subproblem solutions to give a solution to the original problem.  Merge sort  A sorting algorithm based on divide and conquer. Its worst-case running time has a lower order of growth than insertion sort. Because we are dealing with subproblems, we state each subproblem as sorting a subarray A≈íp : : r¬ç. Initially, p D 1 and r D n, but these values change as we recurse through subproblems. To sort A≈íp : : r¬ç: Divide by splitting into two subarrays A≈íp : : q¬ç and A≈íq C 1 : : r¬ç, where q is the Conquer by recursively sorting the two subarrays A≈íp : : q¬ç and A≈íq C 1 : : r¬ç. Combine by merging the two sorted subarrays A≈íp : : q¬ç and A≈íq C 1 : : r¬ç to pro- duce a single sorted subarray A≈íp : : r¬ç. To accomplish this step, we‚Äôll deÔ¨Åne a procedure MERGE.A; p; q; r .  halfway point of A≈íp : : r¬ç.  The recursion bottoms out when the subarray has just 1 element, so that it‚Äôs trivially sorted.  MERGE-SORT.A; p; r   if p < r  q D b.p C r =2c MERGE-SORT.A; p; q  MERGE-SORT.A; q C 1; r  MERGE.A; p; q; r      check for base case    divide    conquer    conquer    combine   2-10  Lecture Notes for Chapter 2: Getting Started  Initial call: MERGE-SORT.A; 1; n  [It isastounding how often students forget how easy itis tocompute the halfway point of p and r as their average .p C r =2. Weof course have to take the Ô¨Çoor toensurethatwegetanintegerindex q. Butitiscommontoseestudentsperform calculationslike p C .r  cid:0  p =2,orevenmoreelaborateexpressions,forgettingthe easywaytocomputeanaverage.]  Example Bottom-up view for n D 8: [Heavy lines demarcate subarrays used in subprob- lems.]  [Examples when n is a power of 2 are most straightforward, but students might alsowantanexamplewhen n isnotapowerof 2.] Bottom-up view for n D 11:  sorted array 3 6 5 2  4 3  5 4  7 6  8 7  1 1  2 2  2  4  5  7  1  2  3  6  2  5  4  7  1  3  2  6  5 1  2 2  1 5  7 4  4 3 6 3 initial array  2 6 7 8  merge  merge  merge  sorted array  1 1  2 2  3 2  4 3  5 4  6 4  7 5  8 6  9 6  10 11 7 7  1  2  4  4  6  7  2  3  5  6  7  2  4  7  1  4  6  3  5  7  2  6  4  7  2  1  6  4  3  7  5  2  6  4 1  7 2  2 3  6 4  1 5  4 6  7 3 7 8  5 9  6 2 10 11  initial array  merge  merge  merge  merge  [Here, atthenext-to-last levelofrecursion, someofthe subproblems have only 1 element. Therecursionbottomsoutonthesesingle-elementsubproblems.]   Lecture Notes for Chapter 2: Getting Started  2-11  Merging  What remains is the MERGE procedure. Input: Array A and indices p; q; r such that  restrictions on p; q; r, neither subarray is empty.   p  q < r.  Subarray A≈íp : : q¬ç is sorted and subarray A≈íq C 1 : : r¬ç is sorted. By the Output: The two subarrays are merged into a single sorted subarray in A≈íp : : r¬ç. We implement it so that it takes ‚Äö.n  time, where n D r  cid:0  p C 1 D the number of elements being merged. What is n? Until now, n has stood for the size of the original problem. But now we‚Äôre using it as the size of a subproblem. We will use this technique when we analyze recursive algorithms. Although we may denote the original problem size by n, in general n will be the size of a given subproblem.  Idea behind linear-time merging Think of two piles of cards.  Each pile is sorted and placed face-up on a table with the smallest cards on top.  We will merge these into a single sorted pile, face-down on the table.  A basic step:   Choose the smaller of the two top cards.  Remove it from its pile, thereby exposing a new top card.  Place the chosen card face-down onto the output pile.   Repeatedly perform basic steps until one input pile is empty.  Once one input pile empties, just take the remaining input pile and place it  face-down onto the output pile.  input piles, and we started with n cards in the input piles.   Each basic step should take constant time, since we check just the two top cards.  There are  n basic steps, since each basic step removes one card from the  Therefore, this procedure should take ‚Äö.n  time. We don‚Äôt actually need to check whether a pile is empty before each basic step.  Put on the bottom of each input pile a special sentinel card. It contains a special value that we use to simplify the code.     We use 1, since that‚Äôs guaranteed to ‚Äúlose‚Äù to any other value.  The only way that 1 cannot lose is when both piles have 1 exposed as their  But when that happens, all the nonsentinel cards have already been placed into  top cards.  the output pile.   We know in advance that there are exactly r  cid:0  p C 1 nonsentinel cards   stop once we have performed r  cid:0  p C 1 basic steps. Never a need to check for sentinels, since they‚Äôll always lose.  Rather than even counting basic steps, just Ô¨Åll up the output array from index p  up through and including index r.   2-12  Lecture Notes for Chapter 2: Getting Started  Pseudocode MERGE.A; p; q; r  n1 D q  cid:0  p C 1 n2 D r  cid:0  q let L≈í1 : : n1 C 1¬ç and R≈í1 : : n2 C 1¬ç be new arrays for i D 1 to n1 for j D 1 to n2 L≈ín1 C 1¬ç D 1 R≈ín2 C 1¬ç D 1 i D 1 j D 1 for k D p to r  L≈íi ¬ç D A≈íp C i  cid:0  1¬ç R≈íj ¬ç D A≈íq C j ¬ç  if L≈íi ¬ç  R≈íj ¬ç A≈ík¬ç D L≈íi ¬ç i D i C 1 else A≈ík¬ç D R≈íj ¬ç j D j C 1  [The book uses a loop invariant to establish that MERGE works correctly. In a lecturesituation,itisprobablybettertouseanexampletoshowthattheprocedure workscorrectly.]  Example A call of MERGE.9; 12; 16    Lecture Notes for Chapter 2: Getting Started  2-13  8 ‚Ä¶  2 4  8 ‚Ä¶  2 4 i  8 ‚Ä¶  2 4  8 ‚Ä¶  2 4  9 1  3 5  9 1  3 5  9 1  3 5 i  9 1  3 5  A  1 2 i  A  1 2  A  1 2  A  1 2  L  L  L  L  3  2  1  7  5  10 11 12 13 14 15 16 4 6 k 4 7  3 3  1 1  R  5  17 ‚Ä¶  5  4 6  2 2 j  10 11 12 13 14 15 16 2 6  2  1  2  3  17 ‚Ä¶  7 k  5  4 7  1 1  2 2  R  5  4 6  3 3 j  10 11 12 13 14 15 16 2 6  2  3  3  4  17 ‚Ä¶  5  4 7  R  2 2  3 3  4 6 j  2 k 1 1  6  5  4  2  3  10 11 12 13 14 15 16 2 6 k 3 3  1 1  2 2  R  5  17 ‚Ä¶  4 6  4 7 i  5  5  j  8 ‚Ä¶  2 4  8 ‚Ä¶  2 4 i  8 ‚Ä¶  2 4 i  8 ‚Ä¶  2 4  8 ‚Ä¶  2 4  9 2 k 3 5  9 1  3 5  9 1  3 5  9 1  3 5  9 1  3 5  A  1 2 i  A  1 2  A  1 2  A  1 2  A  1 2  L  L  L  L  L  10 11 12 13 14 15 16 4 6  5  7  1  2  3  17 ‚Ä¶  5  4 7  2 2  3 3  4 6  5  R  1 1 j  10 11 12 13 14 15 16 2 6  7  1  2  3  17 ‚Ä¶  1 1  R  5  3 3  4 6  5 k 5  4 7  10 11 12 13 14 15 16 2 6  2  2  3  3  17 ‚Ä¶  5  4 7  1 1  2 2  3 3  1 k  R  10 11 12 13 14 15 16 2 6  2  3  4  5  17 ‚Ä¶  1 1  R  3 3  4 7 i  4 7  5  5  i  10 11 12 13 14 15 16 2 7  2  3  4  5  6  1 1  2 2  3 3  R  4 6 j  4 6 j  17 ‚Ä¶ k 4 6  5  5  5  j  2 2 j  3 k 2 2  [Read this Ô¨Ågure row by row. The Ô¨Årst part shows the arrays at the start of the ‚Äúfor k D p tor‚Äùloop,whereA≈íp : : q¬ç iscopiedintoL≈í1 : : n1¬ç andA≈íqC1 : : r¬ç is copiedintoR≈í1 : : n2¬ç.Succeedingpartsshowthesituationatthestartofsuccessive iterations. Entriesin A withslashes havehadtheirvaluescopied toeither L or R andhavenothadavaluecopiedbackinyet. Entriesin L and R withslasheshave been copied back into A. The last part shows that the subarrays are merged back into A≈íp : : r¬ç,whichisnowsorted,andthatonlythesentinels 1 areexposedin thearrays L and R.]  ¬• ¬• ¬• ¬• ¬• ¬• ¬• ¬• ¬• ¬• ¬• ¬• ¬• ¬• ¬• ¬• ¬• ¬•  2-14  Lecture Notes for Chapter 2: Getting Started  Running time The Ô¨Årst two for loops take ‚Äö.n1 C n2  D ‚Äö.n  time. The last for loop makes n iterations, each taking constant time, for ‚Äö.n  time. Total time: ‚Äö.n .  Analyzing divide-and-conquer algorithms  Use a recurrence equation  more commonly, a recurrence  to describe the running time of a divide-and-conquer algorithm. Let T .n  D running time on a problem of size n.  If the problem size is small enough  say, n  c for some constant c , we have a base case. The brute-force solution takes constant time: ‚Äö.1 .  Otherwise, suppose that we divide into a subproblems, each 1=b the size of the    original.  In merge sort, a D b D 2.    Let the time to divide a size-n problem be D.n .  Have a subproblems to solve, each of size n=b   each subproblem takes  Let the time to combine solutions be C.n .  We get the recurrence  T .n=b  time to solve   we spend aT .n=b  time solving subproblems.  T .n  D  ‚Äö.1   if n  c ; aT .n=b  C D.n  C C.n  otherwise :  Analyzing merge sort  For simplicity, assume that n is a power of 2   each divide step yields two sub- problems, both of size exactly n=2. The base case occurs when n D 1. When n  2, time for merge sort steps: Divide: Just compute q as the average of p and r   D.n  D ‚Äö.1 . Conquer: Recursively solve 2 subproblems, each of size n=2   2T .n=2 . Combine: MERGE on an n-element subarray takes ‚Äö.n  time   C.n  D ‚Äö.n . Since D.n  D ‚Äö.1  and C.n  D ‚Äö.n , summed together they give a function that is linear in n: ‚Äö.n    recurrence for merge sort running time is T .n  D  ‚Äö.1   if n D 1 ; if n > 1 :  2T .n=2  C ‚Äö.n   Solving the merge-sort recurrence By the master theorem in Chapter 4, we can show that this recurrence has the solution T .n  D ‚Äö.n lg n . [Reminder: lg n standsforlog2 n.] Compared to insertion sort  ‚Äö.n2  worst-case time , merge sort is faster. Trading a factor of n for a factor of lg n is a good deal.   Lecture Notes for Chapter 2: Getting Started  2-15  On small inputs, insertion sort may be faster. But for large enough inputs, merge sort will always be faster, because its running time grows more slowly than inser- tion sort‚Äôs. We can understand how to solve the merge-sort recurrence without the master the- orem.   Let c be a constant that describes the running time for the base case and also is the time per array element for the divide and conquer steps. [Ofcourse, we cannotnecessarilyusethesameconstantforboth. It‚Äôsnotworthgoingintothis detailatthispoint.]   We rewrite the recurrence as  T .n  D  c  if n D 1 ; 2T .n=2  C cn if n > 1 :   Draw a recursion tree, which shows successive expansions of the recurrence.  For the original problem, we have a cost of cn, plus the two subproblems, each  costing T .n=2 :  cn  T n 2   T n 2    For each of the size-n=2 subproblems, we have a cost of cn=2, plus two sub-  problems, each costing T .n=4 :  cn  cn 2  cn 2  T n 4   T n 4   T n 4   T n 4    Continue expanding until the problem sizes get down to 1:   2-16  Lecture Notes for Chapter 2: Getting Started  cn 2  cn 2  lg n  cn 4  cn 4  cn 4  cn 4  c  c  c  c  c  ‚Ä¶  c  c  cn  n  cn  cn  cn  ‚Ä¶  cn  Total: cn lg n + cn   Each level has cost cn.   The top level has cost cn.  The next level down has 2 subproblems, each contributing cost cn=2.  The next level has 4 subproblems, each contributing cost cn=4.  Each time we go down one level, the number of subproblems doubles but the  cost per subproblem halves   cost per level stays the same.   There are lg n C 1 levels  height is lg n .   Use induction.  Base case: n D 1   1 level, and lg 1 C 1 D 0 C 1 D 1.    Inductive hypothesis is that a tree for a problem size of 2i has lg 2iC1 D iC1 levels.  Because we assume that the problem size is a power of 2, the next problem  size up after 2i is 2iC1.   A tree for a problem size of 2iC1 has one more level than the size-2i tree    Since lg 2iC1 C 1 D i C 2, we‚Äôre done with the inductive argument.  i C 2 levels.   Total cost is sum of costs at each level. Have lg n C 1 levels, each costing cn    total cost is cn lg n C cn. Ignore low-order term of cn and constant coefÔ¨Åcient c   ‚Äö.n lg n .     Solutions for Chapter 2: Getting Started  Solution to Exercise 2.2-2 This solution is also posted publicly  SELECTION-SORT.A  n D A:length for j D 1 to n  cid:0  1 smallest D j for i D j C 1 to n  if A≈íi ¬ç < A≈ísmallest¬ç  smallest D i  exchange A≈íj ¬ç with A≈ísmallest¬ç  The algorithm maintains the loop invariant that at the start of each iteration of the outer for loop, the subarray A≈í1 : : j  cid:0  1¬ç consists of the j  cid:0  1 smallest elements in the array A≈í1 : : n¬ç, and this subarray is in sorted order. After the Ô¨Årst n  cid:0  1 elements, the subarray A≈í1 : : n  cid:0  1¬ç contains the smallest n  cid:0  1 elements, sorted, and therefore element A≈ín¬ç must be the largest element. The running time of the algorithm is ‚Äö.n2  for all cases.  Solution to Exercise 2.2-4 This solution is also posted publicly  Modify the algorithm so it tests whether the input satisÔ¨Åes some special-case con- dition and, if it does, output a pre-computed answer. The best-case running time is generally not a good measure of an algorithm.  Solution to Exercise 2.3-3  The base case is when n D 2, and we have n lg n D 2 lg 2 D 2  1 D 2.   2-18  Solutions for Chapter 2: Getting Started  For the inductive step, our inductive hypothesis is that T .n=2  D .n=2  lg.n=2 . Then T .n  D 2T .n=2  C n  D 2.n=2  lg.n=2  C n D n.lg n  cid:0  1  C n D n lg n  cid:0  n C n D n lg n ;  which completes the inductive proof for exact powers of 2.  Solution to Exercise 2.3-4  Since it takes ‚Äö.n  time in the worst case to insert A≈ín¬ç into the sorted array A≈í1 : : n  cid:0  1¬ç, we get the recurrence T .n  D  ‚Äö.1   if n D 1 ; if n > 1 :  T .n  cid:0  1  C ‚Äö.n   Although the exercise does not ask you to solve this recurrence, its solution is T .n  D ‚Äö.n2 .  Solution to Exercise 2.3-5 This solution is also posted publicly  Procedure BINARY-SEARCH takes a sorted array A, a value , and a range ≈ílow : : high¬ç of the array, in which we search for the value . The procedure com- pares  to the array entry at the midpoint of the range and decides to eliminate half the range from further consideration. We give both iterative and recursive versions, each of which returns either an index i such that A≈íi ¬ç D , or NIL if no entry of A≈ílow : : high¬ç contains the value . The initial call to either version should have the parameters A; ; 1; n.  ITERATIVE-BINARY-SEARCH .A; ; low; high  while low  high mid D b.low C high =2c if  == A≈ímid¬ç return mid  elseif  > A≈ímid¬ç  low D mid C 1 else high D mid  cid:0  1  return NIL   Solutions for Chapter 2: Getting Started  2-19  RECURSIVE-BINARY-SEARCH .A; ; low; high   if low > high  return NIL  mid D b.low C high =2c if  == A≈ímid¬ç return mid  elseif  > A≈ímid¬ç  return RECURSIVE-BINARY-SEARCH .A; ; mid C 1; high  else return RECURSIVE-BINARY-SEARCH .A; ; low; mid  cid:0  1  Both procedures terminate the search unsuccessfully when the range is empty  i.e., low > high  and terminate it successfully if the value  has been found. Based on the comparison of  to the middle element in the searched range, the search continues with the range halved. The recurrence for these procedures is therefore T .n  D T .n=2  C ‚Äö.1 , whose solution is T .n  D ‚Äö.lg n .  Solution to Exercise 2.3-6  The while loop of lines 5‚Äì7 of procedure INSERTION-SORT scans backward through the sorted array A≈í1 : : j  cid:0  1¬ç to Ô¨Ånd the appropriate place for A≈íj ¬ç. The hitch is that the loop not only searches for the proper place for A≈íj ¬ç, but that it also moves each of the array elements that are bigger than A≈íj ¬ç one position to the right  line 6 . These movements can take as much as ‚Äö.j   time, which occurs when all the j  cid:0  1 elements preceding A≈íj ¬ç are larger than A≈íj ¬ç. We can use binary search to improve the running time of the search to ‚Äö.lg j  , but binary search will have no effect on the running time of moving the elements. Therefore, binary search alone cannot improve the worst-case running time of INSERTION-SORT to ‚Äö.n lg n .  Solution to Exercise 2.3-7  The following algorithm solves the problem:  1. Sort the elements in S. 2. Form the set S0 D f¬¥ W ¬¥ D x  cid:0  y for some y 2 Sg. 3. Sort the elements in S0. 4. Merge the two sorted sets S and S0. 5. There exist two elements in S whose sum is exactly x if and only if the same  value appears in consecutive positions in the merged output.  To justify the claim in step 4, Ô¨Årst observe that if any value appears twice in the merged output, it must appear in consecutive positions. Thus, we can restate the condition in step 5 as there exist two elements in S whose sum is exactly x if and only if the same value appears twice in the merged output.   2-20  Solutions for Chapter 2: Getting Started  Solution to Problem 2-1  Suppose that some value w appears twice. Then w appeared once in S and once in S0. Because w appeared in S0, there exists some y 2 S such that w D x  cid:0  y, or x D w C y. Since w 2 S, the elements w and y are in S and sum to x. Conversely, suppose that there are values w; y 2 S such that w C y D x. Then, since x  cid:0  y D w, the value w appears in S0. Thus, w is in both S and S0, and so it will appear twice in the merged output. Steps 1 and 3 require ‚Äö.n lg n  steps. Steps 2, 4, 5, and 6 require O.n  steps. Thus the overall running time is O.n lg n .  A reader submitted a simpler solution that also runs in ‚Äö.n lg n  time. First, sort the elements in S, taking ‚Äö.n lg n  time. Then, for each element y in S, perform a binary search in S for x  cid:0  y. Each binary search takes O.lg n  time, and there are are most n of them, and so the time for all the binary searches is O.n lg n . The overall running time is ‚Äö.n lg n .  Another reader pointed out that since S is a set, if the value x=2 appears in S, it appears in S just once, and so x=2 cannot be a solution.  [ItmaybebettertoassignthisproblemaftercoveringasymptoticnotationinSec- tion3.1;otherwisepart c maybetoodifÔ¨Åcult.]  a. Insertion sort takes ‚Äö.k2  time per k-element list in the worst case. Therefore, sorting n=k lists of k elements each takes ‚Äö.k2n=k  D ‚Äö.nk  worst-case time.  b. Just extending the 2-list merge to merge all the lists at once would take ‚Äö.n  .n=k   D ‚Äö.n2=k  time  n from copying each element once into the result list, n=k from examining n=k lists at each step to select next item for result list . To achieve ‚Äö.n lg.n=k  -time merging, we merge the lists pairwise, then merge the resulting lists pairwise, and so on, until there‚Äôs just one list. The pairwise merging requires ‚Äö.n  work at each level, since we are still working on n el- ements, even if they are partitioned among sublists. The number of levels, starting with n=k lists  with k elements each  and Ô¨Ånishing with 1 list  with n elements , is dlg.n=k e. Therefore, the total running time for the merging is ‚Äö.n lg.n=k  .  c. The modiÔ¨Åed algorithm has the same asymptotic running time as standard merge sort when ‚Äö.nk C n lg.n=k   D ‚Äö.n lg n . The largest asymptotic value of k as a function of n that satisÔ¨Åes this condition is k D ‚Äö.lg n . To see why, Ô¨Årst observe that k cannot be more than ‚Äö.lg n   i.e., it can‚Äôt have a higher-order term than lg n , for otherwise the left-hand expression wouldn‚Äôt be ‚Äö.n lg n   because it would have a higher-order term than n lg n . So all we need to do is verify that k D ‚Äö.lg n  works, which we can do by plugging k D lg n into ‚Äö.nk C n lg.n=k   D ‚Äö.nk C n lg n  cid:0  n lg k  to get   Solution to Problem 2-2  Solutions for Chapter 2: Getting Started  2-21  ‚Äö.n lg n C n lg n  cid:0  n lg lg n  D ‚Äö.2n lg n  cid:0  n lg lg n  ; which, by taking just the high-order term and ignoring the constant coefÔ¨Åcient, equals ‚Äö.n lg n .  d. In practice, k should be the largest list length on which insertion sort is faster  than merge sort.  a. We need to show that the elements of A0 form a permutation of the elements  of A.  b.  Loop invariant: At the start of each iteration of the for loop of lines 2‚Äì4, A≈íj ¬ç D minfA≈ík¬ç W j  k  ng and the subarray A≈íj : : n¬ç is a permuta- tion of the values that were in A≈íj : : n¬ç at the time that the loop started.  element A≈ín¬ç. The loop invariant trivially holds.  Initialization: Initially, j D n, and the subarray A≈íj : : n¬ç consists of single Maintenance: Consider an iteration for a given value of j . By the loop in- variant, A≈íj ¬ç is the smallest value in A≈íj : : n¬ç. Lines 3‚Äì4 exchange A≈íj ¬ç and A≈íj  cid:0  1¬ç if A≈íj ¬ç is less than A≈íj  cid:0  1¬ç, and so A≈íj  cid:0  1¬ç will be the smallest value in A≈íj  cid:0  1 : : n¬ç afterward. Since the only change to the sub- array A≈íj  cid:0  1 : : n¬ç is this possible exchange, and the subarray A≈íj : : n¬ç is a permutation of the values that were in A≈íj : : n¬ç at the time that the loop started, we see that A≈íj  cid:0  1 : : n¬ç is a permutation of the values that were in A≈íj  cid:0  1 : : n¬ç at the time that the loop started. Decrementing j for the next iteration maintains the invariant. Termination: The loop terminates when j reaches i. By the statement of the loop invariant, A≈íi ¬ç D min fA≈ík¬ç W i  k  ng and A≈íi : : n¬ç is a permutation of the values that were in A≈íi : : n¬ç at the time that the loop started.  c.  Loop invariant: At the start of each iteration of the for loop of lines 1‚Äì4, the subarray A≈í1 : : i  cid:0  1¬ç consists of the i  cid:0  1 smallest values originally in A≈í1 : : n¬ç, in sorted order, and A≈íi : : n¬ç consists of the n  cid:0  i C 1 remaining values originally in A≈í1 : : n¬ç.  Initialization: Before the Ô¨Årst iteration of the loop, i D 1. The subarray A≈í1 : : i  cid:0  1¬ç is empty, and so the loop invariant vacuously holds. Maintenance: Consider an iteration for a given value of i. By the loop invari- ant, A≈í1 : : i  cid:0  1¬ç consists of the i smallest values in A≈í1 : : n¬ç, in sorted order. Part  b  showed that after executing the for loop of lines 2‚Äì4, A≈íi ¬ç is the smallest value in A≈íi : : n¬ç, and so A≈í1 : : i ¬ç is now the i smallest values orig- inally in A≈í1 : : n¬ç, in sorted order. Moreover, since the for loop of lines 2‚Äì4 permutes A≈íi : : n¬ç, the subarray A≈íi C 1 : : n¬ç consists of the n  cid:0  i remaining values originally in A≈í1 : : n¬ç. Termination: The for loop of lines 1‚Äì4 terminates when i D n, so that i  cid:0  1 D n  cid:0  1. By the statement of the loop invariant, A≈í1 : : i  cid:0  1¬ç is the subarray   2-22  Solutions for Chapter 2: Getting Started  A≈í1 : : n cid:0 1¬ç, and it consists of the n cid:0 1 smallest values originally in A≈í1 : : n¬ç, in sorted order. The remaining element must be the largest value in A≈í1 : : n¬ç, and it is in A≈ín¬ç. Therefore, the entire array A≈í1 : : n¬ç is sorted.  Note: Tn the second edition, the for loop of lines 1‚Äì4 had an upper bound of A:length. The last iteration of the outer for loop would then result in no iterations of the inner for loop of lines 1‚Äì4, but the termination argument would simplify: A≈í1 : : i  cid:0  1¬ç would be the entire array A≈í1 : : n¬ç, which, by the loop invariant, is sorted.  d. The running time depends on the number of iterations of the for loop of lines 2‚Äì4. For a given value of i, this loop makes n  cid:0  i iterations, and i takes on the values 1; 2; : : : ; n  cid:0  1. The total number of iterations, therefore, is n cid:0 1 XiD1 .n  cid:0  i   D  n  cid:0   n cid:0 1  n cid:0 1  i  n.n  cid:0  1   2  XiD1  XiD1 D n.n  cid:0  1   cid:0  n.n  cid:0  1  D n2 2  cid:0   D  n  2  2  :  Thus, the running time of bubblesort is ‚Äö.n2  in all cases. The worst-case running time is the same as that of insertion sort.  a. The inversions are .1; 5 ; .2; 5 ; .3; 4 ; .3; 5 ; .4; 5 .  Remember that inversions  2 D n.n  cid:0  1 =2.  are speciÔ¨Åed by indices rather than by the values in the array.  b. The array with elements from f1; 2; : : : ; ng with the most inversions is hn; n  cid:0  1; n  cid:0  2; : : : ; 2; 1i. For all 1  i < j  n, there is an inversion .i; j  . The number of such inversions is cid:0 n c. Suppose that the array A starts out with an inversion .k; j  . Then k < j and A≈ík¬ç > A≈íj ¬ç. At the time that the outer for loop of lines 1‚Äì8 sets key D A≈íj ¬ç, the value that started in A≈ík¬ç is still somewhere to the left of A≈íj ¬ç. That is, it‚Äôs in A≈íi ¬ç, where 1  i < j , and so the inversion has become .i; j  . Some iteration of the while loop of lines 5‚Äì7 moves A≈íi ¬ç one position to the right. Line 8 will eventually drop key to the left of this element, thus eliminating the inversion. Because line 5 moves only elements that are greater than key, it moves only elements that correspond to inversions. In other words, each iteration of the while loop of lines 5‚Äì7 corresponds to the elimination of one inversion.  d. We follow the hint and modify merge sort to count the number of inversions in  ‚Äö.n lg n  time.  Solution to Problem 2-4 This solution is also posted publicly   Solutions for Chapter 2: Getting Started  2-23  To start, let us deÔ¨Åne a merge-inversion as a situation within the execution of merge sort in which the MERGE procedure, after copying A≈íp : : q¬ç to L and A≈íq C 1 : : r¬ç to R, has values x in L and y in R such that x > y. Consider an inversion .i; j  , and let x D A≈íi ¬ç and y D A≈íj ¬ç, so that i   y. We claim that if we were to run merge sort, there would be exactly one merge- inversion involving x and y. To see why, observe that the only way in which array elements change their positions is within the MERGE procedure. More- over, since MERGE keeps elements within L in the same relative order to each other, and correspondingly for R, the only way in which two elements can change their ordering relative to each other is for the greater one to appear in L and the lesser one to appear in R. Thus, there is at least one merge-inversion involving x and y. To see that there is exactly one such merge-inversion, ob- serve that after any call of MERGE that involves both x and y, they are in the same sorted subarray and will therefore both appear in L or both appear in R in any given call thereafter. Thus, we have proven the claim. We have shown that every inversion implies one merge-inversion. In fact, the correspondence between inversions and merge-inversions is one-to-one. Sup- pose we have a merge-inversion involving values x and y, where x originally was A≈íi ¬ç and y was originally A≈íj ¬ç. Since we have a merge-inversion, x > y. And since x is in L and y is in R, x must be within a subarray preceding the subarray containing y. Therefore x started out in a position i preceding y‚Äôs original position j , and so .i; j   is an inversion. Having shown a one-to-one correspondence between inversions and merge- inversions, it sufÔ¨Åces for us to count merge-inversions. Consider a merge-inversion involving y in R. Let ¬¥ be the smallest value in L that is greater than y. At some point during the merging process, ¬¥ and y will be the ‚Äúexposed‚Äù values in L and R, i.e., we will have ¬¥ D L≈íi ¬ç and y D R≈íj ¬ç in line 13 of MERGE. At that time, there will be merge-inversions involving y and L≈íi ¬ç; L≈íi C 1¬ç; L≈íi C 2¬ç; : : : ; L≈ín1¬ç, and these n1  cid:0  i C 1 merge-inversions will be the only ones involving y. Therefore, we need to detect the Ô¨Årst time that ¬¥ and y become exposed during the MERGE procedure and add the value of n1  cid:0  i C 1 at that time to our total count of merge-inversions. The following pseudocode, modeled on merge sort, works as we have just de- scribed. It also sorts the array A.  COUNT-INVERSIONS.A; p; r  inersions D 0 if p < r  q D b.p C r =2c inersions D inersions C COUNT-INVERSIONS.A; p; q  inersions D inersions C COUNT-INVERSIONS.A; q C 1; r  inersions D inersions C MERGE-INVERSIONS .A; p; q; r   return inersions   2-24  Solutions for Chapter 2: Getting Started  MERGE-INVERSIONS .A; p; q; r  n1 D q  cid:0  p C 1 n2 D r  cid:0  q let L≈í1 : : n1 C 1¬ç and R≈í1 : : n2 C 1¬ç be new arrays for i D 1 to n1 for j D 1 to n2 L≈ín1 C 1¬ç D 1 R≈ín2 C 1¬ç D 1 i D 1 j D 1 inersions D 0 for k D p to r  L≈íi ¬ç D A≈íp C i  cid:0  1¬ç R≈íj ¬ç D A≈íq C j ¬ç  if R≈íj ¬ç < L≈íi ¬ç  inersions D inersions C n1  cid:0  i C 1 A≈ík¬ç D R≈íj ¬ç j D j C 1 else A≈ík¬ç D L≈íi ¬ç i D i C 1 return inersions  The initial call is COUNT-INVERSIONS.A; 1; n . In MERGE-INVERSIONS, whenever R≈íj ¬ç is exposed and a value greater than R≈íj ¬ç becomes exposed in the L array, we increase inersions by the number of remaining elements in L. Then because R≈íj C 1¬ç becomes exposed, R≈íj ¬ç can never be exposed again. We don‚Äôt have to worry about merge-inversions involving the sentinel 1 in R, since no value in L will be greater than 1. Since we have added only a constant amount of additional work to each pro- cedure call and to each iteration of the last for loop of the merging procedure, the total running time of the above pseudocode is the same as for merge sort: ‚Äö.n lg n .   Lecture Notes for Chapter 3: Growth of Functions   A way to describe behavior of functions in the limit. We‚Äôre studying asymptotic   Describe growth of functions.  Focus on what‚Äôs important by abstracting away low-order terms and constant   How we indicate running times of algorithms.  A way to compare ‚Äúsizes‚Äù of functions:  Chapter 3 overview  efÔ¨Åciency.  factors.  O      ‚Äö  D o  < !  >  Asymptotic notation  O-notation O.g.n   D ff .n  W there exist positive constants c and n0 such that  0  f .n   cg.n  for all n  n0g :  cg n   f n   n  n0  g.n  is an asymptotic upper bound for f .n . If f .n  2 O.g.n  , we write f .n  D O.g.n    will precisely explain this soon .   3-2  Lecture Notes for Chapter 3: Growth of Functions  Example 2n2 D O.n3 , with c D 1 and n0 D 2. Examples of functions in O.n2 :  n2 n2 C n n2 C 1000n 1000n2 C 1000n Also, n n=1000 n1:99999 n2= lg lg lg n  -notation .g.n   D ff .n  W there exist positive constants c and n0 such that  0  cg.n   f .n  for all n  n0g :  f n   cg n   n  n0  g.n  is an asymptotic lower bound for f .n .  Example  pn D .lg n , with c D 1 and n0 D 16.  Examples of functions in .n2 :  n2 n2 C n n2  cid:0  n 1000n2 C 1000n 1000n2  cid:0  1000n Also, n3 n2:00001 n2 lg lg lg n 22n   Lecture Notes for Chapter 3: Growth of Functions  3-3  ‚Äö-notation ‚Äö.g.n   D ff .n  W there exist positive constants c1, c2, and n0 such that  0  c1g.n   f .n   c2g.n  for all n  n0g : c2g n   f n   c1g n   n  n0  g.n  is an asymptotically tight bound for f .n .  Example n2=2  cid:0  2n D ‚Äö.n2 , with c1 D 1=4, c2 D 1=2, and n0 D 8. Theorem f .n  D ‚Äö.g.n   if and only if f D O.g.n   and f D .g.n   :  Leading constants and low-order terms don‚Äôt matter.  Asymptotic notation in equations  When on right-hand side O.n2  stands for some anonymous function in the set O.n2 . 2n2 C 3n C 1 D 2n2 C ‚Äö.n  means 2n2 C 3n C 1 D 2n2 C f .n  for some f .n  2 ‚Äö.n . In particular, f .n  D 3n C 1. By the way, we interpret  of anonymous functions as D  of times the asymptotic notation appears:  n  O.i    XiD1 O.1  C O.2  C  C O.n  not OK: n hidden constants  OK: 1 anonymous function    no clean interpretation  When on left-hand side No matter how the anonymous functions are chosen on the left-hand side, there is a way to choose the anonymous functions on the right-hand side to make the equation valid. Interpret 2n2 C ‚Äö.n  D ‚Äö.n2  as meaning for all functions f .n  2 ‚Äö.n , there exists a function g.n  2 ‚Äö.n2  such that 2n2 C f .n  D g.n .   3-4  Lecture Notes for Chapter 3: Growth of Functions  Can chain together: 2n2 C 3n C 1 D 2n2 C ‚Äö.n   D ‚Äö.n2  :  Interpretation:  First equation: There exists f .n  2 ‚Äö.n  such that 2n2C3nC1 D 2n2Cf .n .  Second equation: For all g.n  2 ‚Äö.n   such as the f .n  used to make the Ô¨Årst  equation hold , there exists h.n  2 ‚Äö.n2  such that 2n2 C g.n  D h.n .  o-notation o.g.n   D ff .n  W for all constants c > 0, there exists a constant  n0 > 0 such that 0  f .n  < cg.n  for all n  n0g :  Another view, probably easier to use:  f .n  g.n  D 0.  lim n!1  !-notation !.g.n   D ff .n  W for all constants c > 0, there exists a constant  n0 > 0 such that 0  cg.n  < f .n  for all n  n0g :  Another view, again, probably easier to use:  f .n  g.n  D 1.  lim n!1  n1:9999 D o.n2  n2= lg n D o.n2  n2 ¬§ o.n2   just like 2 6< 2  n2=1000 ¬§ o.n2   n2:0001 D !.n2  n2 lg n D !.n2  n2 ¬§ !.n2   Comparisons of functions  Relational properties:  Transitivity:  ReÔ¨Çexivity:  f .n  D ‚Äö.f .n  . Same for O and .  Symmetry:  f .n  D ‚Äö.g.n   and g.n  D ‚Äö.h.n     f .n  D ‚Äö.h.n  . Same for O; ; o; and !.  Transpose symmetry:  f .n  D ‚Äö.g.n   if and only if g.n  D ‚Äö.f .n  . f .n  D O.g.n   if and only if g.n  D .f .n  . f .n  D o.g.n   if and only if g.n  D !.f .n  .   Standard notations and common functions  Lecture Notes for Chapter 3: Growth of Functions  3-5  Comparisons:  f .n  is asymptotically smaller than g.n  if f .n  D o.g.n  .  f .n  is asymptotically larger than g.n  if f .n  D !.g.n  . No trichotomy. Although intuitively, we can liken O to ,  to , etc., unlike real numbers, where a   b, we might not be able to compare functions. Example: n1Csin n and n, since 1 C sin n oscillates between 0 and 2.  [You probably do not want to use lecture time going over all the deÔ¨Ånitions and properties given in Section 3.2, but it might be worth spending a few minutes of lecturetimeonsomeofthefollowing.]  Monotonicity  Exponentials   f .n  is monotonically increasing if m  n   f .m   f .n .  f .n  is monotonically decreasing if m  n   f .m   f .n .  f .n  is strictly increasing if m < n   f .m  < f .n .  f .n  is strictly decreasing if m > n   f .m  > f .n .  Useful identities: a cid:0 1 D 1=a ; .am n D amn ; aman D amCn : Can relate rates of growth of polynomials and exponentials: for all real constants a and b such that a > 1,  nb an D 0 ;  lim n!1 which implies that nb D o.an . A suprisingly useful inequality: for all real x, ex  1 C x : As x gets closer to 0, ex gets closer to 1 C x.   3-6  Lecture Notes for Chapter 3: Growth of Functions  Logarithms  Notations:   binary logarithm  ,  natural logarithm  ,  exponentiation  ,  composition  .  lg n D log2 n ln n D loge n lgk n D .lg n k lg lg n D lg.lg n  Logarithm functions apply only to the next term in the formula, so that lg n C k means .lg n  C k, and not lg.n C k . In the expression logb a:      If we hold b constant, then the expression is strictly increasing as a increases. If we hold a constant, then the expression is strictly decreasing as b increases.  Useful identities for all real a > 0, b > 0, c > 0, and n, and where logarithm bases are not 1:  Changing the base of a logarithm from one constant to another only changes the value by a constant factor, so we usually don‚Äôt worry about logarithm bases in asymptotic notation. Convention is to use lg within asymptotic notation, unless the base actually matters. Just as polynomials grow more slowly than exponentials, logarithms grow more  nb an D 0, substitute lg n for n and 2a for a:  a D blogb a ;  logc.ab  D logc a C logc b ; logb an D n logb a ; logb a D ;  logc a logc b  logb.1=a  D  cid:0  logb a ; ;  logb a D loga b alogb c D clogb a :  1  slowly than polynomials. In lim n!1  lgb n .2a lg n D lim n!1  lim n!1 implying that lgb n D o.na .  lgb n na D 0 ;  Factorials  n≈† D 1  2  3  n. Special case: 0≈† D 1. Can use Stirling‚Äôs approximation, en1 C ‚Äö 1 n ; n≈† D to derive that lg.n≈†  D ‚Äö.n lg n .  p2 n n   Solutions for Chapter 3: Growth of Functions  Solution to Exercise 3.1-1  g.n   if f .n   g.n  ; if f .n  < g.n  :  First, let‚Äôs clarify what the function max.f .n ; g.n   is. Let‚Äôs deÔ¨Åne the function h.n  D max.f .n ; g.n  . Then h.n  D  f .n  Since f .n  and g.n  are asymptotically nonnegative, there exists n0 such that f .n   0 and g.n   0 for all n  n0. Thus for n  n0, f .n  C g.n   f .n   0 and f .n  C g.n   g.n   0. Since for any particular n, h.n  is either f .n  or g.n , we have f .n  C g.n   h.n   0, which shows that h.n  D max.f .n ; g.n    c2.f .n  C g.n   for all n  n0  with c2 D 1 in the deÔ¨Ånition of ‚Äö . Similarly, since for any particular n, h.n  is the larger of f .n  and g.n , we have for all n  n0, 0  f .n   h.n  and 0  g.n   h.n . Adding these two inequal- ities yields 0  f .n  C g.n   2h.n , or equivalently 0  .f .n  C g.n  =2  h.n , which shows that h.n  D max.f .n ; g.n    c1.f .n Cg.n   for all n  n0  with c1 D 1=2 in the deÔ¨Ånition of ‚Äö .  Solution to Exercise 3.1-2 This solution is also posted publicly  To show that .n C a b D ‚Äö.nb , we want to Ô¨Ånd constants c1; c2; n0 > 0 such that 0  c1nb  .n C a b  c2nb for all n  n0. Note that n C a  n C jaj   2n  and n C a  n  cid:0  jaj  1  2  n    Thus, when n  2jaj,  when jaj  n ,  when jaj  1  2 n .   3-8  Solutions for Chapter 3: Growth of Functions  1  n  n C a  2n :  0  Since b > 0, the inequality still holds when all parts are raised to the power b:  2  2   .n C a b  .2n b ;  nb 0  1 0  1 2b nb  .n C a b  2bnb : Thus, c1 D .1=2 b, c2 D 2b, and n0 D 2jaj satisfy the deÔ¨Ånition.  Solution to Exercise 3.1-3 This solution is also posted publicly  Solution to Exercise 3.1-4 This solution is also posted publicly  Let the running time be T .n . T .n   O.n2  means that T .n   f .n  for some function f .n  in the set O.n2 . This statement holds for any running time T .n , since the function g.n  D 0 for all n is in O.n2 , and running times are always nonnegative. Thus, the statement tells us nothing about the running time.  2nC1 D O.2n , but 22n ¬§ O.2n . To show that 2nC1 D O.2n , we must Ô¨Ånd constants c; n0 > 0 such that 0  2nC1  c  2n for all n  n0 : Since 2nC1 D 2  2n for all n, we can satisfy the deÔ¨Ånition with c D 2 and n0 D 1. To show that 22n 6D O.2n , assume there exist constants c; n0 > 0 such that 0  22n  c  2n for all n  n0 : Then 22n D 2n  2n  c  2n   2n  c. But no constant is greater than all 2n, and so the assumption leads to a contradiction.  Solution to Exercise 3.1-8  .g.n; m   D ff .n; m  W there exist positive constants c, n0, and m0  such that 0  cg.n; m   f .n; m  for all n  n0 or m  m0g :  ‚Äö.g.n; m   D ff .n; m  W there exist positive constants c1, c2, n0, and m0 such that 0  c1g.n; m   f .n; m   c2g.n; m  for all n  n0 or m  m0g :   Solutions for Chapter 3: Growth of Functions  3-9  Solution to Exercise 3.2-4 This solution is also posted publicly  dlg ne≈† is not polynomially bounded, but dlg lg ne≈† is. Proving that a function f .n  is polynomially bounded is equivalent to proving that lg.f .n   D O.lg n  for the following reasons.  If f is polynomially bounded, then there exist constants c, k, n0 such that for all n  n0, f .n   cnk. Hence, lg.f .n    kc lg n, which, since c and k are constants, means that lg.f .n   D O.lg n .     Similarly, if lg.f .n   D O.lg n , then f is polynomially bounded. In the following proofs, we will make use of the following two facts:  lg.n≈†  D ‚Äö.n lg n   by equation  3.19  .  1. 2. dlg ne D ‚Äö.lg n , because   dlg ne  lg n  dlg ne < lg n C 1  2 lg n for all n  2  lg.dlg ne≈†  D ‚Äö.dlg ne lg dlg ne   D ‚Äö.lg n lg lg n  D !.lg n  :  lg.dlg lg ne≈†  D ‚Äö.dlg lg ne lgdlg lg ne   D ‚Äö.lg lg n lg lg lg n  D o..lg lg n 2  D o.lg2.lg n   D o.lg n  :  Therefore, lg.dlg ne≈†  ¬§ O.lg n , and so dlg ne≈† is not polynomially bounded.  The last step above follows from the property that any polylogarithmic function grows more slowly than any positive polynomial function, i.e., that for constants a; b > 0, we have lgb n D o.na . Substitute lg n for n, 2 for b, and 1 for a, giving lg2.lg n  D o.lg n . Therefore, lg.dlg lg ne≈†  D O.lg n , and so dlg lg ne≈† is polynomially bounded.  Solution to Exercise 3.2-5  lg.lg n  is asymptotically larger because lg.lg n  D lg n  cid:0  1.   3-10  Solutions for Chapter 3: Growth of Functions  Solution to Exercise 3.2-6  Solution to Exercise 3.2-7  Both 2 and  C 1 equal .3 C p5 =2, and both y2 and y C 1 equal .3  cid:0  p5 =2.  We have two base cases: i D 0 and i D 1. For i D 0, we have 0  cid:0  y0 p5  1  cid:0  1 p5  and for i D 1, we have 1  cid:0  y1 p5  D  2p5  .1 C p5   cid:0  .1  cid:0  p5  2p5 2p5  D D 0 D F0 ;  D D 1 D F1 :  For the inductive case, the inductive hypothesis is that Fi cid:0 1 D .i cid:0 1  cid:0  yi cid:0 1 =p5 and Fi cid:0 2 D .i cid:0 2  cid:0  yi cid:0 2 =p5. We have Fi D Fi cid:0 1 C Fi cid:0 2 i cid:0 1  cid:0  yi cid:0 1   inductive hypothesis   i cid:0 2  cid:0  yi cid:0 2   equation  3.22    D  D  D  D  p5  C  p5  i cid:0 2. C 1   cid:0  yi cid:0 2.y C 1   p5 i cid:0 22  cid:0  yi cid:0 2 y2  p5  i  cid:0  yi p5  :   Exercise 3.2-6   Solution to Problem 3-3  a. Here is the ordering, where functions on the same line are in the same equiva-  lence class, and those higher on the page are  of those below them:   Solutions for Chapter 3: Growth of Functions  3-11  22nC1 22n .n C 1 ≈† n≈† en n  2n 2n .3=2 n .lg n lg n D nlg lg n .lg n ≈† n3 n2 D 4lg n n lg n and lg.n≈†  n D 2lg n .p2 lg n.D pn  2p2 lg n lg2 n ln n  plg n  see justiÔ¨Åcation 7 see justiÔ¨Åcation 1  see identity 1 see justiÔ¨Åcations 2, 8  see identity 2 see justiÔ¨Åcation 6 see identity 3 see identity 6, justiÔ¨Åcation 3 see identity 5, justiÔ¨Åcation 4  see justiÔ¨Åcation 5  ln ln n 2lg n lg n and lg.lg n  lg.lg n n1= lg n.D 2  and 1 Much of the ranking is based on the following properties:  see identity 7  see identity 4   Exponential functions grow faster than polynomial functions, which grow  faster than polylogarithmic functions.   The base of a logarithm doesn‚Äôt matter asymptotically, but the base of an  exponential and the degree of a polynomial do matter.  We have the following identities: 1. .lg n lg n D nlg lg n because alogb c D clogb a. 2. 4lg n D n2 because alogb c D clogb a. 3. 2lg n D n. 4. 2 D n1= lg n by raising identity 3 to the power 1= lg n. 5. 2p2 lg n D np2= lg n by raising identity 4 to the powerp2 lg n. 6.  cid:0 p2lg n D 2.1=2  lg n D 2lgpn D pn. 7. lg.lg n  D .lg n   cid:0  1. The following justiÔ¨Åcations explain some of the rankings: 1. en D 2n.e=2 n D !.n2n , since .e=2 n D !.n . 2. .lg n ≈† D !.n3  by taking logs: approximation, lg.n3  D 3 lg n. lg lg n D !.3 .  D pn because cid:0 p2lg n  lg.lg n ≈† D ‚Äö.lg n lg lg n  by Stirling‚Äôs   3-12  Solutions for Chapter 3: Growth of Functions  3. .p2 lg n D !2p2 lg n by taking logs: lg.p2 lg n D .1=2  lg n, lg 2p2 lg n D p2 lg n. .1=2  lg n D !.p2 lg n . 4. 2p2 lg n D !.lg2 n  by taking logs: lg 2p2 lg n Dp2 lg n, lg lg2 n D 2 lg lg n. p2 lg n D !.2 lg lg n . 5. ln ln n D !.2lg n  by taking logs: lg 2lg n D lg n. lg ln ln n D !.lg n . 6. lg.n≈†  D ‚Äö.n lg n   equation  3.19  . 7. n≈† D ‚Äö.nnC1=2e cid:0 n  by dropping constants and low-order terms in equa- 8. .lg n ≈† D ‚Äö..lg n lg nC1=2e cid:0  lg n  by substituting lg n for n in the previous  justiÔ¨Åcation. .lg n ≈† D ‚Äö..lg n lg nC1=2n cid:0  lg e  because alogb c D clogb a.  tion  3.18 .  b. The following f .n  is nonnegative, and for all functions gi .n  in part  a , f .n   is neither O.gi .n   nor .gi .n  .  f .n  D  22nC2  0  if n is even ; if n is odd :   Lecture Notes for Chapter 4: Divide-and-Conquer  Chapter 4 overview  Recall the divide-and-conquer paradigm, which we used for merge sort:  Divide the problem into a number of subproblems that are smaller instances of the  same problem.  Conquer the subproblems by solving them recursively.  Base case: If the subproblems are small enough, just solve them by brute force.  Combine the subproblem solutions to give a solution to the original problem.  We look at two more algorithms based on divide-and-conquer.  Analyzing divide-and-conquer algorithms  Use a recurrence to characterize the running time of a divide-and-conquer algo- rithm. Solving the recurrence gives us the asymptotic running time. A recurrence is a function is deÔ¨Åned in terms of      one or more base cases, and itself, with smaller arguments.  Examples  if n D 1 ; T .n  cid:0  1  C 1 if n > 1 :   T .n  D  1 Solution: T .n  D n.  T .n  D  1 Solution: T .n  D n lg n C n.  T .n  D  0 Solution: T .n  D lg lg n.  if n D 2 ; T .pn  C 1 if n > 2 :  if n D 1 ; 2T .n=2  C n if n  1 :   4-2  Lecture Notes for Chapter 4: Divide-and-Conquer   T .n  D  1 Solution: T .n  D ‚Äö.n lg n .  if n D 1 ; T .n=3  C T .2n=3  C n if n > 1 :  [The notes for this chapter are fairly brief because we teach recurrences in much greaterdetailinaseparatediscretemathcourse.] Many technical issues:   Floors and ceilings  [Floorsandceilings caneasily beremovedanddon‚Äôt affect thesolution tothe recurrence. Theyarebetterlefttoadiscretemathcourse.]   Exact vs. asymptotic functions  Boundary conditions  In algorithm analysis, we usually express both the recurrence and its solution using asymptotic notation.  Example: T .n  D 2T .n=2  C ‚Äö.n , with solution T .n  D ‚Äö.n lg n .  The boundary conditions are usually expressed as ‚ÄúT .n  D O.1  for sufÔ¨Å-  When we desire an exact, rather than an asymptotic, solution, we need to deal  ciently small n.‚Äù    with boundary conditions. In practice, we just use asymptotics most of the time, and we ignore boundary conditions.  [In my course, there are only two acceptable ways of solving recurrences: the substitution method andthemastermethod. Unlesstherecursion treeiscarefully accounted for,Idonotacceptitasaproofofasolution, thoughIcertainly accept a recursion tree as a way to generate a guess for substitution method. You may choosetoallowrecursiontreesasproofsinyourcourse,inwhichcasesomeofthe substitutionproofsinthesolutionsforthischapterbecomerecursiontrees. I also never use the iteration method, which had appeared in the Ô¨Årst edition of Introduction to Algorithms. I Ô¨Ånd that it is too easy to make an error in paren- thesization, and that recursion trees give a better intuitive idea than iterating the recurrenceofhowtherecurrenceprogresses.]  Maximum-subarray problem  Input: An array A≈í1 : : n¬ç of numbers. [Assume that some of the numbers are  negative,becausethisproblemistrivialwhenallnumbersarenonnegative.]  Output: Indices i and j such that A≈íi : : j ¬ç has the greatest sum of any nonempty,  contiguous subarray of A, along with the sum of the values in A≈íi : : j ¬ç.   Lecture Notes for Chapter 4: Divide-and-Conquer  4-3  Scenario   You have the prices that a stock traded at over a period of n consecutive days.  When should you have bought the stock? When should you have sold the stock?  Even though it‚Äôs in retrospect, you can yell at your stockbroker for not recom-  mending these buy and sell dates.  To convert to a maximum-subarray problem, let A≈íi ¬ç D  price after day i   cid:0   price after day .i  cid:0  1   : [Assumingthatwestartwithapriceafterday0,i.e.,justbeforeday1.] Then the nonempty, contiguous subarray with the greatest sum brackets the days that you should have held the stock. If the maximum subarray is A≈íi : : j ¬ç, then should have bought just before day i  i.e., just after day .i  cid:0  1   and sold just after day j . Why do we need to Ô¨Ånd the maximum subarray? Why not just ‚Äúbuy low, sell high‚Äù?   Lowest price might occur after the highest price.  But wouldn‚Äôt the optimal strategy involve buying at the lowest price or selling  at the highest price?   Not necessarily:  11 10 9 8 7 6  0  1  2  3  4  Maximum proÔ¨Åt is $3 per share, from buying after day 2 and selling after day 3. Yet lowest price occurs after day 4 and highest occurs after day 1.  Can solve by brute force: check all  cid:0 n computation so that each subarray A≈íi : : j ¬ç takes O.1  time, given that you‚Äôve computed A≈íi : : j  cid:0  1¬ç, so that the brute-force solution takes ‚Äö.n2  time.  2 D ‚Äö.n2  subarrays. Can organize the  Solving by divide-and-conquer  Use divide-and-conquer to solve in O.n lg n  time. [Maximum subarray might not be unique, though its value is, so we speak of a maximumsubarray,ratherthan the maximumsubarray.] Subproblem: Find a maximum subarray of A≈ílow : : high¬ç. In original call, low D 1, high D n.   4-4  Lecture Notes for Chapter 4: Divide-and-Conquer  Divide the subarray into two subarrays of as equal size as possible. Find the midpoint mid of the subarrays, and consider the subarrays A≈ílow : : mid¬ç and A≈ímid C 1 : : high¬ç.  Conquer by Ô¨Ånding a maximum subarrays of A≈ílow : : mid¬ç and A≈ímidC1 : : high¬ç. Combine by Ô¨Ånding a maximum subarray that crosses the midpoint, and using the best solution out of the three  the subarray crossing the midpoint and the two solutions found in the conquer step .  This strategy works because any subarray must either lie entirely on one side of the midpoint or cross the midpoint.  Finding the maximum subarray that crosses the midpoint Not a smaller instance of the original problem: has the added restriction that the subarray must cross the midpoint. Again, could use brute force. If size of A≈ílow : : high¬ç is n, would have n=2 choices for left endpoint and n=2 choices right endpoint, so would have ‚Äö.n2  combina- tions altogether. Can solve in linear time.   Any subarray crossing the midpoint A≈ímid¬ç is made of two subarrays A≈íi : : mid¬ç  and A≈ímid C 1 : : j ¬ç, where low  i  mid and mid < j  high. combine them.   Find maximum subarrays of the form A≈íi : : mid¬ç and A≈ímid C 1 : : j ¬ç and then  Procedure to take array A and indices low, mid, high and return a tuple giving indices of maximum subarray that crosses the midpoint, along with the sum in this maximum subarray:  FIND-MAX-CROSSING-SUBARRAY .A; low; mid; high     Find a maximum subarray of the form A≈íi : : mid¬ç. left-sum D  cid:0 1 sum D 0 for i D mid downto low sum D sum C A≈íi ¬ç if sum > left-sum left-sum D sum max-left D i     Find a maximum subarray of the form A≈ímid C 1 : : j ¬ç. right-sum D  cid:0 1 sum D 0 for j D mid C 1 to high sum D sum C A≈íj ¬ç if sum > right-sum right-sum D sum max-right D j     Return the indices and the sum of the two subarrays. return .max-left; max-right; left-sum C right-sum    Lecture Notes for Chapter 4: Divide-and-Conquer  4-5  Time: The two loops together consider each index in the range low; : : : ; high ex- actly once, and each iteration takes ‚Äö.1  time   procedure takes ‚Äö.n  time.  Divide-and-conquer procedure for the maximum-subarray problem FIND-MAXIMUM-SUBARRAY.A; low; high   if high == low  return .low; high; A≈ílow¬ç  else mid D b.low C high =2c     base case: only one element  FIND-MAXIMUM-SUBARRAY.A; low; mid   FIND-MAXIMUM-SUBARRAY.A; mid C 1; high  FIND-MAX-CROSSING-SUBARRAY.A; low; mid; high   .left-low; left-high; left-sum  D .right-low; right-high; right-sum  D .cross-low; cross-high; cross-sum  D if left-sum  right-sum and left-sum  cross-sum elseif right-sum  left-sum and right-sum  cross-sum return .right-low; right-high; right-sum  else return .cross-low; cross-high; cross-sum   return .left-low; left-high; left-sum   Initial call: FIND-MAXIMUM-SUBARRAY.A; 1; n    Divide by computing mid.  Conquer by the two recursive calls to FIND-MAXIMUM-SUBARRAY.  Combine by calling FIND-MAX-CROSSING-SUBARRAY and then determining  which of the three results gives the maximum sum.  Base case is when the subarray has only 1 element.  Analysis  Simplifying assumption: Original problem size is a power of 2, so that all sub- problem sizes are integer. [We made the same simplifying assumption when we analyzedmergesort.] Let T .n  denote the running time of FIND-MAXIMUM-SUBARRAY on a subarray of n elements. Base case: Occurs when high equals low, so that n D 1. The procedure just returns   T .n  D ‚Äö.1 . Recursive case: Occurs when n > 1.   Dividing takes ‚Äö.1  time.  Conquering solves two subproblems, each on a subarray of n=2 elements. Takes  T .n=2  time for each subproblem   2T .n=2  time for conquering.  Combining consists of calling FIND-MAX-CROSSING-SUBARRAY, which takes ‚Äö.n  time, and a constant number of constant-time tests   ‚Äö.n C ‚Äö.1  time for combining.   4-6  Lecture Notes for Chapter 4: Divide-and-Conquer  Recurrence for recursive case becomes T .n  D ‚Äö.1  C 2T .n=2  C ‚Äö.n  C ‚Äö.1   D 2T .n=2  C ‚Äö.n  The recurrence for all cases:  T .n  D  ‚Äö.1   2T .n=2  C ‚Äö.n   if n D 1 ; if n > 1 :   absorb ‚Äö.1  terms into ‚Äö.n   :  Same recurrence as for merge sort. Can use the master method to show that it has solution T .n  D ‚Äö.n lg n . Thus, with divide-and-conquer, we have developed a ‚Äö.n lg n -time solution. Better than the ‚Äö.n2 -time brute-force solution. [Canactuallysolvethisproblemin ‚Äö.n  time. SeeExercise4.1-5.]  Strassen‚Äôs algorithm for matrix multiplication  Input: Two n  n  square  matrices, A D .aij   and B D .bij  . Output: n  n matrix C D .cij  , where C D A  B, i.e.,  n  XkD1  ai kbkj  cij D for i; j D 1; 2; : : : ; n.  Need to compute n2 entries of C . Each entry is the sum of n values.  Obvious method  [Usingashorterprocedurenamethaninthebook.]  SQUARE-MAT-MULT.A; B; n  let C be a new n  n matrix for i D 1 to n cij D 0 for k D 1 to n  for j D 1 to n  return C  cij D cij C ai k  bkj  Analysis: Three nested loops, each iterates n times, and innermost loop body takes constant time   ‚Äö.n3 .   Lecture Notes for Chapter 4: Divide-and-Conquer  4-7  Is ‚Äö.n3  the best we can do? Can we multiply matrices in o.n3  time? Seems like any algorithm to multiply matrices must take .n3  time:   Must compute n2 entries.  Each entry is the sum of n terms.  But with Strassen‚Äôs method, we can multiply matrices in o.n3  time.   Strassen‚Äôs algorithm runs in ‚Äö.nlg 7  time.    2:80  lg 7  2:81.   Hence, runs in O.n2:81  time.  Simple divide-and-conquer method  B21 B22 ; A21 A22  B11 B12  A21 A22 ; B D B11 B12  C21 C22 : B21 B22 ; C D C11 C12  As with the other divide-and-conquer algorithms, assume that n is a power of 2. Partition each of A; B; C into four n=2  n=2 matrices: A D A11 A12 Rewrite C D A  B as C21 C22 D A11 A12  C11 C12 giving the four equations C11 D A11  B11 C A12  B21 ; C12 D A11  B12 C A12  B22 ; C21 D A21  B11 C A22  B21 ; C22 D A21  B12 C A22  B22 : Each of these equations multiplies two n=2  n=2 matrices and then adds their n=2  n=2 products. Use these equations to get a divide-and-conquer algorithm: [Using ashorter pro- cedurenamethaninthebook.]  REC-MAT-MULT.A; B; n  let C be a new n  n matrix if n == 1  else partition A, B, and C into n=2  n=2 submatrices  c11 D a11  b11 C11 D REC-MAT-MULT.A11; B11; n=2  C REC-MAT-MULT.A12; B21; n=2  C12 D REC-MAT-MULT.A11; B12; n=2  C REC-MAT-MULT.A12; B22; n=2  C21 D REC-MAT-MULT.A21; B11; n=2  C REC-MAT-MULT.A22; B21; n=2  C22 D REC-MAT-MULT.A21; B12; n=2  C REC-MAT-MULT.A22; B22; n=2   return C  [ThebookbrieÔ¨Çydiscussesthequestionofhowtoavoidcopyingentrieswhenpar- titioningmatrices. Canpartitionmatriceswithoutcopyingentriesbyinsteadusing index calculations. Identify a submatrix by ranges of row and column matrices   4-8  Lecture Notes for Chapter 4: Divide-and-Conquer  from the original matrix. End up representing a submatrix differently from how we represent the original matrix. The advantage of avoiding copying is that par- titioning would take only constant time, instead of ‚Äö.n2  time. The result of the asymptotic analysis won‚Äôt change, but using index calculations to avoid copying givesbetterconstantfactors.]  Analysis Let T .n  be the time to multiply two n=2  n=2 matrices. Base case: n D 1. Perform one scalar multiplication: ‚Äö.1 . Recursive case: n > 1.  8T .n=2 .   Dividing takes ‚Äö.1  time, using index calculations. [Otherwise, ‚Äö.n2  time.]  Conquering makes 8 recursive calls, each multiplying n=2  n=2 matrices    Combining takes ‚Äö.n2  time to add n=2  n=2 matrices four times. [Doesn‚Äôt evenmatterasymptotically whether weuseindex calculations orcopy: would be ‚Äö.n2  eitherway.]  Recurrence is  if n D 1 ; if n > 1 :  8T .n=2  C ‚Äö.n2   T .n  D  ‚Äö.1  Can use master method to show that it has solution T .n  D ‚Äö.n3 . Asymptotically, no better than the obvious method. Constant factors and recurrences: When setting up recurrences, can absorb con- stant factors into asymptotic notation, but cannot absorb a constant number of sub- probems. Although we absorb the 4 additions of n=2n=2 matrices into the ‚Äö.n2  time, we cannot lose the 8 in front of the T .n=2  term. If we absorb the constant number of subproblems, then the recursion tree would not be ‚Äúbushy‚Äù and would instead just be a linear chain.  Strassen‚Äôs method  Idea: Make the recursion tree less bushy. Perform only 7 recursive multiplications of n=2  n=2 matrices, rather than 8. Will cost several additions of n=2  n=2 matrices, but just a constant number more   can still absorb the constant factor for matrix additions into the ‚Äö.n=2  term. The algorithm: 1. As in the recursive method, partition each of the matrices into four n=2  n=2 2. Create 10 matrices S1; S2; : : : ; S10. Each is n=2  n=2 and is the sum or dif- ference of two matrices created in previous step. Time: ‚Äö.n2  to create all 10 matrices.  submatrices. Time: ‚Äö.1 .  3. Recursively compute 7 matrix products P1; P2; : : : ; P7, each n=2  n=2. 4. Compute n=2  n=2 submatrices of C by adding and subtracting various com-  binations of the Pi. Time: ‚Äö.n2 .   Lecture Notes for Chapter 4: Divide-and-Conquer  4-9  Analysis Recurrence will be  T .n  D  ‚Äö.1  By the master method, solution is T .n  D ‚Äö.nlg 7 .  7T .n=2  C ‚Äö.n2   if n D 1 ; if n > 1 :  Details Step 2: Create the 10 matrices S1 D B12  cid:0  B22 ; S2 D A11 C A12 ; S3 D A21 C A22 ; S4 D B21  cid:0  B11 ; S5 D A11 C A22 ; S6 D B11 C B22 ; S7 D A12  cid:0  A22 ; S8 D B21 C B22 ; S9 D A11  cid:0  A21 ; S10 D B11 C B12 : Add or subtract n=2  n=2 matrices 10 times   time is ‚Äö.n=2 . Step 3: Create the 7 matrices P1 D A11  S1 D A11  B12  cid:0  A11  B22 ; P2 D S2  B22 D A11  B22 C A12  B22 ; P3 D S3  B11 D A21  B11 C A22  B11 ; P4 D A22  S4 D A22  B21  cid:0  A22  B11 ; P5 D S5  S6 D A11  B11 C A11  B22 C A22  B11 C A22  B22 ; P6 D S7  S8 D A12  B21 C A12  B22  cid:0  A22  B21  cid:0  A22  B22 ; P7 D S9  S10 D A11  B11 C A11  B12  cid:0  A21  B11  cid:0  A21  B12 : The only multiplications needed are in the middle column; right-hand column just shows the products in terms of the original submatrices of A and B. Step 4: Add and subtract the Pi to construct submatrices of C : C11 D P5 C P4  cid:0  P2 C P6 ; C12 D P1 C P2 ; C21 D P3 C P4 ; C22 D P5 C P1  cid:0  P3  cid:0  P7 : To see how these computations work, expand each right-hand side, replacing each Pi with the submatrices of A and B that form it, and cancel terms: [We expandoutallfourright-handsideshere. Youmightwanttodojustoneortwoof them,toconvincestudentsthatitworks.]   4-10  Lecture Notes for Chapter 4: Divide-and-Conquer  A11 B11 C A11 B22 C A22 B11 C A22 B22   cid:0  A22 B11  C A22 B21   cid:0  A11 B22   cid:0  A12 B22   cid:0  A22 B22  cid:0  A22 B21 C A12 B22 C A12 B21 C A12 B21  A11 B11  A11 B12  cid:0  A11 B22  C A11 B22 C A12 B22 C A12 B22  A11 B12  A21 B11 C A22 B11   cid:0  A22 B11 C A22 B21 C A22 B21  A21 B11  A11 B11 C A11 B22 C A22 B11 C A22 B22   cid:0  A11 B22   cid:0  A22 B11   cid:0  A11 B11   cid:0  A21 B11  C A11 B12  cid:0  A11 B12 C A21 B11 C A21  B12 C A21  B12  A22 B22  Theoretical and practical notes  Strassen‚Äôs algorithm was the Ô¨Årst to beat ‚Äö.n3  time, but it‚Äôs not the asymptotically fastest known. A method by Coppersmith and Winograd runs in O.n2:376  time. Practical issues against Strassen‚Äôs algorithm:  Higher constant factor than the obvious ‚Äö.n3 -time method.  Not good for sparse matrices.  Not numerically stable: larger errors accumulate than in the obvious method.  Submatrices consume space, especially if copying. Numerical stability problem is not as bad as previously thought. And can use index calculations to reduce space requirement. Various researchers have tried to Ô¨Ånd the crossover point, where Strassen‚Äôs algo- rthm runs faster than the obvious ‚Äö.n3 -time method. Analyses  that ignore caches and hardware pipelines  have produced crossover points as low as n D 8, and ex- periments have found crossover points as low as n D 400.  Substitution method  1. Guess the solution. 2. Use induction to Ô¨Ånd the constants and show that the solution works.   Lecture Notes for Chapter 4: Divide-and-Conquer  4-11  Example  if n D 1 ; 2T .n=2  C n if n > 1 :  T .n  D  1 1. Guess: T .n  D n lg n C n. [Here, we have a recurrence with an exact func- tion, rather thanasymptotic notation, andthesolution isalsoexactrather than asymptotic. We‚Äôllhavetocheckboundaryconditionsandthebasecase.]  2. Induction:  2 C n n 2 C  Basis: n D 1   n lg n C n D 1 D T .n  Inductive step: Inductive hypothesis is that T .k  D k lg k C k for all k < n. We‚Äôll use this inductive hypothesis for T .n=2 . T .n  D 2T  n D 2 n 2 C n 2 n D n lg 2 C n C n D n.lg n  cid:0  lg 2  C n C n D n lg n  cid:0  n C n C n D n lg n C n :   by inductive hypothesis   lg  n  Generally, we use asymptotic notation:  We would write T .n  D 2T .n=2  C ‚Äö.n .  We assume T .n  D O.1  for sufÔ¨Åciently small n.  We express the solution by asymptotic notation: T .n  D ‚Äö.n lg n .  We don‚Äôt worry about boundary cases, nor do we show base cases in the substi-  tution proof.   T .n  is always constant for any constant n.  Since we are ultimately interested in an asymptotic solution to a recurrence,  it will always be possible to choose base cases that work.   When we want an asymptotic solution to a recurrence, we don‚Äôt worry about  the base cases in our proofs.   When we want an exact solution, then we have to deal with base cases.  For the substitution method:   Name the constant in the additive term.  Show the upper  O  and lower    bounds separately. Might need to use dif-  ferent constants for each.  Example T .n  D 2T .n=2  C ‚Äö.n . If we want to show an upper bound of T .n  D 2T .n=2  C O.n , we write T .n   2T .n=2  C cn for some positive constant c.   4-12  Lecture Notes for Chapter 4: Divide-and-Conquer  1. Upper bound:  Guess: T .n   d n lg n for some positive constant d . We are given c in the recurrence, and we get to choose d as any positive constant. It‚Äôs OK for d to depend on c. Substitution: T .n   2T .n=2  C cn 2 C cn D 2d 2 n D d n lg 2 C cn D d n lg n  cid:0  d n C cn  d n lg n  lg  n  n  if  cid:0 d n C cn  0 ; d  c  Therefore, T .n  D O.n lg n .  2. Lower bound: Write T .n   2T .n=2  C cn for some positive constant c.  n  n  lg  Guess: T .n   d n lg n for some positive constant d . Substitution: T .n   2T .n=2  C cn D 2d 2 C cn 2 n D d n lg 2 C cn D d n lg n  cid:0  d n C cn  d n lg n  if  cid:0 d n C cn  0 ; d  c  Therefore, T .n  D .n lg n .  Therefore, T .n  D ‚Äö.n lg n . [Forthisparticularrecurrence,wecanused D c for boththeupper-boundandlower-boundproofs. Thatwon‚Äôtalwaysbethecase.]  Make sure you show the same exact form when doing a substitution proof. Consider the recurrence T .n  D 8T .n=2  C ‚Äö.n2  : For an upper bound: T .n   8T .n=2  C cn2 : Guess: T .n   d n3. T .n   8d.n=2 3 C cn2 D 8d.n3=8  C cn2 D d n3 C cn2 6 d n3  doesn‚Äôt work!  Remedy: Subtract off a lower-order term.   Recursion trees  Lecture Notes for Chapter 4: Divide-and-Conquer  4-13  Guess: T .n   d n3  cid:0  d 0n2. T .n   8.d.n=2 3  cid:0  d 0.n=2 2  C cn2 D 8d.n3=8   cid:0  8d 0.n2=4  C cn2 D d n3  cid:0  2d 0n2 C cn2 D d n3  cid:0  d 0n2  cid:0  d 0n2 C cn2  d n3  cid:0  d 0n2  if  cid:0 d 0n2 C cn2  0 ; d 0  c  Be careful when using asymptotic notation. The false proof for the recurrence T .n  D 4T .n=4  C n, that T .n  D O.n : T .n   4.c.n=4   C n   cn C n D O.n   wrong!  Because we haven‚Äôt proven the exact form of our inductive hypothesis  which is that T .n   cn , this proof is false.  Use to generate a guess. Then verify by substitution method.  Example T .n  D T .n=3  C T .2n=3  C ‚Äö.n . For upper bound, rewrite as T .n   T .n=3  C T .2n=3  C cn; for lower bound, as T .n   T .n=3  C T .2n=3  C cn. By summing across each level, the recursion tree shows the cost at each level of recursion  minus the costs of recursive calls, which appear in subtrees :  cn  ‚Ä¶  c n 3   c 2n 3   c n 9   c 2n 9   c 2n 9   c 4n 9   cn  cn  cn  leftmost branch peters out after log3 n levels  rightmost branch peters out after log3 2 n levels   There are log3 n full levels, and after log3=2 n levels, the problem size is down  to 1.   Each level contributes  cn.  Lower bound guess:  d n log3 n D .n lg n  for some positive constant d .   4-14  Lecture Notes for Chapter 4: Divide-and-Conquer   Upper bound guess:  d n log3=2 n D O.n lg n  for some positive constant d .  Then prove by substitution.  1. Upper bound:  Guess: T .n   d n lg n. Substitution: T .n   T .n=3  C T .2n=3  C cn   d.n=3  lg.n=3  C d.2n=3  lg.2n=3  C cn D .d.n=3  lg n  cid:0  d.n=3  lg 3   C .d.2n=3  lg n  cid:0  d.2n=3  lg.3=2   C cn D d n lg n  cid:0  d..n=3  lg 3 C .2n=3  lg.3=2   C cn D d n lg n  cid:0  d..n=3  lg 3 C .2n=3  lg 3  cid:0  .2n=3  lg 2  C cn D d n lg n  cid:0  d n.lg 3  cid:0  2=3  C cn  d n lg n  if  cid:0 d n.lg 3  cid:0  2=3  C cn  0 ;  c  Therefore, T .n  D O.n lg n . Note: Make sure that the symbolic constants used in the recurrence  e.g., c  and the guess  e.g., d   are different.  d   :  lg 3  cid:0  2=3  2. Lower bound:  Guess: T .n   d n lg n. Substitution: Same as for the upper bound, but replacing  by . End up needing  c  :  0 < d  Therefore, T .n  D .n lg n .  lg 3  cid:0  2=3  Since T .n  D O.n lg n  and T .n  D .n lg n , we conclude that T .n  D ‚Äö.n lg n .  Master method  Used for many divide-and-conquer recurrences of the form T .n  D aT .n=b  C f .n  ; where a  1, b > 1, and f .n  > 0. Based on the master theorem  Theorem 4.1 . Compare nlogb a vs. f .n : Case 1: f .n  D O.nlogb a cid:0   for some constant  > 0.   f .n  is polynomially smaller than nlogb a.  Solution: T .n  D ‚Äö.nlogb a .  Intuitively: cost is dominated by leaves.    Lecture Notes for Chapter 4: Divide-and-Conquer  4-15  Case 2: f .n  D ‚Äö.nlogb a lgk n , where k  0.  [ThisformulationofCase2ismoregeneralthaninTheorem4.1,anditisgiven inExercise4.6-2.]  f .n  is within a polylog factor of nlogb a, but not smaller.  Solution: T .n  D ‚Äö.nlogb a lgkC1 n .  Intuitively: cost is nlogb a lgk n at each level, and there are ‚Äö.lg n  levels.  Simple case: k D 0   f .n  D ‚Äö.nlogb a    T .n  D ‚Äö.nlogb a lg n .  Case 3: f .n  D .nlogb aC  for some constant  > 0 and f .n  satisÔ¨Åes the regu- larity condition af .n=b   cf .n  for some constant c < 1 and all sufÔ¨Åciently large n.  f .n  is polynomially greater than nlogb a.  Solution: T .n  D ‚Äö.f .n  .  Intuitively: cost is dominated by root.   What‚Äôs with the Case 3 regularity condition?   Generally not a problem.    It always holds whenever f .n  D nk and f .n  D .nlogb aC  for constant  > 0. [Proving this makes a nice homework exercise. See below.] So you don‚Äôt need to check it when f .n  is a polynomial.  [Here‚Äôs aproof that the regularity condition holds when f .n  D nk and f .n  D .nlogb aC  forconstant  > 0. Since f .n  D .nlogb aC  and f .n  D nk, we have that k > logb a. Using a base of b and treating both sides asexponents, wehave bk > blogb a D a, and so a=bk < 1. Since a, b,and k areconstants,ifwelet c D a=bk,then c isaconstant strictlylessthan 1. Wehavethat af .n=b  D a.n=b k D .a=bk nk D cf .n ,and sotheregularityconditionissatisÔ¨Åed.]  Examples  T .n  D 5T .n=2  C ‚Äö.n2   nlog2 5 vs. n2 Since log2 5  cid:0   D 2 for some constant  > 0, use Case 1   T .n  D ‚Äö.nlg 5    T .n  D 27T .n=3  C ‚Äö.n3 lg n   nlog3 27 D n3 vs. n3 lg n Use Case 2 with k D 1   T .n  D ‚Äö.n3 lg2 n    T .n  D 5T .n=2  C ‚Äö.n3   nlog2 5 vs. n3 Now lg 5 C  D 3 for some constant  > 0 Check regularity condition  don‚Äôt really need to since f .n  is a polynomial : af .n=b  D 5.n=2 3 D 5n3=8  cn3 for c D 5=8 < 1 Use Case 3   T .n  D ‚Äö.n3    T .n  D 27T .n=3  C ‚Äö.n3= lg n   nlog3 27 D n3 vs. n3= lg n D n3 lg cid:0 1 n ¬§ ‚Äö.n3 lgk n  for any k  0. Cannot use the master method.   4-16  Lecture Notes for Chapter 4: Divide-and-Conquer  [Wedon‚Äôtprovethemastertheoreminouralgorithmscourse. Wesometimesprove asimpliÔ¨ÅedversionforrecurrencesoftheformT .n  D aT .n=b Cnc.Section4.6 ofthetexthasthefullproofofthemastertheorem.]   Solutions for Chapter 4: Divide-and-Conquer  Solution to Exercise 4.1-1  Solution to Exercise 4.1-2  If the index of the greatest element of A is i, it returns .i; i; A≈íi ¬ç .  MAX-SUBARRAY-BRUTE-FORCE.A  n D A:length max-so-far D  cid:0 1 for l D 1 to n sum D 0 for h D l to n  sum D sum C A≈íh¬ç if sum > max-so-far  max-so-far D sum low D l high D h  return .low; high   Solution to Exercise 4.1-4  If the algorithm returns a negative sum, toss out the answer and use an empty subarray instead.   4-18  Solutions for Chapter 4: Divide-and-Conquer  Solution to Exercise 4.1-5  MAX-SUBARRAY-LINEAR.A  n D A:length max-sum D  cid:0 1 ending-here-sum D  cid:0 1 for j D 1 to n ending-here-high D j if ending-here-sum > 0  ending-here-sum D ending-here-sum C A≈íj ¬ç else ending-here-low D j ending-here-sum D A≈íj ¬ç if ending-here-sum > max-sum max-sum D ending-here-sum low D ending-here-low high D ending-here-high  return .low; high; max-sum   The variables are intended as follows:        low and high demarcate a maximum subarray found so far.   max-sum gives the sum of the values in a maximum subarray found so far.  ending-here-low and ending-here-high demarcate a maximum subarray ending at index j . Since the high end of any subarray ending at index j must be j , every iteration of the for loop automatically sets ending-here-high D j . ending-here-sum gives the sum of the values in a maximum subarray ending at index j .  test within the for loop determines whether a maximum subarray The Ô¨Årst ending at index j contains just A≈íj ¬ç. As we enter an iteration of the loop, ending-here-sum has the sum of the values in a maximum subarray ending at j  cid:0  1. If ending-here-sum C A≈íj ¬ç > A≈íj ¬ç, then we extend the maximum subarray end- ing at index j  cid:0  1 to include index j .  The test in the if statement just subtracts out A≈íj ¬ç from both sides.  Otherwise, we start a new subarray at index j , so both its low and high ends have the value j and its sum is A≈íj ¬ç. Once we know the maximum subarray ending at index j , we test to see whether it has a greater sum than the maximum subarray found so far, ending at any position less than or equal to j . If it does, then we update low, high, and max-sum appropriately. Since each iteration of the for loop takes constant time, and the loop makes n iterations, the running time of MAX-SUBARRAY-LINEAR is ‚Äö.n .   Solutions for Chapter 4: Divide-and-Conquer  4-19  Solution to Exercise 4.2-2  STRASSEN.A; B  n D A:rows let C be a new n  n matrix if n == 1  else partition A and B in equations  4.9   c11 D a11  b11 let C11, C12, C21, and C22 be n=2  n=2 matrices create n=2  n=2 matrices S1; S2; : : : ; S10 and P1; P2; : : : ; P7 S1 D B12  cid:0  B22 S2 D A11 C A12 S3 D A12 C A22 S4 D B21  cid:0  B11 S5 D A11 C A22 S6 D B11 C B22 S7 D A12  cid:0  A22 S8 D B21 C B22 S9 D A11  cid:0  A21 S10 D B11 C B12 P1 D STRASSEN.A11; S1  P2 D STRASSEN.S2; B22  P3 D STRASSEN.S3; B11  P4 D STRASSEN.A22; S4  P5 D STRASSEN.S5; S6  P6 D STRASSEN.S7; S8  P7 D STRASSEN.S9; S10  C11 D P5 C P4  cid:0  P2 C P6 C12 D P1 C P2 C21 D P3 C P4 C22 D P5 C P1  cid:0  P3  cid:0  P7 combine C11, C12, C21, and C22 into C  return C  Solution to Exercise 4.2-4 This solution is also posted publicly  If you can multiply 3  3 matrices using k multiplications, then you can multiply n  n matrices by recursively multiplying n=3  n=3 matrices, in time T .n  D kT .n=3  C ‚Äö.n2 . Using the master method to solve this recurrence, consider the ratio of nlog3 k and n2:    If log3 k D 2, case 2 applies and T .n  D ‚Äö.n2 lg n . In this case, k D 9 and T .n  D o.nlg 7 .   4-20  Solutions for Chapter 4: Divide-and-Conquer      If log3 k < 2, case 3 applies and T .n  D ‚Äö.n2 . In this case, k < 9 and T .n  D o.nlg 7 . If log3 k > 2, case 1 applies and T .n  D ‚Äö.nlog3 k . In this case, k > 9. T .n  D o.nlg 7  when log3 k < lg 7, i.e., when k < 3lg 7  21:85. The largest such integer k is 21.  Thus, k D 21 and the running time is ‚Äö.nlog3 k  D ‚Äö.nlog3 21  D O.n2:80   since log3 21  2:77 .  Solution to Exercise 4.3-1  We guess that T .n   cn2 for some constant c > 0. We have T .n  D T .n  cid:0  1  C n  c.n  cid:0  1 2 C n D cn2  cid:0  2cn C c C n D cn2 C c.1  cid:0  2n  C n :  This last quantity is less than or equal to cn2 if c.1  cid:0  2n  C n  0 or, equivalently, c  n=.2n  cid:0  1 . This last condition holds for all n  1 and c  1. For the boundary condition, we set T .1  D 1, and so T .1  D 1  c  12. Thus, we can choose n0 D 1 and c D 1.  Solution to Exercise 4.3-7  If we were to try a straight substitution proof, assuming that T .n   cnlog3 4, we would get stuck: T .n   4.c.n=3 log3 4  C n 4  C n  D 4c nlog3 4 D cnlog3 4 C n ;  which is greater than cnlog3 4. Instead, we subtract off a lower-order term and as- sume that T .n   cnlog3 4  cid:0  d n. Now we have T .n   4.c.n=3 log3 4  cid:0  d n=3  C n 3  C n d n C n ;  D 4 cnlog3 4 D cnlog3 4  cid:0    cid:0   d n  3  4  4  which is less than or equal to cnlog3 4  cid:0  d n if d  3.   Solutions for Chapter 4: Divide-and-Conquer  4-21  Solution to Exercise 4.4-6 This solution is also posted publicly  Solution to Exercise 4.4-9 This solution is also posted publicly  The shortest path from the root to a leaf in the recursion tree is n ! .1=3 n ! .1=3 2n !  ! 1. Since .1=3 kn D 1 when k D log3 n, the height of the part of the tree in which every node has two children is log3 n. Since the values at each of these levels of the tree add up to cn, the solution to the recurrence is at least cn log3 n D .n lg n .  T .n  D T .Àõ n  C T ..1  cid:0  Àõ n  C cn We saw the solution to the recurrence T .n  D T .n=3 C T .2n=3 C cn in the text. This recurrence can be similarly solved. Without loss of generality, let Àõ  1 cid:0 Àõ, so that 0 < 1 cid:0 Àõ  1=2 and 1=2  Àõ < 1.  cÀõ n  c.1  cid:0  Àõ n  log1=.1 cid:0 Àõ  n  log1=Àõ n  cÀõ2n  cÀõ.1  cid:0  Àõ n  cÀõ.1  cid:0  Àõ n  c.1  cid:0  Àõ 2n  cn  ‚Ä¶  cn  cn  cn  ‚Ä¶  Total: O.n lg n   The recursion tree is full for log1=.1 cid:0 Àõ  n levels, each contributing cn, so we guess .n log1=.1 cid:0 Àõ  n  D .n lg n . It has log1=Àõ n levels, each contributing  cn, so we guess O.n log1=Àõ n  D O.n lg n . Now we show that T .n  D ‚Äö.n lg n  by substitution. To prove the upper bound, we need to show that T .n   d n lg n for a suitable constant d > 0. T .n  D T .Àõ n  C T ..1  cid:0  Àõ n  C cn   dÀõ n lg.Àõ n  C d.1  cid:0  Àõ n lg..1  cid:0  Àõ n  C cn D dÀõ n lg Àõ C dÀõ n lg n C d.1  cid:0  Àõ n lg.1  cid:0  Àõ  C d.1  cid:0  Àõ n lg n C cn D d n lg n C d n.Àõ lg Àõ C .1  cid:0  Àõ  lg.1  cid:0  Àõ   C cn  d n lg n ;  if d n.Àõ lg Àõ C .1  cid:0  Àõ  lg.1  cid:0  Àõ   C cn  0. This condition is equivalent to d.Àõ lg Àõ C .1  cid:0  Àõ  lg.1  cid:0  Àõ     cid:0 c :   4-22  Solutions for Chapter 4: Divide-and-Conquer  Since 1=2  Àõ < 1 and 0 < 1 cid:0  Àõ  1=2, we have that lg Àõ < 0 and lg.1 cid:0  Àõ  < 0. Thus, Àõ lg Àõ C .1  cid:0  Àõ  lg.1  cid:0  Àõ  < 0, so that when we multiply both sides of the inequality by this factor, we need to reverse the inequality:  d  or  Àõ lg Àõ C .1  cid:0  Àõ  lg.1  cid:0  Àõ    cid:0 c  c  :   cid:0 Àõ lg Àõ C  cid:0 .1  cid:0  Àõ  lg.1  cid:0  Àõ   d  The fraction on the right-hand side is a positive constant, and so it sufÔ¨Åces to pick any value of d that is greater than or equal to this fraction. To prove the lower bound, we need to show that T .n   d n lg n for a suitable constant d > 0. We can use the same proof as for the upper bound, substituting  for , and we get the requirement that 0 < d  Therefore, T .n  D ‚Äö.n lg n .   cid:0 Àõ lg Àõ  cid:0  .1  cid:0  Àõ  lg.1  cid:0  Àõ   c  :  Solution to Exercise 4.5-2  Solution to Problem 4-1  We need to Ô¨Ånd the largest integer a such that log4 a < lg 7. The answer is a D 48.  Note: In parts  a ,  b , and  d  below, we are applying case 3 of the master theorem, which requires the regularity condition that af .n=b   cf .n  for some constant c < 1. In each of these parts, f .n  has the form nk. The regularity condition is satisÔ¨Åed because af .n=b  D ank=bk D .a=bk nk D .a=bk f .n , and in each of the cases below, a=bk is a constant strictly less than 1. a. T .n  D 2T .n=2  C n3 D ‚Äö.n3 . This is a divide-and-conquer recurrence with a D 2, b D 2, f .n  D n3, and nlogb a D nlog2 2 D n. Since n3 D .nlog2 2C2  and a=bk D 2=23 D 1=4 < 1, case 3 of the master theorem applies, and T .n  D ‚Äö.n3 .  b. T .n  D T .9n=10  C n D ‚Äö.n . This is a divide-and-conquer recurrence with a D 1, b D 10=9, f .n  D n, and nlogb a D nlog10=9 1 D n0 D 1. Since n D .nlog10=9 1C1  and a=bk D 1=.10=9 1 D 9=10 < 1, case 3 of the master theorem applies, and T .n  D ‚Äö.n .  c. T .n  D 16T .n=4  C n2 D ‚Äö.n2 lg n . This is another divide-and-conquer recurrence with a D 16, b D 4, f .n  D n2, and nlogb a D nlog4 16 D n2. Since n2 D ‚Äö.nlog4 16 , case 2 of the master theorem applies, and T .n  D ‚Äö.n2 lg n .   Solutions for Chapter 4: Divide-and-Conquer  4-23  d. T .n  D 7T .n=3  C n2 D ‚Äö.n2 . This is a divide-and-conquer recurrence with a D 7, b D 3, f .n  D n2, and nlogb a D nlog3 7. Since 1 < log3 7 < 2, we have that n2 D .nlog3 7C  for some constant  > 0. We also have a=bk D 7=32 D 7=9 < 1, so that case 3 of the master theorem applies, and T .n  D ‚Äö.n2 .  e. T .n  D 7T .n=2  C n2 D O.nlg 7 . This is a divide-and-conquer recurrence with a D 7, b D 2, f .n  D n2, and nlogb a D nlog2 7. Since 2 < lg 7 < 3, we have that n2 D O.nlog2 7 cid:0   for some constant  > 0. Thus, case 1 of the master theorem applies, and T .n  D ‚Äö.nlg 7 . f. T .n  D 2T .n=4  C pn D ‚Äö.pn lg n . This is another divide-and-conquer recurrence with a D 2, b D 4, f .n  D pn, and nlogb a D nlog4 2 D pn. Since pn D ‚Äö.nlog4 2 , case 2 of the master theorem applies, and T .n  D ‚Äö.pn lg n .  g. T .n  D T .n  cid:0  1  C n  Using the recursion tree shown below, we get a guess of T .n  D ‚Äö.n2 .  n  n  n-1  n-2  2  1  n  n-1  n-2  :::  2  1  ‚Äö.n2   First, we prove the T .n  D .n2  part by induction. The inductive hypothesis is T .n   cn2 for some constant c > 0. T .n  D T .n  cid:0  1  C n  c.n  cid:0  1 2 C n D cn2  cid:0  2cn C c C n  cn2  if  cid:0 2cn C n C c  0 or, equivalently, n.1  cid:0  2c  C c  0. This condition holds when n  0 and 0 < c  1=2. For the upper bound, T .n  D O.n2 , we use the inductive hypothesis that T .n   cn2 for some constant c > 0. By a similar derivation, we get that T .n   cn2 if  cid:0 2cn C n C c  0 or, equivalently, n.1  cid:0  2c  C c  0. This condition holds for c D 1 and n  1. Thus, T .n  D .n2  and T .n  D O.n2 , so we conclude that T .n  D ‚Äö.n2 .   4-24  Solutions for Chapter 4: Divide-and-Conquer  h. T .n  D T .pn  C 1  The easy way to do this is with a change of variables, as on page 86 of the text. Let m D lg n and S.m  D T .2m . T .2m  D T .2m=2  C 1, so S.m  D S.m=2  C 1. Using the master theorem, nlogb a D nlog2 1 D n0 D 1 and f .n  D 1. Since 1 D ‚Äö.1 , case 2 applies and S.m  D ‚Äö.lg m . There- fore, T .n  D ‚Äö.lg lg n .  Solution to Problem 4-3  [Thisproblemissolvedonlyforpartsa,c,e,f,g,h,andi.] a. T .n  D 3T .n=2  C n lg n  We have f .n  D n lg n and nlogb a D nlg 3  n1:585. Since n lg n D O.nlg 3 cid:0   for any 0 <   0:58, by case 1 of the master theorem, we have T .n  D ‚Äö.nlg 3 .  c. T .n  D 4T .n=2  C n2pn We have f .n  D n2pn D n5=2 and nlogb a D nlog2 4 D n2. Since n5=2 D .n2C  for  D 1=2, we look at the regularity condition in case 3 of the master theorem. We have af .n=b  D 4.n=2 2pn=2 D n5=2=p2  cn5=2 for 1=p2  c < 1. Case 3 applies, and we have T .n  D ‚Äö.n2pn . e. T .n  D 2T .n=2  C n= lg n  We can get a guess by means of a recursion tree:  n=2  lg.n=2   n=2  lg.n=2   lg n  n=4  lg.n=4   n=4  lg.n=4   n=4  lg.n=4   n=4  lg.n=4   n lg n  ‚Ä¶  n lg n  lg n  cid:0  1  n  n  lg n  cid:0  2  n  ‚Ä¶ lg n  cid:0  i D ‚Äö.n lg lg n   lg n cid:0 1  XiD0  We get the sum on each level by observing that at depth i, we have 2i nodes, each with a numerator of n=2i and a denominator of lg.n=2i   D lg n  cid:0  i, so that the cost at depth i is  2i   n=2i  lg n  cid:0  i D  n  :  lg n  cid:0  i   Solutions for Chapter 4: Divide-and-Conquer  4-25  The sum for all levels is lg n cid:0 1 lg n  n  XiD0  n  i  lg n  XiD1 XiD1  lg n  cid:0  i D n D n D n  ‚Äö.lg lg n  D ‚Äö.n lg lg n  :  1= i   by equation  A.7 , the harmonic series   We can use this analysis as a guess that T .n  D ‚Äö.n lg lg n . If we were to do a straight substitution proof, it would be rather involved. Instead, we will show by substitution that T .n   n.1 C Hblg nc  and T .n   n  Hdlg ne, where Hk is the kth harmonic number: Hk D 1=1 C 1=2 C 1=3 C  C 1=k. We also deÔ¨Åne H0 D 0. Since Hk D ‚Äö.lg k , we have that Hblg nc D ‚Äö.lg blg nc  D ‚Äö.lg lg n  and Hdlg ne D ‚Äö.lg dlg ne  D ‚Äö.lg lg n . Thus, we will have that T .n  D ‚Äö.n lg lg n . The base case for the proof is for n D 1, and we use T .1  D 1. Here, lg n D 0, so that lg n D blg nc D dlg ne. Since H0 D 0, we have T .1  D 1  1.1 C H0  and T .1  D 1  0 D 1  H0. For the upper bound of T .n   n.1 C Hblg nc , we have T .n  D 2T .n=2  C n= lg n   2..n=2 .1 C Hblg.n=2 c   C n= lg n D n.1 C Hblg n cid:0 1c  C n= lg n D n.1 C Hblg nc cid:0 1 C 1= lg n   n.1 C Hblg nc cid:0 1 C 1=blg nc  D n.1 C Hblg nc  ;  where the last line follows from the identity Hk D Hk cid:0 1 C 1=k. The upper bound of T .n   n  Hdlg ne is similar: T .n  D 2T .n=2  C n= lg n   2..n=2   Hdlg.n=2 e  C n= lg n D n  Hdlg n cid:0 1e C n= lg n D n  .Hdlg ne cid:0 1 C 1= lg n   n  .Hdlg ne cid:0 1 C 1=dlg ne  D n  Hdlg ne :  Thus, T .n  D ‚Äö.n lg lg n .  f. T .n  D T .n=2  C T .n=4  C T .n=8  C n  Using the recursion tree shown below, we get a guess of T .n  D ‚Äö.n .   4-26  Solutions for Chapter 4: Divide-and-Conquer  n  n 4  n 2  n 8  n 4  n 16  n 8  n 16  n 32  n 16  n 32  n 64  log4 n  n  n 8  n. 4C2C1  8    D 7 8 n  log8 n  n. 1  32 C 1 64    64  8 C 3  4 C 2 16 C 2 D n 16C16C12C4C1 D n 49 64 D 7 n :::  2  8  XiD1 7 8i We use the substitution method to prove that T .n  D O.n . Our inductive hypothesis is that T .n   cn for some constant c > 0. We have T .n  D T .n=2  C T .n=4  C T .n=8  C n  n D ‚Äö.n   log n   cn=2 C cn=4 C cn=8 C n D 7cn=8 C n D .1 C 7c=8 n  cn  if c  8 :  Therefore, T .n  D O.n . Showing that T .n  D .n  is easy: T .n  D T .n=2  C T .n=4  C T .n=8  C n  n : Since T .n  D O.n  and T .n  D .n , we have that T .n  D ‚Äö.n .  g. T .n  D T .n  cid:0  1  C 1=n  This recurrence corresponds to the harmonic series, so that T .n  D Hn, where Hn D 1=1C1=2C1=3CC1=n. For the base case, we have T .1  D 1 D H1. For the inductive step, we assume that T .n  cid:0  1  D Hn cid:0 1, and we have T .n  D T .n  cid:0  1  C 1=n  D Hn cid:0 1 C 1=n D Hn :  h. T .n  D T .n  cid:0  1  C lg n  Since Hn D ‚Äö.lg n  by equation  A.7 , we have that T .n  D ‚Äö.lg n .  We guess that T .n  D ‚Äö.n lg n . To prove the upper bound, we will show that T .n  D O.n lg n . Our inductive hypothesis is that T .n   cn lg n for some constant c. We have   Solutions for Chapter 4: Divide-and-Conquer  4-27  T .n  D T .n  cid:0  1  C lg n   c.n  cid:0  1  lg.n  cid:0  1  C lg n D cn lg.n  cid:0  1   cid:0  c lg.n  cid:0  1  C lg n  cn lg.n  cid:0  1   cid:0  c lg.n=2  C lg n  D cn lg.n  cid:0  1   cid:0  c lg n C c C lg n < cn lg n  cid:0  c lg n C c C lg n  cn lg n ;  if  cid:0 c lg n C c C lg n  0. Equivalently,  cid:0 c lg n C c C lg n  0   since lg.n  cid:0  1   lg.n=2  for n  2   c  .c  cid:0  1  lg n lg n  c=.c  cid:0  1  : This works for c D 2 and all n  4. To prove the lower bound, we will show that T .n  D .n lg n . Our inductive hypothesis is that T .n   cn lg n C d n for constants c and d . We have T .n  D T .n  cid:0  1  C lg n   c.n  cid:0  1  lg.n  cid:0  1  C d.n  cid:0  1  C lg n D cn lg.n  cid:0  1   cid:0  c lg.n  cid:0  1  C d n  cid:0  d C lg n  cn lg.n=2   cid:0  c lg.n  cid:0  1  C d n  cid:0  d C lg n  since lg.n  cid:0  1   lg.n=2  for n  2  D cn lg n  cid:0  cn  cid:0  c lg.n  cid:0  1  C d n  cid:0  d C lg n  cn lg n ;  if  cid:0 cn  cid:0  c lg.n  cid:0  1  C d n  cid:0  d C lg n  0. Since  cid:0 cn  cid:0  c lg.n  cid:0  1  C d n  cid:0  d C lg n >   cid:0 cn  cid:0  c lg.n  cid:0  1  C d n  cid:0  d C lg.n  cid:0  1  ;  it sufÔ¨Åces to Ô¨Ånd conditions in which  cid:0 cn cid:0 c lg.n cid:0 1 Cd n cid:0 dClg.n cid:0 1   0. Equivalently,  cid:0 cn  cid:0  c lg.n  cid:0  1  C d n  cid:0  d C lg.n  cid:0  1   0  .d  cid:0  c n  .c  cid:0  1  lg.n  cid:0  1  C d :  This works for c D 1, d D 2, and all n  2. Since T .n  D O.n lg n  and T .n  D .n lg n , we conclude that T .n  D ‚Äö.n lg n .  i. T .n  D T .n  cid:0  2  C 2 lg n  We guess that T .n  D ‚Äö.n lg n . We show the upper bound of T .n  D O.n lg n  by means of the inductive hypothesis T .n   cn lg n for some con- stant c > 0. We have T .n  D T .n  cid:0  2  C 2 lg n   c.n  cid:0  2  lg.n  cid:0  2  C 2 lg n  c.n  cid:0  2  lg n C 2 lg n D .cn  cid:0  2c C 2  lg n   4-28  Solutions for Chapter 4: Divide-and-Conquer  D cn lg n C .2  cid:0  2c  lg n if c > 1 :  cn lg n  Therefore, T .n  D O.n lg n . For the lower bound of T .n  D .n lg n , we‚Äôll show that T .n   cn lg nCd n, for constants c; d > 0 to be chosen. We assume that n  4, which implies that 1. lg.n  cid:0  2   lg.n=2 , 2. n=2  lg n, and 3. n=2  2.  We‚Äôll use these inequalities as we go along.  We have T .n   c.n  cid:0  2  lg.n  cid:0  2  C d.n  cid:0  2  C 2 lg n  D cn lg.n  cid:0  2   cid:0  2c lg.n  cid:0  2  C d n  cid:0  2d C 2 lg n > cn lg.n  cid:0  2   cid:0  2c lg n C d n  cid:0  2d C 2 lg n   since  cid:0  lg n <  cid:0  lg.n  cid:0  2    D cn lg.n  cid:0  2   cid:0  2.c  cid:0  1  lg n C d n  cid:0  2d  cn lg.n=2   cid:0  2.c  cid:0  1  lg n C d n  cid:0  2d D cn lg n  cid:0  cn  cid:0  2.c  cid:0  1  lg n C d n  cid:0  2d  cn lg n ;   by inequality  1  above   if  cid:0 cn  cid:0  2.c  cid:0  1  lg n C d n  cid:0  2d  0 or, equivalently, d n  cn C 2.c  cid:0  1  lg n C 2d . Pick any constant c > 1=2, and then pick any constant d such that  d  2.2c  cid:0  1  :  The requirement that c > 1=2 means that d is positive.  Then  d=2  2c  cid:0  1 D c C .c  cid:0  1  ; and adding d=2 to both sides, we have  d  c C .c  cid:0  1  C d=2 : Multiplying by n yields  d n  cn C .c  cid:0  1 n C d n=2 ; and then both multiplying and dividing the middle term by 2 gives  d n  cn C 2.c  cid:0  1 n=2 C d n=2 : Using inequalities  2  and  3  above, we get d n  cn C 2.c  cid:0  1  lg n C 2d ; which is what we needed to show. Thus T .n  D .n lg n . Since T .n  D O.n lg n  and T .n  D .n lg n , we conclude that T .n  D ‚Äö.n lg n .   Lecture Notes for Chapter 5: Probabilistic Analysis and Randomized Algorithms  The hiring problem  [This chapter introduces probabilistic analysis and randomized algorithms. It as- sumesthatthestudentisfamiliarwiththebasicprobabilitymaterialinAppendixC. Theprimarygoalsofthesenotesareto   explain the difference between probabilistic analysis and randomized algo-   presentthetechniqueofindicatorrandomvariables,and  giveanotherexampleoftheanalysisofarandomizedalgorithm  permutingan  rithms,  arrayinplace .  Thesenotesomitthetechniqueofpermutinganarraybysorting,andtheyomitthe starredSection5.4.]  Scenario  You are using an employment agency to hire a new ofÔ¨Åce assistant.  The agency sends you one candidate each day.  You interview the candidate and must immediately decide whether or not to hire that person. But if you hire, you must also Ô¨Åre your current ofÔ¨Åce assis- tant‚Äîeven if it‚Äôs someone you have recently hired.   Cost to interview is ci per candidate  interview fee paid to agency .  Cost to hire is ch per candidate  includes cost to Ô¨Åre current ofÔ¨Åce assistant +  hiring fee paid to agency .   Assume that ch > ci .  You are committed to having hired, at all times, the best candidate seen so far. Meaning that whenever you interview a candidate who is better than your current ofÔ¨Åce assistant, you must Ô¨Åre the current ofÔ¨Åce assistant and hire the candidate. Since you must have someone hired at all times, you will always hire the Ô¨Årst candidate that you interview.  Goal Determine what the price of this strategy will be.   5-2  Lecture Notes for Chapter 5: Probabilistic Analysis and Randomized Algorithms  Pseudocode to model this scenario Assumes that the candidates are numbered 1 to n and that after interviewing each candidate, we can determine if it‚Äôs better than the current ofÔ¨Åce assistant. Uses a dummy candidate 0 that is worse than all others, so that the Ô¨Årst candidate is always hired.     candidate 0 is a least-qualiÔ¨Åed dummy candidate  HIRE-ASSISTANT.n  best D 0 for i D 1 to n  interview candidate i if candidate i is better than candidate best  best D i hire candidate i  Cost If n candidates, and we hire m of them, the cost is O.nci C mch .  Have to pay nci to interview, no matter how many we hire.  So we focus on analyzing the hiring cost mch.  mch varies with each run‚Äîit depends on the order in which we interview the  candidates.   This is a model of a common paradigm: we need to Ô¨Ånd the maximum or minimum in a sequence by examining each element and maintaining a current ‚Äúwinner.‚Äù The variable m denotes how many times we change our notion of which element is currently winning.  Worst-case analysis  In the worst case, we hire all n candidates. This happens if each one is better than all who came before. In other words, if the candidates appear in increasing order of quality. If we hire all n, then the cost is O.nci C nch  D O.nch   since ch > ci .  Probabilistic analysis  In general, we have no control over the order in which candidates appear. We could assume that they come in a random order:   Assign a rank to each candidate: rank.i   is a unique integer in the range 1 to n.  The ordered list hrank.1 ; rank.2 ; : : : ; rank.n i is a permutation of the candi-  The list of ranks is equally likely to be any one of the n≈† permutations.  Equivalently, the ranks form a uniform random permutation: each of the pos-  date numbers h1; 2; : : : ; ni.  sible n≈† permutations appears with equal probability.   Lecture Notes for Chapter 5: Probabilistic Analysis and Randomized Algorithms  5-3  Essential idea of probabilistic analysis We must use knowledge of, or make assumptions about, the distribution of inputs.   The expectation is over this distribution.  This technique requires that we can make a reasonable characterization of the  input distribution.  Randomized algorithms  We might not know the distribution of inputs, or we might not be able to model it computationally. Instead, we use randomization within the algorithm in order to impose a distribu- tion on the inputs.  For the hiring problem Change the scenario:   The employment agency sends us a list of all n candidates in advance.  On each day, we randomly choose a candidate from the list to interview  but    considering only those we have not yet interviewed . Instead of relying on the candidates being presented to us in a random order, we take control of the process and enforce a random order.  What makes an algorithm randomized An algorithm is randomized if its behavior is determined in part by values pro- duced by a random-number generator.  RANDOM.a; b  returns an integer r, where a  r  b and each of the b cid:0  aC 1 possible values of r is equally likely. In practice, RANDOM is implemented by a pseudorandom-number generator, which is a deterministic method returning numbers that ‚Äúlook‚Äù random and pass statistical tests.    Indicator random variables  A simple yet powerful technique for computing the expected value of a random variable. Helpful in situations in which there may be dependence. Given a sample space and an event A, we deÔ¨Åne the indicator random variable  IfAg D  1 if A occurs ;  0 if A does not occur :  Lemma For an event A, let XA D IfAg. Then E ≈íXA¬ç D PrfAg.   5-4  Lecture Notes for Chapter 5: Probabilistic Analysis and Randomized Algorithms  Proof Letting A be the complement of A, we have E ≈íXA¬ç D E ≈íIfAg¬ç  D 1  PrfAg C 0  PrÀöA cid:9   deÔ¨Ånition of expected value  D PrfAg :   lemma   Simple example Determine the expected number of heads when we Ô¨Çip a fair coin one time.  Sample space is fH; T g.  PrfHg D PrfT g D 1=2.  DeÔ¨Åne indicator random variable XH D IfHg. XH counts the number of heads  Since PrfHg D 1=2, lemma says that E ≈íXH ¬ç D 1=2.  in one Ô¨Çip.  Slightly more complicated example Determine the expected number of heads in n coin Ô¨Çips.   Let X be a random variable for the number of heads in n Ô¨Çips.    kD0 k  PrfX D kg. In fact, this is what the book  does in equation  C.37 . Instead, we‚Äôll use indicator random variables.   Could compute E ≈íX ¬ç D Pn  For i D 1; 2; : : : ; n, deÔ¨Åne Xi D Ifthe ith Ô¨Çip results in event Hg.  Then X DPn  Lemma says that E ≈íXi ¬ç D PrfHg D 1=2 for i D 1; 2; : : : ; n.  Expected number of heads is E ≈íX ¬ç D E ≈íPn  Problem: We want E ≈íPn  iD1 Xi ¬ç. We have only the individual expectations  Solution: Linearity of expectation says that the expectation of the sum equals  E ≈íX1¬ç ; E ≈íX2¬ç ; : : : ; E ≈íXn¬ç.  iD1 Xi ¬ç.  iD1 Xi.  the sum of the expectations. Thus,  Xi  n  D  E ≈íX ¬ç D E" n XiD1 XiD1 XiD1 D D n=2 :  n  1=2  E ≈íXi ¬ç   Linearity of expectation applies even when there is dependence among the ran- dom variables. [Not an issue in this example, but it can be a great help. The hat-checkproblemofExercise5.2-4isaproblemwithlotsofdependence. See thesolutiononpage5-11ofthismanual.]   Lecture Notes for Chapter 5: Probabilistic Analysis and Randomized Algorithms  5-5  Analysis of the hiring problem  Assume that the candidates arrive in a random order. Let X be a random variable that equals the number of times we hire a new ofÔ¨Åce assistant. DeÔ¨Åne indicator random variables X1; X2; : : : ; Xn, where Xi D Ifcandidate i is hiredg : Useful properties:  X D X1 C X2 C  C Xn.  Lemma   E ≈íXi ¬ç D Prfcandidate i is hiredg. We need to compute Prfcandidate i is hiredg.  Candidate i is hired if and only if candidate i is better than each of candidates  1; 2; : : : ; i  cid:0  1.  Assumption that the candidates arrive in random order   candidates 1; 2; : : : ; i arrive in random order   any one of these Ô¨Årst i candidates is equally likely to be the best one so far.  Xi   Thus, Prfcandidate i is the best so farg D 1= i.  Which implies E ≈íXi ¬ç D 1= i. Now compute E ≈íX ¬ç: E ≈íX ¬ç D E" n XiD1 XiD1 XiD1  E ≈íXi ¬ç  D  1= i  D D ln n C O.1   n  n   equation  A.7 : the sum is a harmonic series  .  Thus, the expected hiring cost is O.ch ln n , which is much better than the worst- case cost of O.nch .  Randomized algorithms  Instead of assuming a distribution of the inputs, we impose a distribution.  The hiring problem  For the hiring problem, the algorithm is deterministic:   For any given input, the number of times we hire a new ofÔ¨Åce assistant will  always be the same.   5-6  Lecture Notes for Chapter 5: Probabilistic Analysis and Randomized Algorithms     The number of times we hire a new ofÔ¨Åce assistant depends only on the input. In fact, it depends only on the ordering of the candidates‚Äô ranks that it is given.  Some rank orderings will always produce a high hiring cost. Example: h1; 2; 3; 4; 5; 6i, where each candidate is hired.  Some will always produce a low hiring cost. Example: any ordering in which the best candidate is the Ô¨Årst one interviewed. Then only the best candidate is hired.   Some may be in between.  Instead of always interviewing the candidates in the order presented, what if we Ô¨Årst randomly permuted this order?   The randomization is now in the algorithm, not in the input distribution.  Given a particular input, we can no longer say what its hiring cost will be. Each    time we run the algorithm, we can get a different hiring cost. In other words, each time we run the algorithm, the execution depends on the random choices made.   No particular input always elicits worst-case behavior.  Bad behavior occurs only if we get ‚Äúunlucky‚Äù numbers from the random-  number generator.  Pseudocode for randomized hiring problem RANDOMIZED-HIRE-ASSISTANT.n   randomly permute the list of candidates HIRE-ASSISTANT .n   Lemma The expected hiring cost of RANDOMIZED-HIRE-ASSISTANT is O.ch ln n .  Proof After permuting the input array, we have a situation identical to the proba- bilistic analysis of deterministic HIRE-ASSISTANT.  Randomly permuting an array  [Thebookconsiderstwomethodsofrandomlypermutingan n-elementarray. The Ô¨Årstmethodassignsarandompriorityintherange1ton3 toeachpositionandthen reorders the array elements into increasing priority order. We omit this method from these notes. The second method is better: it works in place  unlike the priority-basedmethod ,itrunsinlineartimewithoutrequiringsorting,anditneeds fewer random bits  n random numbers in therange 1to n rather than the range 1 to n3 . Wepresentandanalyzethesecondmethodinthesenotes.]  Goal Produce a uniform random permutation  each of the n≈† permutations is equally likely to be produced .   Lecture Notes for Chapter 5: Probabilistic Analysis and Randomized Algorithms  5-7  Non-goal: Show that for each element A≈íi ¬ç, the probability that A≈íi ¬ç moves to position j is 1=n.  See Exercise 5.3-4, whose solution is on page 5-14 of this manual.  The following procedure permutes the array A≈í1 : : n¬ç in place  i.e., no auxiliary array is required .  RANDOMIZE-IN-PLACE .A; n  for i D 1 to n  swap A≈íi ¬ç with A≈íRANDOM.i; n ¬ç  Idea    In iteration i, choose A≈íi ¬ç randomly from A≈íi : : n¬ç.   Will never alter A≈íi ¬ç after iteration i.  Time O.1  per iteration   O.n  total.  Correctness Given a set of n elements, a k-permutation is a sequence containing k of the n elements. There are n≈†=.n  cid:0  k ≈† possible k-permutations. Lemma RANDOMIZE-IN-PLACE computes a uniform random permutation.  Proof Use a loop invariant:  Loop invariant: Just prior to the ith iteration of the for loop, for each possible .i  cid:0  1 -permutation, subarray A≈í1 : : i  cid:0  1¬ç contains this .i  cid:0  1 - permutation with probability .n  cid:0  i C 1 ≈†=n≈†.  Initialization: Just before Ô¨Årst iteration, i D 1. Loop invariant says that for each possible 0-permutation, subarray A≈í1 : : 0¬ç contains this 0-permutation with probability n≈†=n≈† D 1. A≈í1 : : 0¬ç is an empty subarray, and a 0-permutation has no elements. So, A≈í1 : : 0¬ç contains any 0-permutation with probability 1. Maintenance: Assume that just prior to the ith iteration, each possible .i  cid:0  1 - permutation appears in A≈í1 : : i  cid:0  1¬ç with probability .n cid:0  i C 1 ≈†=n≈†. Will show that after the ith iteration, each possible i-permutation appears in A≈í1 : : i ¬ç with probability .n cid:0  i  ≈†=n≈†. Incrementing i for the next iteration then maintains the invariant. Consider a particular i-permutation  D hx1; x2; : : : ; xii. It consists of an .i  cid:0  1 -permutation 0 D hx1; x2; : : : ; xi cid:0 1i, followed by xi. Let E1 be the event that the algorithm actually puts 0 into A≈í1 : : i  cid:0  1¬ç. By the loop invariant, PrfE1g D .n  cid:0  i C 1 ≈†=n≈†. Let E2 be the event that the ith iteration puts xi into A≈íi ¬ç.   5-8  Lecture Notes for Chapter 5: Probabilistic Analysis and Randomized Algorithms  We get the i-permutation  in A≈í1 : : i ¬ç if and only if both E1 and E2 occur   the probability that the algorithm produces  in A≈í1 : : i ¬ç is PrfE2 \ E1g. Equation  C.14    PrfE2 \ E1g D PrfE2 j E1g PrfE1g. The algorithm chooses xi randomly from the n  cid:0  i C 1 possibilities in A≈íi : : n¬ç   PrfE2 j E1g D 1=.n  cid:0  i C 1 . Thus, PrfE2 \ E1g D PrfE2 j E1g PrfE1g  .n  cid:0  i C 1 ≈†  n≈†  1  n  cid:0  i C 1  .n  cid:0  i  ≈†  :  n≈†  D  D  Termination: At termination, i D n C 1, so we conclude that A≈í1 : : n¬ç is a given  lemma   n-permutation with probability .n  cid:0  n ≈†=n≈† D 1=n≈†.   Solutions for Chapter 5: Probabilistic Analysis and Randomized Algorithms  Solution to Exercise 5.1-3  To get an unbiased random bit, given only calls to BIASED-RANDOM, call BIASED-RANDOM twice. Repeatedly do so until the two calls return different values, and when this occurs, return the Ô¨Årst of the two bits:  UNBIASED-RANDOM  while TRUE  x D BIASED-RANDOM y D BIASED-RANDOM if x ¬§ y  return x  To see that UNBIASED-RANDOM returns 0 and 1 each with probability 1=2, ob- serve that the probability that a given iteration returns 0 is Prfx D 0 and y D 1g D .1  cid:0  p p ; and the probability that a given iteration returns 1 is Prfx D 1 and y D 0g D p.1  cid:0  p  :  We rely on the bits returned by BIASED-RANDOM being independent.  Thus, the probability that a given iteration returns 0 equals the probability that it returns 1. Since there is no other way for UNBIASED-RANDOM to return a value, it returns 0 and 1 each with probability 1=2. Assuming that each iteration takes O.1  time, the expected running time of UNBIASED-RANDOM is linear in the expected number of iterations. We can view each iteration as a Bernoulli trial, where ‚Äúsuccess‚Äù means that the iteration returns a value. The probability of success equals the probability that 0 is returned plus the probability that 1 is returned, or 2p.1  cid:0  p . The number of trials until a success occurs is given by the geometric distribution, and by equation  C.32 , the expected number of trials for this scenario is 1=.2p.1  cid:0  p  . Thus, the expected running time of UNBIASED-RANDOM is ‚Äö.1=.2p.1  cid:0  p  .   5-10  Solutions for Chapter 5: Probabilistic Analysis and Randomized Algorithms  Solution to Exercise 5.2-1 This solution is also posted publicly  Since HIRE-ASSISTANT always hires candidate 1, it hires exactly once if and only if no candidates other than candidate 1 are hired. This event occurs when candi- date 1 is the best candidate of the n, which occurs with probability 1=n. HIRE-ASSISTANT hires n times if each candidate is better than all those who were interviewed  and hired  before. This event occurs precisely when the list of ranks given to the algorithm is h1; 2; : : : ; ni, which occurs with probability 1=n≈†.  Solution to Exercise 5.2-2  We make three observations:  1. Candidate 1 is always hired. 2. The best candidate, i.e., the one whose rank is n, is always hired. 3. If the best candidate is candidate 1, then that is the only candidate hired.  Therefore, in order for HIRE-ASSISTANT to hire exactly twice, candidate 1 must have rank i  n cid:0  1 and all candidates whose ranks are i C 1; i C 2; : : : ; n cid:0  1 must be interviewed after the candidate whose rank is n.  When i D n  cid:0  1, this second condition vacuously holds.  Let Ei be the event in which candidate 1 has rank i; clearly, PrfEig D 1=n for any given value of i. Letting j denote the position in the interview order of the best candidate, let F be the event in which candidates 2; 3; : : : ; j  cid:0  1 have ranks strictly less than the rank of candidate 1. Given that event Ei has occurred, event F occurs when the best candidate is the Ô¨Årst one interviewed out of the n  cid:0  i candidates whose ranks are i C 1; i C 2; : : : ; n. Thus, PrfF j Eig D 1=.n  cid:0  i  . Our Ô¨Ånal event is A, which occurs when HIRE-ASSISTANT hires exactly twice. Noting that the events E1; E2; : : : ; En are disjoint, we have A D F \ .E1 [ E2 [  [ En cid:0 1   D .F \ E1  [ .F \ E2  [  [ .F \ En cid:0 1  :  and  n cid:0 1  XiD1  PrfF \ Eig :  PrfAg D By equation  C.14 , PrfF \ Eig D PrfF j Eig PrfEig  D  1  n  cid:0  i   1  n  ;   Solutions for Chapter 5: Probabilistic Analysis and Randomized Algorithms  5-11  and so  PrfAg D  1  n  n cid:0 1  1  1  n  n cid:0 1  XiD1 n  cid:0  i  1 XiD1 n  cid:0  i n 1 n  cid:0  1 C 1 n  Hn cid:0 1 ;  1  D  D  D  1  n  cid:0  2 C  C  1  1  where Hn cid:0 1 is the nth harmonic number.  Solution to Exercise 5.2-4 This solution is also posted publicly  Another way to think of the hat-check problem is that we want to determine the  A Ô¨Åxed point of a expected number of Ô¨Åxed points in a random permutation. permutation  is a value i for which .i   D i.  We could enumerate all n≈† per- mutations, count the total number of Ô¨Åxed points, and divide by n≈† to determine the average number of Ô¨Åxed points per permutation. This would be a painstak- ing process, and the answer would turn out to be 1. We can use indicator random variables, however, to arrive at the same answer much more easily. DeÔ¨Åne a random variable X that equals the number of customers that get back their own hat, so that we want to compute E ≈íX ¬ç. For i D 1; 2; : : : ; n, deÔ¨Åne the indicator random variable Xi D Ifcustomer i gets back his own hatg : Then X D X1 C X2 C  C Xn. Since the ordering of hats is random, each customer has a probability of 1=n of getting back his or her own hat. In other words, PrfXi D 1g D 1=n, which, by Lemma 5.1, implies that E ≈íXi ¬ç D 1=n. Thus, E ≈íX ¬ç D E" n XiD1 XiD1 XiD1 D D 1 ;   linearity of expectation   Xi  and so we expect that exactly 1 customer gets back his own hat.  E ≈íXi ¬ç  1=n  D  n  n   5-12  Solutions for Chapter 5: Probabilistic Analysis and Randomized Algorithms  Note that this is a situation in which the indicator random variables are not inde- pendent. For example, if n D 2 and X1 D 1, then X2 must also equal 1. Con- versely, if n D 2 and X1 D 0, then X2 must also equal 0. Despite the dependence, PrfXi D 1g D 1=n for all i, and linearity of expectation holds. Thus, we can use the technique of indicator random variables even in the presence of dependence.  Solution to Exercise 5.2-5 This solution is also posted publicly  Let Xij be an indicator random variable for the event where the pair A≈íi ¬ç; A≈íj ¬ç for i   A≈íj ¬ç. More precisely, we deÔ¨Åne Xij D IfA≈íi ¬ç > A≈íj ¬çg for 1  i < j  n. We have PrfXij D 1g D 1=2, because given two distinct random numbers, the probability that the Ô¨Årst is bigger than the second is 1=2. By Lemma 5.1, E ≈íXij ¬ç D 1=2. Let X be the the random variable denoting the total number of inverted pairs in the array, so that  We want the expected number of inverted pairs, so we take the expectation of both sides of the above equation to obtain  We use linearity of expectation to get  X D  n cid:0 1  XiD1  n  XjDiC1  Xij :  Xij :  Xij  E ≈íXij ¬ç  n  n  n  D  n cid:0 1  E ≈íX ¬ç D E"n cid:0 1 XiD1 XjDiC1 E ≈íX ¬ç D E"n cid:0 1 XiD1 XjDiC1 XiD1 XjDiC1 XiD1 XjDiC1 D D  n 2! 1 n.n  cid:0  1  D n.n  cid:0  1  D  n cid:0 1  2  2  4  1  2    :  n  1=2  Thus the expected number of inverted pairs is n.n  cid:0  1 =4.   Solutions for Chapter 5: Probabilistic Analysis and Randomized Algorithms  5-13  Solution to Exercise 5.3-1  Here‚Äôs the rewritten procedure:  RANDOMIZE-IN-PLACE .A  n D A:length swap A≈í1¬ç with A≈íRANDOM.1; n ¬ç for i D 2 to n  swap A≈íi ¬ç with A≈íRANDOM.i; n ¬ç  The loop invariant becomes  Loop invariant: Just prior to the iteration of the for loop for each value of i D 2; : : : ; n, for each possible .i cid:0 1 -permutation, the subarray A≈í1 : : i cid:0 1¬ç contains this .i  cid:0  1 -permutation with probability .n  cid:0  i C 1 ≈†=n≈†.  The maintenance and termination parts remain the same. The initialization part is for the subarray A≈í1 : : 1¬ç, which contains any 1-permutation with probability .n  cid:0  1 ≈†=n≈† D 1=n.  Solution to Exercise 5.3-2 This solution is also posted publicly  Although PERMUTE-WITHOUT-IDENTITY will not produce the identity permuta- tion, there are other permutations that it fails to produce. For example, consider its operation when n D 3, when it should be able to produce the n≈†  cid:0  1 D 5 non- identity permutations. The for loop iterates for i D 1 and i D 2. When i D 1, the call to RANDOM returns one of two possible values  either 2 or 3 , and when i D 2, the call to RANDOM returns just one value  3 . Thus, PERMUTE-WITHOUT- IDENTITY can produce only 2  1 D 2 possible permutations, rather than the 5 that are required.  Solution to Exercise 5.3-3  The PERMUTE-WITH-ALL procedure does not produce a uniform random per- mutation. Consider the permutations it produces when n D 3. The procedure makes 3 calls to RANDOM, each of which returns one of 3 values, and so calling PERMUTE-WITH-ALL has 27 possible outcomes. Since there are 3≈† D 6 permuta- tions, if PERMUTE-WITH-ALL did produce a uniform random permutation, then each permutation would occur 1=6 of the time. That would mean that each permu- tation would have to occur an integer number m times, where m=27 D 1=6. No integer m satisÔ¨Åes this condition. In fact, if we were to work out the possible permutations of h1; 2; 3i and how often they occur with PERMUTE-WITH-ALL, we would get the following probabilities:   5-14  Solutions for Chapter 5: Probabilistic Analysis and Randomized Algorithms  probability  permutation h1; 2; 3i h1; 3; 2i h2; 1; 3i h2; 3; 1i h3; 1; 2i h3; 2; 1i Although these probabilities sum to 1, none are equal to 1=6.  4=27 5=27 5=27 5=27 4=27 4=27  Solution to Exercise 5.3-4 This solution is also posted publicly  Solution to Exercise 5.3-7  PERMUTE-BY-CYCLIC chooses offset as a random integer in the range 1  offset  n, and then it performs a cyclic rotation of the array. That is, B≈í..i C offset  cid:0  1  mod n  C 1¬ç D A≈íi ¬ç for i D 1; 2; : : : ; n.  The subtraction and addition of 1 in the index calculation is due to the 1-origin indexing. If we had used 0-origin indexing instead, the index calculation would have simplied to B≈í.i C offset  mod n¬ç D A≈íi ¬ç for i D 0; 1; : : : ; n  cid:0  1.  Thus, once offset is determined, so is the entire permutation. Since each value of offset occurs with probability 1=n, each element A≈íi ¬ç has a probability of ending up in position B≈íj ¬ç with probability 1=n. This procedure does not produce a uniform random permutation, however, since it can produce only n different permutations. Thus, n permutations occur with probability 1=n, and the remaining n≈†  cid:0  n permutations occur with probability 0.  Since each recursive call reduces m by 1 and makes only one call to RANDOM, it‚Äôs easy to see that there are a total of m calls to RANDOM. Moreover, since each recursive call adds exactly one element to the set, it‚Äôs easy to see that the resulting set S contains exactly m elements. Because the elements of set S are chosen independently of each other, it sufÔ¨Åces to show that each of the n values appears in S with probability m=n. We use an inductive proof. The inductive hypothesis is that a call to RANDOM-SUBSET.m; n  returns a set S of m elements, each appearing with probability m=n. The base cases are for m D 0 and m D 1. When m D 0, the returned set is empty, and so it contains each element with probability 0. When m D 1, the returned set has one element, and it is equally likely to be any number in f1; 2; 3; : : : ; ng. For the inductive step, we assume that the call RANDOM-SUBSET.m  cid:0  1; n  cid:0  1  returns a set S0 of m cid:0 1 elements in which each value in f1; 2; 3; : : : ; n  cid:0  1g occurs with probability .m  cid:0  1 =.n  cid:0  1 . After the line i D RANDOM.1; n , i is equally likely to be any value in f1; 2; 3; : : : ; ng. We consider separately the probabilities   Solutions for Chapter 5: Probabilistic Analysis and Randomized Algorithms  5-15  that S contains j < n and that S contains n. Let Rj be the event that the call RANDOM.1; n  returns j , so that PrfRjg D 1=n. For j < n, the event that j 2 S is the union of two disjoint events:      j 2 S0, and j 62 S0 and Rj  these events are independent ,  Thus, Prfj 2 Sg   the events are disjoint    by the inductive hypothesis   1  D Prfj 2 S0g C Prfj 62 S0 and Rjg D  n  cid:0  1  m  cid:0  1 n n  cid:0  1  m  cid:0  1  n  cid:0  1 C1  cid:0  m  cid:0  1 n  cid:0  1 C n  cid:0  1 m  cid:0  1 n  cid:0  1  cid:0  n  cid:0  m m  cid:0  1 n  cid:0  1  n  cid:0  1  .m  cid:0  1 n C .n  cid:0  m   n n C  D  n  n  1  1  D D  D  D D  .n  cid:0  1 n  mn  cid:0  n C n  cid:0  m  .n  cid:0  1 n  m.n  cid:0  1  .n  cid:0  1 n m n  :  The event that n 2 S is also the union of two disjoint events:  Rn, and  Rj and j 2 S0 for some j < n  these events are independent . Thus, Prfn 2 Sg D PrfRng C PrfRj and j 2 S0 for some j < ng  the events are disjoint  D D D D  n  cid:0  1  n n  cid:0  1 n  cid:0  1 C  m  cid:0  1 n  cid:0  1 n  cid:0  1 n  1 n C m  cid:0  1 1 n  n  cid:0  1 n  cid:0  1 C nm  cid:0  n  cid:0  m C 1 nm  cid:0  m n.n  cid:0  1  m.n  cid:0  1  n.n  cid:0  1  m n  n.n  cid:0  1   D D    :   by the inductive hypothesis    5-16  Solutions for Chapter 5: Probabilistic Analysis and Randomized Algorithms  Solution to Exercise 5.4-6  First we determine the expected number of empty bins. We deÔ¨Åne a random vari- able X to be the number of empty bins, so that we want to compute E ≈íX ¬ç. Next, for i D 1; 2; : : : ; n, we deÔ¨Åne the indicator random variable Yi D Ifbin i is emptyg. Thus,  Yi  Yi ;  n  XiD1  X D and so  n  E ≈íX ¬ç D E" n XiD1 XiD1 XiD1  D  D  n  E ≈íYi ¬ç   by linearity of expectation   Prfbin i is emptyg   by Lemma 5.1  .  n  :  1  1  1  Thus,  E ≈íX ¬ç D  nn 1 n0  Let us focus on a speciÔ¨Åc bin, say bin i. We view a toss as a success if it misses bin i and as a failure if it lands in bin i. We have n independent Bernoulli trials, each with probability of success 1  cid:0  1=n. In order for bin i to be empty, we need n successes in n trials. Using a binomial distribution, therefore, we have that Prfbin i is emptyg D  n D 1  cid:0  XiD11  cid:0  nn nn D n1  cid:0   n!1  cid:0  nn  By equation  3.14 , as n approaches 1, the quantity .1  cid:0  1=n n approaches 1=e, and so E ≈íX ¬ç approaches n=e. Now we determine the expected number of bins with exactly one ball. We re- deÔ¨Åne X to be number of bins with exactly one ball, and we redeÔ¨Åne Yi to be Ifbin i gets exactly one ballg. As before, we Ô¨Ånd that E ≈íX ¬ç D Again focusing on bin i, we need exactly n cid:0 1 successes in n independent Bernoulli trials, and so  Prfbin i gets exactly one ballg :  XiD1  1  :  n   Solutions for Chapter 5: Probabilistic Analysis and Randomized Algorithms  5-17  1  Prfbin i gets exactly one ballg D   n D n 1  cid:0  D 1  cid:0   n  cid:0  1!1  cid:0  nn cid:0 1 1 nn cid:0 1  1  1  ;  n  nn cid:0 1 1 n1  and so  E ≈íX ¬ç D  Because  n  1  1  XiD11  cid:0  D n1  cid:0  nn cid:0 1  nn cid:0 1 nn cid:0 1 nn n cid:0 1  cid:0  1 1  cid:0  1  D  :  ;  1  n1  cid:0  as n approaches 1, we Ô¨Ånd that E ≈íX ¬ç approaches 1  cid:0  1=n D  e.n  cid:0  1   n=e  n2  :  n  a. To determine the expected value represented by the counter after n INCREMENT  operations, we deÔ¨Åne some random variables:  For j D 1; 2; : : : ; n, let Xj denote the increase in the value represented by  Let Vn be the value represented by the counter after n INCREMENT opera-  the counter due to the j th INCREMENT operation.  tions.  Then Vn D X1 C X2 C  C Xn. We want to compute E ≈íVn¬ç. By linearity of expectation, E ≈íVn¬ç D E ≈íX1 C X2 C  C Xn¬ç D E ≈íX1¬ç C E ≈íX2¬ç C  C E ≈íXn¬ç : We shall show that E ≈íXj ¬ç D 1 for j D 1; 2; : : : ; n, which will prove that E ≈íVn¬ç D n. We actually show that E ≈íXj ¬ç D 1 in two ways, the second more rigorous than the Ô¨Årst:  1. Suppose that at the start of the j th INCREMENT operation, the counter holds the value i, which represents ni. If the counter increases due to this INCRE- MENT operation, then the value it represents increases by niC1  cid:0  ni. The counter increases with probability 1=.niC1  cid:0  ni  , and so  Solution to Problem 5-1   5-18  Solutions for Chapter 5: Probabilistic Analysis and Randomized Algorithms  E ≈íXj ¬ç D .0  Prfcounter does not increaseg   C ..niC1  cid:0  ni    Prfcounter increasesg  niC1  cid:0  ni C.niC1  cid:0  ni     1  1  niC1  cid:0  ni  D 0 1  cid:0  D 1 ;  and so E ≈íXj ¬ç D 1 regardless of the value held by the counter. 2. Let Cj be the random variable denoting the value held in the counter at the start of the j th INCREMENT operation. Since we can ignore values of Cj greater than 2b  cid:0  1, we use a formula for conditional expectation: E ≈íXj ¬ç D E ≈íE ≈íXj j Cj ¬ç¬ç  D  E ≈íXj j Cj D i ¬ç  PrfCj D ig :  2b cid:0 1  XiD0  To compute E ≈íXj j Cj D i ¬ç, we note that  PrfXj D 0 j Cj D ig D 1  cid:0  1=.niC1  cid:0  ni  ,  PrfXj D niC1  cid:0  ni j Cj D ig D 1=.niC1  cid:0  ni  , and  PrfXj D k j Cj D ig D 0 for all other k. Thus, E ≈íXj j Cj D i ¬ç D Xk  k  PrfXj D k j Cj D ig  1  niC1  cid:0  ni C.niC1  cid:0  ni     1  niC1  cid:0  ni  D 0 1  cid:0  D 1 : Therefore, noting that  PrfCj D ig D 1 ;  2b cid:0 1  XiD0  we have  E ≈íXj ¬ç D  1  PrfCj D ig  2b cid:0 1  XiD0 D 1 :  Why is the second way more rigorous than the Ô¨Årst? Both ways condition on the value held in the counter, but only the second way incorporates the conditioning into the expression for E ≈íXj ¬ç.  b. DeÔ¨Åning Vn and Xj as in part  a , we want to compute Var ≈íVn¬ç, where ni D 100i. The Xj are pairwise independent, and so by equation  C.29 , Var ≈íVn¬ç D Var ≈íX1¬ç C Var ≈íX2¬ç C  C Var ≈íXn¬ç. Since ni D 100i, we see that niC1 cid:0  ni D 100.i C 1  cid:0  100i D 100. Therefore, with probability 99=100, the increase in the value represented by the counter due to the j th INCREMENT operation is 0, and with probability 1=100, the   Solutions for Chapter 5: Probabilistic Analysis and Randomized Algorithms  5-19  value represented increases by 100. Thus, by equation  C.27 ,  j  cid:0  E2 ≈íXj ¬ç 100 C1002   99  1  100  cid:0  12  Var ≈íXj ¬ç D EX 2 D 02  D 100  cid:0  1 D 99 :  Summing up the variances of the Xj gives Var ≈íVn¬ç D 99n.   Lecture Notes for Chapter 6: Heapsort  Chapter 6 overview  Heapsort  Heaps  Heap data structure   O.n lg n  worst case‚Äîlike merge sort.  Sorts in place‚Äîlike insertion sort.  Combines the best of both algorithms.  To understand heapsort, we‚Äôll cover heaps and heap operations, and then we‚Äôll take a look at priority queues.   Heap A  not garbage-collected storage  is a nearly complete binary tree.   Height of node =  of edges on a longest simple path from the node down to  a leaf.   Height of heap D height of root D ‚Äö.lg n .   A heap can be stored as an array A.   Root of tree is A≈í1¬ç.  Parent of A≈íi ¬ç D A≈íbi=2c¬ç.  Left child of A≈íi ¬ç D A≈í2i ¬ç.  Right child of A≈íi ¬ç D A≈í2i C 1¬ç.  Computing is fast with binary representation implementation.  [Inbook,havelength andheap-size attributes. Here,webypasstheseattributesand useparametervaluesinstead.]   6-2  Lecture Notes for Chapter 6: Heapsort  Example Of a max-heap. [Arcsaboveandbelow thearray ontheright gobetween parents and children. ThereisnosigniÔ¨Åcance towhether an arcisdrawnabove orbelow thearray.]  4 8  5 7  6 9  7 3  1  2  3  4  16 14 10 8  5  7  6  9  7  3  8  2  9  4  10  1  1 16  2 14  3 10  8 2  9 4  10 1  Heap property   For max-heaps  largest element at root , max-heap property: for all nodes i,   For min-heaps  smallest element at root , min-heap property: for all nodes i,  excluding the root, A≈íPARENT.i  ¬ç  A≈íi ¬ç. excluding the root, A≈íPARENT.i  ¬ç  A≈íi ¬ç.  By induction and transitivity of , the max-heap property guarantees that the max- imum element of a max-heap is at the root. Similar argument for min-heaps. The heapsort algorithm we‚Äôll show uses max-heaps. Note: In general, heaps can be k-ary tree instead of binary.  Maintaining the heap property  MAX-HEAPIFY is important for manipulating max-heaps. It is used to maintain the max-heap property.   Before MAX-HEAPIFY, A≈íi ¬ç may be smaller than its children.  Assume left and right subtrees of i are max-heaps.  After MAX-HEAPIFY, subtree rooted at i is a max-heap.  MAX-HEAPIFY.A; i; n  l D LEFT.i   r D RIGHT.i   if l  n and A≈íl¬ç > A≈íi ¬ç largest D l else largest D i if r  n and A≈ír¬ç > A≈ílargest¬ç largest D r if largest ¬§ i exchange A≈íi ¬ç with A≈ílargest¬ç MAX-HEAPIFY.A; largest; n    Lecture Notes for Chapter 6: Heapsort  6-3  [Parameter n replacesattribute A:heap-size.] The way MAX-HEAPIFY works:   Compare A≈íi ¬ç, A≈íLEFT.i  ¬ç, and A≈íRIGHT.i  ¬ç.    If necessary, swap A≈íi ¬ç with the larger of the two children to preserve heap property.   Continue this process of comparing and swapping down the heap, until subtree rooted at i is max-heap. If we hit a leaf, then the subtree rooted at the leaf is trivially a max-heap.  Run MAX-HEAPIFY on the following heap example.  3 10  5 7  6 9  7 3  2 14  4 4  i 8 2  9 8  10 1  1 16   b   4 14  i  9 8  10 1  2 4  2 14  4 8  i  9 4  10 1  8 2  8 2  1 16   a   1 16   c   5 7  5 7  6 9  6 9  3 10  3 10  7 3  7 3   Node 2 violates the max-heap property.  Compare node 2 with its children, and then swap it with the larger of the two   Continue down the tree, swapping until the value is properly placed at the root  of a subtree that is a max-heap. In this case, the max-heap is a leaf.  children.  Time O.lg n .  Analysis [Insteadofbook‚Äôsformalanalysiswithrecurrence,justcomeupwithO.lg n  intu- itively.] Heap is almost-complete binary tree, hence must process O.lg n  levels, with constant work at each level  comparing 3 items and maybe swapping 2 .   6-4  Lecture Notes for Chapter 6: Heapsort  Building a heap  BUILD-MAX-HEAP.A; n  for i D bn=2c downto 1  MAX-HEAPIFY.A; i; n   The following procedure, given an unordered array, will produce a max-heap.  [Parameter n replacesbothattributes A:length and A:heap-size.]  Example Building a max-heap from the following unsorted array results in the Ô¨Årst heap example.    i starts off as 5.   MAX-HEAPIFY is applied to subtrees rooted at nodes  in order : 16, 2, 3, 1, 4.  1 4  2 1  3 3  4 2  7  6  5 9 16 9 10 14 8  8  10 7  A  1 4  3 3  1 16  2 14  3 10  5 16  6 9  7 10  4 8  5 7  6 9  7 3  8 2  9 4  10 1  2 1  i 10 7  4 2  8 14  9 8  Correctness  Loop invariant: At start of every iteration of for loop, each node i C 1, i C 2, . . . , n is root of a max-heap.  Initialization: By Exercise 6.1-7, we know that each node bn=2c C 1, bn=2c C 2, . . . , n is a leaf, which is the root of a trivial max-heap. Since i D bn=2c before the Ô¨Årst iteration of the for loop, the invariant is initially true. Maintenance: Children of node i are indexed higher than i, so by the loop invari- ant, they are both roots of max-heaps. Correctly assuming that iC1; iC2; : : : ; n are all roots of max-heaps, MAX-HEAPIFY makes node i a max-heap root. Decrementing i reestablishes the loop invariant at each iteration.  Termination: When i D 0, the loop terminates. By the loop invariant, each node,  notably node 1, is the root of a max-heap.   Lecture Notes for Chapter 6: Heapsort  6-5  Analysis   Simple bound: O.n  calls to MAX-HEAPIFY, each of which takes O.lg n  time   O.n lg n .  Note: A good approach to analysis in general is to start by proving easy bound, then try to tighten it.   Tighter analysis: Observation: Time to run MAX-HEAPIFY is linear in the height of the node it‚Äôs run on, and most nodes have small heights. Have  Àôn=2hC1 nodes of height h  see Exercise 6.3-3 , and height of heap is blg nc   Exercise 6.1-2 . The time required by MAX-HEAPIFY when called on a node of height h is O.h , so the total cost of BUILD-MAX-HEAP is  h  blg ncXhD0  2h! :  2hC1m O.h  D O n blg ncXhD0l n Evaluate the last summation by substituting x D 1=2 in the formula  A.8   cid:0 P1kD0 kxk, which yields 1XhD0 .1  cid:0  1=2 2  h 2h D  1=2  Thus, the running time of BUILD-MAX-HEAP is O.n . Building a min-heap from an unordered array can be done by calling MIN- HEAPIFY instead of MAX-HEAPIFY, also taking linear time.  D 2 :  Given an input array, the heapsort algorithm acts as follows:  Builds a max-heap from the array.  Starting with the root  the maximum element , the algorithm places the maxi- mum element into the correct place in the array by swapping it with the element in the last position in the array. ‚ÄúDiscard‚Äù this last node  knowing that it is in its correct place  by decreasing the heap size, and calling MAX-HEAPIFY on the new  possibly incorrectly-placed  root.     Repeat this ‚Äúdiscarding‚Äù process until only one node  the smallest element   remains, and therefore is in the correct place in the array.  HEAPSORT.A; n   BUILD-MAX-HEAP.A; n  for i D n downto 2  exchange A≈í1¬ç with A≈íi ¬ç MAX-HEAPIFY.A; 1; i  cid:0  1   [Parameter n replaces A:length,andparametervalue i  cid:0  1 in MAX-HEAPIFY call replacesdecrementingof A:heap-size.]  The heapsort algorithm   6-6  Lecture Notes for Chapter 6: Heapsort  Example Sort an example heap on the board. [Nodeswithheavyoutlinearenolongerinthe heap.]  2  3  1  i  7  4   b   2  1  3  i  4  7   d   A  1 2  3 4  7  3  1  3  7   a   3   c   1   e   2  7  7  4  2  1  i  4  i  2  4  Analysis   BUILD-MAX-HEAP: O.n       for loop: n  cid:0  1 times exchange elements: O.1   MAX-HEAPIFY: O.lg n   Total time: O.n lg n . Though heapsort is a great algorithm, a well-implemented quicksort usually beats it in practice.  Heap implementation of priority queue  Heaps efÔ¨Åciently implement priority queues. These notes will deal with max- priority queues implemented with max-heaps. Min-priority queues are imple- mented with min-heaps similarly. A heap gives a good compromise between fast insertion but slow extraction and vice versa. Both operations take O.lg n  time.  Priority queue   Maintains a dynamic set S of elements.  Each set element has a key‚Äîan associated value.   Lecture Notes for Chapter 6: Heapsort  6-7   Max-priority queue supports dynamic-set operations:    INSERT.S; x : inserts element x into set S.   MAXIMUM.S  : returns element of S with largest key.  EXTRACT-MAX.S  : removes and returns element of S with largest key.    INCREASE-KEY.S; x; k : increases value of element x‚Äôs key to k. Assume k  x‚Äôs current key value.   Example max-priority queue application: schedule jobs on shared computer.  Min-priority queue supports similar operations:    INSERT.S; x : inserts element x into set S.   MINIMUM.S  : returns element of S with smallest key.  EXTRACT-MIN.S  : removes and returns element of S with smallest key.  DECREASE-KEY .S; x; k : decreases value of element x‚Äôs key to k. Assume  k  x‚Äôs current key value.   Example min-priority queue application: event-driven simulator.  Note: Actual implementations often have a handle in each heap element that allows access to an object in the application, and objects in the application often have a handle  likely an array index  to access the heap element. Will examine how to implement max-priority queue operations.  Finding the maximum element  Getting the maximum element is easy: it‚Äôs the root.  HEAP-MAXIMUM.A   return A≈í1¬ç  Time ‚Äö.1 .  Extracting max element  Given the array A:   Make sure heap is not empty.  Make a copy of the maximum element  the root .  Make the last node in the tree the new root.  Re-heapify the heap, with one fewer node.  Return the copy of the maximum element.  Note: Because we need to decrement the heap size n in the following pseudocode, assume that it is passed by reference, not by value. [This issue does not come up in the pseudocode in the book, because it uses the attribute A:heap-size insteadofpassingintheheapsizeasaparameter.]   6-8  Lecture Notes for Chapter 6: Heapsort  HEAP-EXTRACT-MAX .A; n   if n < 1  error ‚Äúheap underÔ¨Çow‚Äù  max D A≈í1¬ç A≈í1¬ç D A≈ín¬ç n D n  cid:0  1 MAX-HEAPIFY.A; 1; n  return max     remakes heap  Analysis Constant-time assignments plus time for MAX-HEAPIFY.  Time O.lg n .  Example Run HEAP-EXTRACT-MAX on Ô¨Årst heap example.   Take 16 out of node 1.  Move 1 from node 10 to node 1.  Erase node 10.  MAX-HEAPIFY from the root to preserve max-heap property.  Note that successive extractions will remove items in reverse sorted order.  Increasing key value  Given set S, element x, and new key value k:  Make sure k  x‚Äôs current key.  Update x‚Äôs key value to k.  Traverse the tree upward comparing x to its parent and swapping keys if neces-  sary, until x‚Äôs key is smaller than its parent‚Äôs key.  HEAP-INCREASE-KEY.A; i; key   if key < A≈íi ¬ç  error ‚Äúnew key is smaller than current key‚Äù  A≈íi ¬ç D key while i > 1 and A≈íPARENT.i  ¬ç < A≈íi ¬ç exchange A≈íi ¬ç with A≈íPARENT.i  ¬ç i D PARENT.i    Analysis Upward path from node i has length O.lg n  in an n-element heap.   Lecture Notes for Chapter 6: Heapsort  6-9  Time O.lg n .  Example Increase key of node 9 in Ô¨Årst heap example to have value 15. Exchange keys of nodes 4 and 9, then of nodes 2 and 4.  Inserting into the heap  Given a key k to insert into the heap:        Increment the heap size. Insert a new node in the last position in the heap, with key  cid:0 1. Increase the  cid:0 1 key to k using the HEAP-INCREASE-KEY procedure deÔ¨Åned above.  Note: Again, the parameter n is passed by reference, not by value.  MAX-HEAP-INSERT .A; key; n  n D n C 1 A≈ín¬ç D  cid:0 1 HEAP-INCREASE-KEY .A; n; key   Analysis Constant time assignments C time for HEAP-INCREASE-KEY.  Time O.lg n .  Min-priority queue operations are implemented similarly with min-heaps.   Solutions for Chapter 6: Heapsort  Solution to Exercise 6.1-1 This solution is also posted publicly  Since a heap is an almost-complete binary tree  complete at all levels except pos- sibly the lowest , it has at most 2hC1  cid:0  1 elements  if it is complete  and at least 2h cid:0  1C 1 D 2h elements  if the lowest level has just 1 element and the other levels are complete .  Solution to Exercise 6.1-2 This solution is also posted publicly  Given an n-element heap of height h, we know from Exercise 6.1-1 that 2h  n  2hC1  cid:0  1 < 2hC1 : Thus, h  lg n < h C 1. Since h is an integer, h D blg nc  by deÔ¨Ånition of b c .  Solution to Exercise 6.1-3  Assume the claim is false‚Äîi.e., that there is a subtree whose root is not the largest element in the subtree. Then the maximum element is somewhere else in the sub- tree, possibly even at more than one location. Let m be the index at which the maximum appears  the lowest such index if the maximum appears more than once . Since the maximum is not at the root of the subtree, node m has a parent. Since the parent of a node has a lower index than the node, and m was chosen to be the smallest index of the maximum value, A≈íPARENT.m ¬ç < A≈ím¬ç. But by the max- heap property, we must have A≈íPARENT.m ¬ç  A≈ím¬ç. So our assumption is false, and the claim is true.   Solutions for Chapter 6: Heapsort  6-11  Solution to Exercise 6.2-6 This solution is also posted publicly  If you put a value at the root that is less than every value in the left and right subtrees, then MAX-HEAPIFY will be called recursively until a leaf is reached. To make the recursive calls traverse the longest path to a leaf, choose values that make MAX-HEAPIFY always recurse on the left child. It follows the left branch when the left child is greater than or equal to the right child, so putting 0 at the root and 1 at all the other nodes, for example, will accomplish that. With such values, MAX-HEAPIFY will be called h times  where h is the heap height, which is the number of edges in the longest path from the root to a leaf , so its running time will be ‚Äö.h   since each call does ‚Äö.1  work , which is ‚Äö.lg n . Since we have a case in which MAX-HEAPIFY‚Äôs running time is ‚Äö.lg n , its worst-case running time is .lg n .  Solution to Exercise 6.3-3  Let H be the height of the heap. Two subtleties to beware of:   Be careful not to confuse the height of a node  longest distance from a leaf     with its depth  distance from the root . If the heap is not a complete binary tree  bottom level is not full , then the nodes at a given level  depth  don‚Äôt all have the same height. For example, although all nodes at depth H have height 0, nodes at depth H  cid:0  1 can have either height 0 or height 1. For a complete binary tree, it‚Äôs easy to show that there are Àôn=2hC1 nodes of  height h. But the proof for an incomplete tree is tricky and is not derived from the proof for a complete tree.  Proof By induction on h.  Basis: Show that it‚Äôs true for h D 0  i.e., that  of leaves Àôn=2hC1 D dn=2e . In fact, we‚Äôll show that the  of leaves D dn=2e. The tree leaves  nodes at height 0  are at depths H and H  cid:0  1. They consist of      all nodes at depth H , and the nodes at depth H  cid:0  1 that are not parents of depth-H nodes.  Let x be the number of nodes at depth H ‚Äîthat is, the number of nodes in the bottom  possibly incomplete  level. Note that n  cid:0  x is odd, because the n  cid:0  x nodes above the bottom level form a complete binary tree, and a complete binary tree has an odd number of nodes  1 less than a power of 2 . Thus if n is odd, x is even, and if n is even, x is odd.   6-12  Solutions for Chapter 6: Heapsort  To prove the base case, we must consider separately the case in which n is even  x is odd  and the case in which n is odd  x is even . Here are two ways to do this: The Ô¨Årst requires more cleverness, and the second requires more algebraic manipulation.  1. First method of proving the base case:  If n is odd, then x is even, so all nodes have siblings‚Äîi.e., all internal nodes have 2 children. Thus  see Exercise B.5-3 ,  of internal nodes D  of leaves  cid:0  1. So, n D  of nodes D  of leavesC  of internal nodes D 2  of leaves cid:0  1. Thus,  of leaves D .nC1 =2 D dn=2e.  The latter equality holds because n is odd.  If n is even, then x is odd, and some leaf doesn‚Äôt have a sibling. If we gave it a sibling, we would have n C 1 nodes, where n C 1 is odd, so the case we analyzed above would apply. Observe that we would also increase the number of leaves by 1, since we added a node to a parent that already had a child. By the odd-node case above,  of leaves C 1 D d.n C 1 =2e D dn=2e C 1.  The latter equality holds because n is even.   In either case,  of leaves D dn=2e.  2. Second method of proving the base case:  Note that at any depth d < H there are 2d nodes, because all such tree levels are complete.  If x is even, there are x=2 nodes at depth H  cid:0  1 that are parents of depth H nodes, hence 2H cid:0 1  cid:0  x=2 nodes at depth H  cid:0  1 that are not parents of depth- H nodes. Thus, total  of height-0 nodes D x C 2H cid:0 1  cid:0  x=2  D 2H cid:0 1 C x=2 D .2H C x =2 D Àô.2H C x  cid:0  1 =2 D dn=2e :   because x is even    n D 2H C x  cid:0  1 because the complete tree down to depth H  cid:0  1 has 2H  cid:0  1 nodes and depth H has x nodes.  If x is odd, by an argument similar to the even case, we see that  of height-0 nodes D x C 2H cid:0 1  cid:0  .x C 1 =2          D 2H cid:0 1 C .x  cid:0  1 =2 D .2H C x  cid:0  1 =2 D n=2 D dn=2e   because x odd   n even  :  Inductive step: Show that if it‚Äôs true for height h  cid:0  1, it‚Äôs true for h. Let nh be the number of nodes at height h in the n-node tree T .   Solutions for Chapter 6: Heapsort  6-13  Consider the tree T 0 formed by removing the leaves of T . It has n0 D n cid:0  n0 nodes. We know from the base case that n0 D dn=2e, so n0 D n  cid:0  n0 D n  cid:0  dn=2e D bn=2c. Note that the nodes at height h in T would be at height h  cid:0  1 if the leaves of the tree were removed‚Äîthat is, they are at height h  cid:0  1 in T 0. Letting n0h cid:0 1 denote the number of nodes at height h  cid:0  1 in T 0, we have nh D n0h cid:0 1 : By induction, we can bound n0h cid:0 1: nh D n0h cid:0 1 Àôn0=2h DÀôbn=2c =2h Àô.n=2 =2h DÀôn=2hC1 :  Alternative solution  An alternative solution relies on four facts:  1. Every node not on the unique simple path from the last leaf to the root is the  root of a complete binary subtree.  2. A node that is the root of a complete binary subtree and has height h is the  ancestor of 2h leaves.  3. By Exercise 6.1-7, an n-element heap has dn=2e leaves. 4. For nonnegative reals a and b, we have dae  b  dabe. The proof is by contradiction. Assume that an n-element heap contains at least Àôn=2hC1 C 1 nodes of height h. Exactly one node of height h is on the unique the heap contains at leastÀôn=2hC1, are the roots of complete binary subtrees, and  simple path from the last leaf to the root, and the subtree rooted at this node has at least one leaf  that being the last leaf . All other nodes of height h, of which  each such node is the root of a subtree with 2h leaves. Moreover, each subtree whose root is at height h is disjoint. Therefore, the number of leaves in the entire heap is at least  2hC1m  2h C 1  l n l n D l n which contradicts the property that an n-element heap has dn=2e leaves.  2hC1  2hm C 1 2m C 1 ;   6-14  Solutions for Chapter 6: Heapsort  Solution to Exercise 6.4-1 This solution is also posted publicly  25   a   13   d   5   g   8  4  5  2  13  20  13  17  13  8  7  17  2  7  4  2  8  7  4  2  5  4  i  20  25  8  i 25  5  2  7  4  i  17  2  4  13  i  17  2  i  8  13  17  20  25  20  25  20  25  5  4  i  7  8  13  17  7  8  13  17  7  8  13  17  20  25  20  25  20  25  5  i  i  4  A  2 4  5 7  8 13 17 20 25  20   b   8   e   4   h   7  2  5  5  5  17   c   7   f   2   i    Solutions for Chapter 6: Heapsort  6-15  5  12  5  12  8 8  7  4  0  6  2  1  4  0  6  2  1  Solution to Exercise 6.5-2 This solution is also posted publicly  13  13  9  9  8 8  -¬•  i 10  7  7  15   a   15   c   13  13  9  i  10  i 10  15   b   15   d   5  12  5  12  9  7  4  0  6  2  1  8  4  0  6  2  1  8  Solution to Exercise 6.5-6  Change the procedure to the following:  HEAP-INCREASE-KEY .A; i; key   if key < A≈íi ¬ç  error ‚Äúnew key is smaller than current key‚Äù  A≈íi ¬ç D key while i > 1 and A≈íPARENT.i  ¬ç < A≈íi ¬ç  A≈íi ¬ç D A≈íPARENT.i  ¬ç i D PARENT.i    A≈íi ¬ç D key  Solution to Problem 6-1 This solution is also posted publicly  a. The procedures BUILD-MAX-HEAP and BUILD-MAX-HEAP0 do not always create the same heap when run on the same input array. Consider the following counterexample.   6-16  Solutions for Chapter 6: Heapsort  Input array A:  A  1  2 3  BUILD-MAX-HEAP.A :  2  3  1  1  -¬•  3  2  1  -¬•  2  1  BUILD-MAX-HEAP0.A :  A  3 2  1  3  1  2  A  3 1  2  b. An upper bound of O.n lg n  time follows immediately from there being n  cid:0  1 calls to MAX-HEAP-INSERT, each taking O.lg n  time. For a lower bound of .n lg n , consider the case in which the input array is given in strictly in- creasing order. Each call to MAX-HEAP-INSERT causes HEAP-INCREASE- KEY to go all the way up to the root. Since the depth of node i is blg ic, the total time is n XiD1  ‚Äö.blg dn=2ec   ‚Äö.blg ic    n  n  XiDdn=2e XiDdn=2e XiDdn=2e  n    ‚Äö.blg.n=2 c   ‚Äö.blg n  cid:0  1c   D  n=2  ‚Äö.lg n  D .n lg n  :  In the worst case, therefore, BUILD-MAX-HEAP0 requires ‚Äö.n lg n  time to build an n-element heap.  Solution to Problem 6-2  a. We can represent a d -ary heap in a 1-dimensional array as follows. The root resides in A≈í1¬ç, its d children reside in order in A≈í2¬ç through A≈íd C 1¬ç, their children reside in order in A≈íd C 2¬ç through A≈íd 2 C d C 1¬ç, and so on. The following two procedures map a node with index i to its parent and to its j th child  for 1  j  d  , respectively. D-ARY-PARENT.i   return b.i  cid:0  2 =d C 1c D-ARY-CHILD.i; j   return d.i  cid:0  1  C j C 1   Solutions for Chapter 6: Heapsort  6-17  To convince yourself that these procedures really work, verify that D-ARY-PARENT.D-ARY-CHILD.i; j    D i ; for any 1  j  d . Notice that the binary heap procedures are a special case of the above procedures when d D 2.  b. Since each node has d children, the height of a d -ary heap with n nodes is  ‚Äö.logd n  D ‚Äö.lg n= lg d  .  c. The procedure HEAP-EXTRACT-MAX given in the text for binary heaps works Ô¨Åne for d -ary heaps too. The change needed to support d -ary heaps is in MAX- HEAPIFY, which must compare the argument node to all d children instead of just 2 children. The running time of HEAP-EXTRACT-MAX is still the running time for MAX-HEAPIFY, but that now takes worst-case time proportional to the product of the height of the heap by the number of children examined at each node  at most d  , namely ‚Äö.d logd n  D ‚Äö.d lg n= lg d  .  d. The procedure MAX-HEAP-INSERT given in the text for binary heaps works Ô¨Åne for d -ary heaps too, assuming that HEAP-INCREASE-KEY works for d -ary heaps. The worst-case running time is still ‚Äö.h , where h is the height of the heap.  Since only parent pointers are followed, the number of children a node has is irrelevant.  For a d -ary heap, this is ‚Äö.logd n  D ‚Äö.lg n= lg d  .  e. The HEAP-INCREASE-KEY procedure with two small changes works for d -ary heaps. First, because the problem speciÔ¨Åes that the new key is given by the parameter k, change instances of the variable key to k. Second, change calls of PARENT to calls of D-ARY-PARENT from part  a . In the worst case, the entire height of the tree must be traversed, so the worst- case running time is ‚Äö.h  D ‚Äö.logd n  D ‚Äö.lg n= lg d  .   Lecture Notes for Chapter 7: Quicksort  Chapter 7 overview  [ThetreatmentinthesecondandthirdeditionsdiffersfromthatoftheÔ¨Årstedition. Weuseadifferentpartitioning method‚Äîknown as‚ÄúLomutopartitioning‚Äù‚Äîin the secondandthirdeditions,ratherthanthe‚ÄúHoarepartitioning‚ÄùusedintheÔ¨Årstedi- tion. UsingLomutopartitioning helpssimplify theanalysis, whichusesindicator randomvariablesinthesecondedition.]  Quicksort   Worst-case running time: ‚Äö.n2 .  Expected running time: ‚Äö.n lg n .  Constants hidden in ‚Äö.n lg n  are small.  Sorts in place.  Description of quicksort  Quicksort is based on the three-step process of divide-and-conquer.   To sort the subarray A≈íp : : r¬ç:  Divide: Partition A≈íp : : r¬ç, into two  possibly empty  subarrays A≈íp : : q  cid:0  1¬ç and A≈íq C 1 : : r¬ç, such that each element in the Ô¨Årst subarray A≈íp : : q  cid:0  1¬ç is  A≈íq¬ç and A≈íq¬ç is  each element in the second subarray A≈íq C 1 : : r¬ç.  Conquer: Sort the two subarrays by recursive calls to QUICKSORT. Combine: No work is needed to combine the subarrays, because they are sorted  in place.   Perform the divide step by a procedure PARTITION, which returns the index q  that marks the position separating the subarrays.   7-2  Lecture Notes for Chapter 7: Quicksort  QUICKSORT.A; p; r   if p < r  q D PARTITION.A; p; r  QUICKSORT.A; p; q  cid:0  1  QUICKSORT.A; q C 1; r   Initial call is QUICKSORT.A; 1; n .  Partitioning  Partition subarray A≈íp : : r¬ç by the following procedure:  PARTITION.A; p; r  x D A≈ír¬ç i D p  cid:0  1 for j D p to r  cid:0  1 i D i C 1 exchange A≈íi ¬ç with A≈íj ¬ç  if A≈íj ¬ç  x  exchange A≈íi C 1¬ç with A≈ír¬ç return i C 1  PARTITION always selects the last element A≈ír¬ç in the subarray A≈íp : : r¬ç as the  pivot‚Äîthe element around which to partition.   As the procedure executes, the array is partitioned into four regions, some of  which may be empty:  Loop invariant: 1. All entries in A≈íp : : i ¬ç are  pivot. 2. All entries in A≈íi C 1 : : j  cid:0  1¬ç are > pivot. 3. A≈ír¬ç D pivot.  It‚Äôs not needed as part of the loop invariant, but the fourth region is A≈íj : : r cid:0 1¬ç, whose entries have not yet been examined, and so we don‚Äôt know how they compare to the pivot.  Example On an 8-element subarray.   Lecture Notes for Chapter 7: Quicksort  7-3  i  i  p,j 8  p 8  p,i 1  p,i 1  p 1  p 1  p 1  p 1  p 1  1  j 1  8  8  i 4  6  4  0  3  9  6  j 6  6  4  0  3  9  0  3  9  3  9  0  j 0  6  8  3  9  i 0  4  j 3  6  4  0  6  8  9  j 9  4  0  6  8  9  4  0  5  8  9  4  j 4  8  i 3  i 3  i 3  r 5  r 5  r 5  r 5  r 5  r 5  r 5  r 5  r 6  A[r]:  A[j .. r‚Äì1]:  A[i+1 .. j‚Äì1]:  known to be > pivot A[p .. i]:   pivot  pivot not yet examined known to be ¬£  [Theindexj disappearsbecauseitisnolongerneededoncetheforloopisexited.]  Correctness Use the loop invariant to prove correctness of PARTITION:  Initialization: Before the loop starts, all the conditions of the loop invariant are satisÔ¨Åed, because r is the pivot and the subarrays A≈íp : : i ¬ç and A≈íi C 1 : : j  cid:0  1¬ç are empty. Maintenance: While the loop is running, if A≈íj ¬ç  pivot, then A≈íj ¬ç and A≈íi C 1¬ç are swapped and then i and j are incremented. If A≈íj ¬ç > pivot, then increment only j .  Termination: When the loop terminates, j D r, so all elements in A are parti- tioned into one of the three cases: A≈íp : : i ¬ç  pivot, A≈íi C 1 : : r  cid:0  1¬ç > pivot, and A≈ír¬ç D pivot.  The last two lines of PARTITION move the pivot element from the end of the array to between the two subarrays. This is done by swapping the pivot and the Ô¨Årst element of the second subarray, i.e., by swapping A≈íi C 1¬ç and A≈ír¬ç.  Time for partitioning ‚Äö.n  to partition an n-element subarray.   7-4  Lecture Notes for Chapter 7: Quicksort  Performance of quicksort  The running time of quicksort depends on the partitioning of the subarrays: If the subarrays are balanced, then quicksort can run as fast as mergesort. If they are unbalanced, then quicksort can run as slowly as insertion sort.      Worst case   Occurs when the subarrays are completely unbalanced.  Have 0 elements in one subarray and n  cid:0  1 elements in the other subarray.  Get the recurrence  T .n  D T .n  cid:0  1  C T .0  C ‚Äö.n   D T .n  cid:0  1  C ‚Äö.n  D ‚Äö.n2  :   Same running time as insertion sort.    In fact, the worst-case running time occurs when quicksort takes a sorted array as input, but insertion sort runs in O.n  time in this case.  Best case   Occurs when the subarrays are completely balanced every time.  Each subarray has  n=2 elements.  Get the recurrence  T .n  D 2T .n=2  C ‚Äö.n   D ‚Äö.n lg n  :  Balanced partitioning   Quicksort‚Äôs average running time is much closer to the best case than to the  worst case. Imagine that PARTITION always produces a 9-to-1 split.     Get the recurrence  T .n   T .9n=10  C T .n=10  C ‚Äö.n   D O.n lg n  :    Intuition: look at the recursion tree.     Except that here the constants are different; we get log10 n full levels and  It‚Äôs like the one for T .n  D T .n=3  C T .2n=3  C O.n  in Section 4.4. log10=9 n levels that are nonempty.   As long as it‚Äôs a constant, the base of the log doesn‚Äôt matter in asymptotic   Any split of constant proportionality will yield a recursion tree of depth  notation.  ‚Äö.lg n .   Lecture Notes for Chapter 7: Quicksort  7-5  Intuition for the average case   Splits in the recursion tree will not always be constant.  There will usually be a mix of good and bad splits throughout the recursion  tree.   To see that this doesn‚Äôt affect the asymptotic running time of quicksort, assume  that levels alternate between best-case and worst-case splits.  n  0  n‚Äì1   n‚Äì1  2 ‚Äì 1   n‚Äì1  2   n   n   n    n‚Äì1  2   n‚Äì1  2   The extra level in the left-hand Ô¨Ågure only adds to the constant hidden in the  ‚Äö-notation.   There are still the same number of subarrays to sort, and only twice as much  work was done to get to that point.   Both Ô¨Ågures result in O.n lg n  time, though the constant for the Ô¨Ågure on the  left is higher than that of the Ô¨Ågure on the right.  Randomized version of quicksort   We have assumed that all input permutations are equally likely.  This is not always true.  To correct this, we add randomization to quicksort.  We could randomly permute the input array.    Instead, we use random sampling, or picking one element at random.   Don‚Äôt always use A≈ír¬ç as the pivot. Instead, randomly pick an element from the  subarray that is being sorted.  RANDOMIZED-PARTITION .A; p; r  i D RANDOM.p; r  exchange A≈ír¬ç with A≈íi ¬ç return PARTITION.A; p; r   RANDOMIZED-QUICKSORT.A; p; r   if p < r  q D RANDOMIZED-PARTITION.A; p; r  RANDOMIZED-QUICKSORT.A; p; q  cid:0  1  RANDOMIZED-QUICKSORT.A; q C 1; r   Randomly selecting the pivot element will, on average, cause the split of the input array to be reasonably well balanced.  Q Q  7-6  Lecture Notes for Chapter 7: Quicksort  Randomization of quicksort stops any speciÔ¨Åc type of array from causing worst- case behavior. For example, an already-sorted array causes worst-case behavior in non-randomized QUICKSORT, but not in RANDOMIZED-QUICKSORT.  Analysis of quicksort  We will analyze      the worst-case running time of QUICKSORT and RANDOMIZED-QUICKSORT  the same , and the expected  average-case  running time of RANDOMIZED-QUICKSORT.  Worst-case analysis  We will prove that a worst-case split at every level produces a worst-case running time of O.n2 .   Recurrence for the worst-case running time of QUICKSORT:  T .n  D max 0qn cid:0 1  .T .q  C T .n  cid:0  q  cid:0  1   C ‚Äö.n  :  from 0 to n  cid:0  1.   Because PARTITION produces two subproblems, totaling size n  cid:0  1, q ranges  Guess: T .n   cn2, for some c.  Substituting our guess into the above recurrence: .cq2 C c.n  cid:0  q  cid:0  1 2  C ‚Äö.n  .q2 C .n  cid:0  q  cid:0  1 2  C ‚Äö.n  :  0qn cid:0 1 D c  max 0qn cid:0 1  T .n    max   The maximum value of .q2 C .n  cid:0  q  cid:0  1 2  occurs when q is either 0 or n  cid:0  1.   Second derivative with respect to q is positive.  Therefore,  max  0qn cid:0 1  .q2 C .n  cid:0  q  cid:0  1 2   .n  cid:0  1 2  D n2  cid:0  2n C 1 :   And thus,  T .n   cn2  cid:0  c.2n  cid:0  1  C ‚Äö.n    cn2  if c.2n  cid:0  1   ‚Äö.n  :   Pick c so that c.2n  cid:0  1  dominates ‚Äö.n .  Therefore, the worst-case running time of quicksort is O.n2 .  Can also show that the recurrence‚Äôs solution is .n2 . Thus, the worst-case  running time is ‚Äö.n2 .   Lecture Notes for Chapter 7: Quicksort  7-7  Average-case analysis   The dominant cost of the algorithm is partitioning.  PARTITION removes the pivot element from future consideration each time.  Thus, PARTITION is called at most n times.  QUICKSORT recurses on the partitions.  The amount of work that each call to PARTITION does is a constant plus the  number of comparisons that are performed in its for loop.   Let X D the total number of comparisons performed in all calls to PARTITION.  Therefore, the total work done over the entire execution is O.n C X  . We will now compute a bound on the overall number of comparisons. For ease of analysis:   Rename the elements of A as ¬¥1; ¬¥2; : : : ; ¬¥n, with ¬¥i being the ith smallest  element.   DeÔ¨Åne the set Zij D f¬¥i ; ¬¥iC1; : : : ; ¬¥jg to be the set of elements between ¬¥i  and ¬¥j , inclusive.  Each pair of elements is compared at most once, because elements are compared only to the pivot element, and then the pivot element is never in any later call to PARTITION. Let Xij D If¬¥i is compared to ¬¥jg.  Considering whether ¬¥i is compared to ¬¥j at any time during the entire quicksort algorithm, not just during one call of PARTITION.  Since each pair is compared at most once, the total number of comparisons per- formed by the algorithm is  Take expectations of both sides, use Lemma 5.1 and linearity of expectation:  n cid:0 1  n  n  Xij :  X D  XiD1  XjDiC1 E ≈íX ¬ç D E"n cid:0 1 XiD1 XjDiC1 XiD1 XjDiC1 XiD1 XjDiC1  n cid:0 1  n cid:0 1  D  D  n  n  Xij  E ≈íXij ¬ç  Prf¬¥i is compared to ¬¥jg :  Now all we have to do is Ô¨Ånd the probability that two elements are compared.   Think about when two elements are not compared.   For example, numbers in separate partitions will not be compared.    In the previous example, h8; 1; 6; 4; 0; 3; 9; 5i and the pivot is 5, so that none of the set f1; 4; 0; 3g will ever be compared to any of the set f8; 6; 9g.   7-8  Lecture Notes for Chapter 7: Quicksort   Once a pivot x is chosen such that ¬¥i < x < ¬¥j , then ¬¥i and ¬¥j will never be    compared at any later time. If either ¬¥i or ¬¥j is chosen before any other element of Zij , then it will be compared to all the elements of Zij , except itself.   The probability that ¬¥i is compared to ¬¥j is the probability that either ¬¥i or ¬¥j  is the Ô¨Årst element chosen.   There are j  cid:0 iC1 elements, and pivots are chosen randomly and independently. Thus, the probability that any particular one of them is the Ô¨Årst one chosen is 1=.j  cid:0  i C 1 .  Therefore, Prf¬¥i is compared to ¬¥jg D Prf¬¥i or ¬¥j is the Ô¨Årst pivot chosen from Zijg  D Prf¬¥i is the Ô¨Årst pivot chosen from Zijg  C Prf¬¥j is the Ô¨Årst pivot chosen from Zijg  1  j  cid:0  i C 1  D  D  1  j  cid:0  i C 1 C j  cid:0  i C 1  2  :  [Thesecondlinefollowsbecausethetwoeventsaremutuallyexclusive.] Substituting into the equation for E ≈íX ¬ç:  Evaluate by using a change in variables  k D j  cid:0  i  and the bound on the harmonic series in equation  A.7 :  E ≈íX ¬ç D  n cid:0 1  XiD1  n  XjDiC1  2  :  j  cid:0  i C 1  E ≈íX ¬ç D  2  j  cid:0  i C 1 2  k C 1 2  D  <  n  n cid:0 i  XjDiC1 XkD1 XkD1  n  k  n cid:0 1  n cid:0 1  XiD1 XiD1 XiD1 XiD1  n cid:0 1  n cid:0 1  O.lg n   D D O.n lg n  :  So the expected running time of quicksort, using RANDOMIZED-PARTITION, is O.n lg n .   Solutions for Chapter 7: Quicksort  Solution to Exercise 7.2-3 This solution is also posted publicly  PARTITION does a ‚Äúworst-case partitioning‚Äù when the elements are in decreasing order. It reduces the size of the subarray under consideration by only 1 at each step, which we‚Äôve seen has running time ‚Äö.n2 . In particular, PARTITION, given a subarray A≈íp : : r¬ç of distinct elements in de- creasing order, produces an empty partition in A≈íp : : q  cid:0  1¬ç, puts the pivot  orig- inally in A≈ír¬ç  into A≈íp¬ç, and produces a partition A≈íp C 1 : : r¬ç with only one fewer element than A≈íp : : r¬ç. The recurrence for QUICKSORT becomes T .n  D T .n  cid:0  1  C ‚Äö.n , which has the solution T .n  D ‚Äö.n2 .  Solution to Exercise 7.2-5 This solution is also posted publicly  The minimum depth follows a path that always takes the smaller part of the parti- tion‚Äîi.e., that multiplies the number of elements by Àõ. One iteration reduces the number of elements from n to Àõ n, and i iterations reduces the number of elements to Àõi n. At a leaf, there is just one remaining element, and so at a minimum-depth leaf of depth m, we have Àõmn D 1. Thus, Àõm D 1=n. Taking logs, we get m lg Àõ D  cid:0  lg n, or m D  cid:0  lg n= lg Àõ. Similarly, maximum depth corresponds to always taking the larger part of the par- tition, i.e., keeping a fraction 1  cid:0  Àõ of the elements each time. The maximum depth M is reached when there is one element left, that is, when .1  cid:0  Àõ M n D 1. Thus, M D  cid:0  lg n= lg.1  cid:0  Àõ . All these equations are approximate because we are ignoring Ô¨Çoors and ceilings.   7-10  Solutions for Chapter 7: Quicksort  Solution to Exercise 7.3-1  Solution to Exercise 7.4-2  We may be interested in the worst-case performance, but in that case, the random- ization is irrelevant: it won‚Äôt improve the worst case. What randomization can do is make the chance of encountering a worst-case scenario small.  To show that quicksort‚Äôs best-case running time is .n lg n , we use a technique similar to the one used in Section 7.4.1 to show that its worst-case running time is O.n2 . Let T .n  be the best-case time for the procedure QUICKSORT on an input of size n. We have the recurrence T .n  D min 1qn cid:0 1 We guess that T .n   cn lg n for some constant c. Substituting this guess into the recurrence, we obtain T .n    .T .q  C T .n  cid:0  q  cid:0  1   C ‚Äö.n  :  .cq lg q C c.n  cid:0  q  cid:0  1  lg.n  cid:0  q  cid:0  1   C ‚Äö.n  .q lg q C .n  cid:0  q  cid:0  1  lg.n  cid:0  q  cid:0  1   C ‚Äö.n  :  min 1qn cid:0 1 D c  min 1qn cid:0 1  As we‚Äôll show below, the expression q lg q C .n  cid:0  q  cid:0  1  lg.n  cid:0  q  cid:0  1  achieves a minimum over the range 1  q  n cid:0 1 when q D n cid:0 q cid:0 1, or q D .n cid:0 1 =2, since the Ô¨Årst derivative of the expression with respect to q is 0 when q D .n  cid:0  1 =2 and the second derivative of the expression is positive.  It doesn‚Äôt matter that q is not an integer when n is even, since we‚Äôre just trying to determine the minimum value of a function, knowing that when we constrain q to integer values, the function‚Äôs value will be no lower.  Choosing q D .n  cid:0  1 =2 gives us the bound min .q lg q C .n  cid:0  q  cid:0  1  lg.n  cid:0  q  cid:0  1  1qn cid:0 1 2 Cn  cid:0  n  cid:0  1 n  cid:0  1  2  cid:0  1 lgn  cid:0  n  cid:0  1  2  cid:0  1 n  cid:0  1  n  cid:0  1   D .n  cid:0  1  lg  lg  2  2  :  Continuing with our bounding of T .n , we obtain, for n  2, T .n   c.n  cid:0  1  lg  2 C ‚Äö.n   n  cid:0  1  D c.n  cid:0  1  lg.n  cid:0  1   cid:0  c.n  cid:0  1  C ‚Äö.n  D cn lg.n  cid:0  1   cid:0  c lg.n  cid:0  1   cid:0  c.n  cid:0  1  C ‚Äö.n   cn lg.n=2   cid:0  c lg.n  cid:0  1   cid:0  c.n  cid:0  1  C ‚Äö.n  D cn lg n  cid:0  cn  cid:0  c lg.n  cid:0  1   cid:0  cn C c C ‚Äö.n  D cn lg n  cid:0  .2cn C c lg.n  cid:0  1   cid:0  c  C ‚Äö.n   cn lg n ;   since n  2    Solutions for Chapter 7: Quicksort  7-11  since we can pick the constant c small enough so that the ‚Äö.n  term dominates the quantity 2cn C c lg.n  cid:0  1   cid:0  c. Thus, the best-case running time of quicksort is .n lg n . Letting f .q  D q lg q C .n  cid:0  q  cid:0  1  lg.n  cid:0  q  cid:0  1 , we now show how to Ô¨Ånd the minimum value of this function in the range 1  q  n  cid:0  1. We need to Ô¨Ånd the value of q for which the derivative of f with respect to q is 0. We rewrite this function as  f .q  D and so f 0.q  D  q ln q C .n  cid:0  q  cid:0  1  ln.n  cid:0  q  cid:0  1   ;  ln 2  d  ln 2  dq  q ln q C .n  cid:0  q  cid:0  1  ln.n  cid:0  q  cid:0  1  ln q C 1  cid:0  ln.n  cid:0  q  cid:0  1   cid:0  1 ln q  cid:0  ln.n  cid:0  q  cid:0  1   ln 2  :    ln 2  D D  The derivative f 0.q  is 0 when q D n  cid:0  q  cid:0  1, or when q D .n  cid:0  1 =2. To verify that q D .n  cid:0  1 =2 is indeed a minimum  not a maximum or an inÔ¨Çection point , we need to check that the second derivative of f is positive at q D .n  cid:0  1 =2: f 00.q   D  f 00 n  cid:0  1  D 2  D D > 0    1  d  ln 2 1  dq ln q  cid:0  ln.n  cid:0  q  cid:0  1  n  cid:0  q  cid:0  1 ln 2 1 q C ln 2 2 n  cid:0  1 n  cid:0  1 C 1 ln 2  n  cid:0  1  2  1  4   since n  2  :  Solution to Problem 7-2  a. If all elements are equal, then when PARTITION returns, q D r and all elements in A≈íp : : q cid:0 1¬ç are equal. We get the recurrence T .n  D T .n cid:0 1 CT .0 C‚Äö.n  for the running time, and so T .n  D ‚Äö.n2 .   7-12  Solutions for Chapter 7: Quicksort  b. The PARTITION0 procedure:  PARTITION0.A; p; r  x D A≈íp¬ç i D h D p for j D p C 1 to r     Invariant: A≈íp : : i  cid:0  1¬ç < x, A≈íi : : h¬ç D x, A≈íh C 1 : : j  cid:0  1¬ç > x, A≈íj : : r¬ç unknown. if A≈íj ¬ç < x y D A≈íj ¬ç A≈íj ¬ç D A≈íh C 1¬ç A≈íh C 1¬ç D A≈íi ¬ç A≈íi ¬ç D y i D i C 1 h D h C 1 elseif A≈íj ¬ç == x exchange A≈íh C 1¬ç with A≈íj ¬ç h D h C 1  return .i; h   c. RANDOMIZED-PARTITION0 is the same as RANDOMIZED-PARTITION, but  with the call to PARTITION replaced by a call to PARTITION0.  QUICKSORT0.A; p; r   if p < r  .q; t   D RANDOMIZED-PARTITION0.A; p; r  QUICKSORT0.A; p; q  cid:0  1  QUICKSORT0.A; t C 1; r   d. Putting elements equal to the pivot in the same partition as the pivot can only help us, because we do not recurse on elements equal to the pivot. Thus, the subproblem sizes with QUICKSORT0, even with equal elements, are no larger than the subproblem sizes with QUICKSORT when all elements are distinct.  Solution to Problem 7-4  a. QUICKSORT0 does exactly what QUICKSORT does; hence it sorts correctly.  QUICKSORT and QUICKSORT0 do the same partitioning, and then each calls itself with arguments A; p; q  cid:0  1. QUICKSORT then calls itself again, with arguments A; q C 1; r. QUICKSORT0 instead sets p D q C 1 and performs another iteration of its while loop. This executes the same operations as calling itself with A; q C 1; r, because in both cases, the Ô¨Årst and third arguments  A and r  have the same values as before, and p has the old value of q C 1.  b. The stack depth of QUICKSORT0 will be ‚Äö.n  on an n-element input array if there are ‚Äö.n  recursive calls to QUICKSORT0. This happens if every call to PARTITION.A; p; r  returns q D r. The sequence of recursive calls in this scenario is   Solutions for Chapter 7: Quicksort  7-13  QUICKSORT0.A; 1; n  ; QUICKSORT0.A; 1; n  cid:0  1  ; QUICKSORT0.A; 1; n  cid:0  2  ;  :::  QUICKSORT0.A; 1; 1  :  Any array that is already sorted in increasing order will cause QUICKSORT0 to behave this way.  c. The problem demonstrated by the scenario in part  b  is that each invocation of QUICKSORT0 calls QUICKSORT0 again with almost the same range. To avoid such behavior, we must change QUICKSORT0 so that the recursive call is on a smaller interval of the array. The following variation of QUICKSORT0 checks which of the two subarrays returned from PARTITION is smaller and recurses on the smaller subarray, which is at most half the size of the current array. Since the array size is reduced by at least half on each recursive call, the number of recursive calls, and hence the stack depth, is ‚Äö.lg n  in the worst case. Note that this method works no matter how partitioning is performed  as long as the PARTITION procedure has the same functionality as the procedure given in Section 7.1 .  QUICKSORT00.A; p; r   while p < r     Partition and sort the small subarray Ô¨Årst. q D PARTITION.A; p; r  if q  cid:0  p < r  cid:0  q QUICKSORT00.A; p; q  cid:0  1  p D q C 1 else QUICKSORT00.A; q C 1; r  r D q  cid:0  1  The expected running time is not affected, because exactly the same work is done as before: the same partitions are produced, and the same subarrays are sorted.   Lecture Notes for Chapter 8: Sorting in Linear Time  Chapter 8 overview  How fast can we sort?  Comparison sorting  We will prove a lower bound, then beat it by playing a different game.   The only operation that may be used to gain order information about a sequence   All sorts seen so far are comparison sorts: insertion sort, selection sort, merge  is comparison of pairs of elements.  sort, quicksort, heapsort, treesort.  Lower bounds for sorting  Lower bounds   .n  to examine all the input.  All sorts seen so far are .n lg n .  We‚Äôll show that .n lg n  is a lower bound for comparison sorts.  Decision tree   Abstraction of any comparison sort.  Represents comparisons made by   a speciÔ¨Åc sorting algorithm  on inputs of a given size.   Abstracts away everything else: control and data movement.  We‚Äôre counting only comparisons.   8-2  Lecture Notes for Chapter 8: Sorting in Linear Time  For insertion sort on 3 elements:  compare A[1] to A[2]  A[1] ¬£   A[2]  1:2  A[1] > A[2]  swap in array   >  A[1] ¬£   A[2] ¬£  √Ü 1,2,3√¶  A[3]  2:3  A[1] ¬£  A[2] A[2] > A[3]  >  1:3  A[1] > A[2] A[1] > A[3]  >  1:3  √Ü 2,1,3√¶  2:3  √Ü 1,3,2√¶  > √Ü 3,1,2√¶  √Ü 2,3,1√¶  > √Ü 3,2,1√¶  [Each internal node is labeled by indices of array elements from their original positions. Each leaf is labeled by the permutation of orders that the algorithm determines.] How many leaves on the decision tree? There are  n≈† leaves, because every permutation appears at least once. For any comparison sort,    1 tree for each n.   View the tree as if the algorithm splits in two at each node, based on the infor-  mation it has determined up to that point.   The tree models all possible execution traces.  What is the length of the longest path from root to leaf?   Depends on the algorithm    Insertion sort: ‚Äö.n2   Merge sort: ‚Äö.n lg n   Lemma Any binary tree of height h has  2h leaves. In other words:    l D  of leaves,   h D height,  Then l  2h.   We‚Äôll prove this lemma later.  Why is this useful?  Theorem Any decision tree that sorts n elements has height .n lg n .  ¬£ ¬£ ¬£ ¬£ ¬£  Lecture Notes for Chapter 8: Sorting in Linear Time  8-3   By lemma, n≈†  l  2h or 2h  n≈†  Take logs: h  lg.n≈†   Use Stirling‚Äôs approximation: n≈† > .n=e n  by equation  3.17    Proof    l  n≈†  h  lg.n=e n D n lg.n=e  D n lg n  cid:0  n lg e D .n lg n  :  Now to prove the lemma:   theorem   Proof By induction on h. Basis: h D 0. Tree is just one node, which is a leaf. 2h D 1. Inductive step: Assume true for height D h  cid:0  1. Extend tree of height h  cid:0  1 by making as many new leaves as possible. Each leaf becomes parent to two new leaves.  of leaves for height h D 2    of leaves for height h  cid:0  1   D 2  2h cid:0 1 D 2h :   ind. hypothesis   lemma   Corollary Heapsort and merge sort are asymptotically optimal comparison sorts.  Sorting in linear time  Non-comparison sorts.  Counting sort  Depends on a key assumption: numbers to be sorted are integers in f0; 1; : : : ; kg. Input: A≈í1 : : n¬ç, where A≈íj ¬ç 2 f0; 1; : : : ; kg for j D 1; 2; : : : ; n. Array A and Output: B≈í1 : : n¬ç, sorted. B is assumed to be already allocated and is given as a  values n and k are given as parameters.  parameter.  Auxiliary storage: C ≈í0 : : k¬ç   8-4  Lecture Notes for Chapter 8: Sorting in Linear Time  COUNTING-SORT.A; B; n; k   let C ≈í0 : : k¬ç be a new array for i D 0 to k C ≈íi ¬ç D 0 for j D 1 to n C ≈íA≈íj ¬ç¬ç D C ≈íA≈íj ¬ç¬ç C 1 for i D 1 to k C ≈íi ¬ç D C ≈íi ¬ç C C ≈íi  cid:0  1¬ç for j D n downto 1 B≈íC ≈íA≈íj ¬ç¬ç¬ç D A≈íj ¬ç C ≈íA≈íj ¬ç¬ç D C ≈íA≈íj ¬ç¬ç  cid:0  1  Analysis ‚Äö.n C k , which is ‚Äö.n  if k D O.n . How big a k is practical?   Good for sorting 32-bit values? No.        16-bit? Probably not. 8-bit? Maybe, depending on n. 4-bit? Probably  unless n is really small .  Counting sort will be used in radix sort.  Radix sort  Do an example for A D 21; 51; 31; 01; 22; 32; 02; 33 Counting sort is stable  keys with same value appear in same order in output as they did in input  because of how the last loop works.  How IBM made its money. Punch card readers for census tabulation in early 1900‚Äôs. Card sorters, worked on one column at a time. It‚Äôs the algorithm for using the machine that extends the technique to multi-column sorting. The human operator was part of the algorithm! Key idea: Sort least signiÔ¨Åcant digits Ô¨Årst. To sort d digits:  RADIX-SORT.A; d   for i D 1 to d  use a stable sort to sort array A on digit i   Lecture Notes for Chapter 8: Sorting in Linear Time  8-5  Example  326 453 608 835 751 435 704 690  690 751 453 704 835 435 326 608  Correctness  sorted  704 608 326 835 435 751 453 690  326 435 453 608 690 704 751 835    Induction on number of passes  i in pseudocode .   Assume digits 1; 2; : : : ; i  cid:0  1 are sorted.  Show that a stable sort on digit i leaves digits 1; : : : ; i sorted:      If 2 digits in position i are different, ordering by position i is correct, and positions 1; : : : ; i  cid:0  1 are irrelevant. If 2 digits in position i are equal, numbers are already in the right order  by inductive hypothesis . The stable sort on digit i leaves them in the right order.  This argument shows why it‚Äôs so important to use a stable sort for intermediate sort.  Analysis Assume that we use counting sort as the intermediate sort.  ‚Äö.n C k  per pass  digits in range 0; : : : ; k   d passes  ‚Äö.d.n C k   total    If k D O.n , time D ‚Äö.d n . How to break each key into digits?   n words.    b bits word.   Break into r-bit digits. Have d D db=re.  Use counting sort, k D 2r  cid:0  1.  Example: 32-bit words, 8-bit digits. b D 32, r D 8, d D d32=8e D 4, k D 28  cid:0  1 D 255.   Time D ‚Äö  b How to choose r? Balance b=r and n C 2r. Choosing r  lg n gives us lg n .n C n  D ‚Äö.bn= lg n . ‚Äö cid:0  b  r .n C 2r   .   8-6  Lecture Notes for Chapter 8: Sorting in Linear Time      If we choose r   b= lg n, and n C 2r term doesn‚Äôt improve. If we choose r > lg n, then n C 2r term gets big. Example: r D 2 lg n   2r D 22 lg n D .2lg n 2 D n2.  So, to sort 216 32-bit numbers, use r D lg 216 D 16 bits. db=re D 2 passes. Compare radix sort to merge sort and quicksort:    1 million .220  32-bit integers.  Radix sort: d32=20e D 2 passes.  Merge sort quicksort: lg n D 20 passes.  Remember, though, that each radix sort ‚Äúpass‚Äù is really 2 passes‚Äîone to take  census, and one to move data.  How does radix sort violate the ground rules for a comparison sort?   Using counting sort allows us to gain information about keys by means other  than directly comparing 2 keys.   Used keys as array indices.  Bucket sort  Assumes the input is generated by a random process that distributes elements uni- formly over ≈í0; 1 .  Idea  Divide ≈í0; 1  into n equal-sized buckets.  Distribute the n input values into the buckets.  Sort each bucket.  Then go through buckets in order, listing elements in each one. Input: A≈í1 : : n¬ç, where 0  A≈íi ¬ç < 1 for all i. Auxiliary array: B≈í0 : : n  cid:0  1¬ç of linked lists, each list initially empty. BUCKET-SORT.A; n  let B≈í0 : : n  cid:0  1¬ç be a new array for i D 1 to n  cid:0  1 for i D 1 to n for i D 0 to n  cid:0  1 concatenate lists B≈í0¬ç; B≈í1¬ç; : : : ; B≈ín  cid:0  1¬ç together in order return the concatenated lists  insert A≈íi ¬ç into list B≈íbn  A≈íi ¬çc¬ç sort list B≈íi ¬ç with insertion sort  make B≈íi ¬ç an empty list   Lecture Notes for Chapter 8: Sorting in Linear Time  8-7  Correctness Consider A≈íi ¬ç, A≈íj ¬ç. Assume without loss of generality that A≈íi ¬ç  A≈íj ¬ç. Then bn  A≈íi ¬çc  bn  A≈íj ¬çc. So A≈íi ¬ç is placed into the same bucket as A≈íj ¬ç or into a bucket with a lower index.      If same bucket, insertion sort Ô¨Åxes up. If earlier bucket, concatenation of lists Ô¨Åxes up.  Analysis  Relies on no bucket getting too many values.  All lines of algorithm except insertion sorting take ‚Äö.n  altogether.    Intuitively, if each bucket gets a constant number of elements, it takes O.1  time to sort each bucket   O.n  sort time for all buckets. per bucket.   We ‚Äúexpect‚Äù each bucket to have few elements, since the average is 1 element   But we need to do a careful analysis.  DeÔ¨Åne a random variable:  ni D the number of elements placed in bucket B≈íi ¬ç. Because insertion sort runs in quadratic time, bucket sort time is   linearity of expectation    E ≈íaX ¬ç D aE ≈íX ¬ç   O.n2  T .n  D ‚Äö.n  C Take expectations of both sides:  i   :  n cid:0 1  XiD0  n cid:0 1  O.n2  E ≈íT .n ¬ç D E"‚Äö.n  C i   XiD0 XiD0 i   EO.n2 D ‚Äö.n  C XiD0 i  O.En2  D ‚Äö.n  C  n cid:0 1  n cid:0 1  Claim E ≈ín2  i ¬ç D 2  cid:0  .1=n  for i D 0; : : : ; n  cid:0  1.  Proof of claim DeÔ¨Åne indicator random variables:  Xij D IfA≈íj ¬ç falls in bucket ig  PrfA≈íj ¬ç falls in bucket ig D 1=n  ni D  Xij  n  XjD1   8-8  Lecture Notes for Chapter 8: Sorting in Linear Time  Then  i D E"  n XjD1 En2 D E" n XjD1 XjD1 EX 2 D  Xij!2 XjD1 XkDjC1 X 2 ij C 2 XjD1 XkDjC1 ij C 2  n cid:0 1  n cid:0 1  n  n  n  Xij Xi k  E ≈íXij Xi k¬ç   linearity of expectation   EX 2  1  ij D 02  PrfA≈íj ¬ç doesn‚Äôt fall in bucket ig C 12  PrfA≈íj ¬ç falls in bucket ig D 0 1  cid:0  D  n C 1   n  n  1  1  E ≈íXij Xi k¬ç for j ¬§ k: Since j ¬§ k, Xij and Xi k are independent random variables   E ≈íXij Xi k¬ç D E ≈íXij ¬ç E ≈íXi k¬ç  1  n  1 n  1 n2  D D  Therefore:  En2  i D  1 n2  n  n cid:0 1  n  1 XjD1 XkDjC1 XjD1 n C 2 n C 2 n 2! 1 D n  n2 n.n  cid:0  1   1  1 n2    2  D 1 C 2  n  cid:0  1 D 1 C n 1 D 1 C 1  cid:0  D 2  cid:0   n  n  1  Therefore:  E ≈íT .n ¬ç D ‚Äö.n  C  O.2  cid:0  1=n   n cid:0 1  XiD0 D ‚Äö.n  C O.n  D ‚Äö.n   array.   claim    Again, not a comparison sort. Used a function of key values to index into an   Lecture Notes for Chapter 8: Sorting in Linear Time  8-9   This is a probabilistic analysis‚Äîwe used probability to analyze an algorithm  whose running time depends on the distribution of inputs.   Different from a randomized algorithm, where we use randomization to impose  a distribution.   With bucket sort, if the input isn‚Äôt drawn from a uniform distribution on ≈í0; 1 ,  all bets are off  performance-wise, but the algorithm is still correct .   Solutions for Chapter 8: Sorting in Linear Time  Solution to Exercise 8.1-3 This solution is also posted publicly  If the sort runs in linear time for m input permutations, then the height h of the portion of the decision tree consisting of the m corresponding leaves and their ancestors is linear. Use the same argument as in the proof of Theorem 8.1 to show that this is impos- sible for m D n≈†=2, n≈†=n, or n≈†=2n. We have 2h  m, which gives us h  lg m. For all the possible m‚Äôs given here, lg m D .n lg n , hence h D .n lg n . In particular,  lg  lg  lg  n≈† 2 D lg n≈†  cid:0  1  n lg n  cid:0  n lg e  cid:0  1 ; n≈† n D lg n≈†  cid:0  lg n  n lg n  cid:0  n lg e  cid:0  lg n ; n≈† 2n D lg n≈†  cid:0  n  n lg n  cid:0  n lg e  cid:0  n :  Let S be a sequence of n elements divided into n=k subsequences each of length k where all of the elements in any subsequence are larger than all of the elements of a preceding subsequence and smaller than all of the elements of a succeeding subsequence.  Claim Any comparison-based sorting algorithm to sort s must take .n lg k  time in the worst case.  Proof First notice that, as pointed out in the hint, we cannot prove the lower bound by multiplying together the lower bounds for sorting each subsequence. That would only prove that there is no faster algorithm that sorts the subsequences  Solution to Exercise 8.1-4   Solutions for Chapter 8: Sorting in Linear Time  8-11  independently. This was not what we are asked to prove; we cannot introduce any extra assumptions. Now, consider the decision tree of height h for any comparison sort for S. Since the elements of each subsequence can be in any order, any of the k≈† permutations cor- respond to the Ô¨Ånal sorted order of a subsequence. And, since there are n=k such subsequences, each of which can be in any order, there are .k≈† n=k permutations of S that could correspond to the sorting of some input order. Thus, any decision tree for sorting S must have at least .k≈† n=k leaves. Since a binary tree of height h has no more than 2h leaves, we must have 2h  .k≈† n=k or h  lg..k≈† n=k . We therefore obtain h  lg..k≈† n=k  D .n=k  lg.k≈†   .n=k  lg..k=2 k=2  D .n=2  lg.k=2  :  The third line comes from k≈† having its k=2 largest terms being at least k=2 each.  We implicitly assume here that k is even. We could adjust with Ô¨Çoors and ceilings if k were odd.  Since there exists at least one path in any decision tree for sorting S that has length at least .n=2  lg.k=2 , the worst-case running time of any comparison-based sort- ing algorithm for S is .n lg k .  Solution to Exercise 8.2-3 This solution is also posted publicly  [ThefollowingsolutionalsoanswersExercise8.2-2.] Notice that the correctness argument in the text does not depend on the order in which A is processed. The algorithm is correct no matter what order is used! But the modiÔ¨Åed algorithm is not stable. As before, in the Ô¨Ånal for loop an element equal to one taken from A earlier is placed before the earlier one  i.e., at a lower index position  in the output arrray B. The original algorithm was stable because an element taken from A later started out with a lower index than one taken earlier. But in the modiÔ¨Åed algorithm, an element taken from A later started out with a higher index than one taken earlier. In particular, the algorithm still places the elements with value k in positions C ≈ík  cid:0  1¬ç C 1 through C ≈ík¬ç, but in the reverse order of their appearance in A.  Solution to Exercise 8.2-4  Compute the C array as is done in counting sort. The number of integers in the range ≈ía : : b¬ç is C ≈íb¬ç  cid:0  C ≈ía  cid:0  1¬ç, where we interpret C ≈í cid:0 1¬ç as 0.   8-12  Solutions for Chapter 8: Sorting in Linear Time  Solution to Exercise 8.3-2  Solution to Exercise 8.3-3 This solution is also posted publicly  Insertion sort is stable. When inserting A≈íj ¬ç into the sorted sequence A≈í1 : : : j  cid:0 1¬ç, we do it the following way: compare A≈íj ¬ç to A≈íi ¬ç, starting with i D j  cid:0  1 and going down to i D 1. Continue at long as A≈íj ¬ç < A≈íi ¬ç. Merge sort as deÔ¨Åned is stable, because when two elements compared are equal, the tie is broken by taking the element from array L which keeps them in the original order. Heapsort and quicksort are not stable. One scheme that makes a sorting algorithm stable is to store the index of each element  the element‚Äôs place in the original ordering  with the element. When comparing two elements, compare them by their values and break ties by their indices. Additional space requirements: For n elements, their indices are 1 : : : n. Each can be written in lg n bits, so together they take O.n lg n  additional space. Additional time requirements: The worst case is when all elements are equal. The asymptotic time does not change because we add a constant amount of work to each comparison.  Basis: If d D 1, there‚Äôs only one digit, so sorting on that digit sorts the array. Inductive step: Assuming that radix sort works for d  cid:0  1 digits, we‚Äôll show that it works for d digits. Radix sort sorts separately on each digit, starting from digit 1. Thus, radix sort of d digits, which sorts on digits 1; : : : ; d is equivalent to radix sort of the low-order d  cid:0  1 digits followed by a sort on digit d . By our induction hypothesis, the sort of the low-order d  cid:0  1 digits works, so just before the sort on digit d , the elements are in order according to their low-order d  cid:0  1 digits. The sort on digit d will order the elements by their d th digit. Consider two ele- ments, a and b, with d th digits ad and bd respectively.        If ad < bd , the sort will put a before b, which is correct, since a < b regardless of the low-order digits. If ad > bd , the sort will put a after b, which is correct, since a > b regardless of the low-order digits. If ad D bd , the sort will leave a and b in the same order they were in, because it is stable. But that order is already correct, since the correct order of a and b is determined by the low-order d  cid:0  1 digits when their d th digits are equal, and the elements are already sorted by their low-order d  cid:0  1 digits. If the intermediate sort were not stable, it might rearrange elements whose d th digits were equal‚Äîelements that were in the right order after the sort on their lower-order digits.   Solutions for Chapter 8: Sorting in Linear Time  8-13  Solution to Exercise 8.3-4 This solution is also posted publicly  Treat the numbers as 3-digit numbers in radix n. Each digit ranges from 0 to n cid:0  1. Sort these 3-digit numbers with radix sort. There are 3 calls to counting sort, each taking ‚Äö.n C n  D ‚Äö.n  time, so that the total time is ‚Äö.n .  Solution to Exercise 8.4-2  The worst-case running time for the bucket-sort algorithm occurs when the assump- tion of uniformly distributed input does not hold. If, for example, all the input ends up in the Ô¨Årst bucket, then in the insertion sort phase it needs to sort all the input, which takes O.n2  time. A simple change that will preserve the linear expected running time and make the worst-case running time O.n lg n  is to use a worst-case O.n lg n -time algorithm, such as merge sort, instead of insertion sort when sorting the buckets.  Solution to Problem 8-1 This solution is also posted publicly  a. For a comparison algorithm A to sort, no two input permutations can reach the same leaf of the decision tree, so there must be at least n≈† leaves reached in TA, one for each possible input permutation. Since A is a deterministic algorithm, it must always reach the same leaf when given a particular permutation as input, so at most n≈† leaves are reached  one for each permutation . Therefore exactly n≈† leaves are reached, one for each input permutation. These n≈† leaves will each have probability 1=n≈†, since each of the n≈† possible permutations is the input with the probability 1=n≈†. Any remaining leaves will have probability 0, since they are not reached for any input. Without loss of generality, we can assume for the rest of this problem that paths leading only to 0-probability leaves aren‚Äôt in the tree, since they cannot affect the running time of the sort. That is, we can assume that TA consists of only the n≈† leaves labeled 1=n≈† and their ancestors.  b. If k > 1, then the root of T is not a leaf. This implies that all of T ‚Äôs leaves are leaves in LT and RT . Since every leaf at depth h in LT or RT has depth hC 1 in T , D.T   must be the sum of D.LT  , D.RT  , and k, the total number of leaves. To prove this last assertion, let dT .x  D depth of node x in tree T . Then,   8-14  Solutions for Chapter 8: Sorting in Linear Time  dT .x   D.T   D Xx2leaves.T   D Xx2leaves.LT   D Xx2leaves.LT   D Xx2leaves.LT   D D.LT   C D.RT   C k :  dT .x  C Xx2leaves.RT   .dLT .x  C 1  C Xx2leaves.RT   dLT .x  C Xx2leaves.RT    dT .x   .dRT .x  C 1  dRT .x  C Xx2leaves.T    1  c. To show that d.k  D min1ik cid:0 1 fd.i   C d.k  cid:0  i   C kg we will show sepa-  1ik cid:0 1fd.i   C d.k  cid:0  i   C kg  1ik cid:0 1fd.i   C d.k  cid:0  i   C kg :  rately that d.k   min and d.k   min  To show that d.k   min1ik cid:0 1 fd.i   C d.k  cid:0  i   C kg, we need only show that d.k   d.i   C d.k  cid:0  i   C k, for i D 1; 2; : : : ; k  cid:0  1. For any i from 1 to k  cid:0  1 we can Ô¨Ånd trees RT with i leaves and LT with k  cid:0  i leaves such that D.RT   D d.i   and D.LT   D d.k  cid:0  i  . Construct T such that RT and LT are the right and left subtrees of T ‚Äôs root respectively. Then  by deÔ¨Ånition of d as min D.T   value  d.k   D.T    D D.RT   C D.LT   C k  by part  b   D d.i   C d.k  cid:0  i   C k   by choice of RT and LT   .   To show that d.k   min1ik cid:0 1 fd.i   C d.k  cid:0  i   C kg, we need only show that d.k   d.i   C d.k  cid:0  i   C k, for some i in f1; 2; : : : ; k  cid:0  1g. Take the tree T with k leaves such that D.T   D d.k , let RT and LT be T ‚Äôs right and left subtree, respecitvely, and let i be the number of leaves in RT . Then k  cid:0  i is the number of leaves in LT and d.k  D D.T     by choice of T    D D.RT   C D.LT   C k  by part  b    d.i   C d.k  cid:0  i   C k   by deÔ¨Åntion of d as min D.T   value  . Neither i nor k  cid:0  i can be 0  and hence 1  i  k  cid:0  1 , since if one of these were 0, either RT or LT would contain all k leaves of T , and that k-leaf subtree would have a D equal to D.T    cid:0  k  by part  b  , contradicting the choice of T as the k-leaf tree with the minimum D.  d. Let fk.i   D i lg i C .k  cid:0  i   lg.k  cid:0  i  . To Ô¨Ånd the value of i that minimizes fk,  Ô¨Ånd the i for which the derivative of fk with respect to i is 0: f 0k.i   D  d  ln 2  d i  i ln i C .k  cid:0  i   ln.k  cid:0  i   ln i C 1  cid:0  ln.k  cid:0  i    cid:0  1 ln 2 ln i  cid:0  ln.k  cid:0  i      ln 2  D D   Solutions for Chapter 8: Sorting in Linear Time  8-15  is 0 at i D k=2. To verify this is indeed a minimum  not a maximum , check that the second derivative of fk is positive at i D k=2: f 00k .i   D  d  D    1  ln 2 1  d i  ln i  cid:0  ln.k  cid:0  i   k  cid:0  i : ln 2 1 i C k ln 2 2 k C 4 1 ln 2   k  2  1  D > 0  f 00k .k=2  D  since k > 1 .  Now we use substitution to prove d.k  D .k lg k . The base case of the induction is satisÔ¨Åed because d.1   0 D c  1  lg 1 for any constant c. For the inductive step we assume that d.i    ci lg i for 1  i  k  cid:0  1, where c is some constant to be determined. d.k  D min 1ik cid:0 1fd.i   C d.k  cid:0  i   C kg 1ik cid:0 1fc.i lg i C .k  cid:0  i   lg.k  cid:0  i    C kg min  D min 1ik cid:0 1fcfk.i   C kg D c k 2k  cid:0  2 C k 2 C k D ck lg k D c.k lg k  cid:0  k  C k D ck lg k C .k  cid:0  ck   ck lg k if c  1 ;  2 lgk  cid:0   lg  k  k  k  2  and so d.k  D .k lg k .  e. Using the result of part  d  and the fact that TA  as modiÔ¨Åed in our solution to  part  a   has n≈† leaves, we can conclude that D.TA   d.n≈†  D .n≈† lg.n≈†   : D.TA  is the sum of the decision-tree path lengths for sorting all input per- mutations, and the path lengths are proportional to the run time. Since the n≈† permutations have equal probability 1=n≈†, the expected time to sort n random elements  1 input permutation  is the total time for all permutations divided by n≈†: .n≈† lg.n≈†    n≈†  D .lg.n≈†   D .n lg n  :  f. We will show how to modify a randomized decision tree  algorithm  to deÔ¨Åne a deterministic decision tree  algorithm  that is at least as good as the randomized one in terms of the average number of comparisons. At each randomized node, pick the child with the smallest subtree  the subtree with the smallest average number of comparisons on a path to a leaf . Delete all   8-16  Solutions for Chapter 8: Sorting in Linear Time  Solution to Problem 8-3  the other children of the randomized node and splice out the randomized node itself. The deterministic algorithm corresponding to this modiÔ¨Åed tree still works, be- cause the randomized algorithm worked no matter which path was taken from each randomized node. The average number of comparisons for the modiÔ¨Åed algorithm is no larger than the average number for the original randomized tree, since we discarded the higher-average subtrees in each case. In particular, each time we splice out a randomized node, we leave the overall average less than or equal to what it was, because      the same set of input permutations reaches the modiÔ¨Åed subtree as before, but those inputs are handled in less than or equal to average time than before, and the rest of the tree is unmodiÔ¨Åed.  The randomized algorithm thus takes at least as much time on average as the corresponding deterministic one.  We‚Äôve shown that the expected running time for a deterministic comparison sort is .n lg n , hence the expected time for a randomized comparison sort is also .n lg n .   a. The usual, unadorned radix sort algorithm will not solve this problem in the required time bound. The number of passes, d , would have to be the number of digits in the largest integer. Suppose that there are m integers; we always have m  n. In the worst case, we would have one integer with n=2 digits and n=2 integers with one digit each. We assume that the range of a single digit is constant. Therefore, we would have d D n=2 and m D n=2 C 1, and so the running time would be ‚Äö.d m  D ‚Äö.n2 . Let us assume without loss of generality that all the integers are positive and have no leading zeros.  If there are negative integers or 0, deal with the positive numbers, negative numbers, and 0 separately.  Under this assumption, we can observe that integers with more digits are always greater than integers with fewer digits. Thus, we can Ô¨Årst sort the integers by number of digits  using counting sort , and then use radix sort to sort each group of integers with the same length. Noting that each integer has between 1 and n digits, let mi be the number of integers with i digits, for i D 1; 2; : : : ; n. Since there are n digits altogether, we havePn It takes O.n  time to compute how many digits all the integers have and, once the numbers of digits have been computed, it takes O.m C n  D O.n  time to group the integers by number of digits. To sort the group with mi digits by radix sort takes ‚Äö.i  mi   time. The time to sort all groups, therefore, is ‚Äö.i  mi   D ‚Äö  n XiD1 XiD1 D ‚Äö.n  :  iD1 i  mi D n.  i  mi!  n   Solutions for Chapter 8: Sorting in Linear Time  8-17  b. One way to solve this problem is by a radix sort from right to left. Since the strings have varying lengths, however, we have to pad out all strings that are shorter than the longest string. The padding is on the right end of the string, and it‚Äôs with a special character that is lexicographically less than any other character  e.g., in C, the character ‚Äô\0‚Äô with ASCII value 0 . Of course, we don‚Äôt have to actually change any string; if we want to know the j th character of a string whose length is k, then if j > k, the j th character is the pad character. Unfortunately, this scheme does not always run in the required time bound. Suppose that there are m strings and that the longest string has d characters. In the worst case, one string has n=2 characters and, before padding, n=2 strings have one character each. As in part  a , we would have d D n=2 and m D n=2 C 1. We still have to examine the pad characters in each pass of radix sort, even if we don‚Äôt actually create them in the strings. Assuming that the range of a single character is constant, the running time of radix sort would be ‚Äö.d m  D ‚Äö.n2 . To solve the problem in O.n  time, we use the property that, if the Ô¨Årst letter of string x is lexicographically less that the Ô¨Årst letter of string y, then x is lexicographically less than y, regardless of the lengths of the two strings. We take advantage of this property by sorting the strings on the Ô¨Årst letter, using counting sort. We take an empty string as a special case and put it Ô¨Årst. We gather together all strings with the same Ô¨Årst letter as a group. Then we recurse, within each group, based on each string with the Ô¨Årst letter removed. The correctness of this algorithm is straightforward. Analyzing the running time is a bit trickier. Let us count the number of times that each string is sorted by a call of counting sort. Suppose that the ith string, si , has length li. Then si is sorted by at most li C 1 counting sorts.  The ‚ÄúC1‚Äù is because it may have to be sorted as an empty string at some point; for example, ab and a end up in the same group in the Ô¨Årst pass and are then ordered based on b and the empty string in the second pass. The string a is sorted its length, 1, time plus one more time.  A call of counting sort on t strings takes ‚Äö.t   time  remembering that the number of different characters on which we are sorting is a constant.  Thus, the total time for all calls of counting sort is  O  m .li C 1 ! D O  m XiD1 XiD1 D O.n C m  D O.n  ;  li C m!  where the second line follows fromPm m  n.  iD1 li D n, and the last line is because  Solution to Problem 8-4  a. Compare each red jug with each blue jug. Since there are n red jugs and n blue  jugs, that will take ‚Äö.n2  comparisons in the worst case.   8-18  Solutions for Chapter 8: Sorting in Linear Time  b. To solve the problem, an algorithm has to perform a series of comparisons until it has enough information to determine the matching. We can view the computation of the algorithm in terms of a decision tree. Every internal node is labeled with two jugs  one red, one blue  which we compare, and has three outgoing edges  red jug smaller, same size, or larger than the blue jug . The leaves are labeled with a unique matching of jugs. The height of the decision tree is equal to the worst-case number of comparisons the algorithm has to make to determine the matching. To bound that size, let us Ô¨Årst compute the number of possible matchings for n red and n blue jugs. If we label the red jugs from 1 to n and we also label the blue jugs from 1 to n before starting the comparisons, every outcome of the algorithm can be represented as a set f.i; .i    W 1  i  n and  is a permutation on f1; : : : ; ngg ; which contains the pairs of red jugs  Ô¨Årst component  and blue jugs  second component  that are matched up. Since every permutation  corresponds to a different outcome, there must be exactly n≈† different results. Now we can bound the height h of our decision tree. Every tree with a branch- ing factor of 3  every inner node has at most three children  has at most 3h leaves. Since the decison tree must have at least n≈† children, it follows that 3h  n≈†  .n=e n   h  n log3 n  cid:0  n log3 e D .n lg n  : So any algorithm solving the problem must use .n lg n  comparisons.  c. Assume that the red jugs are labeled with numbers 1; 2; : : : ; n and so are the blue jugs. The numbers are arbitrary and do not correspond to the volumes of jugs, but are just used to refer to the jugs in the algorithm description. Moreover, the output of the algorithm will consist of n distinct pairs .i; j  , where the red jug i and the blue jug j have the same volume. The procedure MATCH-JUGS takes as input two sets representing jugs to be matched: R  f1; : : : ; ng, representing red jugs, and B  f1; : : : ; ng, rep- resenting blue jugs. We will call the procedure only with inputs that can be matched; one necessary condition is that jRj D jBj.   Solutions for Chapter 8: Sorting in Linear Time  8-19     sets are empty     sets contain just one jug each  MATCH-JUGS.R; B  if jRj == 0 return if jRj == 1  let R D frg and B D fbg output ‚Äú.r; b ‚Äù return  else r D a randomly chosen jug in R  compare r to every jug of B B< D the set of jugs in B that are smaller than r B> D the set of jugs in B that are larger than r b D the one jug in B with the same size as r compare b to every jug of R  cid:0  frg R< D the set of jugs in R that are smaller than b R> D the set of jugs in R that are larger than b output ‚Äú.r; b ‚Äù MATCH-JUGS.R<; B<  MATCH-JUGS.R>; B>   Correctness can be seen as follows  remember that jRj D jBj in each call . Once we pick r randomly from R, there will be a matching among the jugs in volume smaller than r  which are in the sets R< and B< , and likewise between the jugs larger than r  which are in R> and B> . Termination is also easy to see: since jR j < jRj in every recursive step, the size of the Ô¨Årst parameter reduces with every recursive call. It eventually must reach 0 or 1, in which case the recursion terminates. What about the running time? The analysis of the expected number of com- parisons is similar to that of the quicksort algorithm in Section 7.4.2. Let us order the jugs as r1; : : : ; rn and b1; : : : ; bn where ri < riC1 and bi < biC1 for i D 1; : : : ; n, and ri D bi. Our analysis uses indicator random variables Xij D Ifred jug ri is compared to blue jug bjg : As in quicksort, a given pair ri and bj is compared at most once. When we compare ri to every jug in B, jug ri will not be put in either R . When we compare bi to every jug in R  cid:0  frig, jug bi is not put into either B . The total number of comparisons is  X D  n cid:0 1  XiD1  n  XjDiC1  Xij :  To calculate the expected value of X, we follow the quicksort analysis to arrive at  E ≈íX ¬ç D  n cid:0 1  XiD1  n  XjDiC1  Prfri is compared to bjg :  As in the quicksort analysis, once we choose a jug rk such that ri < rk < bj , we will put ri in R , and so ri and bj will never be compared   8-20  Solutions for Chapter 8: Sorting in Linear Time  Solution to Problem 8-7  again. Let us denote Rij D fri ; : : : ; rjg. Then jugs ri and bj will be compared if and only if the Ô¨Årst jug in Rij to be chosen is either ri or rj . Still following the quicksort analysis, until a jug from Rij is chosen, the entire set Rij is together. Any jug in Rij is equally likely to be Ô¨Årst one chosen. Since jRijj D j  cid:0  i C 1, the probability of any given jug being the Ô¨Årst one chosen in Rij is 1=.j  cid:0 iC1 . The remainder of the analysis is the same as the quicksort analysis, and we arrive at the solution of O.n lg n  comparisons. Just like in quicksort, in the worst case we always choose the largest  or small- est  jug to partition the sets, which reduces the set sizes by only 1. The running time then obeys the recurrence T .n  D T .n  cid:0  1  C ‚Äö.n , and the number of comparisons we make in the worst case is T .n  D ‚Äö.n2 .  a. A≈íq¬ç must go the wrong place, because it goes where A≈íp¬ç should go. Since A≈íp¬ç is the smallest value in array A that goes to the wrong array location, A≈íp¬ç must be smaller than A≈íq¬ç.  b. From how we have deÔ¨Åned the array B, we have that if A≈íi ¬ç  A≈íj ¬ç then B≈íi ¬ç  B≈íj ¬ç. Therefore, algorithm X performs the same sequence of exchanges on array B as it does on array A. The output produced on array A is of the form : : : A≈íq¬ç : : : A≈íp¬ç : : :, and so the output produced on array B is of the form : : : B≈íq¬ç : : : B≈íp¬ç : : :, or : : : 1 : : : 0 : : :. Hence algorithm X fails to sort array B correctly.  c. The even steps perform Ô¨Åxed permutations. The odd steps sort each column by some sorting algorithm, which might not be an oblivious compare-exchange algorithm. But the result of sorting each column would be the same as if we did use an oblivious compare-exchange algorithm.  d. After step 1, each column has 0s on top and 1s on the bottom, with at most one transition between 0s and 1s, and it is a 0 ! 1 transition.  As we read the array in column-major order, all 1 ! 0 transitions occur between adjacent columns.  After step 2, therefore, each consecutive group of r=s rows, read in row-major order, has at most one transition, and again it is a 0 ! 1 transition. All 1 ! 0 transitions occur at the end of a group of r=s rows. Since there are s groups of r=s rows, there are at most s dirty rows, and the rest of the rows are clean. Step 3 moves the 0s to the top rows and the 1s to the bottom rows. The s dirty rows are somewhere in the middle.  e. The dirty area after step 3 is at most s rows high and s columns wide, and so its area is at most s2. Step 4 turns the clean 0s in the top rows into a clean area on the left, the clean 1s in the bottom rows into a clean area on the right, and the dirty area of size s2 is between the two clean areas.  f. First, we argue that if the dirty area after step 4 has size at most r=2, then steps 5‚Äì8 complete the sorting. If the dirty area has size at most r=2  half a column , then it either resides entirely in one column or it resides in the bottom   Solutions for Chapter 8: Sorting in Linear Time  8-21  half of one column and the top half of the next column. In the former case, step 5 sorts the column containing the dirty area, and steps 6‚Äì8 maintain that the array is sorted. In the latter case, step 5 cannot increase the size of the dirty area, step 6 moves the entire dirty area into the same column, step 7 sorts it, and step 8 moves it back. Second, we argue that the dirty area after step 4 has size at most r=2. But that follows immediately from the requirement that r  2s2 and the property that after step 4, the dirty area has size at most s2.  g. If s does not divide r, then after step 2, we can see up to s 0 ! 1 transitions and s  cid:0  1 1 ! 0 transitions in the rows. After step 3, we would have up to 2s  cid:0  1 dirty rows, for a dirty area size of at most 2s2 cid:0  s. To push the correctness proof through, we need 2s2  cid:0  s  r=2, or r  4s2  cid:0  2s.  h. We can reduce the number of transitions in the rows after step 2 back down to at most s by sorting every other column in reverse order in step 1. Now if we have a transition  either 1 ! 0 or 0 ! 1  between columns after step 1, then either one of the columns had all 1s or the other had all 0s, in which case we would not have a transition within one of the columns.   Lecture Notes for Chapter 9: Medians and Order Statistics  Chapter 9 overview    i th order statistic is the ith smallest element of a set of n elements.   The minimum is the Ô¨Årst order statistic  i D 1 .  The maximum is the nth order statistic  i D n .  A median is the ‚Äúhalfway point‚Äù of the set.  When n is odd, the median is unique, at i D .n C 1 =2.  When n is even, there are two medians:  The lower median, at i D n=2, and  The upper median, at i D n=2 C 1.  We mean lower median when we use the phrase ‚Äúthe median.‚Äù  The selection problem: Input: A set A of n distinct numbers and a number i, with 1  i  n. Output: The element x 2 A that is larger than exactly i  cid:0  1 other elements in A.  In other words, the ith smallest element of A.  We can easily solve the selection problem in O.n lg n  time:   Sort the numbers using an O.n lg n -time algorithm, such as heapsort or merge  sort.   Then return the ith element in the sorted array.  There are faster algorithms, however.   First, we‚Äôll look at the problem of selecting the minimum and maximum of a   Then, we‚Äôll look at a simple general selection algorithm with a time bound of  set of elements.  O.n  in the average case.   Finally, we‚Äôll look at a more complicated general selection algorithm with a  time bound of O.n  in the worst case.   9-2  Lecture Notes for Chapter 9: Medians and Order Statistics  Minimum and maximum  We can easily obtain an upper bound of n cid:0 1 comparisons for Ô¨Ånding the minimum of a set of n elements.  Examine each element in turn and keep track of the smallest one.  This is the best we can do, because each element, except the minimum, must be  compared to a smaller element at least once.  The following pseudocode Ô¨Ånds the minimum element in array A≈í1 : : n¬ç:  MINIMUM.A; n  min D A≈í1¬ç for i D 2 to n  if min > A≈íi ¬ç  min D A≈íi ¬ç  return min  The maximum can be found in exactly the same way by replacing the > with < in the above algorithm.  Simultaneous minimum and maximum  Some applications need both the minimum and maximum of a set of elements.  For example, a graphics program may need to scale a set of .x; y  data to Ô¨Åt onto a rectangular display. To do so, the program must Ô¨Årst Ô¨Ånd the minimum and maximum of each coordinate.  A simple algorithm to Ô¨Ånd the minimum and maximum is to Ô¨Ånd each one indepen- dently. There will be n  cid:0  1 comparisons for the minimum and n  cid:0  1 comparisons for the maximum, for a total of 2n  cid:0  2 comparisons. This will result in ‚Äö.n  time. In fact, at most 3bn=2c comparisons sufÔ¨Åce to Ô¨Ånd both the minimum and maxi- mum:  Maintain the minimum and maximum of elements seen so far.  Don‚Äôt compare each element to the minimum and maximum separately.  Process elements in pairs.  Compare the elements of a pair to each other.  Then compare the larger element to the maximum so far, and compare the  smaller element to the minimum so far.  This leads to only 3 comparisons for every 2 elements. Setting up the initial values for the min and max depends on whether n is odd or even.      If n is even, compare the Ô¨Årst two elements and assign the larger to max and the smaller to min. Then process the rest of the elements in pairs. If n is odd, set both min and max to the Ô¨Årst element. Then process the rest of the elements in pairs.   Lecture Notes for Chapter 9: Medians and Order Statistics  9-3  Analysis of the total number of comparisons    If n is even, we do 1 initial comparison and then 3.n cid:0  2 =2 more comparisons.  of comparisons D D D D  3.n  cid:0  2  C 1 2 3n  cid:0  6 2 C 1 3n 2  cid:0  3 C 1 3n 2  cid:0  2 :    If n is odd, we do 3.n  cid:0  1 =2 D 3bn=2c comparisons.  In either case, the maximum number of comparisons is  3bn=2c.  Selection in expected linear time  Selection of the ith smallest element of the array A can be done in ‚Äö.n  time. The function RANDOMIZED-SELECT uses RANDOMIZED-PARTITION from the quicksort algorithm in Chapter 7. RANDOMIZED-SELECT differs from quicksort because it recurses on one side of the partition only.  RANDOMIZED-SELECT .A; p; r; i    if p == r  return A≈íp¬ç  q D RANDOMIZED-PARTITION.A; p; r  k D q  cid:0  p C 1 if i == k return A≈íq¬ç     pivot value is the answer  elseif i < k  return RANDOMIZED-SELECT.A; p; q  cid:0  1; i    else return RANDOMIZED-SELECT.A; q C 1; r; i  cid:0  k  After the call to RANDOMIZED-PARTITION, the array is partitioned into two sub- arrays A≈íp : : q  cid:0  1¬ç and A≈íq C 1 : : r¬ç, along with a pivot element A≈íq¬ç.  The elements of subarray A≈íp : : q  cid:0  1¬ç are all  A≈íq¬ç.  The elements of subarray A≈íq C 1 : : r¬ç are all > A≈íq¬ç.  The pivot element is the kth element of the subarray A≈íp : : r¬ç, where k D  q  cid:0  p C 1. If the pivot element is the ith smallest element  i.e., i D k , return A≈íq¬ç.  Otherwise, recurse on the subarray containing the ith smallest element.        If i < k, this subarray is A≈íp : : q  cid:0  1¬ç, and we want the ith smallest element. If i > k, this subarray is A≈íq C 1 : : r¬ç and, since there are k elements in A≈íp : : r¬ç that precede A≈íq C 1 : : r¬ç, we want the .i  cid:0  k th smallest element of this subarray.   9-4  Lecture Notes for Chapter 9: Medians and Order Statistics  Analysis  Worst-case running time ‚Äö.n2 , because we could be extremely unlucky and always recurse on a subarray that is only 1 element smaller than the previous subarray.  Expected running time RANDOMIZED-SELECT works well on average. Because it is randomized, no par- ticular input brings out the worst-case behavior consistently. The running time of RANDOMIZED-SELECT is a random variable that we denote by T .n . We obtain an upper bound on E ≈íT .n ¬ç as follows:   RANDOMIZED-PARTITION is equally likely to return any element of A as the  pivot.   For each k such that 1  k  n, the subarray A≈íp : : q¬ç has k elements  all  pivot  with probability 1=n. [Note that we‚Äôre now considering asubarray that includesthepivot,alongwithelementslessthanthepivot.]   For k D 1; 2; : : : ; n, deÔ¨Åne indicator random variable Xk D Ifsubarray A≈íp : : q¬ç has exactly k elementsg :   Since Prfsubarray A≈íp : : q¬ç has exactly k elementsg D 1=n, Lemma 5.1 says that E ≈íXk¬ç D 1=n.  When we call RANDOMIZED-SELECT, we don‚Äôt know if it will terminate im- mediately with the correct answer, recurse on A≈íp : : q  cid:0  1¬ç, or recurse on A≈íq C 1 : : r¬ç. It depends on whether the ith smallest element is less than, equal to, or greater than the pivot element A≈íq¬ç.  To obtain an upper bound, we assume that T .n  is monotonically increasing  and that the ith smallest element is always in the larger subarray.  and Xk D 0 for all other k.   For a given call of RANDOMIZED-SELECT, Xk D 1 for exactly one value of k,  When Xk D 1, the two subarrays have sizes k  cid:0  1 and n  cid:0  k.  For a subproblem of size n, RANDOMIZED-PARTITION takes O.n  time. [Ac- tually,ittakes‚Äö.n  time,butO.n  sufÔ¨Åces,sincewe‚Äôreobtainingonlyanupper boundontheexpectedrunningtime.]   Therefore, we have the recurrence  n  n  XkD1 XkD1  T .n    Xk  .T .max.k  cid:0  1; n  cid:0  k   C O.n    D  Xk  T .max.k  cid:0  1; n  cid:0  k   C O.n  :   Taking expected values gives  E ≈íT .n ¬ç   E" n XkD1  Xk  T .max.k  cid:0  1; n  cid:0  k   C O.n    Lecture Notes for Chapter 9: Medians and Order Statistics  9-5  D  D  D  n  n  XkD1 XkD1 XkD1  n  E ≈íXk  T .max.k  cid:0  1; n  cid:0  k  ¬ç C O.n    linearity of expectation   E ≈íXk¬ç  E ≈íT .max.k  cid:0  1; n  cid:0  k  ¬ç C O.n   equation  C.24    1 n  E ≈íT .max.k  cid:0  1; n  cid:0  k  ¬ç C O.n  :  in order to apply equation  C.24 .   We rely on Xk and T .max.k  cid:0  1; n  cid:0  k   being independent random variables  Looking at the expression max.k  cid:0  1; n  cid:0  k , we have max.k  cid:0  1; n  cid:0  k  D  k  cid:0  1 if k > dn=2e ; n  cid:0  k if k  dn=2e : If n is even, each term from T .dn=2e  up to T .n  cid:0  1  appears exactly twice in the summation. If n is odd, these terms appear twice and T .bn=2c  appears once.       Either way,  E ≈íT .n ¬ç   E ≈íT .k ¬ç C O.n  :  2  n  n cid:0 1  XkDbn=2c   Solve this recurrence by substitution:  of the recurrence.   Guess that T .n   cn for some constant c that satisÔ¨Åes the initial conditions  Assume that T .n  D O.1  for n < some constant. We‚Äôll pick this constant  Also pick a constant a such that the function described by the O.n  term is  later.  bounded from above by an for all n > 0.   Using this guess and constants c and a, we have  E ≈íT .n ¬ç   2  n cid:0 1  n  2c  2c  bn=2c cid:0 1  ck C an k! C an XkD1 .bn=2c  cid:0  1 bn=2c  cid:0   XkDbn=2c n   n cid:0 1 XkD1 k  cid:0  n  .n  cid:0  1 n n  .n  cid:0  1 n  cid:0  n  n2  cid:0  n n2=4  cid:0  3n=2 C 2 2  cid:0  2  cid:0  2 C an n 3n2 n 4 C  .n=2  cid:0  2 .n=2  cid:0  1   C an  2c  2c  2  2  2  2  2  c   C an  C an  D  D    D  D   9-6  Lecture Notes for Chapter 9: Medians and Order Statistics  2  n C an  2  cid:0  an :  1 2  cid:0   D c 3n 4 C c 3cn 4 C 2 C an  c D cn  cid:0  cn 4  cid:0  cn=4  cid:0  c=2  cid:0  an  0  cn=4  cid:0  an  c=2 n.c=4  cid:0  a   c=2   To complete this proof, we choose c such that  n   n   c=2 c=4  cid:0  a :  2c  c  cid:0  4a   Thus, as long as we assume that T .n  D O.1  for n < 2c=.c cid:0  4a , we have  E ≈íT .n ¬ç D O.n .  Therefore, we can determine any order statistic in linear time on average.  Selection in worst-case linear time  We can Ô¨Ånd the ith smallest element in O.n  time in the worst case. We‚Äôll describe a procedure SELECT that does so. SELECT recursively partitions the input array.    Idea: Guarantee a good split when the array is partitioned.   Will use the deterministic procedure PARTITION, but with a small modiÔ¨Åca- tion. Instead of assuming that the last element of the subarray is the pivot, the modiÔ¨Åed PARTITION procedure is told which element to use as the pivot.  SELECT works on an array of n > 1 elements. It executes the following steps: 1. Divide the n elements into groups of 5. Get dn=5e groups: bn=5c groups with exactly 5 elements and, if 5 does not divide n, one group with the remaining n mod 5 elements.  2. Find the median of each of the dn=5e groups:   Run insertion sort on each group. Takes O.1  time per group since each  group has  5 elements.   Then just pick the median from each group, in O.1  time.  3. Find the median x of the dn=5e medians by a recursive call to SELECT.  If dn=5e is even, then follow our convention and Ô¨Ånd the lower median.  4. Using the modiÔ¨Åed version of PARTITION that takes the pivot element as input, partition the input array around x. Let x be the kth element of the array after partitioning, so that there are k  cid:0  1 elements on the low side of the partition and n  cid:0  k elements on the high side.   Lecture Notes for Chapter 9: Medians and Order Statistics  9-7  5. Now there are three possibilities:        If i D k, just return x. If i < k, return the ith smallest element on the low side of the partition by making a recursive call to SELECT. If i > k, return the .i cid:0 k th smallest element on the high side of the partition by making a recursive call to SELECT.  Start by getting a lower bound on the number of elements that are greater than the partitioning element x:  Analysis  x  [Each group is a column. Each white circle is the median of a group, as found instep2. Arrowsgofromlarger elements tosmallerelements, basedonwhatwe knowafterstep4. Elementsintheregiononthelowerrightareknowntobegreater than x.]  At least half of the medians found in step 2 are  x.  Look at the groups containing these medians that are  x. All of them con- tribute 3 elements that are > x  the median of the group and the 2 elements in the group greater than the group‚Äôs median , except for 2 of the groups: the group containing x  which has only 2 elements > x  and the group with < 5 elements.  5m  cid:0  2 groups with 3 ele-  Forget about these 2 groups. That leaves   1 2l n  ments known to be > x.   Thus, we know that at least  5m  cid:0  2  3 1 2l n  elements are > x.  3n 10  cid:0  6  Symmetrically, the number of elements that are < x is at least 3n=10  cid:0  6. Therefore, when we call SELECT recursively in step 5, it‚Äôs on  7n=10 C 6 ele- ments. Develop a recurrence for the worst-case running time of SELECT:   Steps 1, 2, and 4 each take O.n  time:   9-8  Lecture Notes for Chapter 9: Medians and Order Statistics   Step 1: making groups of 5 elements takes O.n  time.  Step 2: sorting dn=5e groups in O.1  time each.  Step 4: partitioning the n-element array around x takes O.n  time.  creasing.   Step 3 takes time T .dn=5e .  Step 5 takes time  T .7n=10 C 6 , assuming that T .n  is monotonically in-  Assume that T .n  D O.1  for small enough n. We‚Äôll use n < 140 as ‚Äúsmall  Thus, we get the recurrence  enough.‚Äù Why 140? We‚Äôll see why later.  T .n    O.1   T .dn=5e  C T .7n=10 C 6  C O.n   Solve this recurrence by substitution:  if n < 140 ; if n  140 :     Assume that c is large enough that T .n   cn for all n < 140. So we are  Pick a constant a such that the function described by the O.n  term in the  Inductive hypothesis: T .n   cn for some constant c and all n > 0. concerned only with the case n  140. recurrence is  an for all n > 0.   Substitute the inductive hypothesis in the right-hand side of the recurrence:  T .n   c dn=5e C c.7n=10 C 6  C an  cn=5 C c C 7cn=10 C 6c C an D 9cn=10 C 7c C an D cn C . cid:0 cn=10 C 7c C an  :   This last quantity is  cn if  cid:0 cn=10 C 7c C an  0 cn=10  cid:0  7c  an  cn  cid:0  70c  10an c.n  cid:0  70   10an  c  10a.n=.n  cid:0  70   :   Because we assumed that n  140, we have n=.n  cid:0  70   2.  Thus, 20a  10a.n=.n cid:0 70  , so choosing c  20a gives c  10a.n=.n cid:0 70  ,  We conclude that T .n  D O.n , so that SELECT runs in linear time in all cases.  Why 140? We could have used any integer strictly greater than 70.  which in turn gives us the condition we need to show that T .n   cn.  constant to work with.   Observe that for n > 70, the fraction n=.n  cid:0  70  decreases as n increases.  We picked n  140 so that the fraction would be  2, which is an easy  We could have picked, say, n  71, so that for all n  71, the fraction would be  71=.71  cid:0  70  D 71. Then we would have had 20a  710a, so we‚Äôd have needed to choose c  710a.   Lecture Notes for Chapter 9: Medians and Order Statistics  9-9  Notice that SELECT and RANDOMIZED-SELECT determine information about the relative order of elements only by comparing elements.   Sorting requires .n lg n  time in the comparison model.  Sorting algorithms that run in linear time need to make assumptions about their  input.  input.   Linear-time selection algorithms do not require any assumptions about their   Linear-time selection algorithms solve the selection problem without sorting  and therefore are not subject to the .n lg n  lower bound.   Solutions for Chapter 9: Medians and Order Statistics  Solution to Exercise 9.1-1  The smallest of n numbers can be found with n  cid:0  1 comparisons by conducting a tournament as follows: Compare all the numbers in pairs. Only the smaller of each pair could possibly be the smallest of all n, so the problem has been reduced to that of Ô¨Ånding the smallest of dn=2e numbers. Compare those numbers in pairs, and so on, until there‚Äôs just one number left, which is the answer. To see that this algorithm does exactly n cid:0  1 comparisons, notice that each number except the smallest loses exactly once. To show this more formally, draw a binary tree of the comparisons the algorithm does. The n numbers are the leaves, and each number that came out smaller in a comparison is the parent of the two numbers that were compared. Each non-leaf node of the tree represents a comparison, and there are n  cid:0  1 internal nodes in an n-leaf full binary tree  see Exercise  B.5-3  , so exactly n  cid:0  1 comparisons are made. In the search for the smallest number, the second smallest number must have come out smallest in every comparison made with it until it was eventually compared with the smallest. So the second smallest is among the elements that were com- pared with the smallest during the tournament. To Ô¨Ånd it, conduct another tourna- ment  as above  to Ô¨Ånd the smallest of these numbers. At most dlg ne  the height of the tree of comparisons  elements were compared with the smallest, so Ô¨Ånding the smallest of these takes dlg ne  cid:0  1 comparisons in the worst case. The total number of comparisons made in the two tournaments was n  cid:0  1 C dlg ne  cid:0  1 D n C dlg ne  cid:0  2 in the worst case.  Solution to Exercise 9.3-1 This solution is also posted publicly  For groups of 7, the algorithm still works in linear time. The number of elements greater than x  and similarly, the number less than x  is at least  4 1 7m  cid:0  2  2l n  2n 7  cid:0  8 ;   Solutions for Chapter 9: Medians and Order Statistics  9-11  and the recurrence becomes  T .n   T .dn=7e  C T .5n=7 C 8  C O.n  ; which can be shown to be O.n  by substitution, as for the groups of 5 case in the text. For groups of 3, however, the algorithm no longer works in linear time. The number of elements greater than x, and the number of elements less than x, is at least  3m  cid:0  2  2 1 2l n  and the recurrence becomes  n 3  cid:0  4 ;  T .n   T .dn=3e  C T .2n=3 C 4  C O.n  ; which does not have a linear solution. We can prove that the worst-case time for groups of 3 is .n lg n . We do so by deriving a recurrence for a particular case that takes .n lg n  time. In counting up the number of elements greater than x  and similarly, the num-  ber less than x , consider the particular case in which there are exactly l 1 2l n 3mm groups with medians  x and in which the ‚Äúleftover‚Äù group does contribute 2 elements greater than x. Then the number of elements greater than x is exactly 2l 1 3mm  cid:0  1 C 1  the  cid:0 1 discounts x‚Äôs group, as usual, and the C1 is con- 2l n tributed by x‚Äôs group  D 2dn=6e  cid:0  1, and the recursive step for elements  x has n  cid:0  .2dn=6e  cid:0  1   n  cid:0  .2.n=6 C 1   cid:0  1  D 2n=3  cid:0  1 elements. Observe also that the O.n  term in the recurrence is really ‚Äö.n , since the partitioning in step 4 takes ‚Äö.n   not just O.n   time. Thus, we get the recurrence  T .n   T .dn=3e  C T .2n=3  cid:0  1  C ‚Äö.n   T .n=3  C T .2n=3  cid:0  1  C ‚Äö.n  ; from which you can show that T .n   cn lg n by substitution. You can also see that T .n  is nonlinear by noticing that each level of the recursion tree sums to n. [Infact,anyoddgroupsize 5 worksinlineartime.]  Solution to Exercise 9.3-3 This solution is also posted publicly  A modiÔ¨Åcation to quicksort that allows it to run in O.n lg n  time in the worst case uses the deterministic PARTITION algorithm that was modiÔ¨Åed to take an element to partition around as an input parameter. SELECT takes an array A, the bounds p and r of the subarray in A, and the rank i of an order statistic, and in time linear in the size of the subarray A≈íp : : r¬ç it returns the ith smallest element in A≈íp : : r¬ç.   9-12  Solutions for Chapter 9: Medians and Order Statistics  BEST-CASE-QUICKSORT .A; p; r   if p < r  i D b.r  cid:0  p C 1 =2c x D SELECT.A; p; r; i   q D PARTITION.x  BEST-CASE-QUICKSORT.A; p; q  cid:0  1  BEST-CASE-QUICKSORT.A; q C 1; r   For an n-element array, the largest subarray that BEST-CASE-QUICKSORT re- curses on has n=2 elements. This situation occurs when n D r  cid:0  p C 1 is even; then the subarray A≈íq C 1 : : r¬ç has n=2 elements, and the subarray A≈íp : : q  cid:0  1¬ç has n=2  cid:0  1 elements. Because BEST-CASE-QUICKSORT always recurses on subarrays that are at most half the size of the original array, the recurrence for the worst-case running time is T .n   2T .n=2  C ‚Äö.n  D O.n lg n .  Solution to Exercise 9.3-5 This solution is also posted publicly  We assume that are given a procedure MEDIAN that takes as parameters an ar- ray A and subarray indices p and r, and returns the value of the median element of A≈íp : : r¬ç in O.n  time in the worst case. Given MEDIAN, here is a linear-time algorithm SELECT0 for Ô¨Ånding the ith small- est element in A≈íp : : r¬ç. This algorithm uses the deterministic PARTITION algo- rithm that was modiÔ¨Åed to take an element to partition around as an input parame- ter.  SELECT0.A; p; r; i    if p == r  return A≈íp¬ç  x D MEDIAN.A; p; r  q D PARTITION.x  k D q  cid:0  p C 1 if i == k return A≈íq¬ç  elseif i < k  return SELECT0.A; p; q  cid:0  1; i    else return SELECT0.A; q C 1; r; i  cid:0  k  Because x is the median of A≈íp : : r¬ç, each of the subarrays A≈íp : : q  cid:0  1¬ç and A≈íq C 1 : : r¬ç has at most half the number of elements of A≈íp : : r¬ç. The recurrence for the worst-case running time of SELECT0 is T .n   T .n=2  C O.n  D O.n .   Solutions for Chapter 9: Medians and Order Statistics  9-13  Solution to Exercise 9.3-8  Let‚Äôs start out by supposing that the median  the lower median, since we know we have an even number of elements  is in X. Let‚Äôs call the median value m, and let‚Äôs suppose that it‚Äôs in X ≈ík¬ç. Then k elements of X are less than or equal to m and n cid:0  k elements of X are greater than or equal to m. We know that in the two arrays combined, there must be n elements less than or equal to m and n elements greater than or equal to m, and so there must be n  cid:0  k elements of Y that are less than or equal to m and n  cid:0  .n  cid:0  k  D k elements of Y that are greater than or equal to m. Thus, we can check that X ≈ík¬ç is the lower median by checking whether Y ≈ín cid:0  k¬ç  X ≈ík¬ç  Y ≈ín  cid:0  k C 1¬ç. A boundary case occurs for k D n. Then n  cid:0  k D 0, and there is no array entry Y ≈í0¬ç; we only need to check that X ≈ín¬ç  Y ≈í1¬ç. Now, if the median is in X but is not in X ≈ík¬ç, then the above condition will not hold. If the median is in X ≈ík0¬ç, where k0 < k, then X ≈ík¬ç is above the median, and Y ≈ín  cid:0  k C 1¬ç   k, then X ≈ík¬ç is below the median, and X ≈ík¬ç < Y ≈ín  cid:0  k¬ç. Thus, we can use a binary search to determine whether there is an X ≈ík¬ç such that either k < n and Y ≈ín cid:0 k¬ç  X ≈ík¬ç  Y ≈ín cid:0 kC1¬ç or k D n and X ≈ík¬ç  Y ≈ín cid:0 kC1¬ç; if we Ô¨Ånd such an X ≈ík¬ç, then it is the median. Otherwise, we know that the median is in Y , and we use a binary search to Ô¨Ånd a Y ≈ík¬ç such that either k < n and X ≈ín  cid:0  k¬ç  Y ≈ík¬ç  X ≈ín  cid:0  k C 1¬ç or k D n and Y ≈ík¬ç  X ≈ín  cid:0  k C 1¬ç; such a Y ≈ík¬ç is the median. Since each binary search takes O.lg n  time, we spend a total of O.lg n  time. Here‚Äôs how we write the algorithm in pseudocode:  TWO-ARRAY-MEDIAN.X; Y   n D X:length median D FIND-MEDIAN.X; Y; n; 1; n  if median == NOT-FOUND     n also equals Y:length  median D FIND-MEDIAN.Y; X; n; 1; n   return median  FIND-MEDIAN.A; B; n; low; high   if low > high  return NOT-FOUND  else k D b.low C high =2c  return A≈ín¬ç  return A≈ík¬ç  if k == n and A≈ín¬ç  B≈í1¬ç elseif k < n and B≈ín  cid:0  k¬ç  A≈ík¬ç  B≈ín  cid:0  k C 1¬ç elseif A≈ík¬ç > B≈ín  cid:0  k C 1¬ç return FIND-MEDIAN.A; B; n; low; k  cid:0  1  else return FIND-MEDIAN.A; B; n; k C 1; high    9-14  Solutions for Chapter 9: Medians and Order Statistics  Solution to Exercise 9.3-9  In order to Ô¨Ånd the optimal placement for Professor Olay‚Äôs pipeline, we need only Ô¨Ånd the median s  of the y-coordinates of his oil wells, as the following proof explains.  Claim The optimal y-coordinate for Professor Olay‚Äôs east-west oil pipeline is as follows:      If n is even, then on either the oil well whose y-coordinate is the lower median or the one whose y-coordinate is the upper median, or anywhere between them. If n is odd, then on the oil well whose y-coordinate is the median.  Proof We examine various cases. In each case, we will start out with the pipeline at a particular y-coordinate and see what happens when we move it. We‚Äôll denote by s the sum of the north-south spurs with the pipeline at the starting location, and s0 will denote the sum after moving the pipeline. We start with the case in which n is even. Let us start with the pipeline somewhere on or between the two oil wells whose y-coordinates are the lower and upper me- dians. If we move the pipeline by a vertical distance d without crossing either of the median wells, then n=2 of the wells become d farther from the pipeline and n=2 become d closer, and so s0 D s C d n=2  cid:0  d n=2 D s; thus, all locations on or between the two medians are equally good. Now suppose that the pipeline goes through the oil well whose y-coordinate is the upper median. What happens when we increase the y-coordinate of the pipeline by d > 0 units, so that it moves above the oil well that achieves the upper median? All oil wells whose y-coordinates are at or below the upper median become d units farther from the pipeline, and there are at least n=2 C 1 such oil wells  the upper median, and every well at or below the lower median . There are at most n=2  cid:0  1 oil wells whose y-coordinates are above the upper median, and each of these oil wells becomes at most d units closer to the pipeline when it moves up. Thus, we have a lower bound on s0 of s0  s C d.n=2 C 1   cid:0  d.n=2  cid:0  1  D s C 2d > s. We conclude that moving the pipeline up from the oil well at the upper median increases the total spur length. A symmetric argument shows that if we start with the pipeline going through the oil well whose y-coordinate is the lower median and move it down, then the total spur length increases. We see, therefore, that when n is even, an optimal placement of the pipeline is anywhere on or between the two medians. Now we consider the case when n is odd. We start with the pipeline going through the oil well whose y-coordinate is the median, and we consider what happens when we move it up by d > 0 units. All oil wells at or below the median become d units farther from the pipeline, and there are at least .nC 1 =2 such wells  the one at the median and the .n  cid:0  1 =2 at or below the median. There are at most .n  cid:0  1 =2 oil wells above the median, and each of these becomes at most d units closer to the pipeline. We get a lower bound on s0 of s0  s C d.n C 1 =2  cid:0  d.n  cid:0  1 =2 D s C d > s, and we conclude that moving the pipeline up from the oil well at the   Solutions for Chapter 9: Medians and Order Statistics  9-15  median increases the total spur length. A symmetric argument shows that moving the pipeline down from the median also increases the total spur length, and so the  claim  optimal placement of the pipeline is on the median.  Since we know we are looking for the median, we can use the linear-time median- Ô¨Ånding algorithm.  Solution to Problem 9-1 This solution is also posted publicly  We assume that the numbers start out in an array.  a. Sort the numbers using merge sort or heapsort, which take ‚Äö.n lg n  worst-case time.  Don‚Äôt use quicksort or insertion sort, which can take ‚Äö.n2  time.  Put the i largest elements  directly accessible in the sorted array  into the output array, taking ‚Äö.i   time. Total worst-case running time: ‚Äö.n lg n C i   D ‚Äö.n lg n   because i  n .  b. Implement the priority queue as a heap. Build the heap using BUILD-HEAP, which takes ‚Äö.n  time, then call HEAP-EXTRACT-MAX i times to get the i largest elements, in ‚Äö.i lg n  worst-case time, and store them in reverse order of extraction in the output array. The worst-case extraction time is ‚Äö.i lg n  because    i extractions from a heap with O.n  elements takes i  O.lg n  D O.i lg n  time, and  half of the i extractions are from a heap with  n=2 elements, so those i=2  extractions take .i=2 .lg.n=2   D .i lg n  time in the worst case.  Total worst-case running time: ‚Äö.n C i lg n .  c. Use the SELECT algorithm of Section 9.3 to Ô¨Ånd the ith largest number in ‚Äö.n  time. Partition around that number in ‚Äö.n  time. Sort the i largest numbers in ‚Äö.i lg i   worst-case time  with merge sort or heapsort . Total worst-case running time: ‚Äö.n C i lg i  .  Note that method  c  is always asymptotically at least as good as the other two methods, and that method  b  is asymptotically at least as good as  a .  Com- paring  c  to  b  is easy, but it is less obvious how to compare  c  and  b  to  a .  c  and  b  are asymptotically at least as good as  a  because n, i lg i, and i lg n are all O.n lg n . The sum of two things that are O.n lg n  is also O.n lg n .    9-16  Solutions for Chapter 9: Medians and Order Statistics  Solution to Problem 9-2  a. The median x of the elements x1; x2; : : : ; xn, is an element x D xk satisfy- ing jfxi W 1  i  n and xi   xgj  n=2. If each element xi is assigned a weight wi D 1=n, then we get Xxi <x  n  1  1  1  D  wi D Xxi <x n  Xxi <x n  jfxi W 1  i  n and xi < xgj 1 n  1 ;  n  1  2  1  1  1  n  D  wi D Xxi >x n  Xxi >x n  jfxi W 1  i  n and xi > xgj 1 n  1 ;  n  1  2  D  D  D  D  2  2  and  Xxi >x  which proves that x is also the weighted median of x1; x2; : : : ; xn with weights wi D 1=n, for i D 1; 2; : : : ; n.  b. We Ô¨Årst sort the n elements into increasing order by xi values. Then we scan the array of sorted xi ‚Äôs, starting with the smallest element and accumulating weights as we scan, until the total exceeds 1=2. The last element, say xk, whose weight caused the total to exceed 1=2, is the weighted median. Notice that the total weight of all elements smaller than xk is less than 1=2, because xk was the Ô¨Årst element that caused the total weight to exceed 1=2. Similarly, the total weight of all elements larger than xk is also less than 1=2, because the total weight of all the other elements exceeds 1=2. The sorting phase can be done in O.n lg n  worst-case time  using merge sort or heapsort , and the scanning phase takes O.n  time. The total running time in the worst case, therefore, is O.n lg n .  c. We Ô¨Ånd the weighted median in ‚Äö.n  worst-case time using the ‚Äö.n  worst-  Although the Ô¨Årst paragraph of the case median algorithm in Section 9.3. section only claims an O.n  upper bound, it is easy to see that the more precise   Solutions for Chapter 9: Medians and Order Statistics  9-17  running time of ‚Äö.n  applies as well, since steps 1, 2, and 4 of SELECT actually take ‚Äö.n  time.  The weighted-median algorithm works as follows. If n  2, we just return the brute-force solution. Otherwise, we proceed as follows. We Ô¨Ånd the actual median xk of the n elements and then partition around it. We then compute the total weights of the two halves. If the weights of the two halves are each strictly less than 1=2, then the weighted median is xk. Otherwise, the weighted median should be in the half with total weight exceeding 1=2. The total weight of the ‚Äúlight‚Äù half is lumped into the weight of xk, and the search continues within the half that weighs more than 1=2. Here‚Äôs pseudocode, which takes as input a set X D fx1; x2; : : : ; xng: WEIGHTED-MEDIAN .X    if n == 1  return x1  elseif n == 2  if w1  w2 return x1 else return x2  partition the set X around xk  else Ô¨Ånd the median xk of X D fx1; x2; : : : ; xng wi and WG DPxi >xk  compute WL DPxi <xk  if WL < 1=2 and WG < 1=2  wi  return xk  elseif WL > 1=2  wk D wk C WG X0 D fxi 2 X W xi  xkg return WEIGHTED-MEDIAN.X0   else wk D wk C WL  X0 D fxi 2 X W xi  xkg return WEIGHTED-MEDIAN.X0   The recurrence for the worst-case running time of WEIGHTED-MEDIAN is T .n  D T .n=2C 1 C ‚Äö.n , since there is at most one recursive call on half the number of elements, plus the median element xk, and all the work preceding the recursive call takes ‚Äö.n  time. The solution of the recurrence is T .n  D ‚Äö.n . d. Let the n points be denoted by their coordinates x1; x2; : : : ; xn, let the corre- sponding weights be w1; w2; : : : ; wn, and let x D xk be the weighted median. For any point p, let f .p  DPn iD1 wi jp  cid:0  xij; we want to Ô¨Ånd a point p such that f .p  is minimum. Let y be any point  real number  other than x. We show the optimality of the weighted median x by showing that f .y  cid:0  f .x   0. We examine separately the cases in which y > x and x > y. For any x and y, we have   9-18  Solutions for Chapter 9: Medians and Order Statistics  f .y   cid:0  f .x  D  wi jy  cid:0  xij  cid:0   wi jx  cid:0  xij  n  XiD1  D  wi .jy  cid:0  xij  cid:0  jx  cid:0  xij  :  n  n  XiD1 XiD1  n  When y > x, we bound the quantity jy  cid:0  xij  cid:0  jx  cid:0  xij from below by exam- ining three cases: 1. x < y  xi : Here, jx  cid:0  yj C jy  cid:0  xij D jx  cid:0  xij and jx  cid:0  yj D y  cid:0  x, 2. x < xi  y: Here, jy  cid:0  xij  0 and jxi  cid:0  xj  y  cid:0  x, which imply that 3. xi  x < y: Here, jx  cid:0  xij C jy  cid:0  xj D jy  cid:0  xij and jy  cid:0  xj D y  cid:0  x,  which imply that jy  cid:0  xij  cid:0  jx  cid:0  xij D  cid:0 jx  cid:0  yj D x  cid:0  y. jy  cid:0  xij  cid:0  jx  cid:0  xij   cid:0 .y  cid:0  x  D x  cid:0  y. which imply that jy  cid:0  xij  cid:0  jx  cid:0  xij D jy  cid:0  xj D y  cid:0  x.  wi .y  cid:0  x  wi! :  Separating out the Ô¨Årst two cases, in which x < xi, from the third case, in which x  xi, we get XiD1 f .y   cid:0  f .x  D wi .jy  cid:0  xij  cid:0  jx  cid:0  xij   Xx<xi wi .x  cid:0  y  C Xxxi D .y  cid:0  x  Xxxi wi  cid:0  Xx<xi The property that Pxi <x wi < 1=2 implies that Pxxi wi  1=2. This fact, combined with y  cid:0  x > 0 andPx<xi wi  1=2, yields that f .y   cid:0  f .x   0. When x > y, we again bound the quantity jy  cid:0  xij  cid:0  jx  cid:0  xij from below by examining three cases: 1. xi  y < x: Here, jy  cid:0  xij C jx  cid:0  yj D jx  cid:0  xij and jx  cid:0  yj D x  cid:0  y, 2. y  xi < x: Here, jy  cid:0  xij  0 and jx  cid:0  xij  x  cid:0  y, which imply that 3. y < x  xi. Here, jx  cid:0  yj C jx  cid:0  xij D jy  cid:0  xij and jx  cid:0  yj D x  cid:0  y,  which imply that jy  cid:0  xij  cid:0  jx  cid:0  xij D  cid:0 jx  cid:0  yj D y  cid:0  x. jy  cid:0  xij  cid:0  jx  cid:0  xij   cid:0 .x  cid:0  y  D y  cid:0  x. which imply that jy  cid:0  xij  cid:0  jx  cid:0  xij D jx  cid:0  yj D x  cid:0  y.  Separating out the Ô¨Årst two cases, in which x > xi, from the third case, in which x  xi, we get XiD1 f .y   cid:0  f .x  D  Xx>xi D .x  cid:0  y  Xxxi The property that Pxi >x wi  1=2 implies that Pxxi combined with x  cid:0  y > 0 andPx>xi  wi .jy  cid:0  xij  cid:0  jx  cid:0  xij  wi .y  cid:0  x  C Xxxi wi  cid:0  Xx>xi wi > 1=2. This fact, wi   0.  wi .x  cid:0  y  wi! :  n   Solutions for Chapter 9: Medians and Order Statistics  9-19  n  XiD1  wi .jx  cid:0  xij C jy  cid:0  yij  :  e. We are given n 2-dimensional points p1; p2; : : : ; pn, where each pi is a pair of real numbers pi D .xi ; yi  , and positive weights w1; w2; : : : ; wn. The goal is to Ô¨Ånd a point p D .x; y  that minimizes the sum f .x; y  D We can express the cost function of the two variables, f .x; y , as the sum of two functions of one variable each: f .x; y  D g.x  C h.y , where g.x  D iD1 wi jx  cid:0  xij, and h.y  D Pn Pn iD1 wi jy  cid:0  yij. The goal of Ô¨Ånding a point p D .x; y  that minimizes the value of f .x; y  can be achieved by treating each dimension independently, because g does not depend on y and h does not depend on x. Thus, min x;y  x;y  f .x; y  D min D min D min D min  .g.x  C h.y   x min x g.x  C min g.x  C min  .g.x  C h.y   h.y   h.y  :  y  y  y  x  Consequently, Ô¨Ånding the best location in 2 dimensions can be done by Ô¨Ånding the weighted median xk of the x-coordinates and then Ô¨Ånding the weighted median yj of the y-coordinates. The point .xk; yj   is an optimal solution for the 2-dimensional post-ofÔ¨Åce location problem.  a. Our algorithm relies on a particular property of SELECT: that not only does it return the ith smallest element, but that it also partitions the input array so that the Ô¨Årst i positions contain the i smallest elements  though not necessarily in sorted order . To see that SELECT has this property, observe that there are only two ways in which returns a value: when n D 1, and when immediately after partitioning in step 4, it Ô¨Ånds that there are exactly i elements on the low side of the partition. Taking the hint from the book, here is our modiÔ¨Åed algorithm to select the ith smallest element of n elements. Whenever it is called with i  n=2, it just calls SELECT and returns its result; in this case, Ui .n  D T .n . When i < n=2, our modiÔ¨Åed algorithm works as follows. Assume that the input is in a subarray A≈íp C 1 : : p C n¬ç, and let m D bn=2c. In the initial call, p D 1. 1. Divide the input as follows. If n is even, divide the input into two parts: A≈íp C 1 : : p C m¬ç and A≈íp C m C 1 : : p C n¬ç. If n is odd, divide the input into three parts: A≈íp C 1 : : p C m¬ç, A≈íp C mC 1 : : p C n cid:0  1¬ç, and A≈íp C n¬ç as a leftover piece. 2. Compare A≈ípC i ¬ç and A≈ípC i C m¬ç for i D 1; 2; : : : ; m, putting the smaller  of the the two elements into A≈íp C i C m¬ç and the larger into A≈íp C i ¬ç.  Solution to Problem 9-3   9-20  Solutions for Chapter 9: Medians and Order Statistics  3. Recursively Ô¨Ånd the ith smallest element in A≈íp C m C 1 : : p C n¬ç, but with an additional action performed by the partitioning procedure: whenever it exchanges A≈íj ¬ç and A≈ík¬ç  where p C m C 1  j; k  p C 2m , it also exchanges A≈íj  cid:0  m¬ç and A≈ík  cid:0  m¬ç. The idea is that after recursively Ô¨Ånding the ith smallest element in A≈íp C m C 1 : : p C n¬ç, the subarray A≈íp C m C 1 : : p C mC i ¬ç contains the i smallest elements that had been in A≈ípC mC 1 : : pC n¬ç and the subarray A≈ípC 1 : : pC i ¬ç contains their larger counterparts, as found in step 1. The ith smallest element of A≈ípC1 : : pCn¬ç must be either one of the i smallest, as placed into A≈ípCmC1 : : pCmCi ¬ç, or it must be one of the larger counterparts, as placed into A≈íp C 1 : : p C i ¬ç. 4. Collect the subarrays A≈íp C 1 : : p C i ¬ç and A≈íp C m C 1 : : p C m C i ¬ç into a single array B≈í1 : : 2i ¬ç, call SELECT to Ô¨Ånd the ith smallest element of B, and return the result of this call to SELECT.  The number of comparisons in each step is as follows:  1. No comparisons. 2. m D bn=2c comparisons. 3. Since we recurse on A≈íp C m C 1 : : p C n¬ç, which has dn=2e elements, the 4. Since we call SELECT on an array with 2i elements, the number of compar-  number of comparisons is Ui .dn=2e . isons is T .2i  .  Thus, when i < n=2, the total number of comparisons is bn=2cC Ui .dn=2e C T .2i  .  b. We show by substitution that if i < n=2, then Ui .n  D n C O.T .2i   lg.n= i   . In particular, we show that Ui .n   n C cT .2i   lg.n= i    cid:0  d.lg lg n T .2i   D n C cT .2i   lg n  cid:0  cT .2i   lg i  cid:0  d.lg lg n T .2i   for some positive constant c, some positive constant d to be chosen later, and n  4. We have Ui .n  D bn=2c C Ui .dn=2e  C T .2i     bn=2c C dn=2e C cT .2i   lg dn=2e  cid:0  cT .2i   lg i   cid:0  d.lg lgdn=2e T .2i    D n C cT .2i   lgdn=2e  cid:0  cT .2i   lg i  cid:0  d.lg lg dn=2e T .2i    n C cT .2i   lg.n=2 C 1   cid:0  cT .2i   lg i  cid:0  d.lg lg.n=2  T .2i   D n C cT .2i   lg.n=2 C 1   cid:0  cT .2i   lg i  cid:0  d.lg.lg n  cid:0  1  T .2i    n C cT .2i   lg n  cid:0  cT .2i   lg i  cid:0  d.lg lg n T .2i    if cT .2i   lg.n=2C 1  cid:0  d.lg.lg n cid:0  1  T .2i    cT .2i   lg n cid:0  d.lg lg n T .2i  . Simple algebraic manipulations gives the following sequence of equivalent con- ditions: cT .2i   lg.n=2 C 1   cid:0  d.lg.lg n  cid:0  1  T .2i    cT .2i   lg n  cid:0  d.lg lg n T .2i   c lg.n=2 C 1   cid:0  d.lg.lg n  cid:0  1    c lg n  cid:0  d.lg lg n  c.lg.n=2 C 1   cid:0  lg n   d.lg.lg n  cid:0  1   cid:0  lg lg n  clg clg 1 2 C    d lg n  d lg  lg n  cid:0  1 lg n lg n  cid:0  1 lg n  n=2 C 1  n  1   Solutions for Chapter 9: Medians and Order Statistics  9-21  Observe that 1=2C1=n decreases as n increases, but .lg n cid:0 1 = lg n increases as n increases. When n D 4, we have 1=2C1=n D 3=4 and .lg n cid:0 1 = lg n D 1=2. Thus, we just need to choose d such that c lg.3=4   d lg.1=2  or, equivalently, c lg.3=4    cid:0 d . Multiplying both sides by  cid:0 1, we get d   cid:0 c lg.3=4  D c lg.4=3 . Thus, any value of d that is at most c lg.4=3  sufÔ¨Åces.  c. When i is a constant, T .2i   D O.1  and lg.n= i   D lg n  cid:0  lg i D O.lg n .  Thus, when i is a constant less than n=2, we have that Ui .n  D n C O.T .2i   lg.n= i    D n C O.O.1   O.lg n   D n C O.lg n  :  d. Suppose that i D n=k for k  2. Then i  n=2. If k > 2, then i < n=2, and  we have Ui .n  D n C O.T .2i   lg.n= i     D n C O.T .2n=k  lg.n=.n=k   D n C O.T .2n=k  lg k  :  If k D 2, then n D 2i and lg k D 1. We have Ui .n  D T .n   D n C .T .n   cid:0  n   n C .T .2i    cid:0  n  D n C .T .2n=k   cid:0  n  D n C .T .2n=k  lg k  cid:0  n  D n C O.T .2n=k  lg k  :  Solution to Problem 9-4  a. As in the quicksort analysis, elements ¬¥i and ¬¥j will not be compared with each other if any element in f¬¥iC1; ¬¥iC2; : : : ; ¬¥j cid:0 1g is chosen as a pivot element before either ¬¥i or ¬¥j , because ¬¥i and ¬¥j would then lie in separate partitions. There can be another reason that ¬¥i and ¬¥j might not be compared, however. Suppose that k < i, so that ¬¥k < ¬¥i , and suppose further that the element chosen as the pivot is ¬¥l, where k  l < i. In this case, because k  l, the recursion won‚Äôt consider elements indexed higher than l. Therefore, the recursion will never look at ¬¥i or ¬¥j , and they will never be compared with each other. Similarly, if j < k and the pivot element ¬¥l is such that j < l  k, then the recursion won‚Äôt consider elements indexed less than l, and again ¬¥i and ¬¥j will never be compared with each other. The Ô¨Ånal case is when i  k  j  but disallowing i D j  , so that ¬¥i  ¬¥k  ¬¥j ; in this case, we have the same analysis as for quicksort: ¬¥i and ¬¥j are compared with each other only if one of them is chosen as the pivot element. Getting back to the case in which k < i, it is again true that ¬¥i and ¬¥j are compared with each other only if one of them is chosen as the pivot element. As we know, they won‚Äôt be compared with each other if the pivot element is   9-22  Solutions for Chapter 9: Medians and Order Statistics  between them, and we argued above that they won‚Äôt be compared with each other if the pivot element is ¬¥l for l < i. Similarly, when j < k, elements ¬¥i and ¬¥j are compared with each other only if one of them is chosen as the pivot element. Now we need to compute the probability that ¬¥i and ¬¥j are compared with each other. Let Zij k be the set of elements that includes ¬¥i ; : : : ; ¬¥j , along with ¬¥k; : : : ; ¬¥i cid:0 1 if k < i or ¬¥jC1; : : : ; ¬¥k if j < k. In other words,  Zij k D¬Ä f¬¥i ; ¬¥iC1; : : : ; ¬¥jg  f¬¥k; ¬¥kC1; : : : ; ¬¥jg f¬¥i ; ¬¥iC1; : : : ; ¬¥kg  if i  k  j ; if k < i ; if j < k :  With this deÔ¨Ånition of Zij k, we have that jZij kj D max.j  cid:0  i C 1; j  cid:0  k C 1; k  cid:0  i C 1  : As in the quicksort analysis, we observe that until an element from Zij k is chosen as the pivot, the whole set Zij k is together in the same partition, and so each element of Zij k is equally likely to be the Ô¨Årst one chosen as the pivot. Letting C be the event that ¬¥i is compared with ¬¥j when Ô¨Ånding ¬¥k sometime during the execution of the algorithm, we have that E ≈íXij k¬ç D PrfCg  D Prf¬¥i or ¬¥j is the Ô¨Årst pivot chosen from Zij kg D Prf¬¥i is the Ô¨Årst pivot chosen from Zij kg  1  1  C Prf¬¥j is the Ô¨Årst pivot chosen from Zij kg jZij kj C max.j  cid:0  i C 1; j  cid:0  k C 1; k  cid:0  i C 1   jZij kj  2  :  D  D  b. Adding up all the possible pairs that might be compared gives  and so, by linearity of expectation, we have  n cid:0 1  n  n  Xij k ;  Xk D  XiD1  XjDiC1 E ≈íXk¬ç D E"n cid:0 1 XiD1 XjDiC1 XiD1 XjDiC1 XiD1 XjDiC1  n cid:0 1  n cid:0 1  D  D  n  n  Xij k  E ≈íXij k¬ç  2  max.j  cid:0  i C 1; j  cid:0  k C 1; k  cid:0  i C 1   :  We break this sum into the same three cases as before: i  k  j , k < i, and j < k. With k Ô¨Åxed, we vary i and j . We get an inequality because we cannot   Solutions for Chapter 9: Medians and Order Statistics  9-23  1  j  cid:0  k C 1  n  n  1  have i D k D j , but our summation will allow it: E ≈íXk¬ç  2  k j cid:0 1 XiDkC1 XiD1 XjDkC1 k  cid:0  i C 1! XjDkC1  j  cid:0  i C 1 C XiD1 XjDiC1 j  cid:0  i C 1 C  D 2  k XiD1  XjDk C XjDk  k cid:0 1  k cid:0 2  1  1  n  n  k cid:0 2  XiD1  k  cid:0  i  cid:0  1  j  cid:0  k  cid:0  1 j  cid:0  k C 1 C  k  cid:0  i C 1! : c. First, let‚Äôs focus on the latter two summations. Each one sums fractions that are strictly less than 1. The middle summation has n  cid:0  k terms, and the right-hand summation has k cid:0 2 terms, and so the latter two summations sum to less than n. Now we look at the Ô¨Årst summation. Let m D j  cid:0  i. There is only one way for m to equal 0: if i D k D j . There are only two ways for m to equal 1: if i D k  cid:0  1 and j D k, or if i D k and j D k C 1. There are only three ways for m to equal 2: if i D k  cid:0  2 and j D k, if i D k  cid:0  1 and j D k C 1, or if i D k and j D k C 2. Continuing on, we see that there are at most m C 1 ways for j  cid:0  i to equal m. Since j  cid:0  i  n  cid:0  1, we can rewrite the Ô¨Årst summation as n cid:0 1 XmD0 Thus, we have E ≈íXk¬ç < 2.n C n   m C 1 m C 1 D n :  D 4n :  d. To show that RANDOMIZED-SELECT runs in expected time O.n , we adapt Lemma 7.1 for RANDOMIZED-SELECT. The adaptation is trivial: just re- place the variable X in the lemma statement by the random variable Xk that we just analyzed. Thus, the expected running time of RANDOMIZED-SELECT is O.n C Xk  D O.n .   Lecture Notes for Chapter 11: Hash Tables  Chapter 11 overview  Many applications require a dynamic set that supports only the dictionary opera- tions INSERT, SEARCH, and DELETE. Example: a symbol table in a compiler. A hash table is effective for implementing a dictionary.   The expected time to search for an element in a hash table is O.1 , under some  reasonable assumptions.   Worst-case search time is ‚Äö.n , however.  A hash table is a generalization of an ordinary array.   With an ordinary array, we store the element whose key is k in position k of the  array.   Given a key k, we Ô¨Ånd the element whose key is k by just looking in the kth  position of the array. This is called direct addressing.   Direct addressing is applicable when we can afford to allocate an array with  one position for every possible key.  We use a hash table when we do not want to  or cannot  allocate an array with one position per possible key.   Use a hash table when the number of keys actually stored is small relative to  the number of possible keys.   A hash table is an array, but it typically uses a size proportional to the number  of keys to be stored  rather than the number of possible keys .   Given a key k, don‚Äôt just use k as the index into the array.    Instead, compute a function of k, and use that value to index into the array. We call this function a hash function.  Issues that we‚Äôll explore in hash tables:   How to compute hash functions. We‚Äôll look at the multiplication and division  methods.   What to do when the hash function maps multiple keys to the same table entry.  We‚Äôll look at chaining and open addressing.   11-2  Lecture Notes for Chapter 11: Hash Tables  Direct-address tables  isn‚Äôt too large.  Scenario  Maintain a dynamic set.  Each element has a key drawn from a universe U D f0; 1; : : : ; m  cid:0  1g where m  No two elements have the same key. Represent by a direct-address table, or array, T ≈í0 : : : m  cid:0  1¬ç:  Each slot, or position, corresponds to a key in U .    If there‚Äôs an element x with key k, then T ≈ík¬ç contains a pointer to x.   Otherwise, T ≈ík¬ç is empty, represented by NIL.  U   universe of keys  6  0  1  9  4  K   actual keys   2  5  7  3      8  T  key  satellite data  0  1  2  3  4  5  6  7  8  9  2 3  5  8  Dictionary operations are trivial and take O.1  time each:  DIRECT-ADDRESS-SEARCH .T; k   return T ≈ík¬ç  DIRECT-ADDRESS-INSERT.T; x  T ≈íkey≈íx¬ç¬ç D x DIRECT-ADDRESS-DELETE .T; x  T ≈íkey≈íx¬ç¬ç D NIL  Hash tables  The problem with direct addressing is if the universe U is large, storing a table of size jUj may be impractical or impossible. Often, the set K of keys actually stored is small, compared to U , so that most of the space allocated for T is wasted.   Lecture Notes for Chapter 11: Hash Tables  11-3   When K is much smaller than U , a hash table requires much less space than a  direct-address table.   Can reduce storage requirements to ‚Äö.jKj .  Can still get O.1  search time, but in the average case, not the worst case.  Idea Instead of storing an element with key k in slot k, use a function h and store the element in slot h.k .   We call h a hash function.  h W U ! f0; 1; : : : ; m  cid:0  1g, so that h.k  is a legal slot number in T .  We say that k hashes to slot h.k .  Collisions When two or more keys hash to the same slot.  Can happen when there are more possible keys than slots  jUj > m .  For a given set K of keys with jKj  m, may or may not happen. DeÔ¨Ånitely  Therefore, must be prepared to handle collisions in all cases.  Use two methods: chaining and open addressing.  Chaining is usually better than open addressing. We‚Äôll examine both.  happens if jKj > m.  Collision resolution by chaining  Put all elements that hash to the same slot into a linked list.  T  U   universe of keys   K   actual keys   k5     k4  k2  k7    k8    k3  k1  k6  k1  k5  k3 k8  k4  k2  k6  k7  [ThisÔ¨Ågureshowssingly linked lists. Ifwewanttodelete elements, it‚Äôsbetter to usedoublylinkedlists.]   Slot j contains a pointer to the head of the list of all stored elements that hash  to j [ortothesentinelifusingacircular,doublylinkedlistwithasentinel] , If there are no such elements, slot j contains NIL.     11-4  Lecture Notes for Chapter 11: Hash Tables  How to implement dictionary operations with chaining:    Insertion:  CHAINED-HASH-INSERT.T; x   insert x at the head of list T ≈íh.key≈íx¬ç ¬ç   Worst-case running time is O.1 .  Assumes that the element being inserted isn‚Äôt already in the list.    It would take an additional search to check if it was already inserted.   Search:  CHAINED-HASH-SEARCH.T; k   search for an element with key k in list T ≈íh.k ¬ç  Running time is proportional to the length of the list of elements in slot h.k .   Deletion:  CHAINED-HASH-DELETE .T; x   delete x from the list T ≈íh.key≈íx¬ç ¬ç   Given pointer x to the element to delete, so no search is needed to Ô¨Ånd this  element.   Worst-case running time is O.1  time if the lists are doubly linked.    If the lists are singly linked, then deletion takes as long as searching, be- cause we must Ô¨Ånd x‚Äôs predecessor in its list in order to correctly update next pointers.  Analysis of hashing with chaining  Given a key, how long does it take to Ô¨Ånd an element with that key, or to determine that there is no element with that key?  Analysis is in terms of the load factor Àõ D n=m:   n D  of elements in the table.  m D  of slots in the table D  of  possibly empty  linked lists.  Load factor is average number of elements per linked list.  Can have Àõ   1.   Worst case is when all n keys hash to the same slot   get a single list of length n  Average case depends on how well the hash function distributes the keys among    worst-case time to search is ‚Äö.n , plus time to compute hash function. the slots.  We focus on average-case performance of hashing with chaining.   Assume simple uniform hashing: any given element is equally likely to hash  into any of the m slots.   Lecture Notes for Chapter 11: Hash Tables  11-5  n D n0 C n1 C  C nm cid:0 1.   For j D 0; 1; : : : ; m  cid:0  1, denote the length of list T ≈íj ¬ç by nj . Then  Average value of nj is E ≈ínj ¬ç D Àõ D n=m.  Assume that we can compute the hash function in O.1  time, so that the time required to search for the element with key k depends on the length nh.k  of the list T ≈íh.k ¬ç.  We consider two cases:      If the hash table contains no element with key k, then the search is unsuccessful. If the hash table does contain an element with key k, then the search is success- ful.  [Inthetheoremstatementsthatfollow,weomittheassumptionsthatwe‚Äôreresolv- ingcollisionsbychainingandthatsimpleuniformhashingapplies.]  Unsuccessful search Theorem An unsuccessful search takes expected time ‚Äö.1 C Àõ . Proof Simple uniform hashing   any key not already in the table is equally likely to hash to any of the m slots. To search unsuccessfully for any key k, need to search to the end of the list T ≈íh.k ¬ç. This list has expected length E ≈ính.k ¬ç D Àõ. Therefore, the expected number of elements examined in an unsuccessful search is Àõ. Adding in the time to compute the hash function, the total time required is ‚Äö.1 C Àõ .  Successful search  The expected time for a successful search is also ‚Äö.1 C Àõ .  The circumstances are slightly different from an unsuccessful search.  The probability that each list is searched is proportional to the number of ele-  ments it contains.  Theorem A successful search takes expected time ‚Äö.1 C Àõ . Proof Assume that the element x being searched for is equally likely to be any of the n elements stored in the table. The number of elements examined during a successful search for x is 1 more than the number of elements that appear before x in x‚Äôs list. These are the elements inserted after x was inserted  because we insert at the head of the list . So we need to Ô¨Ånd the average, over the n elements x in the table, of how many elements were inserted into x‚Äôs list after x was inserted. For i D 1; 2; : : : ; n, let xi be the ith element inserted into the table, and let ki D key≈íxi ¬ç.   11-6  Lecture Notes for Chapter 11: Hash Tables  For all i and j , deÔ¨Åne indicator random variable Xij D Ifh.ki   D h.kj  g. Simple uniform hashing   Prfh.ki   D h.kj  g D 1=m   E ≈íXij ¬ç D 1=m  by Lemma 5.1 . Expected number of elements examined in a successful search is  E" 1  n  n  XiD1 1 C  n  n  n  n  n  n  1  1  Xij! XjDiC1 XiD1 1 C E ≈íXij ¬ç!  linearity of expectation  XjDiC1 m! XiD1 1 C XjDiC1 XiD1 .n  cid:0  i   nm  n i! XiD1 XiD1 n  cid:0    equation  A.1   nmn2  cid:0  n.n C 1  n  cid:0  1 2m Àõ 2  cid:0   nm  2n  Àõ  1  1  2  :  n  1  n  1  n  D  D  D 1 C  D 1 C  D 1 C  D 1 C D 1 C  Adding in the time for computing the hash function, we get that the expected total time for a successful search is ‚Äö.2 C Àõ=2  cid:0  Àõ=2n  D ‚Äö.1 C Àõ .  Alternative analysis, using indicator random variables even more  For each slot l and for each pair of keys ki and kj , deÔ¨Åne the indicator random variable Xij l D Ifthe search is for xi, h.ki   D l, and h.kj   D lg. Xij l D 1 when keys ki and kj collide at slot l and when we are searching for xi . Simple uniform hashing   Prfh.ki   D lg D 1=m and Prfh.kj   D lg D 1=m. Also have Prfthe search is for xig D 1=n. These events are all independent   PrfXij l D 1g D 1=nm2   E ≈íXij l ¬ç D 1=nm2  by Lemma 5.1 . DeÔ¨Åne, for each element xj , the indicator random variable Yj D Ifxj appears in a list prior to the element being searched forg : Yj D 1 if and only if there is some slot l that has both elements xi and xj in its list, and also i < j  so that xi appears after xj in the list . Therefore,  Yj D  j cid:0 1  XiD1  m cid:0 1  XlD0  Xij l :   Lecture Notes for Chapter 11: Hash Tables  11-7  n  n  j cid:0 1  j cid:0 1  m cid:0 1  m cid:0 1  E ≈íXij l ¬ç  D 1 C   linearity of expectation   Xij l  linearity of expectation   One Ô¨Ånal random variable: Z, which counts how many elements appear in the list prior to the element being searched for: Z DPn jD1 Yj . We must count the element being searched for as well as all those preceding it in its list   compute E ≈íZ C 1¬ç: E ≈íZ C 1¬ç D E"1 C Yj XjD1 D 1 C E" n XjD1 XiD1 XlD0 XjD1 XiD1 XlD0 XjD1 XiD1 XlD0 D 1 C 2!  m  D 1 C n n.n  cid:0  1  D 1 C 2 n  cid:0  1 D 1 C 2m n 2m  cid:0  D 1 C Àõ Àõ 2  cid:0  D 1 C  nm2  nm2  m cid:0 1  j cid:0 1  nm  2m  2n  1  1  1  1    :  n  Adding in the time for computing the hash function, we get that the expected total time for a successful search is ‚Äö.2 C Àõ=2  cid:0  Àõ=2n  D ‚Äö.1 C Àõ .  Interpretation If n D O.m , then Àõ D n=m D O.m =m D O.1 , which means that searching takes constant time on average. Since insertion takes O.1  worst-case time and deletion takes O.1  worst-case time when the lists are doubly linked, all dictionary operations take O.1  time on average.  Hash functions  We discuss some issues regarding hash-function design and present schemes for hash function creation.  What makes a good hash function?    Ideally, the hash function satisÔ¨Åes the assumption of simple uniform hashing.   11-8  Lecture Notes for Chapter 11: Hash Tables    In practice, it‚Äôs not possible to satisfy this assumption, since we don‚Äôt know in advance the probability distribution that keys are drawn from, and the keys may not be drawn independently.   Often use heuristics, based on the domain of the keys, to create a hash function  that performs well.  Keys as natural numbers   Hash functions assume that the keys are natural numbers.  When they‚Äôre not, have to interpret them as natural numbers.  Example: Interpret a character string as an integer expressed in some radix  notation. Suppose the string is CLRS:  ASCII values: C D 67, L D 76, R D 82, S D 83.  There are 128 basic ASCII values.  So interpret CLRS as .67  1283  C .76  1282  C .82  1281  C .83  1280  D  141,764,947.  Division method  h.k  D k mod m : Example: m D 20 and k D 91   h.k  D 11. Advantage: Fast, since requires just one division operation. Disadvantage: Have to avoid certain values of m:   Powers of 2 are bad. If m D 2p for integer p, then h.k  is just the least signiÔ¨Åcant p bits of k. If k is a character string interpreted in radix 2p  as in CLRS example , then m D 2p  cid:0  1 is bad: permuting characters in a string does not change its hash value  Exercise 11.3-3 .    Good choice for m: A prime not too close to an exact power of 2.  Multiplication method  1. Choose constant A in the range 0 < A < 1. 2. Multiply key k by A. 3. Extract the fractional part of kA. 4. Multiply the fractional part by m. 5. Take the Ô¨Çoor of the result. Put another way, h.k  D bm .k A mod 1 c, where k A mod 1 D kA  cid:0  bkAc D fractional part of kA. Disadvantage: Slower than division method. Advantage: Value of m is not critical.   Lecture Notes for Chapter 11: Hash Tables  11-9   Relatively  easy implementation:  Choose m D 2p for some integer p.  Let the word size of the machine be w bits.  Assume that k Ô¨Åts into a single word.  k takes w bits.   Let s be an integer in the range 0 < s < 2w.  s takes w bits.   Restrict A to be of the form s=2w .  w bits  k  s D A  2w  r0 extract p bits  r1  binary point  h.k    Multiply k by s.  Since we‚Äôre multiplying two w-bit words, the result is 2w bits, r12w Cr0, where r1 is the high-order word of the product and r0 is the low-order word. r1 holds the integer part of kA  bkAc  and r0 holds the fractional part of kA  k A mod 1 D kA  cid:0  bkAc . Think of the ‚Äúbinary point‚Äù  analog of decimal point, but for binary representation  as being between r1 and r0. Since we don‚Äôt care about the integer part of kA, we can forget about r1 and just use r0.     Since we want bm .k A mod 1 c, we could get that value by shifting r0 to the left by p D lg m bits and then taking the p bits that were shifted to the left of the binary point.  We don‚Äôt need to shift. The p bits that would have been shifted to the left of the binary point are the p most signiÔ¨Åcant bits of r0. So we can just take these bits after having formed r0 by multiplying k by s.   Example: m D 8  implies p D 3 , w D 5, k D 21. Must have 0 < s < 25;  choose s D 13   A D 13=32.  Using just the formula to compute h.k : kA D 21  13=32 D 273=32 D 8 17   k A mod 1 D 17=32   m .k A mod 1  D 8  17=32 D 17=4 D 4 1 4   bm .k A mod 1 c D 4, so that h.k  D 4.  Using the implementation: ks D 21  13 D 273 D 8  25 C 17   r1 D 8, r0 D 17. Written in w D 5 bits, r0 D 10001. Take the p D 3 most signiÔ¨Å- cant bits of r0, get 100 in binary, or 4 in decimal, so that h.k  D 4.  32  How to choose A:   The multiplication method works with any legal value of A.  But it works better with some values than with others, depending on the keys  being hashed.   Knuth suggests using A  .p5  cid:0  1 =2.  ¬∑  11-10  Lecture Notes for Chapter 11: Hash Tables  Universal hashing  [We just touch on universal hashing in these notes. See the book for a full treat- ment.] Suppose that a malicious adversary, who gets to choose the keys to be hashed, has seen your hashing program and knows the hash function in advance. Then he could choose keys that all hash to the same slot, giving worst-case behavior. One way to defeat the adversary is to use a different hash function each time. You choose one at random at the beginning of your program. Unless the adversary knows how you‚Äôll be randomly choosing which hash function to use, he cannot intentionally defeat you. Just because we choose a hash function randomly, that doesn‚Äôt mean it‚Äôs a good hash function. What we want is to randomly choose a single hash function from a set of good candidates. Consider a Ô¨Ånite collection H of hash functions that map a universe U of keys into the range f0; 1; : : : ; m  cid:0  1g. H is universal if for each pair of keys k; l 2 U , where k ¬§ l, the number of hash functions h 2 H for which h.k  D h.l  is  jHj =m. Put another way, H is universal if, with a hash function h chosen randomly from H , the probability of a collision between two different keys is no more than than 1=m chance of just choosing two slots randomly and independently. Why are universal hash functions good?   They give good hashing behavior:  Theorem Using chaining and universal hashing on key k:      If k is not in the table, the expected length E ≈ính.k ¬ç of the list that k hashes to is  Àõ. If k is in the table, the expected length E ≈ính.k ¬ç of the list that holds k is  1 C Àõ. Corollary Using chaining and universal hashing, the expected time for each SEARCH op- eration is O.1 .   They are easy to design.  [Seebookfordetailsofbehavioranddesignofauniversalclassofhashfunctions.]  Open addressing  An alternative to chaining for handling collisions.   Lecture Notes for Chapter 11: Hash Tables  11-11  Idea  Store all keys in the hash table itself.  Each slot contains either a key or NIL.  To search for key k:   Compute h.k  and examine slot h.k . Examining a slot is known as a probe. If slot h.k  contains key k, the search is successful. If this slot contains NIL, the search is unsuccessful.     There‚Äôs a third possibility: slot h.k  contains a key that is not k. We compute the index of some other slot, based on k and on which probe  count from 0: 0th, 1st, 2nd, etc.  we‚Äôre on.   Keep probing until we either Ô¨Ånd key k  successful search  or we Ô¨Ånd a slot  holding NIL  unsuccessful search .   We need the sequence of slots probed to be a permutation of the slot numbers h0; 1; : : : ; m  cid:0  1i  so that we examine all slots if we have to, and so that we don‚Äôt examine any slot more than once .   Thus, the hash function is h W U  f0; 1; : : : ; m  cid:0  1g ‚Ä¶ probe number  The requirement that the sequence of slots be a permutation of h0; 1; : : : ; m  cid:0  1i is equivalent to requiring that the probe sequence hh.k; 0 ; h.k; 1 ; : : : ; h.k; m  cid:0  1 i be a permutation of h0; 1; : : : ; m  cid:0  1i.  To insert, act as though we‚Äôre searching, and insert at the Ô¨Årst NIL slot we Ô¨Ånd.  ! f0; 1; : : : ; m  cid:0  1g ‚Ä¶ ‚Äû  slot number  ∆í‚Äö  ∆í‚Äö  ‚Äû  .  Pseudocode for searching HASH-SEARCH.T; k  i D 0 repeat  j D h.k; i   if T ≈íj ¬ç == k return j  i D i C 1  until T ≈íj ¬ç == NIL or i D m return NIL  HASH-SEARCH returns the index of a slot containing key k, or NIL if the search is unsuccessful.   11-12  Lecture Notes for Chapter 11: Hash Tables  Pseudocode for insertion HASH-INSERT.T; k  i D 0 repeat  j D h.k; i   if T ≈íj ¬ç == NIL T ≈íj ¬ç D k return j else i D i C 1  until i == m error ‚Äúhash table overÔ¨Çow‚Äù  HASH-INSERT returns the number of the slot that gets key k, or it Ô¨Çags a ‚Äúhash table overÔ¨Çow‚Äù error if there is no empty slot in which to put key k.  Deletion Cannot just put NIL into the slot containing the key we want to delete.   Suppose we want to delete key k in slot j .  And suppose that sometime after inserting key k, we were inserting key k0, and  during this insertion we had probed slot j  which contained key k .   And suppose we then deleted key k by storing NIL into slot j .  And then we search for key k0.  During the search, we would probe slot j before probing the slot into which  key k0 was eventually stored.   Thus, the search would be unsuccessful, even though key k0 is in the table. Solution: Use a special value DELETED instead of NIL when marking a slot as empty during deletion.   Search should treat DELETED as though the slot holds a key that does not match    the one being searched for. Insertion should treat DELETED as though the slot were empty, so that it can be reused.  The disadvantage of using DELETED is that now search time is no longer dependent on the load factor Àõ.  How to compute probe sequences  The ideal situation is uniform hashing: each key is equally likely to have any of the m≈† permutations of h0; 1; : : : ; m  cid:0  1i as its probe sequence.  This generalizes simple uniform hashing for a hash function that produces a whole probe sequence rather than just a single number.  It‚Äôs hard to implement true uniform hashing, so we approximate it with techniques that at least guarantee that the probe sequence is a permutation of h0; 1; : : : ; m cid:0  1i. None of these techniques can produce all m≈† probe sequences. They will make use of auxiliary hash functions, which map U ! f0; 1; : : : ; m  cid:0  1g.   Lecture Notes for Chapter 11: Hash Tables  11-13  Linear probing Given auxiliary hash function h0, the probe sequence starts at slot h0.k  and con- tinues sequentially through the table, wrapping after slot m  cid:0  1 to slot 0. Given key k and probe number i  0  i < m , h.k; i   D .h0.k  C i   mod m. The initial probe determines the entire sequence   only m possible sequences. Linear probing suffers from primary clustering: long runs of occupied sequences build up. And long runs tend to get longer, since an empty slot preceded by i full slots gets Ô¨Ålled next with probability .i C 1 =m. Result is that the average search and insertion times increase.  Quadratic probing As in linear probing, the probe sequence starts at h0.k . Unlike linear probing, it jumps around in the table according to a quadratic function of the probe number: h.k; i   D .h0.k  C c1i C c2i 2  mod m, where c1; c2 ¬§ 0 are constants. Must constrain c1, c2, and m in order to ensure that we get a full permutation of h0; 1; : : : ; m cid:0 1i.  Problem 11-3 explores one way to implement quadratic probing.  Can get secondary clustering: if two distinct keys have the same h0 value, then they have the same probe sequence.  Double hashing Use two auxiliary hash functions, h1 and h2. h1 gives the initial probe, and h2 gives the remaining probes: h.k; i   D .h1.k  C ih2.k   mod m. Must have h2.k  be relatively prime to m  no factors in common other than 1  in order to guarantee that the probe sequence is a full permutation of h0; 1; : : : ; m cid:0 1i.  Could choose m to be a power of 2 and h2 to always produce an odd number  > 1.   Could let m be prime and have 1 < h2.k  < m.  ‚Äö.m2  different probe sequences, since each possible combination of h1.k  and h2.k  gives a different probe sequence.  Analysis of open-address hashing  Assumptions  Analysis is in terms of load factor Àõ. We will assume that the table never  completely Ô¨Ålls, so we always have 0  n < m   0  Àõ < 1.   Assume uniform hashing.  No deletion.    In a successful search, each key is equally likely to be searched for.  Theorem The expected number of probes in an unsuccessful search is at most 1=.1  cid:0  Àõ .   11-14  Lecture Notes for Chapter 11: Hash Tables  Proof Since the search is unsuccessful, every probe is to an occupied slot, except for the last probe, which is to an empty slot. DeÔ¨Åne random variable X D  of probes made in an unsuccessful search. DeÔ¨Åne events Ai, for i D 1; 2; : : :, to be the event that there is an ith probe and that it‚Äôs to an occupied slot. X  i if and only if probes 1; 2; : : : ; i  cid:0  1 are made and are to occupied slots   PrfX  ig D PrfA1 \ A2 \  \ Ai cid:0 1g. By Exercise C.2-5, PrfA1 \ A2 \  \ Ai cid:0 1g D PrfA1g  PrfA2 j A1g  PrfA3 j A1 \ A2g  PrfAi cid:0 1 j A1 \ A2 \  \ Ai cid:0 2g :  Claim PrfAj j A1 \ A2 \  \ Aj cid:0 1g D .n cid:0 j C1 =.m cid:0 j C1 . Boundary case: j D 1   PrfA1g D n=m. Proof For the boundary case j D 1, there are n stored keys and m slots, so the probability that the Ô¨Årst probe is to an occupied slot is n=m. Given that j  cid:0 1 probes were made, all to occupied slots, the assumption of uniform hashing says that the probe sequence is a permutation of h0; 1; : : : ; m cid:0  1i, which in turn implies that the next probe is to a slot that we have not yet probed. There are m  cid:0  j C 1 slots remaining, n  cid:0  j C 1 of which are occupied. Thus, the probability  claim  that the j th probe is to an occupied slot is .n  cid:0  j C 1 =.m  cid:0  j C 1 . Using this claim,  PrfX  ig D  n m  ‚Äû  n  cid:0  1 m  cid:0  1   n  cid:0  2 m  cid:0  2  ∆í‚Äö i  cid:0  1 factors  n  cid:0  i C 2 m  cid:0  i C 2 ‚Ä¶  :  n < m   .n  cid:0  j  =.m  cid:0  j    n=m for j  0, which implies PrfX  ig   n mi cid:0 1 D Àõi cid:0 1 :  By equation  C.25 ,  E ≈íX ¬ç D  PrfX  ig  1XiD1 1XiD1 1XiD0  1  1  cid:0  Àõ  Àõi cid:0 1  Àõi    D  D   equation  A.6   .   theorem    Lecture Notes for Chapter 11: Hash Tables  11-15  Interpretation If Àõ is constant, an unsuccessful search takes O.1  time.      If Àõ D 0:5, then an unsuccessful search takes an average of 1=.1  cid:0  0:5  D 2 probes. If Àõ D 0:9, takes an average of 1=.1  cid:0  0:9  D 10 probes.  Corollary The expected number of probes to insert is at most 1=.1  cid:0  Àõ . Proof Since there is no deletion, insertion uses the same probe sequence as an unsuccessful search.  Theorem The expected number of probes in a successful search is at most  1  Àõ  ln  .  1 1  cid:0  Àõ  Proof A successful search for key k follows the same probe sequence as when key k was inserted. By the previous corollary, if k was the .i C 1 st key inserted, then Àõ equaled i=m at the time. Thus, the expected number of probes made in a search for k is at most 1=.1  cid:0  i=m  D m=.m  cid:0  i  . That was assuming that k was the .i C 1 st key inserted. We need to average over all n keys: XiD0  n cid:0 1  n cid:0 1  m  m  n  1  m  cid:0  i D D  m  1  n  1 XiD0 m  cid:0  i 1 XkDm cid:0 nC1 ÀõZ m  Àõ  k  1  m cid:0 n ln  m  ln  1  m  cid:0  n 1  cid:0  Àõ  1  Àõ 1  Àõ    D D  .1=x  dx   by inequality  A.12     theorem    Solutions for Chapter 11: Hash Tables  Solution to Exercise 11.1-4  We denote the huge array by T and, taking the hint from the book, we also have a stack implemented by an array S. The size of S equals the number of keys actually stored, so that S should be allocated at the dictionary‚Äôs maximum size. The stack has an attribute S:top, so that only entries S ≈í1 : : S:top¬ç are valid. The idea of this scheme is that entries of T and S validate each other. If key k is actually stored in T , then T ≈ík¬ç contains the index, say j , of a valid entry in S, and S ≈íj ¬ç contains the value k. Let us call this situation, in which 1  T ≈ík¬ç  S:top, S ≈íT ≈ík¬ç¬ç D k, and T ≈íS ≈íj ¬ç¬ç D j , a validating cycle. Assuming that we also need to store pointers to objects in our direct-address table, we can store them in an array that is parallel to either T or S. Since S is smaller than T , we‚Äôll use an array S0, allocated to be the same size as S, for these pointers. Thus, if the dictionary contains an object x with key k, then there is a validating cycle and S0≈íT ≈ík¬ç¬ç points to x. The operations on the dictionary work as follows:      Initialization: Simply set S:top D 0, so that there are no valid entries in the stack.  SEARCH: Given key k, we check whether we have a validating cycle, i.e., whether 1  T ≈ík¬ç  S:top and S ≈íT ≈ík¬ç¬ç D k. If so, we return S0≈íT ≈ík¬ç¬ç, and otherwise we return NIL. INSERT: To insert object x with key k, assuming that this object is not already in the dictionary, we increment S:top, set S ≈íS:top¬ç D k, set S0≈íS:top¬ç D x, and set T ≈ík¬ç D S:top.  DELETE: To delete object x with key k, assuming that this object is in the dictionary, we need to break the validating cycle. The trick is to also ensure that we don‚Äôt leave a ‚Äúhole‚Äù in the stack, and we solve this problem by moving the top entry of the stack into the position that we are vacating‚Äîand then Ô¨Åxing up that entry‚Äôs validating cycle. That is, we execute the following sequence of assignments:   Solutions for Chapter 11: Hash Tables  11-17  S ≈íT ≈ík¬ç¬ç D S ≈íS:top¬ç S0≈íT ≈ík¬ç¬ç D S0≈íS:top¬ç T ≈íS ≈íT ≈ík¬ç¬ç¬ç D T ≈ík¬ç T ≈ík¬ç D 0 S:top D S:top  cid:0  1  Solution to Exercise 11.2-1 This solution is also posted publicly  Each of these operations‚Äîinitialization, SEARCH, INSERT, and DELETE‚Äîtakes O.1  time.  Xkl  For each pair of keys k; l, where k ¬§ l, deÔ¨Åne the indicator random variable Xkl D Ifh.k  D h.l g. Since we assume simple uniform hashing, PrfXkl D 1g D Prfh.k  D h.l g D 1=m, and so E ≈íXkl ¬ç D 1=m. Now deÔ¨Åne the random variable Y to be the total number of collisions, so that Y DPk¬§l Xkl. The expected number of collisions is E ≈íY ¬ç D EXk¬§l D Xk¬§l D  n 2! 1 m n.n  cid:0  1  D n.n  cid:0  1  D   linearity of expectation   E ≈íXkl ¬ç  2m   :  m  2  1  Solution to Exercise 11.2-4 This solution is also posted publicly  The Ô¨Çag in each slot will indicate whether the slot is free.   A free slot is in the free list, a doubly linked list of all free slots in the table.  The slot thus contains two pointers.   A used slot contains an element and a pointer  possibly NIL  to the next element that hashes to this slot.  Of course, that pointer points to another slot in the table.    11-18  Solutions for Chapter 11: Hash Tables  Operations    Insertion:      If the element hashes to a free slot, just remove the slot from the free list and store the element there  with a NIL pointer . The free list must be doubly linked in order for this deletion to run in O.1  time. If the element hashes to a used slot j , check whether the element x already there ‚Äúbelongs‚Äù there  its key also hashes to slot j  .      If so, add the new element to the chain of elements in this slot. To do so, allocate a free slot  e.g., take the head of the free list  for the new element and put this new slot at the head of the list pointed to by the hashed-to slot  j  . If not, E is part of another slot‚Äôs chain. Move it to a new slot by allo- cating one from the free list, copying the old slot‚Äôs  j ‚Äôs  contents  ele- ment x and pointer  to the new slot, and updating the pointer in the slot that pointed to j to point to the new slot. Then insert the new element in the now-empty slot as usual. To update the pointer to j , it is necessary to Ô¨Ånd it by searching the chain of elements starting in the slot x hashes to.   Deletion: Let j be the slot the element x to be deleted hashes to.      If x is the only element in j  j doesn‚Äôt point to any other entries , just free the slot, returning it to the head of the free list. If x is in j but there‚Äôs a pointer to a chain of other elements, move the Ô¨Årst pointed-to entry to slot j and free the slot it was in. If x is found by following a pointer from j , just free x‚Äôs slot and splice it out of the chain  i.e., update the slot that pointed to x to point to x‚Äôs successor .  Searching: Check the slot the key hashes to, and if that is not the desired    element, follow the chain of pointers from the slot.  All the operations take expected O.1  times for the same reason they do with the version in the book: The expected time to search the chains is O.1 C Àõ  regardless of where the chains are stored, and the fact that all the elements are stored in the table means that Àõ  1. If the free list were singly linked, then operations that involved removing an arbitrary slot from the free list would not run in O.1  time.  Solution to Exercise 11.2-6  We can view the hash table as if it had m rows and L columns; each row stores one chain. The array has mL entries storing n keys, and mL  cid:0  n empty values. The procedure picks array positions at random until it Ô¨Ånds a key, which it returns. The probability of success on one draw is n=mL, so mL=n D L=Àõ trials are needed. Each trial takes time O.1 , since the individual chain sizes are known. The chain for the last draw needs to be scanned to Ô¨Ånd the desired element, however, costing O.L .   Solutions for Chapter 11: Hash Tables  11-19  Solution to Exercise 11.3-3  First, we observe that we can generate any permutation by a sequence of inter- changes of pairs of characters. One can prove this property formally, but infor- mally, consider that both heapsort and quicksort work by interchanging pairs of elements and that they have to be able to produce any permutation of their input array. Thus, it sufÔ¨Åces to show that if string x can be derived from string y by interchanging a single pair of characters, then x and y hash to the same value. Let us denote the ith character in x by xi, and similarly for y. The interpreta-  iD0 yi 2ip mod .2p  cid:0  1 .  iD0 xi 2ip, and so h.x  D  cid:0 Pn cid:0 1  iD0 xi 2ip mod .2p  cid:0  1 . tion of x in radix 2p isPn cid:0 1 Similarly, h.y  D cid:0 Pn cid:0 1 Suppose that x and y are identical strings of n characters except that the characters in positions a and b are interchanged: xa D yb and ya D xb. Without loss of generality, let a > b. We have h.x   cid:0  h.y  D  n cid:0 1 XiD0 Since 0  h.x ; h.y  < 2p  cid:0  1, we have that  cid:0 .2p  cid:0  1  < h.x   cid:0  h.y  < 2p  cid:0  1. If we show that .h.x   cid:0  h.y   mod .2p  cid:0  1  D 0, then h.x  D h.y . Since the sums in the hash functions are the same except for indices a and b, we have .h.x   cid:0  h.y   mod .2p  cid:0  1   xi 2ip! mod .2p  cid:0  1   cid:0   n cid:0 1 XiD0  yi 2ip! mod .2p  cid:0  1  :  D ..xa2ap C xb2bp   cid:0  .ya2ap C yb2bp   mod .2p  cid:0  1  D ..xa2ap C xb2bp   cid:0  .xb2ap C xa2bp   mod .2p  cid:0  1  D ..xa  cid:0  xb 2ap  cid:0  .xa  cid:0  xb 2bp  mod .2p  cid:0  1  D ..xa  cid:0  xb .2ap  cid:0  2bp   mod .2p  cid:0  1  D ..xa  cid:0  xb 2bp.2.a cid:0 b p  cid:0  1   mod .2p  cid:0  1  :  By equation  A.5 ,  a cid:0 b cid:0 1  2.a cid:0 b p  cid:0  1  ;  2pi D  2p  cid:0  1  XiD0 and multiplying both sides by 2p  cid:0  1, we get 2.a cid:0 b p  cid:0  1 D cid:0 Pa cid:0 b cid:0 1 Thus, .h.x   cid:0  h.y   mod .2p  cid:0  1  D  .xa  cid:0  xb 2bp a cid:0 b cid:0 1 XiD0 D 0 ; since one of the factors is 2p  cid:0  1. We have shown that .h.x   cid:0  h.y   mod .2p  cid:0  1  D 0, and so h.x  D h.y .  2pi! .2p  cid:0  1 ! mod .2p  cid:0  1   iD0  2pi .2p  cid:0  1 .   11-20  Solutions for Chapter 11: Hash Tables  Solution to Exercise 11.3-5  2 D uj .uj  cid:0  1 =2.  Let b D jBj and u D jUj. We start by showing that the total number of collisions is minimized by a hash function that maps u=b elements of U to each of the b values in B. For a given hash function, let uj be the number of elements that map to j 2 B. We have u DPj2B uj . We also have that the number of collisions for a given value of j 2 B is cid:0 uj Lemma The total number of collisions is minimized when uj D u=b for each j 2 B. Proof If uj  u=b, let us call j underloaded, and if uj  u=b, let us call j overloaded. Consider an unbalanced situation in which uj ¬§ u=b for at least one value j 2 B. We can think of converting a balanced situation in which all uj equal u=b into the unbalanced situation by repeatedly moving an element that maps to an underloaded value to map instead to an overloaded value.  If you think of the values of B as representing buckets, we are repeatedly moving elements from buckets containing at most u=b elements to buckets containing at least u=b elements.  We now show that each such move increases the number of collisions, so that all the moves together must increase the number of collisions. Suppose that we move an element from an underloaded value j to an overloaded value k, and we leave all other elements alone. Because j is underloaded and k is overloaded, uj  u=b  uk. Considering just the collisions for values j and k, we have uj .uj  cid:0  1 =2 C uk.uk  cid:0  1 =2 collisions before the move and .uj  cid:0  1 .uj  cid:0  2 =2 C .uk C 1 uk=2 collisions afterward. We wish to show that uj .uj  cid:0  1 =2 C uk.uk  cid:0  1 =2 < .uj  cid:0  1 .uj  cid:0  2 =2 C .uk C 1 uk=2. We have the following sequence of equivalent inequalities:  uj < uk C 1 2uj < 2uk C 2  cid:0 uk < uk  cid:0  2uj C 2 k  cid:0  uk < u2  j  cid:0  3uj C 2 C u2  k C uk  u2 j  cid:0  uj C u2  uj .uj  cid:0  1  C uk.uk  cid:0  1  < .uj  cid:0  1 .uj  cid:0  2  C .uk C 1 uk  uj .uj  cid:0  1 =2 C uk.uk  cid:0  1 =2 < .uj  cid:0  1 .uj  cid:0  2 =2 C .uk C 1 uk=2 : Thus, each move increases the number of collisions. We conclude that the number of collisions is minimized when uj D u=b for each j 2 B. By the above lemma, for any hash function, the total number of collisions must  be at least b.u=b .u=b  cid:0  1 =2. The number of pairs of distinct elements is cid:0 u 2 D u.u  cid:0  1 =2. Thus, the number of collisions per pair of distinct elements must be at least   Solutions for Chapter 11: Hash Tables  11-21  b.u=b .u=b  cid:0  1 =2  u.u  cid:0  1 =2  u=b  cid:0  1 u  cid:0  1 u=b  cid:0  1 u 1 1 b  cid:0   u  :  D  >  D  Thus, the bound  on the probability of a collision for any pair of distinct elements can be no less than 1=b  cid:0  1=u D 1=jBj  cid:0  1=jUj.  Solution to Problem 11-1  a. Since we assume uniform hashing, we can use the same observation as is used in Corollary 11.7: that inserting a key entails an unsuccessful search followed by placing the key into the Ô¨Årst empty slot found. As in the proof of Theo- rem 11.6, if we let X be the random variable denoting the number of probes in an unsuccessful search, then PrfX  ig  Àõi cid:0 1. Since n  m=2, we have Àõ  1=2. Letting i D k C 1, we have PrfX > kg D PrfX  k C 1g  .1=2 .kC1  cid:0 1 D 2 cid:0 k.  b. Substituting k D 2 lg n into the statement of part  a  yields that the probability that the ith insertion requires more than k D 2 lg n probes is at most 2 cid:0 2 lg n D .2lg n  cid:0 2 D n cid:0 2 D 1=n2. We must deal with the possibility that 2 lg n is not an integer, however. Then the event that the ith insertion requires more than 2 lg n probes is the same as the event that the ith insertion requires more than b2 lg nc probes. Since b2 lg nc > 2 lg n  cid:0  1, we have that the probability of this event is at most 2 cid:0 b2 lg nc < 2 cid:0 .2 lg n cid:0 1  D 2=n2 D O.1=n2 .  c. Let the event A be X > 2 lg n, and for i D 1; 2; : : : ; n, let the event Ai be Xi > 2 lg n. In part  b , we showed that PrfAig D O.1=n2  for i D 1; 2; : : : ; n. From how we deÔ¨Åned these events, A D A1 [ A2 [  [ An. Using Boole‚Äôs inequality,  C.19 , we have PrfAg  PrfA1g C PrfA2g C  C PrfAng   n  O.1=n2  D O.1=n  :  d. We use the deÔ¨Ånition of expectation and break the sum into two parts:   11-22  Solutions for Chapter 11: Hash Tables  Solution to Problem 11-2 This solution is also posted publicly  n  XkD1 d2 lg neXkD1 d2 lg neXkD1  D    E ≈íX ¬ç D  k  PrfX D kg  k  PrfX D kg C  k  PrfX D kg  n  XkDd2 lg neC1  d2 lg ne  PrfX D kg C  n  PrfX D kg  n  XkDd2 lg neC1 XkDd2 lg neC1  n  d2 lg neXkD1  D d2 lg ne  PrfX D kg C n  PrfX D kg : Since X takes on exactly one value, we have that Pd2 lg ne PrfX  d2 lg neg  1 and Pn O.1=n , by part  c . Therefore, E ≈íX ¬ç  d2 lg ne  1 C n  O.1=n   kD1 PrfX D kg D kDd2 lg neC1 PrfX D kg  PrfX > 2 lg ng D  D d2 lg ne C O.1  D O.lg n  :  a. A particular key is hashed to a particular slot with probability 1=n. Suppose we select a speciÔ¨Åc set of k keys. The probability that these k keys are inserted into the slot in question and that all other keys are inserted elsewhere is  :  1  1  nk1  cid:0  nn cid:0 k  1 Since there are cid:0 n k ways to choose our k keys, we get nn cid:0 k n k! : Qk D 1 nk1  cid:0  b. For i D 1; 2; : : : ; n, let Xi be a random variable denoting the number of keys that hash to slot i, and let Ai be the event that Xi D k, i.e., that exactly k keys hash to slot i. From part  a , we have PrfAg D Qk. Then, Pk D PrfM D kg D Prn max 1in D Prfthere exists i such that Xi D k and that Xi  k for i D 1; 2; : : : ; ng  Prfthere exists i such that Xi D kg D PrfA1 [ A2 [  [ Ang  PrfA1g C PrfA2g C  C PrfAng D nQk :  Xi D ko   by inequality  C.19     Solutions for Chapter 11: Hash Tables  11-23  c. We start by showing two facts.  First, 1  cid:0  1=n < 1, which implies .1  cid:0  1=n n cid:0 k < 1. Second, n≈†=.n cid:0 k ≈† D n.n cid:0 1 .n cid:0 2  .n cid:0 kC1  < nk. Using these facts, along with the simpliÔ¨Åcation k≈† > .k=e k of equation  3.18 , we have  1  nn cid:0 k  n≈†  k≈†.n  cid:0  k ≈†  Qk D  1  nk1  cid:0  nkk≈†.n  cid:0  k ≈† 1  n≈†  <  <  <  k≈† ek kk   .1  cid:0  1=n n cid:0 k < 1   n≈†=.n  cid:0  k ≈† < nk   k≈† > .k=e k  .  d. Notice that when n D 2, lg lg n D 0, so to be precise, we need to assume that n  3. In part  c , we showed that Qk < ek=kk for any k; in particular, this inequality k0 < 1=n3 or, equivalently, holds for k0. Thus, it sufÔ¨Åces to show that ek0 =k0 that n3 < k0 Taking logarithms of both sides gives an equivalent condition: 3 lg n < k0.lg k0  cid:0  lg e   k0 =ek0.  c lg n lg lg n  D  .lg c C lg lg n  cid:0  lg lg lg n  cid:0  lg e  :  c  3 <  lg lg n  lg lg lg n  lg lg lg n  Dividing both sides by lg n gives the condition .lg c C lg lg n  cid:0  lg lg lg n  cid:0  lg e  lg c  cid:0  lg e lg lg n  cid:0   lg c  cid:0  lg e lg lg n  cid:0   lg lg n  :  lg lg n  :  Let x be the last expression in parentheses:  D c1 C x D1 C We need to show that there exists a constant c > 1 such that 3 < cx. Noting that limn!1 x D 1, we see that there exists n0 such that x  1=2 for all n  n0. Thus, any constant c > 6 works for n  n0. We handle smaller values of n‚Äîin particular, 3  n < n0‚Äîas follows. Since n is constrained to be an integer, there are a Ô¨Ånite number of n in the range 3  n < n0. We can evaluate the expression x for each such value of n and determine a value of c for which 3 < cx for all values of n. The Ô¨Ånal value of c that we use is the larger of  6, which works for all n  n0, and  max3n<n0 fc W 3 < cxg, i.e., the largest value of c that we chose for the  range 3  n < n0.  Thus, we have shown that Qk0 < 1=n3, as desired. To see that Pk < 1=n2 for k  k0, we observe that by part  b , Pk  nQk for all k. Choosing k D k0 gives Pk0  nQk0 < n  .1=n3  D 1=n2. For   11-24  Solutions for Chapter 11: Hash Tables  k > k0, we will show that we can pick the constant c such that Qk < 1=n3 for all k  k0, and thus conclude that Pk < 1=n2 for all k  k0. To pick c as required, we let c be large enough that k0 > 3 > e. Then e=k < 1 for all k  k0, and so ek=kk decreases as k increases. Thus, Qk < ek=kk  ek0 =kk0 < 1=n3  e. The expectation of M is  for k  k0.  E ≈íM ¬ç D  n  n  k0  k0  D  k  PrfM D kg  k  PrfM D kg C  XkD0 XkD0 XkD0 k0  PrfM D kg C XkD0  k0 D k0  PrfM  k0g C n  PrfM > k0g ;  XkDk0C1 XkDk0C1 XkDk0C1  PrfM D kg C n    k0  n  n  k  PrfM D kg  n  PrfM D kg  PrfM D kg  which is what we needed to show, since k0 D c lg n= lg lg n. To show that E ≈íM ¬ç D O.lg n= lg lg n , note that PrfM  k0g  1 and PrfM > k0g D  PrfM D kg  n  n  Pk  D  XkDk0C1 XkDk0C1 XkDk0C1 < n  .1=n2  D 1=n :  <  n  We conclude that E ≈íM ¬ç  k0  1 C n  .1=n   D k0 C 1 D O.lg n= lg lg n  :  1=n2   by part  d    Solution to Problem 11-3  a. From how the probe-sequence computation is speciÔ¨Åed, it is easy to see that the probe sequence is hh.k ; h.k  C 1; h.k  C 1 C 2; h.k  C 1 C 2 C 3;   Solutions for Chapter 11: Hash Tables  11-25  : : : ; h.k  C 1 C 2 C 3 C  C i ; : : :i, where all the arithmetic is modulo m. Starting the probe numbers from 0, the ith probe is offset  modulo m  from h.k  by  Thus, we can write the probe sequence as  i  D  2  j D  i.i C 1   XjD0 h0.k; i   Dh.k  C  1  2  i 2 C  1  2  i :  1  2  i C  1  2  i 2 mod m ;  which demonstrates that this scheme is a special case of quadratic probing.  b. Let h0.k; i   denote the ith probe of our scheme. We saw in part  a  that h0.k; i   D .h.k  C i.i C 1 =2  mod m. To show that our algorithm exam- ines every table position in the worst case, we show that for a given key, each of the Ô¨Årst m probes hashes to a distinct value. That is, for any key k and for any probe numbers i and j such that 0  i < j < m, we have h0.k; i   ¬§ h0.k; j  . We do so by showing that h0.k; i   D h0.k; j   yields a contradiction. Let us assume that there exists a key k and probe numbers i and j satsifying 0  i < j < m for which h0.k; i   D h0.k; j  . Then h.k  C i.i C 1 =2  h.k  C j.j C 1 =2 .mod m  ; which in turn implies that i.i C 1 =2  j.j C 1 =2 .mod m  ; or j.j C 1 =2  cid:0  i.i C 1 =2  0 .mod m  : Since j.j C 1 =2  cid:0  i.i C 1 =2 D .j  cid:0  i  .j C i C 1 =2, we have .j  cid:0  i  .j C i C 1 =2  0 .mod m  : The factors j  cid:0  i and j C i C 1 must have different parities, i.e., j  cid:0  i is even if and only if j C i C 1 is odd.  Work out the various cases in which i and j are even and odd.  Since .j  cid:0  i  .j C i C 1 =2  0 .mod m , we have .j  cid:0  i  .j C i C 1 =2 D rm for some integer r or, equivalently, .j  cid:0  i  .j C i C 1  D r  2m. Using the assumption that m is a power of 2, let m D 2p for some nonnegative integer p, so that now we have .j  cid:0  i  .j C i C 1  D r  2pC1. Because exactly one of the factors j  cid:0  i and j C i C 1 is even, 2pC1 must divide one of the factors. It cannot be j  cid:0  i, since j  cid:0  i < m < 2pC1. But it also cannot be j C i C 1, since j C i C 1  .m  cid:0  1  C .m  cid:0  2  C 1 D 2m  cid:0  2 < 2pC1. Thus we have derived the contradiction that 2pC1 divides neither of the factors j  cid:0  i and j C i C 1. We conclude that h0.k; i   ¬§ h0.k; j  .   Lecture Notes for Chapter 12: Binary Search Trees  Chapter 12 overview  Search trees   Data structures that support many dynamic-set operations.  Can be used as both a dictionary and as a priority queue.  Basic operations take time proportional to the height of the tree.   For complete binary tree with n nodes: worst case ‚Äö.lg n .  For linear chain of n nodes: worst case ‚Äö.n .   Different types of search trees include binary search trees, red-black trees  cov-  ered in Chapter 13 , and B-trees  covered in Chapter 18 .  We will cover binary search trees, tree walks, and operations on binary search trees.  Binary search trees  Binary search trees are an important data structure for dynamic sets.  Accomplish many dynamic-set operations in O.h  time, where h D height of  As in Section 10.4, we represent a binary tree by a linked data structure in which  tree.  each node is an object.   T:root points to the root of tree T .  Each node contains the attributes   key  and possibly other satellite data .      left: points to left child. right: points to right child.   p: points to parent. T:root:p D NIL.   12-2  Lecture Notes for Chapter 12: Binary Search Trees   Stored keys must satisfy the binary-search-tree property.      If y is in left subtree of x, then y:key  x:key. If y is in right subtree of x, then y:key  x:key.  Draw sample tree. [ThisisFigure12.1 a fromthetext,using A, B, D, F , H, K inplaceof2,3,5, 5,7,8,withalphabetic comparisons. It‚ÄôsOKtohaveduplicate keys,thoughthere arenoneinthisexample. Showthatthebinary-search-tree propertyholds.]  F  B  H  A  D  K  The binary-search-tree property allows us to print keys in a binary search tree in order, recursively, using an algorithm called an inorder tree walk. Elements are printed in monotonically increasing order. How INORDER-TREE-WALK works:   Check to make sure that x is not NIL.  Recursively, print the keys of the nodes in x‚Äôs left subtree.  Print x‚Äôs key.  Recursively, print the keys of the nodes in x‚Äôs right subtree.  INORDER-TREE-WALK .x  if x ¬§ NIL  INORDER-TREE-WALK .x:left  print key≈íx¬ç INORDER-TREE-WALK .x:right   Example Do the inorder tree walk on the example above, getting the output ABDFHK.  Correctness Follows by induction directly from the binary-search-tree property.  Time Intuitively, the walk takes ‚Äö.n  time for a tree with n nodes, because we visit and print each node once. [Bookhasformalproof.]   Lecture Notes for Chapter 12: Binary Search Trees  12-3  Querying a binary search tree  Searching  TREE-SEARCH.x; k   if x == NIL or k == key≈íx¬ç  return x if k < x:key  return TREE-SEARCH.x:left; k   else return TREE-SEARCH.x:right; k   Initial call is TREE-SEARCH.T:root; k .  Example Search for values D and C in the example tree from above.  Time The algorithm recurses, visiting nodes on a downward path from the root. Thus, running time is O.h , where h is the height of the tree. [The text also gives an iterative version of TREE-SEARCH, which is more efÔ¨Å- cientonmostcomputers. Theaboverecursive procedure ismorestraightforward, however.]  Minimum and maximum  The binary-search-tree property guarantees that      the minimum key of a binary search tree is located at the leftmost node, and the maximum key of a binary search tree is located at the rightmost node.  Traverse the appropriate pointers  left or right  until NIL is reached.  TREE-MINIMUM.x  while x:left ¬§ NIL return x  x D x:left  TREE-MAXIMUM.x  while x:right ¬§ NIL return x  x D x:right  Time Both procedures visit nodes that form a downward path from the root to a leaf. Both procedures run in O.h  time, where h is the height of the tree.   12-4  Lecture Notes for Chapter 12: Binary Search Trees  Successor and predecessor  Assuming that all keys are distinct, the successor of a node x is the node y such that y:key is the smallest key > x:key.  We can Ô¨Ånd x‚Äôs successor based entirely on the tree structure. No key comparisons are necessary.  If x has the largest key in the binary search tree, then we say that x‚Äôs successor is NIL. There are two cases: 1. If node x has a non-empty right subtree, then x‚Äôs successor is the minimum in  x‚Äôs right subtree.  2. If node x has an empty right subtree, notice that:   As long as we move to the left up the tree  move up through right children ,   x‚Äôs successor y is the node that x is the predecessor of  x is the maximum  we‚Äôre visiting smaller keys.  in y‚Äôs left subtree .  return TREE-MINIMUM.x:right   TREE-SUCCESSOR .x  if x:right ¬§ NIL y D x:p while y ¬§ NIL and x == y:right x D y y D y:p  return y  Example  15  6  18  3  7  17  20  2  4  13  9  TREE-PREDECESSOR is symmetric to TREE-SUCCESSOR.   Find the successor of the node with key value 15.  Answer: Key value 17   Find the successor of the node with key value 6.  Answer: Key value 7   Find the successor of the node with key value 4.  Answer: Key value 6   Find the predecessor of the node with key value 6.  Answer: Key value 4   Time For both the TREE-SUCCESSOR and TREE-PREDECESSOR procedures, in both cases, we visit nodes on a path down the tree or up the tree. Thus, running time is O.h , where h is the height of the tree.   Lecture Notes for Chapter 12: Binary Search Trees  12-5  Insertion and deletion  Insertion and deletion allows the dynamic set represented by a binary search tree to change. The binary-search-tree property must hold after the change. Insertion is more straightforward than deletion.  Insertion  TREE-INSERT.T; ¬¥  y D NIL x D T:root while x ¬§ NIL y D x if ¬¥:key < x:key x D x:left else x D x:right  ¬¥:p D y if y == NIL     tree T was empty  elseif ¬¥:key < y:key  T:root D ¬¥ y:left D ¬¥ else y:right D ¬¥  To insert value  into the binary search tree, the procedure is given node ¬¥, with  ¬¥:key D , ¬¥:left D NIL, and ¬¥:right D NIL.   Beginning at root of the tree, trace a downward path, maintaining two pointers.   Pointer x: traces the downward path.  Pointer y: ‚Äútrailing pointer‚Äù to keep track of parent of x.   Traverse the tree downward by comparing the value of node at x with , and  move to the left or right child accordingly.   When x is NIL, it is at the correct position for node ¬¥.  Compare ¬¥‚Äôs value with y‚Äôs value, and insert ¬¥ at either y‚Äôs left or right, appro-  priately.  Example Run TREE-INSERT.T; C   on the Ô¨Årst sample binary search tree. Result:  F  B  H  A  D  K  C   12-6  Lecture Notes for Chapter 12: Binary Search Trees  Time Same as TREE-SEARCH. On a tree of height h, procedure takes O.h  time. TREE-INSERT can be used with INORDER-TREE-WALK to sort a given set of num- bers.  See Exercise 12.3-3.   Deletion  [Deletion from a binary search tree changed in the third edition. In the Ô¨Årst two editions, whenthenode ¬¥ passedto TREE-DELETE hadtwochildren, ¬¥‚Äôssucces- sory wasthenodeactuallyremoved,withy‚Äôscontentscopiedinto¬¥. Theproblem withthatapproach isthatifthereareexternalpointers intothebinary searchtree, thenapointer to y fromoutside thebinarysearch treebecomes stale. Inthethird edition,thenode ¬¥ passedto TREE-DELETE isalwaysthenodeactuallyremoved, sothatallexternalpointerstonodesotherthan ¬¥ remainvalid.] Conceptually, deleting node ¬¥ from binary search tree T has three cases:  1. If ¬¥ has no children, just remove it. 2. If ¬¥ has just one child, then make that child take ¬¥‚Äôs position in the tree, drag-  ging the child‚Äôs subtree along.  3. If ¬¥ has two children, then Ô¨Ånd ¬¥‚Äôs successor y and replace ¬¥ by y in the tree. y must be in ¬¥‚Äôs right subtree and have no left child. The rest of ¬¥‚Äôs original right subtree becomes y‚Äôs new right subtree, and ¬¥‚Äôs left subtree becomes y‚Äôs new left subtree. This case is a little tricky because the exact sequence of steps taken depends on whether y is ¬¥‚Äôs right child.  The code organizes the cases a bit differently. Since it will move subtrees around within the binary search tree, it uses a subroutine, TRANSPLANT, to replace one subtree as the child of its parent by another subtree.  TRANSPLANT.T; u;    if u:p == NIL  T:root D  elseif u == u:p:left u:p:left D  else u:p:right D  if  ¬§ NIL  :p D u:p   the root .  left or right child.  TRANSPLANT.T; u;   replaces the subtree rooted at u by the subtree rooted at :   Makes u‚Äôs parent become ‚Äôs parent  unless u is the root, in which case it makes   u‚Äôs parent gets  as either its left or right child, depending on whether u was a   Doesn‚Äôt update :left or :right, leaving that up to TRANSPLANT‚Äôs caller.  TREE-DELETE .T; ¬¥  has four cases when deleting node ¬¥ from binary search tree T :   Lecture Notes for Chapter 12: Binary Search Trees  12-7    If ¬¥ has no left child, replace ¬¥ by its right child. The right child may or may not be NIL.  If ¬¥‚Äôs right child is NIL, then this case handles the situation in which ¬¥ has no children.   q  q  r  l  NIL  r  q  z  q  z  NIL  l  l  l  q  q  z  z  y  NIL  x    If ¬¥ has just one child, and that child is its left child, then replace ¬¥ by its left child.   Otherwise, ¬¥ has two children. Find ¬¥‚Äôs successor y. y must lie in ¬¥‚Äôs right subtree and have no left child  the solution to Exercise 12.2-5 on page 12-15 of this manual shows why . Goal is to replace ¬¥ by y, splicing y out of its current location.    If y is ¬¥‚Äôs right child, replace ¬¥ by y and leave y‚Äôs right child alone.  q  y  NIL  x  x  y  q  z  y  l  NIL  x   Otherwise, y lies within ¬¥‚Äôs right subtree but is not the root of this subtree.  Replace y by its own right child. Then replace ¬¥ by y.  r  l  r  l  r  q  y  x   12-8  Lecture Notes for Chapter 12: Binary Search Trees  TREE-DELETE .T; ¬¥   if ¬¥:left == NIL  elseif ¬¥:right == NIL  TRANSPLANT.T; ¬¥; ¬¥:left   else    ¬¥ has two children.  y D TREE-MINIMUM.¬¥:right  if y:p ¬§ ¬¥  TRANSPLANT.T; ¬¥; ¬¥:right      ¬¥ has no left child     ¬¥ has just a left child     y is ¬¥‚Äôs successor     y lies within ¬¥‚Äôs right subtree but is not the root of this subtree. TRANSPLANT.T; y; y:right  y:right D ¬¥:right y:right:p D y    Replace ¬¥ by y. TRANSPLANT.T; ¬¥; y  y:left D ¬¥:left y:left:p D y  Note that the last three lines execute when ¬¥ has two children, regardless of whether y is ¬¥‚Äôs right child.  Example On this binary search tree T ,  B  I  A  H  G  C  F  E  D  K  J  L  N  OM  run the following. [Youcaneitherstartwiththeoriginaltreeeachtimeorstartwith theresultofthepreviouscall. Thetreeisdesignedsothateitherwaywillelicitall fourcases.]   TREE-DELETE.T; I   shows the case in which the node deleted has no left child.  TREE-DELETE.T; G  shows the case in which the node deleted has a left child  but no right child.   TREE-DELETE.T; K  shows the case in which the node deleted has both chil-  dren and its successor is its right child.   TREE-DELETE.T; B  shows the case in which the node deleted has both chil-  dren and its successor is not its right child.   Lecture Notes for Chapter 12: Binary Search Trees  12-9  Time O.h , on a tree of height h. Everything is O.1  except for the call to TREE- MINIMUM.  Minimizing running time  We‚Äôve been analyzing running time in terms of h  the height of the binary search tree , instead of n  the number of nodes in the tree .   Problem: Worst case for binary search tree is ‚Äö.n ‚Äîno better than linked list.  Solution: Guarantee small height  balanced tree ‚Äîh D O.lg n . In later chapters, by varying the properties of binary search trees, we will be able to analyze running time in terms of n.   Method: Restructure the tree if necessary. Nothing special is required for querying, but there may be extra work when changing the structure of the tree  inserting or deleting .  Red-black trees are a special class of binary trees that avoids the worst-case be- havior of O.n  that we can see in ‚Äúplain‚Äù binary search trees. Red-black trees are covered in detail in Chapter 13.  [These are notes on a starred section in the book. I covered this material in an optionallecture.] Given a set of n distinct keys. Insert them in random order into an initially empty binary search tree.   Each of the n≈† permutations is equally likely.  Different from assuming that every binary search tree on n keys is equally  likely. Try it for n D 3. Will get 5 different binary search trees. When we look at the binary search trees resulting from each of the 3≈† input permutations, 4 trees will appear once and 1 tree will appear twice. [Thisgives theidea forthe solution toExercise12.4-3.]   Forget about deleting keys.  We will show that the expected height of a randomly built binary search tree is O.lg n .  Random variables  DeÔ¨Åne the following random variables:  Xn D height of a randomly built binary search tree on n keys.  Expected height of a randomly built binary search tree   12-10  Lecture Notes for Chapter 12: Binary Search Trees   Yn D 2Xn D exponential height.  Rn D rank of the root within the set of n keys used to build the binary search  tree.  Equally likely to be any element of f1; 2; : : : ; ng.    If Rn D i, then  Left subtree is a randomly-built binary search tree on i  cid:0  1 keys.  Right subtree is a randomly-built binary search tree on n  cid:0  i keys.  Foreshadowing  We will need to relate E ≈íYn¬ç to E ≈íXn¬ç. We‚Äôll use Jensen‚Äôs inequality: E ≈íf .X  ¬ç  f .E ≈íX ¬ç  ; provided  [leaveonboard]    the expectations exist and are Ô¨Ånite, and   f .x  is convex: for all x; y and all 0    1 f .x C .1  cid:0   y   f .x  C .1  cid:0   f .y  :  f y   l f x  +  1‚Äìl  f y   f x   f l x +  1‚Äìl  y   x  l x +  1‚Äìl  y  y  Convex  ‚Äúcurves upward‚Äù  We‚Äôll use Jensen‚Äôs inequality for f .x  D 2x.  Since 2x curves upward, it‚Äôs convex.   Lecture Notes for Chapter 12: Binary Search Trees  12-11  Formula for Yn  Think about Yn, if we know that Rn D i:  i‚Äì1 nodes  n‚Äìi nodes  Height of root is 1 more than the maximum height of its children: Yn D 2  max.Yi cid:0 1; Yn cid:0 i   : Base cases:  Y1 D 1  expected height of a 1-node tree is 20 D 1 .  DeÔ¨Åne Y0 D 0. DeÔ¨Åne indicator random variables Zn;1; Zn;2; : : : ; Zn;n: Zn;i D IfRn D ig : Rn is equally likely to be any element of f1; 2; : : : ; ng   PrfRn D ig D 1=n   E ≈íZn;i ¬ç D 1=n Consider a given n-node binary search tree  which could be a subtree . Exactly one Zn;i is 1, and all others are 0. Hence,   since E ≈íIfAg¬ç D PrfAg   [leaveonboard]  n  XiD1  Zn;i  .2  max.Yi cid:0 1; Yn cid:0 i    :  Yn D [Recall: Yn D 2  max.Yi cid:0 1; Yn cid:0 i   wasassumingthat Rn D i.]  [leaveonboard]  Bounding E ≈íYn¬ç  We will show that E ≈íYn¬ç is polynomial in n, which will imply that E ≈íXn¬ç D O.lg n .  Claim Zn;i is independent of Yi cid:0 1 and Yn cid:0 i . JustiÔ¨Åcation If we choose the root such that Rn D i, the left subtree contains i  cid:0  1 nodes, and it‚Äôs like any other randomly built binary search tree with i  cid:0  1 nodes. Other than the number of nodes, the left subtree‚Äôs structure has nothing to do with it being the left subtree of the root. Hence, Yi cid:0 1 and Zn;i are independent. Similarly, Yn cid:0 i and Zn;i are independent.   claim    12-12  Lecture Notes for Chapter 12: Binary Search Trees  Fact If X and Y are nonnegative random variables, then E ≈ímax.X; Y  ¬ç  E ≈íX ¬çCE ≈íY ¬ç. [Leaveonboard. ThisisExerciseC.3-4fromthetext.]  Thus,  n  n  D  D  E ≈íZn;i ¬ç  E ≈í2  max.Yi cid:0 1; Yn cid:0 i  ¬ç  E ≈íYn¬ç D E" n Zn;i .2  max.Yi cid:0 1; Yn cid:0 i    XiD1 XiD1 E ≈íZn;i  .2  max.Yi cid:0 1; Yn cid:0 i   ¬ç XiD1 1 XiD1 n  E ≈í2  max.Yi cid:0 1; Yn cid:0 i  ¬ç XiD1 XiD1 .E ≈íYi cid:0 1¬ç C E ≈íYn cid:0 i ¬ç   E ≈ímax.Yi cid:0 1; Yn cid:0 i  ¬ç  D  D    n  n  2  2  n  n  n   linearity of expectation    independence    E ≈íZn;i ¬ç D 1=n    E ≈íaX ¬ç D a E ≈íX ¬ç    earlier fact  .  Observe that the last summation is .E ≈íY0¬ç C E ≈íYn cid:0 1¬ç  C .E ≈íY1¬ç C E ≈íYn cid:0 2¬ç  C .E ≈íY2¬ç C E ≈íYn cid:0 3¬ç  E ≈íYi ¬ç ;  C C .E ≈íYn cid:0 1¬ç C E ≈íY0¬ç  D 2  n cid:0 1  XiD0  and so we get the recurrence  E ≈íYi ¬ç :  [leaveonboard]  We will show that for all integers n > 0, this recurrence has the solution  E ≈íYn¬ç   4  n  n cid:0 1  XiD0  Solving the recurrence  E ≈íYn¬ç   1  4 n C 3 3 ! :  Lemma n cid:0 1  XiD0 i C 3  3 ! D n C 3 4 ! :  [ThislemmasolvesExercise12.4-1.]   Lecture Notes for Chapter 12: Binary Search Trees  12-13  3! C  C 4 3! C  C 4  3! C 4 4! 3! C 3 3!   lemma   k  cid:0  1! C n  cid:0  1 k !.  k! D n  cid:0  1 3!, we have  Proof Use Pascal‚Äôs identity  Exercise C.1-7 : n Also using the simple identity 4 4! D 1 D 3 3 ! C n C 2  n C 3 4 ! 4 ! D  n C 2 3 ! C n C 1 3 ! C n C 1 D  n C 2 4 ! 3! C n 3 ! C n 3 ! C n C 1 D  n C 2 4! ::: 3 ! C n C 1 D  n C 2 3 ! C n C 1 D  n C 2 XiD0 i C 3 3 ! : D  3 ! C n 3 ! C n  n cid:0 1  We solve the recurrence by induction on n. Basis: n D 1. 1 D Y1 D E ≈íY1¬ç   4 1 C 3  3 ! D  1  Inductive step: Assume that E ≈íYi ¬ç   1 4  4 D 1 : 4 i C 3  1  3 ! for all i < n. Then  E ≈íYn¬ç   n cid:0 1  4  4  1  1  n  n  n  n cid:0 1  n cid:0 1  E ≈íYi ¬ç   from before   XiD0 3 !  inductive hypothesis  4 i C 3 XiD0 XiD0 i C 3 3 ! n n C 3 4 ! .n C 3 ≈† 1 n  4≈† .n  cid:0  1 ≈† .n C 3 ≈† 1 4  3≈† n≈†   lemma   1    D  D  D  D   12-14  Lecture Notes for Chapter 12: Binary Search Trees  With our bound on E ≈íYn¬ç, we use Jensen‚Äôs inequality to bound E ≈íXn¬ç:  1  4 n C 3 3 ! :  D  Thus, we‚Äôve proven that E ≈íYn¬ç   1  4 n C 3 3 !.  Bounding E ≈íXn¬ç  Thus,  2E≈íXn¬ç  E2Xn D E ≈íYn¬ç : 2E≈íXn¬ç   1  4 n C 3 3 ! .n C 3 .n C 2 .n C 1  1 4   6  D D O.n3  :  Taking logs of both sides gives E ≈íXn¬ç D O.lg n . Done!   Solutions for Chapter 12: Binary Search Trees  Solution to Exercise 12.1-2 This solution is also posted publicly  In a heap, a node‚Äôs key is  both of its children‚Äôs keys. In a binary search tree, a node‚Äôs key is  its left child‚Äôs key, but  its right child‚Äôs key. The heap property, unlike the binary-searth-tree property, doesn‚Äôt help print the nodes in sorted order because it doesn‚Äôt tell which subtree of a node contains the element to print before that node. In a heap, the largest element smaller than the node could be in either subtree. Note that if the heap property could be used to print the keys in sorted order in O.n  time, we would have an O.n -time algorithm for sorting, because building the heap takes only O.n  time. But we know  Chapter 8  that a comparison sort must take .n lg n  time.  Solution to Exercise 12.2-5  Let x be a node with two children. In an inorder tree walk, the nodes in x‚Äôs left subtree immediately precede x and the nodes in x‚Äôs right subtree immediately fol- low x. Thus, x‚Äôs predecessor is in its left subtree, and its successor is in its right subtree. Let s be x‚Äôs successor. Then s cannot have a left child, for a left child of s would come between x and s in the inorder walk.  It‚Äôs after x because it‚Äôs in x‚Äôs right subtree, and it‚Äôs before s because it‚Äôs in s‚Äôs left subtree.  If any node were to come between x and s in an inorder walk, then s would not be x‚Äôs successor, as we had supposed. Symmetrically, x‚Äôs predecessor has no right child.   12-16  Solutions for Chapter 12: Binary Search Trees  Solution to Exercise 12.2-7 This solution is also posted publicly  Note that a call to TREE-MINIMUM followed by n cid:0  1 calls to TREE-SUCCESSOR performs exactly the same inorder walk of the tree as does the procedure INORDER- TREE-WALK. INORDER-TREE-WALK prints the TREE-MINIMUM Ô¨Årst, and by deÔ¨Ånition, the TREE-SUCCESSOR of a node is the next node in the sorted order determined by an inorder tree walk. This algorithm runs in ‚Äö.n  time because:      It requires .n  time to do the n procedure calls. It traverses each of the n  cid:0  1 tree edges at most twice, which takes O.n  time. To see that each edge is traversed at most twice  once going down the tree and once going up , consider the edge between any node u and either of its children, node . By starting at the root, we must traverse .u;   downward from u to , before traversing it upward from  to u. The only time the tree is traversed downward is in code of TREE-MINIMUM, and the only time the tree is traversed upward is in code of TREE-SUCCESSOR when we look for the successor of a node that has no right subtree. Suppose that  is u‚Äôs left child.   Before printing u, we must print all the nodes in its left subtree, which is rooted  at , guaranteeing the downward traversal of edge .u;  .   After all nodes in u‚Äôs left subtree are printed, u must be printed next. Procedure TREE-SUCCESSOR traverses an upward path to u from the maximum element  which has no right subtree  in the subtree rooted at . This path clearly includes edge .u;  , and since all nodes in u‚Äôs left subtree are printed, edge .u;   is never traversed again.  Now suppose that  is u‚Äôs right child.   After u is printed, TREE-SUCCESSOR .u  is called. To get to the minimum element in u‚Äôs right subtree  whose root is  , the edge .u;   must be traversed downward.   After all values in u‚Äôs right subtree are printed, TREE-SUCCESSOR is called on the maximum element  again, which has no right subtree  in the subtree rooted at . TREE-SUCCESSOR traverses a path up the tree to an element after u, since u was already printed. Edge .u;   must be traversed upward on this path, and since all nodes in u‚Äôs right subtree have been printed, edge .u;   is never traversed again.  Hence, no edge is traversed twice in the same direction. Therefore, this algorithm runs in ‚Äö.n  time.   Solutions for Chapter 12: Binary Search Trees  12-17  Solution to Exercise 12.3-3 This solution is also posted publicly  Here‚Äôs the algorithm:  TREE-SORT.A   let T be an empty binary search tree for i D 1 to n INORDER-TREE-WALK .T:root   TREE-INSERT.T; A≈íi ¬ç   Worst case: ‚Äö.n2 ‚Äîoccurs when a linear chain of nodes results from the repeated TREE-INSERT operations. Best case: ‚Äö.n lg n ‚Äîoccurs when a binary tree of height ‚Äö.lg n  results from the repeated TREE-INSERT operations.  Solution to Exercise 12.4-2  We will answer the second part Ô¨Årst. We shall show that if the average depth of a  the Ô¨Årst part by exhibiting that this bound is tight: there is a binary search tree with  node is ‚Äö.lg n , then the height of the tree is O.pn lg n . Then we will answer average node depth ‚Äö.lg n  and height ‚Äö.pn lg n  D !.lg n .  Lemma If the average depth of a node in an n-node binary search tree is ‚Äö.lg n , then the  height of the tree is O.pn lg n .  Proof Suppose that an n-node binary search tree has average depth ‚Äö.lg n  and height h. Then there exists a path from the root to a node at depth h, and the depths of the nodes on this path are 0; 1; : : : ; h. Let P be the set of nodes on this path and Q be all other nodes. Then the average depth of a node is  1  n Xx2P  depth.x  CXy2Q  depth.y !   depth.x   1  h  1  nXx2P XdD0 1 n  ‚Äö.h2  :  n  d  D  D  For the purpose of contradiction, suppose that h is not O.pn lg n , so that h D !.pn lg n . Then we have 1 1 n  ‚Äö.h2  D n  !.n lg n  D !.lg n  ;   12-18  Solutions for Chapter 12: Binary Search Trees  which contradicts the assumption that the average depth is ‚Äö.lg n . Thus, the  height is O.pn lg n .  Here is an example of an n-node binary search tree with average node depth ‚Äö.lg n  but height !.lg n :  n  cid:0 pn lg n  nodes  nodes protrude from below as a single chain. This tree has height  To compute an upper bound on the average depth of a node, we use O.lg n  as  pn lg n nodes In this tree, n  cid:0 pn lg n nodes are a complete binary tree, and the other pn lg n ‚Äö.lg.n  cid:0 pn lg n   Cpn lg n D ‚Äö.pn lg n  D !.lg n  : an upper bound on the depth of each of the n  cid:0 pn lg n nodes in the complete binary tree part and O.lg n Cpn lg n  as an upper bound on the depth of each of the pn lg n nodes in the protruding chain. Thus, the average depth of a node is n  O.pn lg n .lg n Cpn lg n  C .n  cid:0 pn lg n  lg n  D level of the complete binary tree part has ‚Äö.n  cid:0 pn lg n  nodes, and each of these n  ‚Äö..n  cid:0 pn lg n  lg n  D  nodes has depth ‚Äö.lg n . Thus, the average node depth is at least 1  To bound the average depth of a node from below, observe that the bottommost  bounded from above by 1  1 n  O.n lg n   1 n  .n lg n   D O.lg n  :  D .lg n  :  Because the average node depth is both O.lg n  and .lg n , it is ‚Äö.lg n .  Solution to Exercise 12.4-4  We‚Äôll go one better than showing that the function 2x is convex. Instead, we‚Äôll show that the function cx is convex, for any positive constant c. According to the deÔ¨Ånition of convexity on page 1199 of the text, a function f .x  is con- vex if for all x and y and for all 0    1, we have f .x C .1  cid:0   y   f .x  C .1  cid:0   f .y . Thus, we need to show that for all 0    1, we have cxC.1 cid:0  y  cx C .1  cid:0   cy. We start by proving the following lemma.   Solutions for Chapter 12: Binary Search Trees  12-19  Lemma For any real numbers a and b and any positive real number c, ca  cb C .a  cid:0  b cb ln c : Proof We Ô¨Årst show that for all real r, we have cr  1C r ln c. By equation  3.12  from the text, we have ex  1 C x for all real x. Let x D r ln c, so that ex D er ln c D .eln c r D cr. Then we have cr D er ln c  1 C r ln c. Substituting a  cid:0  b for r in the above inequality, we have ca cid:0 b  1 C .a  cid:0  b  ln c.  lemma  Multiplying both sides by cb gives ca  cb C .a  cid:0  b cb ln c. Now we can show that cxC.1 cid:0  y  cx C .1  cid:0   cy for all 0    1. For convenience, let ¬¥ D x C .1  cid:0   y. In the inequality given by the lemma, substitute x for a and ¬¥ for b, giving cx  c¬¥ C .x  cid:0  ¬¥ c¬¥ ln c : Also substitute y for a and ¬¥ for b, giving cy  c¬¥ C .y  cid:0  ¬¥ c¬¥ ln c : If we multiply the Ô¨Årst inequality by  and the second by 1  cid:0   and then add the resulting inequalities, we get cx C .1  cid:0   cy   .c¬¥ C .x  cid:0  ¬¥ c¬¥ ln c  C .1  cid:0   .c¬¥ C .y  cid:0  ¬¥ c¬¥ ln c  D c¬¥ C xc¬¥ ln c  cid:0  ¬¥c¬¥ ln c C .1  cid:0   c¬¥ C .1  cid:0   yc¬¥ ln c   cid:0  .1  cid:0   ¬¥c¬¥ ln c  D . C .1  cid:0    c¬¥ C .x C .1  cid:0   y c¬¥ ln c  cid:0  . C .1  cid:0    ¬¥c¬¥ ln c D c¬¥ C ¬¥c¬¥ ln c  cid:0  ¬¥c¬¥ ln c D c¬¥ D cxC.1 cid:0  y ; as we wished to show.  Solution to Problem 12-2 This solution is also posted publicly  To sort the strings of S, we Ô¨Årst insert them into a radix tree, and then use a preorder tree walk to extract them in lexicographically sorted order. The tree walk outputs strings only for nodes that indicate the existence of a string  i.e., those that are lightly shaded in Figure 12.5 of the text .  Correctness The preorder ordering is the correct order because:   Any node‚Äôs string is a preÔ¨Åx of all its descendants‚Äô strings and hence belongs  before them in the sorted order  rule 2 .   12-20  Solutions for Chapter 12: Binary Search Trees  Solution to Problem 12-3   A node‚Äôs left descendants belong before its right descendants because the corre- sponding strings are identical up to that parent node, and in the next position the left subtree‚Äôs strings have 0 whereas the right subtree‚Äôs strings have 1  rule 1 .  Time ‚Äö.n .    Insertion takes ‚Äö.n  time, since the insertion of each string takes time propor- tional to its length  traversing a path through the tree whose length is the length of the string , and the sum of all the string lengths is n.   The preorder tree walk takes O.n  time. It is just like INORDER-TREE-WALK  it prints the current node and calls itself recursively on the left and right sub- trees , so it takes time proportional to the number of nodes in the tree. The number of nodes is at most 1 plus the sum  n  of the lengths of the binary strings in the tree, because a length-i string corresponds to a path through the root and i other nodes, but a single node may be shared among many string paths.  ties by n gives the desired equation.  a. The total path length P .T   is deÔ¨Åned asPx2T d.x; T  . Dividing both quanti- b. For any node x in TL, we have d.x; TL  D d.x; T    cid:0  1, since the distance to the root of TL is one less than the distance to the root of T . Similarly, for any node x in TR, we have d.x; TR  D d.x; T    cid:0  1. Thus, if T has n nodes, we have  P .T   D P .TL  C P .TR  C n  cid:0  1 ; since each of the n nodes of T  except the root  is in either TL or TR.  c.  If T is a randomly built binary search tree, then the root is equally likely to be any of the n elements in the tree, since the root is the Ô¨Årst element inserted. It follows that the number of nodes in subtree TL is equally likely to be any integer in the set f0; 1; : : : ; n  cid:0  1g. The deÔ¨Ånition of P .n  as the average total path length of a randomly built binary search tree, along with part  b , gives us the recurrence n cid:0 1  P .n  D  .P .i   C P .n  cid:0  i  cid:0  1  C n  cid:0  1  :  d. Since P .0  D 0, and since for k D 1; 2; : : : ; n  cid:0  1, each term P .k  in the summation appears once as P .i   and once as P .n  cid:0  i  cid:0  1 , we can rewrite the equation from part  c  as  1  n  XiD0  2  n  n cid:0 1  XkD1  P .n  D  P .k  C ‚Äö.n  :   Solutions for Chapter 12: Binary Search Trees  12-21  e. Observe that if, in the recurrence  7.6  in part  c  of Problem 7-3, we replace E ≈íT . ¬ç by P .  and we replace q by k, we get almost the same recurrence as in part  d  of Problem 12-3. The remaining difference is that in Problem 12-3 d , the summation starts at 1 rather than 2. Observe, however, that a binary tree with just one node has a total path length of 0, so that P .1  D 0. Thus, we can rewrite the recurrence in Problem 12-3 d  as  P .n  D  P .k  C ‚Äö.n   2  n  n cid:0 1  XkD2  and use the same technique as was used in Problem 7-3 to solve it. We start by solving part  d  of Problem 7-3: showing that  k lg k   n2 lg n  cid:0   1  2  1  8  n2 :  n cid:0 1  XkD2  n cid:0 1  Following the hint in Problem 7-3 d , we split the summation into two parts:  dn=2e cid:0 1  n cid:0 1  k lg k :  XkD2  k lg k C  k lg k D  XkDdn=2e  XkD2 The lg k in the Ô¨Årst summation on the right is less than lg.n=2  D lg n  cid:0  1, and the lg k in the second summation is less than lg n. Thus, n cid:0 1 XkD2  XkDdn=2e  dn=2e cid:0 1  n cid:0 1  k  n cid:0 1  dn=2e cid:0 1  XkD2 k C lg n k lg k < .lg n  cid:0  1  XkD2 XkD2 k  cid:0  n.n  cid:0  1  lg n  cid:0  n2 n2 lg n  cid:0   D lg n 1     2 1  k  2  1  8  1  2 n 2  cid:0  1 n  2  if n  2. Now we show that the recurrence  2  n cid:0 1  XkD2  n  P .k  C ‚Äö.n   P .n  D has the solution P .n  D O.n lg n . We use the substitution method. Assume inductively that P .n   an lg n C b for some positive constants a and b to be determined. We can pick a and b sufÔ¨Åciently large so that an lg n C b  P .1 . Then, for n > 1, we have by substitution  P .n  D  2  n  2  n  n cid:0 1  P .k  C ‚Äö.n   XkD2 XkD2 .ak lg k C b  C ‚Äö.n   n cid:0 1     12-22  Solutions for Chapter 12: Binary Search Trees  2a  n cid:0 1  2b  1  2  n  n  2b  2a  D  k lg k C  XkD2 .n  cid:0  2  C ‚Äö.n  n2 C n  1 n2 lg n  cid:0   n  an lg n  cid:0  n C 2b C ‚Äö.n  D an lg n C b C‚Äö.n  C b  cid:0   an lg n C b ;  n  a  a  4  8  4  .n  cid:0  2  C ‚Äö.n   since we can choose a large enough so that a P .n  D O.n lg n .  4 n dominates ‚Äö.n  C b. Thus,  f. We draw an analogy between inserting an element into a subtree of a binary search tree and sorting a subarray in quicksort. Observe that once an element x is chosen as the root of a subtree T , all elements that will be inserted after x into T will be compared to x. Similarly, observe that once an element y is chosen as the pivot in a subarray S, all other elements in S will be compared to y. Therefore, the quicksort implementation in which the comparisons are the same as those made when inserting into a binary search tree is simply to consider the pivots in the same order as the order in which the elements are inserted into the tree.   Lecture Notes for Chapter 13: Red-Black Trees  Chapter 13 overview  Red-black trees  Red-black trees   A variation of binary search trees.  Balanced: height is O.lg n , where n is the number of nodes.  Operations will take O.lg n  time in the worst case.  [Thesenotes areabitsimpler than the treatment inthebook, tomakethem more amenable to a lecture situation. Our students Ô¨Årst see red-black trees in a course that precedes our algorithms course. This set of lecture notes is intended as a refresher for the students, bearing in mind that some time may have passed since theylastsawred-blacktrees. Theproceduresinthischapterareratherlongsequencesofpseudocode. Youmight wanttomakearrangementstoprojectthemratherthanspendingtimewritingthem onaboard.]  A red-black tree is a binary search tree + 1 bit per node: an attribute color, which is either red or black. All leaves are empty  nil  and colored black.   We use a single sentinel, T:nil, for all the leaves of red-black tree T .  T:nil:color is black.  The root‚Äôs parent is also T:nil.  All other attributes of binary search trees are inherited by red-black trees  key, left, right, and p . We don‚Äôt care about the key in T:nil.  Red-black properties  [Leavetheseupontheboard.]   13-2  Lecture Notes for Chapter 13: Red-Black Trees  1. Every node is either red or black. 2. The root is black. 3. Every leaf  T:nil  is black. 4. If a node is red, then both its children are black.  Hence no two reds in a row  on a simple path from the root to a leaf.   5. For each node, all paths from the node to descendant leaves contain the same  number of black nodes.  Example:  26  h = 4 bh = 2  17  h = 1 bh = 1  41  h = 3 bh = 2  30  h = 2 bh = 1  47  h = 2 bh = 1  38  h = 1 bh = 1  50  h = 1 bh = 1  T.nil  [Nodeswithboldoutlineindicateblacknodes. Don‚Äôtaddheightsandblack-heights yet. Wewon‚Äôtbotherwithdrawing T:nil anymore.]  Height of a red-black tree   Height of a node is the number of edges in a longest path to a leaf.  Black-height of a node x: bh.x  is the number of black nodes  including T:nil  on the path from x to leaf, not counting x. By property 5, black-height is well deÔ¨Åned.  [Nowlabeltheexampletreewithheight h andbh values.]  Claim Any node with height h has black-height  h=2. Proof By property 4,  h=2 nodes on the path from the node to a leaf are red. Hence  h=2 are black.  claim  Claim The subtree rooted at any node x contains  2bh.x   cid:0  1 internal nodes.   Lecture Notes for Chapter 13: Red-Black Trees  13-3  Proof By induction on height of x. Basis: Height of x D 0   x is a leaf   bh.x  D 0. The subtree rooted at x has 0 internal nodes. 20  cid:0  1 D 0. Inductive step: Let the height of x be h and bh.x  D b. Any child of x has height h  cid:0  1 and black-height either b  if the child is red  or b  cid:0  1  if the child is black . By the inductive hypothesis, each child has  2bh.x  cid:0 1  cid:0  1 internal nodes. Thus, the subtree rooted at x contains  2  .2bh.x  cid:0 1  cid:0  1  C 1 D 2bh.x   cid:0  1 internal  claim  nodes.  The C1 is for x itself.  Lemma A red-black tree with n internal nodes has height  2 lg.n C 1 . Proof Let h and b be the height and black-height of the root, respectively. By the above two claims, n  2b  cid:0  1  2h=2  cid:0  1 : Adding 1 to both sides and then taking logs gives lg.n C 1   h=2, which implies  theorem  that h  2 lg.n C 1 .  Operations on red-black trees  The non-modifying binary-search-tree operations MINIMUM, MAXIMUM, SUC- CESSOR, PREDECESSOR, and SEARCH run in O.height  time. Thus, they take O.lg n  time on red-black trees. Insertion and deletion are not so easy. If we insert, what color to make the new node?   Red? Might violate property 4.  Black? Might violate property 5.  If we delete, thus removing a node, what color was the node that was removed?   Red? OK, since we won‚Äôt have changed any black-heights, nor will we have created two red nodes in a row. Also, cannot cause a violation of property 2, since if the removed node was red, it could not have been the root.   Black? Could cause there to be two reds in a row  violating property 4 , and can also cause a violation of property 5. Could also cause a violation of prop- erty 2, if the removed node was the root and its child‚Äîwhich becomes the new root‚Äîwas red.  Rotations   The basic tree-restructuring operation.  Needed to maintain red-black trees as balanced binary search trees.  Changes the local pointer structure.  Only pointers are changed.    13-4  Lecture Notes for Chapter 13: Red-Black Trees   Won‚Äôt upset the binary-search-tree property.  Have both left rotation and right rotation. They are inverses of each other.  A rotation takes a red-black-tree and a node within the tree.     set y    turn y‚Äôs left subtree into x‚Äôs right subtree  y  g  x  b  a  LEFT-ROTATE T, x   RIGHT-ROTATE T, y   a  x  b  y  g     link x‚Äôs parent to y  LEFT-ROTATE.T; x  y D x:right x:right D y:left if y:left ¬§ T:nil y:left:p D x y:p D x:p if x:p == T:nil T:root D y elseif x == x:p:left x:p:left D y else x:p:right D y y:left D x x:p D y The pseudocode for LEFT-ROTATE assumes that  x:right ¬§ T:nil, and root‚Äôs parent is T:nil.     put x on y‚Äôs left    Pseudocode for RIGHT-ROTATE is symmetric: exchange left and right everywhere.  Example [Usetodemonstrate thatrotationmaintainsinorderorderingofkeys. Nodecolors omitted.]  LEFT-ROTATE T, x   11  x  9  18  y  14  19  17  22  7  7  4  4  x  11  19  9  14  22  18  y  17   Lecture Notes for Chapter 13: Red-Black Trees  13-5  keys of y‚Äôs right subtree.   Before rotation: keys of x‚Äôs left subtree  11  keys of y‚Äôs left subtree  18   Rotation makes y‚Äôs left subtree into x‚Äôs right subtree.  After rotation: keys of x‚Äôs left subtree  11  keys of x‚Äôs right subtree  18   keys of y‚Äôs right subtree.  Time O.1  for both LEFT-ROTATE and RIGHT-ROTATE, since a constant number of pointers are modiÔ¨Åed.  Notes  Rotation is a very basic operation, also used in AVL trees and splay trees.  Some books talk of rotating on an edge rather than on a node.  Insertion  Start by doing regular binary-search-tree insertion:  RB-INSERT.T; ¬¥  y D T:nil x D T:root while x ¬§ T:nil y D x if ¬¥:key < x:key x D x:left else x D x:right  ¬¥:p D y if y == T:nil  elseif ¬¥:key < y:key  T:root D ¬¥ y:left D ¬¥ else y:right D ¬¥ ¬¥:left D T:nil ¬¥:right D T:nil ¬¥:color D RED RB-INSERT-FIXUP.T; ¬¥    RB-INSERT ends by coloring the new node ¬¥ red.  Then it calls RB-INSERT-FIXUP because we could have violated a red-black  property.  1. OK.  Which property might be violated?   13-6  Lecture Notes for Chapter 13: Red-Black Trees  2. If ¬¥ is the root, then there‚Äôs a violation. Otherwise, OK. 3. OK. 4. If ¬¥:p is red, there‚Äôs a violation: both ¬¥ and ¬¥:p are red. 5. OK.  Remove the violation by calling RB-INSERT-FIXUP:  RB-INSERT-FIXUP .T; ¬¥  while ¬¥:p:color == RED if ¬¥:p == ¬¥:p:p:left y D ¬¥:p:p:right if y:color == RED  ¬¥:p:color D BLACK y:color D BLACK ¬¥:p:p:color D RED ¬¥ D ¬¥:p:p  else if ¬¥ == ¬¥:p:right  ¬¥ D ¬¥:p LEFT-ROTATE.T; ¬¥   ¬¥:p:color D BLACK ¬¥:p:p:color D RED RIGHT-ROTATE.T; ¬¥:p:p      case 1    case 1    case 1    case 1     case 2    case 2    case 3    case 3    case 3  else  same as then clause with ‚Äúright‚Äù and ‚Äúleft‚Äù exchanged   T:root:color D BLACK Loop invariant: At the start of each iteration of the while loop,  a. ¬¥ is red. b. There is at most one red-black violation:   Property 2: ¬¥ is a red root, or  Property 4: ¬¥ and ¬¥:p are both red.  [Thebookhasathirdpartoftheloopinvariant,butweomititforlecture.]  Initialization: We‚Äôve already seen why the loop invariant holds initially. Termination: The loop terminates because ¬¥:p is black. Hence, property 4 is OK.  Only property 2 might be violated, and the last line Ô¨Åxes it.  Maintenance: We drop out when ¬¥ is the root  since then ¬¥:p is the sentinel T:nil, which is black . When we start the loop body, the only violation is of property 4. There are 6 cases, 3 of which are symmetric to the other 3. The cases are not mutually exclusive. We‚Äôll consider cases in which ¬¥:p is a left child. Let y be ¬¥‚Äôs uncle  ¬¥:p‚Äôs sibling .   Lecture Notes for Chapter 13: Red-Black Trees  13-7  Case 1: y is red  A  a  B  z  g  b  B  g  z  A  a  b  C  C  If z is a right child  D  y e  D  y e  d  d  If z is a left child  new z  C  A  a  B  b  g  new z  C  B  g  A  b  a  D  d  e  D  e  d   ¬¥:p:p  ¬¥‚Äôs grandparent  must be black, since ¬¥ and ¬¥:p are both red and  there are no other violations of property 4.  might now be violated.   Make ¬¥:p and y black   now ¬¥ and ¬¥:p are not both red. But property 5  Make ¬¥:p:p red   restores property 5.  The next iteration has ¬¥:p:p as the new ¬¥  i.e., ¬¥ moves up 2 levels .  Case 2: y is black, ¬¥ is a right child  C  d  y  A  a  B  z  g  b  Case 2  z  A  a  b  B  g  Case 3  C  d  y  B  g  C d  z A  a  b   Left rotate around ¬¥:p   now ¬¥ is a left child, and both ¬¥ and ¬¥:p are  Takes us immediately to case 3.  red.  Case 3: y is black, ¬¥ is a left child   Make ¬¥:p black and ¬¥:p:p red.  Then right rotate on ¬¥:p:p.  No longer have 2 reds in a row.  ¬¥:p is now black   no more iterations.  Analysis  O.lg n  time to get through RB-INSERT up to the call of RB-INSERT-FIXUP.   13-8  Lecture Notes for Chapter 13: Red-Black Trees  Within RB-INSERT-FIXUP:   Each iteration takes O.1  time.  Each iteration is either the last one or it moves ¬¥ up 2 levels.  O.lg n  levels   O.lg n  time.  Also note that there are at most 2 rotations overall.  Thus, insertion into a red-black tree takes O.lg n  time.  Deletion  [Because deletion from a binary search tree changed in the third edition, so did deletion from a red-black tree. As with deletion from a binary search tree, the node ¬¥ deleted from a red-black tree is always the node ¬¥ passed to the deletion procedure.] Based on the TREE-DELETE procedure for binary search trees:  RB-DELETE.T; ¬¥  y D ¬¥ y-original-color D y:color if ¬¥:left == T:nil x D ¬¥:right RB-TRANSPLANT.T; ¬¥; ¬¥:right   elseif ¬¥:right == T:nil  x D ¬¥:left RB-TRANSPLANT.T; ¬¥; ¬¥:left  else y D TREE-MINIMUM.¬¥:right   y-original-color D y:color x D y:right if y:p == ¬¥ x:p D y y:right D ¬¥:right y:right:p D y  RB-TRANSPLANT.T; ¬¥; y  y:left D ¬¥:left y:left:p D y y:color D ¬¥:color RB-DELETE-FIXUP .T; x   if y-original-color == BLACK  else RB-TRANSPLANT.T; y; y:right   RB-DELETE calls a special version of TRANSPLANT  used in deletion from binary search trees , customized for red-black trees:   Lecture Notes for Chapter 13: Red-Black Trees  13-9  RB-TRANSPLANT.T; u;    if u:p == T:nil T:root D  elseif u == u:p:left u:p:left D  else u:p:right D  :p D u:p Differences between RB-TRANSPLANT and TRANSPLANT:   RB-TRANSPLANT references the sentinel T:nil instead of NIL.  Assignment to :p occurs even if  points to the sentinel. In fact, we exploit the  ability to assign to :p when  points to the sentinel.  RB-DELETE has almost twice as many lines as TREE-DELETE, but you can Ô¨Ånd each line of TREE-DELETE within RB-DELETE  with NIL replaced by T:nil and calls to TRANSPLANT replaced by calls to RB-TRANSPLANT . Differences between RB-DELETE and TREE-DELETE:   y is the node either removed from the tree  when ¬¥ has fewer than 2 children   or moved within the tree  when ¬¥ has 2 children .   Need to save y‚Äôs original color  in y-original-color  to test it at the end, because if it‚Äôs black, then removing or moving y could cause red-black properties to be violated.   x is the node that moves into y‚Äôs original position. It‚Äôs either y‚Äôs only child, or  T:nil if y has no children.   Sets x:p to point to the original position of y‚Äôs parent, even if x D T:nil. x:p  is set in one of two ways:      If ¬¥ is not y‚Äôs original parent, x:p is set in the last line of RB-TRANSPLANT. If ¬¥ is y‚Äôs original parent, then y will move up to take ¬¥‚Äôs position in the tree. The assignment x:p D y makes x:p point to the original position of y‚Äôs parent, even if x is T:nil.    If y‚Äôs original color was black, the changes to the tree structure might cause red-black properties to be violated, and we call RB-DELETE-FIXUP at the end to resolve the violations.  If y was originally black, what violations of red-black properties could arise?  1. No violation. 2. If y is the root and x is red, then the root has become red. 3. No violation. 4. Violation if x:p and x are both red. 5. Any simple path containing y now has 1 fewer black node.   Correct by giving x an ‚Äúextra black.‚Äù  Add 1 to count of black nodes on paths containing x.  Now property 5 is OK, but property 1 is not.   13-10  Lecture Notes for Chapter 13: Red-Black Trees   x is either doubly black  if x:color D BLACK  or red & black  if x:color D  The attribute x:color is still either RED or BLACK. No new values for color  RED .    attribute. In other words, the extra blackness on a node is by virtue of x pointing to the node.  Remove the violations by calling RB-DELETE-FIXUP:  if w:left:color == BLACK and w:right:color == BLACK  RB-DELETE-FIXUP .T; x  while x ¬§ T:root and x:color == BLACK  if x == x:p:left  w D x:p:right if w:color == RED  w:color D BLACK x:p:color D RED LEFT-ROTATE.T; x:p  w D x:p:right w:color D RED x D x:p  else if w:right:color == BLACK w:left:color D BLACK w:color D RED RIGHT-ROTATE.T; w  w D x:p:right w:color D x:p:color x:p:color D BLACK w:right:color D BLACK LEFT-ROTATE.T; x:p  x D T:root     case 1    case 1    case 1    case 1     case 2    case 2     case 3    case 3    case 3    case 3    case 4    case 4    case 4    case 4    case 4  else  same as then clause with ‚Äúright‚Äù and ‚Äúleft‚Äù exchanged   x:color D BLACK  Idea Move the extra black up the tree until  x points to a red & black node   turn it into a black node,  x points to the root   just remove the extra black, or  we can do certain rotations and recolorings and Ô¨Ånish.  Within the while loop:   x always points to a nonroot doubly black node.  w is x‚Äôs sibling.  w cannot be T:nil, since that would violate property 5 at x:p.  There are 8 cases, 4 of which are symmetric to the other 4. As with insertion, the cases are not mutually exclusive. We‚Äôll look at cases in which x is a left child.   Lecture Notes for Chapter 13: Red-Black Trees  13-11  Case 1: w is red  x  A  a  b  B  C  D  w  E z  g  d  e  Case 1  D  E  e  z  B  g  x  A  new w  a  b  C d   w must have black children.  Make w black and x:p red.  Then left rotate on x:p.  New sibling of x was a child of w before rotation   must be black.  Go immediately to case 2, 3, or 4.  Case 2: w is black and both of w‚Äôs children are black  x  A  a  b  B  c  C  D  w  E z  g  d  e  Case 2  new x  B  c  A  b  a  D  C  g  d  e  E z  [Nodewithgrayoutlineisofunknowncolor,denotedby c.]  Take 1 black off x    singly black  and off w    red .  Move that black to x:p.  Do the next iteration with x:p as the new x.    If entered this case from case 1, then x:p was red   new x is red & black   color attribute of new x is RED   loop terminates. Then new x is made black in the last line.  Case 3: w is black, w‚Äôs left child is red, and w‚Äôs right child is black  x  A  a  b  B  c  C  D  w  E z  g  d  e  Case 3  B  c  g  x  A  a  b  C  new w  d  D  e  E  z   Make w red and w‚Äôs left child black.  Then right rotate on w.  New sibling w of x is black with a red right child   case 4.   13-12  Lecture Notes for Chapter 13: Red-Black Trees  Case 4: w is black, w‚Äôs left child is black, and w‚Äôs right child is red  x  A  a  b  B  c  D  w  C  c¬¢  d  e  g  E z  Case 4  D  c  B  A  b  a  C  c¬¢  e  g  d  E  z  new x = T.root  [Nowtherearetwonodesofunknowncolors,denotedby c and c0.]   Make w be x:p‚Äôs color  c .  Make x:p black and w‚Äôs right child black.  Then left rotate on x:p.  Remove the extra black on x    x is now singly black  without violating  All done. Setting x to root causes the loop to terminate.  any red-black properties.  Analysis  O.lg n  time to get through RB-DELETE up to the call of RB-DELETE-FIXUP. Within RB-DELETE-FIXUP:   Case 2 is the only case in which more iterations occur.   x moves up 1 level.  Hence, O.lg n  iterations.   Each of cases 1, 3, and 4 has 1 rotation    3 rotations in all.  Hence, O.lg n  time.  [InChapter14,we‚Äôllseeatheoremthatreliesonred-blacktreeoperationscausing at most a constant number of rotations. This is where red-black trees enjoy an advantage overAVLtrees: intheworstcase, anoperation onan n-nodeAVLtree causes .lg n  rotations.]   Solutions for Chapter 13: Red-Black Trees  If we color the root of a relaxed red-black tree black but make no other changes, the resulting tree is a red-black tree. Not even any black-heights change.  Solution to Exercise 13.1-3  Solution to Exercise 13.1-4 This solution is also posted publicly  After absorbing each red node into its black parent, the degree of each node black node is        2, if both children were already black, 3, if one child was black and one was red, or 4, if both children were red.  All leaves of the resulting tree have the same depth.  Solution to Exercise 13.1-5 This solution is also posted publicly  In the longest path, at least every other node is black. In the shortest path, at most every node is black. Since the two paths contain equal numbers of black nodes, the length of the longest path is at most twice the length of the shortest path. We can say this more precisely, as follows: Since every path contains bh.x  black nodes, even the shortest path from x to a descendant leaf has length at least bh.x . By deÔ¨Ånition, the longest path from x to a descendant leaf has length height.x . Since the longest path has bh.x  black nodes and at least half the nodes on the longest path are black  by property 4 , bh.x   height.x =2, so length of longest path D height.x   2  bh.x   twice length of shortest path :   13-14  Solutions for Chapter 13: Red-Black Trees  Solution to Exercise 13.2-4  Since the exercise asks about binary search trees rather than the more speciÔ¨Åc red- black trees, we assume here that leaves are full-Ô¨Çedged nodes, and we ignore the sentinels. Taking the book‚Äôs hint, we start by showing that with at most n  cid:0  1 right rotations, we can convert any binary search tree into one that is just a right-going chain. The idea is simple. Let us deÔ¨Åne the right spine as the root and all descendants of the root that are reachable by following only right pointers from the root. A binary search tree that is just a right-going chain has all n nodes in the right spine. As long as the tree is not just a right spine, repeatedly Ô¨Ånd some node y on the right spine that has a non-leaf left child x and then perform a right rotation on y:  RIGHT-ROTATE T, y   y  g  x  b  a  a  x  b  y  g   In the above Ô¨Ågure, note that any of Àõ, Àá, and  can be an empty subtree.  Observe that this right rotation adds x to the right spine, and no other nodes leave the right spine. Thus, this right rotation increases the number of nodes in the right spine by 1. Any binary search tree starts out with at least one node‚Äîthe root‚Äîin the right spine. Moreover, if there are any nodes not on the right spine, then at least one such node has a parent on the right spine. Thus, at most n  cid:0  1 right rotations are needed to put all nodes in the right spine, so that the tree consists of a single right-going chain. If we knew the sequence of right rotations that transforms an arbitrary binary search tree T to a single right-going chain T 0, then we could perform this sequence in reverse‚Äîturning each right rotation into its inverse left rotation‚Äîto transform T 0 back into T . Therefore, here is how we can transform any binary search tree T1 into any other binary search tree T2. Let T 0 be the unique right-going chain consist- ing of the nodes of T1  which is the same as the nodes of T2 . Let r D hr1; r2; : : : ; rki be a sequence of right rotations that transforms T1 to T 0, and let r0 D hr01; r02; : : : ; r0k0i be a sequence of right rotations that transforms T2 to T 0. We know that there exist sequences r and r0 with k; k0  n  cid:0  1. For each right rotation r0i, let l0i be the corresponding inverse left rotation. Then the sequence hr1; r2; : : : ; rk; l0k0 ; l0k0 cid:0 1; : : : ; l02; l01i transforms T1 to T2 in at most 2n  cid:0  2 rotations.  Solution to Exercise 13.3-3 This solution is also posted publicly  In Figure 13.5, nodes A, B, and D have black-height k C 1 in all cases, because each of their subtrees has black-height k and a black root. Node C has black-   Solutions for Chapter 13: Red-Black Trees  13-15  height k C 1 on the left  because its red children have black-height k C 1  and black-height kC2 on the right  because its black children have black-height kC1 .   a   A  k+1 a  y  D  k+1 e  A  k+1 a  D  k+1 e  d  k+1  C  z  B  k+1  b  g  k+1  C  d  d  k+2  C  B  k+1  b  g  k+2  C   b   k+1  B  k+1 a  A  z  b  g  y  D  k+1 e  k+1  B  g  k+1 a  A  b  D  k+1  d  e  In Figure 13.6, nodes A, B, and C have black-height k C 1 in all cases. At left and in the middle, each of A‚Äôs and B‚Äôs subtrees has black-height k and a black root, while C has one such subtree and a red child with black-height k C 1. At the right, each of A‚Äôs and C ‚Äôs subtrees has black-height k and a black root, while B‚Äôs red children each have black-height k C 1.  k+1  C  k+1  A  a  d  y  k+1  B  z  B  k+1  b  g  Case 2  g  k+1 a  A  z  b  Case 3  k+1  C  d  y  B  k+1  k+1 a  A  b  k+1  C d  g  Property 5 is preserved by the transformations. We have shown above that the black-height is well-deÔ¨Åned within the subtrees pictured, so property 5 is preserved within those subtrees. Property 5 is preserved for the tree containing the subtrees pictured, because every path through these subtrees to a leaf contributes kC2 black nodes.  Solution to Exercise 13.3-4  Colors are set to red only in cases 1 and 3, and in both situations, it is ¬¥:p:p that is reddened. If ¬¥:p:p is the sentinel, then ¬¥:p is the root. By part  b  of the loop invariant and line 1 of RB-INSERT-FIXUP, if ¬¥:p is the root, then we have dropped out of the loop. The only subtlety is in case 2, where we set ¬¥ D ¬¥:p before coloring ¬¥:p:p red. Because we rotate before the recoloring, the identity of ¬¥:p:p is the same before and after case 2, so there‚Äôs no problem.   13-16  Solutions for Chapter 13: Red-Black Trees  Solution to Exercise 13.4-6  Case 1 occurs only if x‚Äôs sibling w is red. If x:p were red, then there would be two reds in a row, namely x:p  which is also w:p  and w, and we would have had these two reds in a row even before calling RB-DELETE.  Solution to Exercise 13.4-7  No, the red-black tree will not necessarily be the same. Here are two examples: one in which the tree‚Äôs shape changes, and one in which the shape remains the same but the node colors change.  3  3  2  2  insert 1  delete 1  2  3  3  4  1  2  2  3  3  4  2  insert 1  delete 1  4  1  Solution to Problem 13-1 This solution is also posted publicly  a. When inserting key k, all nodes on the path from the root to the added node  a new leaf  must change, since the need for a new child pointer propagates up from the new node to all of its ancestors. When deleting a node, let y be the node actually removed and ¬¥ be the node given to the delete procedure.      If ¬¥ has at most one child, it will be spliced out, so that all ancestors of ¬¥ will be changed.  As with insertion, the need for a new child pointer propagates up from the removed node.  If ¬¥ has two children, then its successor y will be spliced out and moved to ¬¥‚Äôs position. Therefore all ancestors of both ¬¥ and y must be changed. Because ¬¥ is an ancestor of y, we can just say that all ancestors of y must be changed.  In either case, y‚Äôs children  if any  are unchanged, because we have assumed that there is no parent attribute.   Solutions for Chapter 13: Red-Black Trees  13-17  b. We assume that we can call two procedures:   MAKE-NEW-NODE.k  creates a new node whose key attribute has value k and with left and right attributes NIL, and it returns a pointer to the new node.  COPY-NODE.x  creates a new node whose key, left, and right attributes have the same values as those of node x, and it returns a pointer to the new node.  Here are two ways to write PERSISTENT-TREE-INSERT. The Ô¨Årst is a version of TREE-INSERT, modiÔ¨Åed to create new nodes along the path to where the new node will go, and to not use parent attributes. It returns the root of the new tree.  PERSISTENT-TREE-INSERT .T; k  ¬¥ D MAKE-NEW-NODE.k  new-root D COPY-NODE.T:root  y D NIL x D new-root while x ¬§ NIL  y D x if ¬¥:key < x:key  x D COPY-NODE.x:left  y:left D x else x D COPY-NODE.x:right  y:right D x  if y == NIL  new-root D ¬¥ elseif ¬¥:key < y:key y:left D ¬¥ else y:right D ¬¥ return new-root  PERSISTENT-TREE-INSERT .r; k   if r == NIL  x D MAKE-NEW-NODE.k  if k < r:key  else x D COPY-NODE.r   The second is a rather elegant recursive procedure. The initial call should have T:root as its Ô¨Årst argument. It returns the root of the new tree.  x:left D PERSISTENT-TREE-INSERT.r:left; k   else x:right D PERSISTENT-TREE-INSERT.r:right; k   return x  c. Like TREE-INSERT, PERSISTENT-TREE-INSERT does a constant amount of work at each node along the path from the root to the new node. Since the length of the path is at most h, it takes O.h  time. Since it allocates a new node  a constant amount of space  for each ancestor of the inserted node, it also needs O.h  space.   13-18  Solutions for Chapter 13: Red-Black Trees  d. If there were parent attributes, then because of the new root, every node of the tree would have to be copied when a new node is inserted. To see why, observe that the children of the root would change to point to the new root, then their children would change to point to them, and so on. Since there are n nodes, this change would cause insertion to create .n  new nodes and to take .n  time.  e. From parts  a  and  c , we know that insertion into a persistent binary search tree of height h, like insertion into an ordinary binary search tree, takes worst- case time O.h . A red-black tree has h D O.lg n , so insertion into an ordinary red-black tree takes O.lg n  time. We need to show that if the red-black tree is persistent, insertion can still be done in O.lg n  time. To do this, we will need to show two things:   How to still Ô¨Ånd the parent pointers we need in O.1  time without using a parent attribute. We cannot use a parent attribute because a persistent tree with parent attributes uses .n  time for insertion  by part  d  .   That the additional node changes made during red-black tree operations  by rotation and recoloring  don‚Äôt cause more than O.lg n  additional nodes to change.  Each parent pointer needed during insertion can be found in O.1  time without having a parent attribute as follows: To insert into a red-black tree, we call RB-INSERT, which in turn calls RB- INSERT-FIXUP. Make the same changes to RB-INSERT as we made to TREE- INSERT for persistence. Additionally, as RB-INSERT walks down the tree to Ô¨Ånd the place to insert the new node, have it build a stack of the nodes it tra- verses and pass this stack to RB-INSERT-FIXUP. RB-INSERT-FIXUP needs parent pointers to walk back up the same path, and at any given time it needs parent pointers only to Ô¨Ånd the parent and grandparent of the node it is working on. As RB-INSERT-FIXUP moves up the stack of parents, it needs only parent pointers that are at known locations a constant distance away in the stack. Thus, the parent information can be found in O.1  time, just as if it were stored in a parent attribute. Rotation and recoloring change nodes as follows:   RB-INSERT-FIXUP performs at most 2 rotations, and each rotation changes the child pointers in 3 nodes  the node around which we rotate, that node‚Äôs parent, and one of the children of the node around which we rotate . Thus, at most 6 nodes are directly modiÔ¨Åed by rotation during RB-INSERT-FIXUP. In a persistent tree, all ancestors of a changed node are copied, so RB-INSERT- FIXUP‚Äôs rotations take O.lg n  time to change nodes due to rotation.  Ac- tually, the changed nodes in this case share a single O.lg n -length path of ancestors.    RB-INSERT-FIXUP recolors some of the inserted node‚Äôs ancestors, which are being changed anyway in persistent insertion, and some children of an- cestors  the ‚Äúuncles‚Äù referred to in the algorithm description . There are at most O.lg n  ancestors, hence at most O.lg n  color changes of uncles. Recoloring uncles doesn‚Äôt cause any additional node changes due to persis- tence, because the ancestors of the uncles are the same nodes  ancestors of   Solutions for Chapter 13: Red-Black Trees  13-19  the inserted node  that are being changed anyway due to persistence. Thus, recoloring does not affect the O.lg n  running time, even with persistence.  We could show similarly that deletion in a persistent tree also takes worst-case time O.h .   We already saw in part  a  that O.h  nodes change.  We could write a persistent RB-DELETE procedure that runs in O.h  time, analogous to the changes we made for persistence in insertion. But to do so without using parent pointers we need to walk down the tree to the node to be deleted, to build up a stack of parents as discussed above for insertion. This is a little tricky if the set‚Äôs keys are not distinct, because in order to Ô¨Ånd the path to the node to delete‚Äîa particular node with a given key‚Äîwe have to make some changes to how we store things in the tree, so that duplicate keys can be distinguished. The easiest way is to have each key take a second part that is unique, and to use this second part as a tiebreaker when comparing keys.  Then the problem of showing that deletion needs only O.lg n  time in a persis- tent red-black tree is the same as for insertion.   As for insertion, we can show that the parents needed by RB-DELETE- FIXUP can be found in O.1  time  using the same technique as for insertion .  Also, RB-DELETE-FIXUP performs at most 3 rotations, which as discussed above for insertion requires O.lg n  time to change nodes due to persistence. It also does O.lg n  color changes, which  as for insertion  take only O.lg n  time to change ancestors due to persistence, because the number of copied nodes is O.lg n .   Lecture Notes for Chapter 14: Augmenting Data Structures  Chapter 14 overview  We‚Äôll be looking at methods for designing algorithms. In some cases, the design will be intermixed with analysis. In other cases, the analysis is easy, and it‚Äôs the design that‚Äôs harder.  Augmenting data structures      It‚Äôs unusual to have to design an all-new data structure from scratch. It‚Äôs more common to take a data structure that you know and store additional information in it.   With the new information, the data structure can support new operations.  But you have to Ô¨Ågure out how to correctly maintain the new information with-  out loss of efÔ¨Åciency.  We‚Äôll look at a couple of situations in which we augment red-black trees.  Dynamic order statistics  We want to support the usual dynamic-set operations from R-B trees, plus:   OS-SELECT.x; i  : return pointer to node containing the ith smallest key of the   OS-RANK.T; x : return the rank of x in the linear order determined by an  subtree rooted at x.  inorder walk of T .  Augment by storing in each node x: x:size D  of nodes in subtree rooted at x :    Includes x itself.   Does not include leaves  sentinels . DeÔ¨Åne for sentinel T:nil:size D 0. Then x:size D x:left:size C x:right:size C 1.   14-2  Lecture Notes for Chapter 14: Augmenting Data Structures  i=5 r=2  R  M C  5 8  M A  8 1  B  i=5 r=6  B  M M  8 8  B  i=3 r=2  M F  8 3  R  M D  8 1  R  i=1 r=1  M H  8 1  B  M P  2 8  R  M Q  8 1  [Example above: Ignore colors, butlegalcoloring shownwith‚ÄúR‚Äùand‚ÄúB‚Äùnota- tions. Valuesof i and r arefortheexamplebelow.] Note: OK for keys to not be distinct. Rank is deÔ¨Åned with respect to position in inorder walk. So if we changed D to C, rank of original C is 2, rank of D changed to C is 3.  OS-SELECT.x; i   r D x:left:size C 1 if i == r  return x  elseif i < r  return OS-SELECT.x:left; i    else return OS-SELECT.x:right; i  cid:0  r  Initial call: OS-SELECT.T:root; i   Try OS-SELECT.T:root; 5 . [Values shown inÔ¨Ågure above. Returns node whose keyisH.]  Correctness r D rank of x within subtree rooted at x.        If i D r, then we want x. If i < r, then ith smallest element is in x‚Äôs left subtree, and we want the ith smallest element in the subtree. If i > r, then ith smallest element is in x‚Äôs right subtree, but subtract off the r elements in x‚Äôs subtree that precede those in x‚Äôs right subtree.   Like the randomized SELECT algorithm.  Analysis Each recursive call goes down one level. Since R-B tree has O.lg n  levels, have O.lg n  calls   O.lg n  time.   Lecture Notes for Chapter 14: Augmenting Data Structures  14-3  OS-RANK.T; x  r D x:left:size C 1 y D x while y ¬§ T:root  if y == y:p:right  y D y:p  return r  Demo: Node D. Why does this work?  r D r C y:p:left:size C 1  Loop invariant: At start of each iteration of while loop, r D rank of x:key in subtree rooted at y.  Initialization: Initially, r D rank of x:key in subtree rooted at x, and y D x. Termination: Loop terminates when y D T:root   subtree rooted at y is entire tree. Therefore, r D rank of x:key in entire tree. Maintenance: At end of each iteration, set y D y:p. So, show that if r D rank of x:key in subtree rooted at y at start of loop body, then r D rank of x:key in subtree rooted at y:p at end of loop body.  y  x  [r D ofnodesinsubtreerootedat y preceding x ininorderwalk] Must add nodes in y‚Äôs sibling‚Äôs subtree.      If y is a left child, its sibling‚Äôs subtree follows all nodes in y‚Äôs subtree   don‚Äôt change r. If y is a right child, all nodes in y‚Äôs sibling‚Äôs subtree precede all nodes in y‚Äôs subtree   add size of y‚Äôs sibling‚Äôs subtree, plus 1 for y:p, into r.  y.p  y.p.left  y  Analysis y goes up one level in each iteration   O.lg n  time.   14-4  Lecture Notes for Chapter 14: Augmenting Data Structures  Maintaining subtree sizes   Need to maintain size attributes during insert and delete operations.  Need to maintain them efÔ¨Åciently. Otherwise, might have to recompute them  all, at a cost of .n .  Will see how to maintain without increasing O.lg n  time for insert and delete.  Insert  During pass downward, we know that the new node will be a descendant of each node we visit, and only of these nodes. Therefore, increment size attribute of each node visited.   Then there‚Äôs the Ô¨Åxup pass:   Goes up the tree.  Changes colors O.lg n  times.  Performs  2 rotations.   Color changes don‚Äôt affect subtree sizes.  Rotations do!  But we can determine new sizes based on old sizes and sizes of children.  M C  5 8  x  LEFT-ROTATE T, x   y  M F  5 8  M A  8 1  M F  8 3  y  M D  8 1  M H  8 1  x  M C  8 3  M A  8 1  M D  8 1  M H  8 1  y:size D x:size x:size D x:left:size C x:right:size C 1   Similar for right rotation.  Therefore, can update in O.1  time per rotation   O.1  time spent updating size attributes during Ô¨Åxup.  Therefore, O.lg n  to insert.  Delete Also 2 phases:  1. Splice out some node y. 2. Fixup.   Methodology for augmenting a data structure  Lecture Notes for Chapter 14: Augmenting Data Structures  14-5  After splicing out y, traverse a path y ! root, decrementing size in each node on path. O.lg n  time. During Ô¨Åxup, like insertion, only color changes and rotations.   3 rotations   O.1  time spent updating size attributes during Ô¨Åxup.  Therefore, O.lg n  to delete.  Done!  1. Choose an underlying data structure. 2. Determine additional information to maintain. 3. Verify that we can maintain additional information for existing data structure  operations.  4. Develop new operations.  Don‚Äôt need to do these steps in strict order! Usually do a little of each, in parallel. How did we do them for OS trees?  1. R-B tree. 2. x:size. 3. Showed how to maintain size during insert and delete. 4. Developed OS-SELECT and OS-RANK.  Red-black trees are particularly amenable to augmentation.  Theorem Augment a R-B tree with attribute f , where x:f depends only on information in x, x:left, and x:right  including x:left:f and x:right:f  . Then can maintain values of f in all nodes during insert and delete without affecting O.lg n  performance.  Proof Since x:f depends only on x and its children, when we alter information in x, changes propagate only upward  to x:p; x:p:p; x:p:p:p; : : : ; root . Height = O.lg n    O.lg n  updates, at O.1  each.  Insertion Insert a node as child of existing node. Even if can‚Äôt update f on way down, can go up from inserted node to update f . During Ô¨Åxup, only changes come from color changes  no effect on f   and rotations. Each rotation affects f of  3 nodes  x,y, and parent , and can recompute each in O.1  time. Then, if necessary, propagate changes up the tree. Therefore, O.lg n  time per rotation. Since  2 rotations, O.lg n  time to update f during Ô¨Åxup.   14-6  Lecture Notes for Chapter 14: Augmenting Data Structures  Delete Same idea. After splicing out a node, go up from there to update f . Fixup has  3 rotations. O.lg n  per rotation   O.lg n  to update f during Ô¨Åxup.  theorem  For some attributes, can get away with O.1  per rotation. Example: size attribute.  Interval trees  Maintain a set of intervals. For instance, time intervals.  low[i] = 7  high[i] = 10  i=[7,10]  7  10  11  5  4  17  19  8  15  18  21  23  [leaveonboard]  Operations        INTERVAL-INSERT .T; x : x:int already Ô¨Ålled in. INTERVAL-DELETE.T; x  INTERVAL-SEARCH .T; i  : return pointer to a node x in T such that x:int over- laps interval i. Any overlapping node in T is OK. Return pointer to sen- tinel T:nil if no overlapping node in T .  Interval i has i:low, i:high. i and j overlap if and only if i:low  j:high and j:low  i:high.  Go through examples of proper inclusion, overlap without proper inclusion, no overlap.  Another way: i and j don‚Äôt overlap if and only if i:low > j:high or j:low > i:high. [leavethisonboard] Recall the 4-part methodology.  For interval trees  1. Use R-B trees.   Each node x contains interval x:int.  Key is low endpoint  x:int:low .    Inorder walk would list intervals sorted by low endpoint.   Lecture Notes for Chapter 14: Augmenting Data Structures  14-7  2. Each node x contains  x:max D max endpoint value in subtree rooted at x :  int  max  M [21,23] 23 8  M [17,19]  23 8  M [15,18] 18 8  M [4,8] 8  8  M [5,11] 18 8  M [7,10] 10 8  [leaveonboard]  x:max D max8< :  x:int:high ; x:left:max ; x:right:max  3. Maintaining the information.   This is easy‚Äîx:max depends only on:        information in x: x:int:high information in x:left: x:left:max information in x:right: x:right:max   Apply the theorem.  4. Developing new operations.  INTERVAL-SEARCH .T; i   x D T:root while x ¬§ T:nil and i does not overlap x:int if x:left ¬§ T:nil and x:left:max  i:low x D x:left else x D x:right  return x  Examples Search for ≈í14; 16¬ç and ≈í12; 14¬ç.  Time O.lg n .  Could x:left:max > x:right:max? Sure. Position in tree is determined only by low endpoints, not high endpoints.    In fact, can update max on way down during insertion, and in O.1  time per rotation.   14-8  Lecture Notes for Chapter 14: Augmenting Data Structures  Correctness Key idea: need check only 1 of node‚Äôs 2 children.  Theorem If search goes right, then either:   There is an overlap in right subtree, or  There is no overlap in either subtree.  If search goes left, then either:   There is an overlap in left subtree, or  There is no overlap in either subtree.  Proof If search goes right:      If there is an overlap in right subtree, done. If there is no overlap in right, show there is no overlap in left. Went right because  x:left D T:nil   no overlap in left.  OR   x:left:max < i:low   no overlap in left.  i  x.left.max = highest endpoint in left  If search goes left:      If there is an overlap in left subtree, done. If there is no overlap in left, show there is no overlap in right.   Went left because:  i:low  x:left:max  D j:high for some j in left subtree :   Since there is no overlap in left, i and j don‚Äôt overlap.  Refer back to: no overlap if  i:low > j:high or j:low > i:high :   Since i:low  j:high, must have j:low > i:high.  Now consider any interval k in right subtree.  Because keys are low endpoint,  :  j:low   k:low ‚Äû∆í‚Äö‚Ä¶ in right  in left  ‚Äû∆í‚Äö‚Ä¶  Therefore, i:high < j:low  k:low.  Therefore, i:high < k:low.  Therefore, i and k do not overlap.   theorem    Solutions for Chapter 14: Augmenting Data Structures  Solution to Exercise 14.1-5  Given an element x in an n-node order-statistic tree T and a natural number i, the following procedure retrieves the ith successor of x in the linear order of T :  OS-SUCCESSOR.T; x; i   r D OS-RANK.T; x  s D r C i return OS-SELECT.T:root; s   Since OS-RANK and OS-SELECT each take O.lg n  time, so does the procedure OS-SUCCESSOR.  Solution to Exercise 14.1-6  When inserting node ¬¥, we search down the tree for the proper place for ¬¥. For each node x on this path, add 1 to x:rank if ¬¥ is inserted within x‚Äôs left subtree, and leave x:rank unchanged if ¬¥ is inserted within x‚Äôs right subtree. Similarly when deleting, subtract 1 from x:rank whenever the spliced-out node had been in x‚Äôs left subtree. We also need to handle the rotations that occur during the Ô¨Åxup procedures for in- sertion and deletion. Consider a left rotation on node x, where the pre-rotation right child of x is y  so that x becomes y‚Äôs left child after the left rotation . We leave x:rank unchanged, and letting r D y:rank before the rotation, we set y:rank D r C x:rank. Right rotations are handled in an analogous manner.  Solution to Exercise 14.1-7 This solution is also posted publicly  Let A≈í1 : : n¬ç be the array of n distinct numbers. One way to count the inversions is to add up, for each element, the number of larger elements that precede it in the array:   14-10  Solutions for Chapter 14: Augmenting Data Structures   of inversions D  jIn.j  j ;  n  XjD1  where In.j   D fi W i   A≈íj ¬çg. Note that jIn.j  j is related to A≈íj ¬ç‚Äôs rank in the subarray A≈í1 : : j ¬ç because the elements in In.j   are the reason that A≈íj ¬ç is not positioned according to its rank. Let r.j   be the rank of A≈íj ¬ç in A≈í1 : : j ¬ç. Then j D r.j   C jIn.j  j, so we can compute jIn.j  j D j  cid:0  r.j   by inserting A≈í1¬ç; : : : ; A≈ín¬ç into an order-statistic tree and using OS-RANK to Ô¨Ånd the rank of each A≈íj ¬ç in the tree immediately after it is inserted into the tree.  This OS-RANK value is r.j  .  Insertion and OS-RANK each take O.lg n  time, and so the total time for n ele- ments is O.n lg n .  Solution to Exercise 14.2-2 This solution is also posted publicly  Yes, we can maintain black-heights as attributes in the nodes of a red-black tree without affecting the asymptotic performance of the red-black tree operations. We appeal to Theorem 14.1, because the black-height of a node can be computed from the information at the node and its two children. Actually, the black-height can be computed from just one child‚Äôs information: the black-height of a node is the black-height of a red child, or the black height of a black child plus one. The second child does not need to be checked because of property 5 of red-black trees. Within the RB-INSERT-FIXUP and RB-DELETE-FIXUP procedures are color changes, each of which potentially cause O.lg n  black-height changes. Let us show that the color changes of the Ô¨Åxup procedures cause only local black-height changes and thus are constant-time operations. Assume that the black-height of each node x is kept in the attribute x:bh. For RB-INSERT-FIXUP, there are 3 cases to examine.   Solutions for Chapter 14: Augmenting Data Structures  14-11  Case 1: ¬¥‚Äôs uncle is red.  k+1  C  z  B  k+1  b  g  k+1  C  d  d  k+2  C  B  k+1  b  g  k+2  C   a   A  k+1 a  y  D  k+1 e  A  k+1 a  D  k+1 e  d   b   k+1  B  k+1 a  A  z  b  g  y  D  k+1 e  k+1  B  g  k+1 a  A  b  D  k+1  d  e   Before color changes, suppose that all subtrees Àõ; Àá; ; ƒ±;  have the same black-height k with a black root, so that nodes A, B, C , and D have black- heights of k C 1.  After color changes, the only node whose black-height changed is node C . To Ô¨Åx that, add ¬¥:p:p:bh D ¬¥:p:p:bhC 1 after line 7 in RB-INSERT-FIXUP.  Since the number of black nodes between ¬¥:p:p and ¬¥ remains the same, nodes above ¬¥:p:p are not affected by the color change.  Case 2: ¬¥‚Äôs uncle y is black, and ¬¥ is a right child. Case 3: ¬¥0‚Äôs uncle y is black, and ¬¥ is a left child.  k+1  C  k+1  A  a  d  y  k+1  B  z  B  k+1  b  g  Case 2  g  k+1 a  A  z  b  Case 3  k+1  C  d  y  B  k+1  k+1 a  A  b  k+1  C d  g   With subtrees Àõ; Àá; ; ƒ±;  of black-height k, we see that even with color changes and rotations, the black-heights of nodes A, B, and C remain the same  k C 1 .  Thus, RB-INSERT-FIXUP maintains its original O.lg n  time. For RB-DELETE-FIXUP, there are 4 cases to examine.   14-12  Solutions for Chapter 14: Augmenting Data Structures  Case 1: x‚Äôs sibling w is red.  x  A  a  b  B  C  D  w  E z  g  d  e  Case 1  D  E  e  z  B  g  x  A  new w  a  b  C d   Even though case 1 changes colors of nodes and does a rotation, black-   Case 1 changes the structure of the tree, but waits for cases 2, 3, and 4 to  heights are not changed.  deal with the ‚Äúextra black‚Äù on x.  Case 2: x‚Äôs sibling w is black, and both of w‚Äôs children are black.  x  A  a  b  B  c  C  D  w  E z  g  d  e  Case 2  new x  B  c  A  b  a  D  C  g  d  e  E z   w is colored red, and x‚Äôs ‚Äúextra‚Äù black is moved up to x:p.  Now we can add x:p:bh D x:bh after line 10 in RB-DELETE-FIXUP.  This is a constant-time update. Then, keep looping to deal with the extra  black on x:p.  Case 3: x‚Äôs sibling w is black, w‚Äôs left child is red, and w‚Äôs right child is black.  x  A  a  b  B  c  C  D  w  E z  g  d  e  Case 3  B  c  g  x  A  a  b  C  new w  d  D  e  E  z   Regardless of the color changes and rotation of this case, the black-heights  don‚Äôt change.   Case 3 just sets up the structure of the tree, so it can fall correctly into case 4.  Case 4: x‚Äôs sibling w is black, and w‚Äôs right child is red.  x  A  a  b  B  c  D  w  C  c¬¢  d  e  g  E z  Case 4  D  c  B  A  b  a  C  c¬¢  e  g  d  E  z  new x = root[T]   Solutions for Chapter 14: Augmenting Data Structures  14-13   Nodes A, C , and E keep the same subtrees, so their black-heights don‚Äôt   Add these two constant-time assignments in RB-DELETE-FIXUP after  change.  line 20:  x:p:bh D x:bh C 1 x:p:p:bh D x:p:bh C 1   The extra black is taken care of. Loop terminates.  Thus, RB-DELETE-FIXUP maintains its original O.lg n  time. Therefore, we conclude that black-heights of nodes can be maintained as attributes in red-black trees without affecting the asymptotic performance of red-black tree operations. For the second part of the question, no, we cannot maintain node depths without affecting the asymptotic performance of red-black tree operations. The depth of a node depends on the depth of its parent. When the depth of a node changes, the depths of all nodes below it in the tree must be updated. Updating the root node causes n  cid:0  1 other nodes to be updated, which would mean that operations on the tree that change node depths might not run in O.n lg n  time.  As it travels down the tree, INTERVAL-SEARCH Ô¨Årst checks whether current node x overlaps the query interval i and, if it does not, goes down to either the left or right child. If node x overlaps i, and some node in the right subtree overlaps i, but no node in the left subtree overlaps i, then because the keys are low endpoints, this order of checking  Ô¨Årst x, then one child  will return the overlapping interval with the minimum low endpoint. On the other hand, if there is an interval that overlaps i in the left subtree of x, then checking x before the left subtree might cause the procedure to return an interval whose low endpoint is not the minimum of those that overlap i. Therefore, if there is a possibility that the left subtree might contain an interval that overlaps i, we need to check the left subtree Ô¨Årst. If there is no overlap in the left subtree but node x overlaps i, then we return x. We check the right subtree under the same conditions as in INTERVAL-SEARCH: the left subtree cannot contain an interval that overlaps i, and node x does not overlap i, either. Because we might search the left subtree Ô¨Årst, it is easier to write the pseudocode to use a recursive procedure MIN-INTERVAL-SEARCH-FROM.T; x; i  , which returns the node overlapping i with the minimum low endpoint in the subtree rooted at x, or T:nil if there is no such node.  MIN-INTERVAL-SEARCH .T; i    return MIN-INTERVAL-SEARCH-FROM.T; T:root; i    Solution to Exercise 14.3-3   14-14  Solutions for Chapter 14: Augmenting Data Structures  Solution to Exercise 14.3-6  MIN-INTERVAL-SEARCH-FROM.T; x; i   if x:left ¬§ T:nil and x:left:max  i:low  y D MIN-INTERVAL-SEARCH-FROM.T; x:left; i   if y ¬§ T:nil return y elseif i overlaps x:int  return x  else return T:nil elseif i overlaps x:int  return x  else return MIN-INTERVAL-SEARCH-FROM.T; x:right; i    The call MIN-INTERVAL-SEARCH .T; i   takes O.lg n  time, since each recursive call of MIN-INTERVAL-SEARCH-FROM goes one node lower in the tree, and the height of the tree is O.lg n .  1. Underlying data structure:  A red-black tree in which the numbers in the set are stored simply as the keys of the nodes. SEARCH is then just the ordinary TREE-SEARCH for binary search trees, which runs in O.lg n  time on red-black trees.  2. Additional information:  The red-black tree is augmented by the following attributes in each node x:   x:min-gap contains the minimum gap in the subtree rooted at x. It has the magnitude of the difference of the two closest numbers in the subtree rooted at x. If x is a leaf  its children are all T:nil , let x:min-gap D 1.   x:min-al contains the minimum value  key  in the subtree rooted at x.  x:max-al contains the maximum value  key  in the subtree rooted at x.  3. Maintaining the information:  The three attributes added to the tree can each be computed from information in the node and its children. Hence by Theorem 14.1, they can be maintained during insertion and deletion without affecting the O.lg n  running time:  if there is a left subtree ; otherwise ;  if there is a right subtree ; otherwise ;  x:key  x:min-al D  x:left:min-al x:max-al D  x:right:max-al x:min-gap D min‚Äû x:left:min-gap  x:key  x:right:min-gap x:key  cid:0  x:left:max-al x:right:min-al  cid:0  x:key   1 if no left subtree  ;  1 if no right subtree  ;  1 if no left subtree  ;  1 if no right subtree  :   Solution to Exercise 14.3-7 This solution is also posted publicly  Solutions for Chapter 14: Augmenting Data Structures  14-15  In fact, the reason for deÔ¨Åning the min-al and max-al attributes is to make it possible to compute min-gap from information at the node and its children.  4. New operation:  MIN-GAP simply returns the min-gap stored at the tree root. Thus, its running time is O.1 . Note that in addition  not asked for in the exercise , it is possible to Ô¨Ånd the two closest numbers in O.lg n  time. Starting from the root, look for where the minimum gap  the one stored at the root  came from. At each node x, simulate the computation of x:min-gap to Ô¨Ågure out where x:min-gap came from. If it came from a subtree‚Äôs min-gap attribute, continue the search in that subtree. If it came from a computation with x‚Äôs key, then x and that other number are the closest numbers.  General idea: Move a sweep line from left to right, while maintaining the set of rectangles currently intersected by the line in an interval tree. The interval tree will organize all rectangles whose x interval includes the current position of the sweep line, and it will be based on the y intervals of the rectangles, so that any overlapping y intervals in the interval tree correspond to overlapping rectangles. Details: 1. Sort the rectangles by their x-coordinates.  Actually, each rectangle must ap- pear twice in the sorted list‚Äîonce for its left x-coordinate and once for its right x-coordinate.   2. Scan the sorted list  from lowest to highest x-coordinate .   When an x-coordinate of a left edge is found, check whether the rectangle‚Äôs y-coordinate interval overlaps an interval in the tree, and insert the rectangle  keyed on its y-coordinate interval  into the tree.   When an x-coordinate of a right edge is found, delete the rectangle from the  interval tree.  The interval tree always contains the set of ‚Äúopen‚Äù rectangles intersected by the sweep line. If an overlap is ever found in the interval tree, there are overlapping rectangles.  Time: O.n lg n   O.n lg n  to sort the rectangles  we can use merge sort or heap sort .  O.n lg n  for interval-tree operations  insert, delete, and check for overlap .  Solution to Problem 14-1  a. Assume for the purpose of contradiction that there is no point of maximum overlap in an endpoint of a segment. The maximum overlap point p is in the   14-16  Solutions for Chapter 14: Augmenting Data Structures  interior of m segments. Actually, p is in the interior of the intersection of those m segments. Now look at one of the endpoints p0 of the intersection of the m segments. Point p0 has the same overlap as p because it is in the same intersec- tion of m segments, and so p0 is also a point of maximum overlap. Moreover, p0 is in the endpoint of a segment  otherwise the intersection would not end there , which contradicts our assumption that there is no point of maximum overlap in an endpoint of a segment. Thus, there is always a point of maximum overlap which is an endpoint of one of the segments.  b. Keep a balanced binary search tree of the endpoints. That is, to insert an in- terval, we insert its endpoints separately. With each left endpoint e, associate a value p.e  D C1  increasing the overlap by 1 . With each right endpoint e associate a value p.e  D  cid:0 1  decreasing the overlap by 1 . When multiple end- points have the same value, insert all the left endpoints with that value before inserting any of the right endpoints with that value. Here‚Äôs some intuition. Let e1, e2, . . . , en be the sorted sequence of endpoints corresponding to our intervals. Let s.i; j   denote the sum p.ei   C p.eiC1  C  C p.ej   for 1  i  j  n. We wish to Ô¨Ånd an i maximizing s.1; i  . For each node x in the tree, let l.x  and r.x  be the indices in the sorted order of the leftmost and rightmost endpoints, respectively, in the subtree rooted at x. Then the subtree rooted at x contains the endpoints el.x ; el.x C1; : : : ; er.x . Each node x stores three new attributes. We store x: D s.l.x ; r.x  , the sum of the values of all nodes in the subtree rooted at x. We also store x:m, the maximum value obtained by the expression s.l.x ; i   for any i in fl.x ; l.x  C 1; : : : ; r.x g. Finally, we store x:o as the value of i for which x:m achieves its maximum. For the sentinel, we deÔ¨Åne T:nil: D T:nil:m D 0. We can compute these attributes in a bottom-up fashion to satisfy the require- ments of Theorem 14.1: x: D x:left: C p.x  C x:right: ;  x:m D max¬Ä x:left:m  x:left: C p.x  x:left: C p.x  C x:right:m  max is in x‚Äôs right subtree  :   max is in x‚Äôs left subtree  ;  max is at x  ;  Computing x: is straightforward. Computing x:m bears further explanation. Recall that it is the maximum value of the sum of the p values for the nodes in the subtree rooted at x, starting at the node for el.x , which is the leftmost endpoint in x‚Äôs subtree, and ending at any node for ei in x‚Äôs subtree. The endpoint ei that maximizes this sum‚Äîlet‚Äôs call it ei ‚Äîcorresponds to either a node in x‚Äôs left subtree, x itself, or a node in x‚Äôs right subtree. If ei  corresponds to a node in x‚Äôs left subtree, then x:left:m represents a sum starting at the node for el.x  and ending at a node in x‚Äôs left subtree, and hence x:m D x:left:m. If ei  corresponds to x itself, then x:m represents the sum of all p values in x‚Äôs left subtree, plus p.x , so that x:m D x:left: C p.x . Finally, if ei  corresponds to a node in x‚Äôs right subtree, then x:m represents the sum of all p values in x‚Äôs left subtree, plus p.x , plus the sum of some subset of p values in x‚Äôs right subtree. Moreover, the values taken from x‚Äôs right subtree must start from the leftmost endpoint stored in the right subtree. To maximize this sum,   Solution to Problem 14-2  Solutions for Chapter 14: Augmenting Data Structures  14-17  we need to maximize the sum from the right subtree, and that value is precisely x:right:m. Hence, in this case, x:m D x:left: C p.x  C x:right:m. Once we understand how to compute x:m, it is straightforward to compute x:o from the information in x and its two children. Thus, we can implement the operations as follows:    INTERVAL-INSERT: insert two nodes, one for each endpoint of the interval.  FIND-POM: return the interval whose endpoint is represented by T:root:o.   Note that because we are building a binary search tree of all the endpoints and then determining T:root:o, we have no need to delete any nodes from the tree.  Because of how we have deÔ¨Åned the new attributes, Theorem 14.1 says that each operation runs in O.lg n  time. In fact, FIND-POM takes only O.1  time.  a. We use a circular list in which each element has two attributes, key and next. At the beginning, we initialize the list to contain the keys 1; 2; : : : ; n in that order. This initialization takes O.n  time, since there is only a constant amount of work per element  i.e., setting its key and its next attributes . We make the list circular by letting the next attribute of the last element point to the Ô¨Årst element. We then start scanning the list from the beginning. We output and then delete every mth element, until the list becomes empty. The output sequence is the .n; m -Josephus permutation. This process takes O.m  time per element, for a total time of O.mn . Since m is a constant, we get O.mn  D O.n  time, as required.  b. We can use an order-statistic tree, straight out of Section 14.1. Why? Suppose that we are at a particular spot in the permutation, and let‚Äôs say that it‚Äôs the j th largest remaining person. Suppose that there are k  n people remaining. Then we will remove person j , decrement k to reÔ¨Çect having removed this person, and then go on to the .j Cm cid:0 1 th largest remaining person  subtract 1 because we have just removed the j th largest . But that assumes that j C m  k. If not, then we use a little modular arithmetic, as shown below. In detail, we use an order-statistic tree T , and we call the procedures OS- INSERT, OS-DELETE, OS-RANK, and OS-SELECT:   14-18  Solutions for Chapter 14: Augmenting Data Structures  JOSEPHUS.n; m   initialize T to be empty for j D 1 to n  create a node x with x:key == j OS-INSERT.T; x   k D n j D m while k > 2  x D OS-SELECT.T:root; j   print x:key OS-DELETE.T; x  k D k  cid:0  1 j D ..j C m  cid:0  2  mod k  C 1  print OS-SELECT.T:root; 1 :key  JOSEPHUS.n; m   initialize T to be empty for j D 1 to n  create a node x with x:key == j OS-INSERT.T; x   j D 1 for k D n downto 1  j D ..j C m  cid:0  2  mod k  C 1 x D OS-SELECT.T:root; j   print x:key OS-DELETE.T; x   The above procedure is easier to understand. Here‚Äôs a streamlined version:  Either way, it takes O.n lg n  time to build up the order-statistic tree T , and then we make O.n  calls to the order-statistic-tree procedures, each of which takes O.lg n  time. Thus, the total time is O.n lg n .   Lecture Notes for Chapter 15: Dynamic Programming  Dynamic Programming   Not a speciÔ¨Åc algorithm, but a technique  like divide-and-conquer .  Developed back in the day when ‚Äúprogramming‚Äù meant ‚Äútabular method‚Äù  like  linear programming . Doesn‚Äôt really refer to computer programming.   Used for optimization problems:   Find a solution with the optimal value.  Minimization or maximization.  We‚Äôll see both.   Four-step method  1. Characterize the structure of an optimal solution. 2. Recursively deÔ¨Åne the value of an optimal solution. 3. Compute the value of an optimal solution, typically in a bottom-up fashion. 4. Construct an optimal solution from computed information.  Rod cutting  [Newinthethirdeditionofthebook.] How to cut steel rods into pieces in order to maximize the revenue you can get? Each cut is free. Rod lengths are always an integral number of inches. Input: A length n and table of prices pi , for i D 1; 2; : : : ; n. Output: The maximum revenue obtainable for rods whose lengths sum to n, com-  puted as the sum of the prices for the individual rods.  If pn is large enough, an optimal solution might require no cuts, i.e., just leave the rod as n inches long.   15-2  Lecture Notes for Chapter 15: Dynamic Programming  Example: [UsingtheÔ¨Årst 8 valuesfromtheexampleinthebook.] length i price pi  5 10  8 20  7 17  6 17  2 5  1 1  3 8  4 9  Can cut up a rod in 2n cid:0 1 different ways, because can choose to cut or not cut after each of the Ô¨Årst n  cid:0  1 inches. Here are all 8 ways to cut a rod of length 4, with the costs from the example:  9  1  8  5  5  8  1  1  1  5  1  5  1  5  1  1  1  1  1  1  The best way is to cut it into two 2-inch pieces, getting a revenue of p2 C p2 D 5 C 5 D 10. Let ri be the maximum revenue for a rod of length i. Can express a solution as a sum of individual rod lengths. Can determine optimal revenues ri for the example, by inspection:  i 1 2 3 4 5 6 7 8  ri 1 5 8 10 13 17 18 22  optimal solution 1  no cuts  2  no cuts  3  no cuts  2 C 2 2 C 3 6  no cuts  1 C 6 or 2 C 2 C 3 2 C 6  Can determine optimal revenue rn by taking the maximum of  pn: the price we get by not making a cut,        r1 C rn cid:0 1: the maximum revenue from a rod of 1 inch and a rod of n cid:0  1 inches, r2 C rn cid:0 2: the maximum revenue from a rod of 2 inches and a rod of n  cid:0  2 inches, . . . rn cid:0 1 C r1.  That is, rn D max.pn; r1 C rn cid:0 1; r2 C rn cid:0 2; : : : ; rn cid:0 1 C r1  : Optimal substructure: To solve the original problem of size n, solve subproblems on smaller sizes. After making a cut, we have two subproblems. The optimal solution to the original problem incorporates optimal solutions to the subproblems. We may solve the subproblems independently. Example: For n D 7, one of the optimal solutions makes a cut at 3 inches, giving two subproblems, of lengths 3 and 4. We need to solve both of them optimally. The optimal solution for the problem of length 4, cutting into 2 pieces, each of length 2, is used in the optimal solution to the original problem with length 7.   Lecture Notes for Chapter 15: Dynamic Programming  15-3  A simpler way to decompose the problem: Every optimal solution has a leftmost cut. In other words, there‚Äôs some cut that gives a Ô¨Årst piece of length i cut off the left end, and a remaining piece of length n  cid:0  i on the right.  Need to divide only the remainder, not the Ô¨Årst piece.  Leaves only one subproblem to solve, rather than two subproblems.  Say that the solution with no cuts has Ô¨Årst piece size i D n with revenue pn,  Gives a simpler version of the equation for rn:  and remainder size 0 with revenue r0 D 0.  rn D max 1in  .pi C rn cid:0 i   :  Recursive top-down solution  Direct implementation of the simpler equation for rn. The call CUT-ROD.p; n  returns the optimal revenue rn:  CUT-ROD.p; n   if n == 0  return 0  q D  cid:0 1 for i D 1 to n return q  q D max.q; p≈íi ¬ç C CUT-ROD.p; n  cid:0  i     This procedure works, but it is terribly inefÔ¨Åcient. If you code it up and run it, it could take more than an hour for n D 40. Running time almost doubles each time n increases by 1. Why so inefÔ¨Åcient?: CUT-ROD calls itself repeatedly, even on subproblems it has already solved. Here‚Äôs a tree of recursive calls for n D 4. Inside each node is the value of n for the call represented by the node:  0  1  0  0  0  4  2  1  0  3  1  0  2  0  1  0  Lots of repeated subproblems. Solve the subproblem for size 2 twice, for size 1 four times, and for size 0 eight times. Exponential growth: Let T .n  equal the number of calls to CUT-ROD with second parameter equal to n. Then   15-4  Lecture Notes for Chapter 15: Dynamic Programming  T .n  D¬Ä 1  1 C  n cid:0 1  XjD0  if n D 0 ; if n  1 :  T .j    Summation counts calls where second parameter is j D n  cid:0  i. Solution to recurrence is T .n  D 2n.  Dynamic-programming solution  Instead of solving the same subproblems repeatedly, arrange to solve each sub- problem just once. Save the solution to a subproblem in a table, and refer back to the table whenever we revisit the subproblem. ‚ÄúStore, don‚Äôt recompute‚Äù   time-memory trade-off. Can turn an exponential-time solution into a polynomial-time solution. Two basic approaches: top-down with memoization, and bottom-up.  Top-down with memoization Solve recursively, but store each result in a table. To Ô¨Ånd the solution to a subproblem, Ô¨Årst look in the table. If the answer is there, use it. Otherwise, compute the solution to the subproblem and then store the solu- tion in the table for future use. Memoizing is remembering what we have computed previously. Memoized version of the recursive solution, storing the solution to the subproblem of length i in array entry r≈íi ¬ç:  MEMOIZED-CUT-ROD.p; n   let r≈í0 : : n¬ç be a new array for i D 0 to n r≈íi ¬ç D  cid:0 1 return MEMOIZED-CUT-ROD-AUX.p; n; r   MEMOIZED-CUT-ROD-AUX .p; n; r  if r≈ín¬ç  0 if n == 0  return r≈ín¬ç  else q D  cid:0 1  q D 0 for i D 1 to n  r≈ín¬ç D q return q  q D max.q; p≈íi ¬ç C MEMOIZED-CUT-ROD-AUX .p; n  cid:0  i; r     Lecture Notes for Chapter 15: Dynamic Programming  15-5  Bottom-up Sort the subproblems by size and solve the smaller ones Ô¨Årst. That way, when solving a subproblem, have already solved the smaller subproblems we need.  BOTTOM-UP-CUT-ROD .p; n   let r≈í0 : : n¬ç be a new array r≈í0¬ç D 0 for j D 1 to n q D  cid:0 1 for i D 1 to j r≈íj ¬ç D q  return r≈ín¬ç  q D max.q; p≈íi ¬ç C r≈íj  cid:0  i ¬ç   Running time Both the top-down and bottom-up versions run in ‚Äö.n2  time.   Bottom-up: Doubly nested loops. Number of iterations of inner for loop forms  an arithmetic series.   Top-down: MEMOIZED-CUT-ROD solves each subproblem just once, and it solves subproblems for sizes 0; 1; : : : ; n. To solve a subproblem of size n, the for loop iterates n times   over all recursive calls, total number of iterations forms an arithmetic series. [Actually using aggregate analysis, which Chap- ter17covers.]  Subproblem graphs  How to understand the subproblems involved and how they depend on each other. Directed graph:   One vertex for each distinct subproblem.  Has a directed edge .x; y  if computing an optimal solution to subproblem x  directly requires knowing an optimal solution to subproblem y.  Example: For rod-cutting problem with n D 4:  4  3  2  1  0   15-6  Lecture Notes for Chapter 15: Dynamic Programming  Can think of the subproblem graph as a collapsed version of the tree of recursive calls, where all nodes for the same subproblem are collapsed into a single vertex, and all edges go from parent to child. Subproblem graph can help determine running time. Because we solve each sub- problem just once, running time is sum of times needed to solve each subproblem.   Time to compute solution to a subproblem is typically linear in the out-degree   number of outgoing edges  of its vertex.   Number of subproblems equals number of vertices.  When these conditions hold, running time is linear in number of vertices and edges.  Reconstructing a solution  So far, have focused on computing the value of an optimal solution, rather than the choices that produced an optimal solution. Extend the bottom-up approach to record not just optimal values, but optimal choices. Save the optimal choices in a separate table. Then use a separate pro- cedure to print the optimal choices.  EXTENDED-BOTTOM-UP-CUT-ROD .p; n   let r≈í0 : : n¬ç and s≈í0 : : n¬ç be new arrays r≈í0¬ç D 0 for j D 1 to n q D  cid:0 1 for i D 1 to j  if q < p≈íi ¬ç C r≈íj  cid:0  i ¬ç  q D p≈íi ¬ç C r≈íj  cid:0  i ¬ç s≈íj ¬ç D i  r≈íj ¬ç D q return r and s  Saves the Ô¨Årst cut made in an optimal solution for a problem of size i in s≈íi ¬ç. To print out the cuts made in an optimal solution:  PRINT-CUT-ROD-SOLUTION .p; n  .r; s  D EXTENDED-BOTTOM-UP-CUT-ROD.p; n  while n > 0  print s≈ín¬ç n D n  cid:0  s≈ín¬ç  Example: For the example, EXTENDED-BOTTOM-UP-CUT-ROD returns  i  0 1 2 3  8 r≈íi ¬ç 0 1 5 8 10 13 17 18 22 s≈íi ¬ç 0 1 2 3 2  4  2  7  6  1  2  6  5  A call to PRINT-CUT-ROD-SOLUTION .p; 8  calls EXTENDED-BOTTOM-UP- CUT-ROD to compute the above r and s tables. Then it prints 2, sets n to 6, prints 6, and Ô¨Ånishes  because n becomes 0 .   Lecture Notes for Chapter 15: Dynamic Programming  15-7  Longest common subsequence  Problem: Given 2 sequences, X D hx1; : : : ; xmi and Y D hy1; : : : ; yni. Find a subsequence common to both whose length is longest. A subsequence doesn‚Äôt have to be consecutive, but it has to be in order. [To come up with examples of longest common subsequences, search the dictio- naryforallwordsthatcontainthewordyouarelookingforasasubsequence. On a UNIX system, for example, to Ô¨Ånd all the words with pine as a subsequence, usethecommand grep ‚Äô.*p.*i.*n.*e.*‚Äô dict,where dict isyourlo- caldictionary. Thencheckifthatwordisactuallyalongestcommonsubsequence. WorkingCcodeforÔ¨Åndingalongestcommonsubsequenceoftwostringsappears athttp:  www.cs.dartmouth.edu Àúthc code lcs.c]  Examples [Theexamplesareofdifferenttypesoftrees.] s p r i n g t i m e  h o r s e b a c k  p i o n e e r  s n o w f l a k e  m a e l s t r o m  h e r o i c a l l y  b e c a l m  s c h o l a r l y  Brute-force algorithm: For every subsequence of X, check whether it‚Äôs a subsequence of Y . Time: ‚Äö.n2m .   Each subsequence takes ‚Äö.n  time to check: scan Y for Ô¨Årst letter, from there    2m subsequences of X to check.  scan for second, and so on.  Optimal substructure  Notation: Xi D preÔ¨Åx hx1; : : : ; xii Yi D preÔ¨Åx hy1; : : : ; yii Theorem Let Z D h¬¥1; : : : ; ¬¥ki be any LCS of X and Y . 1. If xm D yn, then ¬¥k D xm D yn and Zk cid:0 1 is an LCS of Xm cid:0 1 and Yn cid:0 1. 2. If xm ¬§ yn, then ¬¥k ¬§ xm   Z is an LCS of Xm cid:0 1 and Y . 3. If xm ¬§ yn, then ¬¥k ¬§ yn   Z is an LCS of X and Yn cid:0 1.   15-8  Lecture Notes for Chapter 15: Dynamic Programming  Proof 1. First show that ¬¥k D xm D yn. Suppose not. Then make a subsequence It‚Äôs a common subsequence of X and Y and has Z0 D h¬¥1; : : : ; ¬¥k; xmi. length k C 1   Z0 is a longer common subsequence than Z   contradicts Z being an LCS. Now show Zk cid:0 1 is an LCS of Xm cid:0 1 and Yn cid:0 1. Clearly, it‚Äôs a common subse- quence. Now suppose there exists a common subsequence W of Xm cid:0 1 and Yn cid:0 1 that‚Äôs longer than Zk cid:0 1   length of W  k. Make subsequence W 0 by ap- pending xm to W . W 0 is common subsequence of X and Y , has length  k C 1   contradicts Z being an LCS. 2. If ¬¥k ¬§ xm, then Z is a common subsequence of Xm cid:0 1 and Y . Suppose there exists a subsequence W of Xm cid:0 1 and Y with length > k. Then W is a common subsequence of X and Y   contradicts Z being an LCS.  3. Symmetric to 2.   theorem   Therefore, an LCS of two sequences contains as a preÔ¨Åx an LCS of preÔ¨Åxes of the sequences.  Recursive formulation  DeÔ¨Åne c≈íi; j ¬ç D length of LCS of Xi and Yj . We want c≈ím; n¬ç. if i D 0 or j D 0 ; if i; j > 0 and xi D yj ; if i; j > 0 and xi ¬§ yj :  c≈íi  cid:0  1; j  cid:0  1¬ç C 1 max.c≈íi  cid:0  1; j ¬ç; c≈íi; j  cid:0  1¬ç   c≈íi; j ¬ç D¬Ä 0  Again, we could write a recursive algorithm based on this formulation. Try with bozo, bat.  4,3  3,3  3,3  2,3  3,2  3,2  4,1  1,3  2,2  2,2  3,1  2,2  3,1  3,1  4,0  0,3  1,2  1,2  2,1  1,2  2,1  2,1  3,0   Lots of repeated subproblems.    Instead of recomputing, store in a table.   Lecture Notes for Chapter 15: Dynamic Programming  15-9  Compute length of optimal solution  LCS-LENGTH.X; Y; m; n   let b≈í1 : : m; 1 : : n¬ç and c≈í0 : : m; o : : n¬ç be new tables for i D 1 to m c≈íi; 0¬ç D 0 for j D 0 to n c≈í0; j ¬ç D 0 for i D 1 to m for j D 1 to n if xi == yj  c≈íi; j ¬ç D c≈íi  cid:0  1; j  cid:0  1¬ç C 1 b≈íi; j ¬ç D ‚Äú-‚Äù  else if c≈íi  cid:0  1; j ¬ç  c≈íi; j  cid:0  1¬ç c≈íi; j ¬ç D c≈íi  cid:0  1; j ¬ç b≈íi; j ¬ç D ‚Äú"‚Äù else c≈íi; j ¬ç D c≈íi; j  cid:0  1¬ç b≈íi; j ¬ç D ‚Äú ‚Äù  return c and b  PRINT-LCS.b; X; i; j   if i == 0 or j D 0 if b≈íi; j ¬ç == ‚Äú-‚Äù  return  PRINT-LCS.b; X; i  cid:0  1; j  cid:0  1  print xi  elseif b≈íi; j ¬ç == ‚Äú"‚Äù PRINT-LCS.b; X; i  cid:0  1; j   else PRINT-LCS.b; X; i; j  cid:0  1  Initial call is PRINT-LCS.b; X; m; n . b≈íi; j ¬ç points to table entry whose subproblem we used in solving LCS of Xi and Yj .       When b≈íi; j ¬ç D -, we have extended LCS by one character. So longest com-  mon subsequence D entries with - in them.  Demonstration What do spanking and amputation have in common? [Showonly c≈íi; j ¬ç.]   15-10  Lecture Notes for Chapter 15: Dynamic Programming  u  0  0  1  1  1  1  1  1  1  t  0  0  1  1  1  1  1  1  1  t  0  0  1  2  2  2  2  2  2  a  0  0  1  2  2  2  2  2  2  a  i  0  0  1  2  2  2  3  3  3  i  o  0  0  1  2  2  2  3  3  3  n  0  0  1  2  3  3  3  4  4  n  pma  0  0  1  1  1  1  1  1  1  p  0  0  0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  1  0  0  0  1  1  1  1  1  1  s  p  a  n  k  i  n  g  Answer: pain.  Time  ‚Äö.mn   Optimal binary search trees   < kn .  [Addedinthesecondedition.]  Given sequence K D hk1; k2; : : : ; kni of n distinct keys, sorted  k1 < k2 <  Want to build a binary search tree from the keys.  For ki , have probability pi that a search is for ki .  Want BST with minimum expected search cost.  Actual cost D  of items examined.  For key ki, cost D depthT .ki   C 1, where depthT .ki   D depth of ki in BST T .  E ≈ísearch cost in T ¬ç  n  n  D  XiD1 XiD1 D 1 C  D  n  .depthT .ki   C 1   pi XiD1 depthT .ki    pi  depthT .ki    pi C XiD1  n  pi  [Keepequation * onboard.]   since probabilities sum to 1        Lecture Notes for Chapter 15: Dynamic Programming  15-11  [SimilartooptimalBSTprobleminthebook,butsimpliÔ¨Åedhere: weassumethat all searches are successful. Book has probabilities of searches between keys in tree.]  Example  i pi  1 .25  2 .2  3 .05  4 .2  5 .3  k1  k4  depthT .ki    depthT .ki    pi  Therefore, E ≈ísearch cost¬ç D 2:15.  k1  k5  k4  depthT .ki    depthT .ki    pi  k2  k3  1 0 2 1 2  k2  k3  1 0 3 2 1  i 1 2 3 4 5  i 1 2 3 4 5  k5  .25 0 .1 .2 .6 1.15  .25 0 .15 .4 .3 1.10  Therefore, E ≈ísearch cost¬ç D 2:10, which turns out to be optimal.  Observations  Optimal BST might not have smallest height.  Optimal BST might not have highest-probability key at root.  Build by exhaustive checking?   15-12  Lecture Notes for Chapter 15: Dynamic Programming   Construct each n-node BST.  For each, put in keys.  Then compute expected search cost.  But there are .4n=n3=2  different BSTs with n nodes.  Optimal substructure  Consider any subtree of a BST. It contains keys in a contiguous range ki ; : : : ; kj for some 1  i  j  n.  T  T'  If T is an optimal BST and T contains subtree T 0 with keys ki ; : : : ; kj , then T 0 must be an optimal BST for keys ki ; : : : ; kj .  Proof Cut and paste.  Use optimal substructure to construct an optimal solution to the problem from op- timal solutions to subproblems:   Given keys ki ; : : : ; kj  the problem .  One of them, kr, where i  r  j , must be the root.  Left subtree of kr contains ki ; : : : ; kr cid:0 1.  Right subtree of kr contains krC1; : : : ; kj .  kr  ki  kr‚Äì1  kr+1  kj    If  we examine all candidate roots kr, for i  r  j , and  we determine all optimal BSTs containing ki ; : : : ; kr cid:0 1 and containing  krC1; : : : ; kj ,  then we‚Äôre guaranteed to Ô¨Ånd an optimal BST for ki ; : : : ; kj .   Lecture Notes for Chapter 15: Dynamic Programming  15-13  Recursive solution  Subproblem domain:  Find optimal BST for ki ; : : : ; kj , where i  1; j  n; j  i  cid:0  1.  When j D i  cid:0  1, the tree is empty. DeÔ¨Åne e≈íi; j ¬ç D expected search cost of optimal BST for ki ; : : : ; kj . If j D i  cid:0  1, then e≈íi; j ¬ç D 0. If j  i,  Select a root kr, for some i  r  j .  Make an optimal BST with ki ; : : : ; kr cid:0 1 as the left subtree.  Make an optimal BST with krC1; : : : ; kj as the right subtree.  Note: when r D i, left subtree is ki ; : : : ; ki cid:0 1; when r D j , right subtree is  kjC1; : : : ; kj .  When a subtree becomes a subtree of a node:   Depth of every node in subtree goes up by 1.  Expected search cost increases by  w.i; j   D   refer to equation     .  pl  j  XlDi  If kr is the root of an optimal BST for ki ; : : : ; kj : e≈íi; j ¬ç D pr C .e≈íi; r  cid:0  1¬ç C w.i; r  cid:0  1   C .e≈ír C 1; j ¬ç C w.r C 1; j    : But w.i; j   D w.i; r  cid:0  1  C pr C w.r C 1; j  . Therefore, e≈íi; j ¬ç D e≈íi; r  cid:0  1¬ç C e≈ír C 1; j ¬ç C w.i; j  . This equation assumes that we already know which key is kr. We don‚Äôt. Try all candidates, and pick the best one:  e≈íi; j ¬ç D  0  min irj fe≈íi; r  cid:0  1¬ç C e≈ír C 1; j ¬ç C w.i; j  g  Could write a recursive algorithm. . .  if j D i  cid:0  1 ; if i  j :  Computing an optimal solution  As ‚Äúusual,‚Äù we‚Äôll store the values in a table:  ¬ç  ; 0 : : n can store e≈í1; 0¬ç  e≈í 1 : : n C 1 ‚Äû ∆í‚Äö ‚Ä¶ can store e≈ín C 1; n¬ç  Will use only entries e≈íi; j ¬ç, where j  i  cid:0  1.  ‚Äû∆í‚Äö‚Ä¶   15-14  Lecture Notes for Chapter 15: Dynamic Programming   Will also compute  root≈íi; j ¬ç D root of subtree with keys ki ; : : : ; kj , for 1  i  j  n :  One other table: don‚Äôt recompute w.i; j   from scratch every time we need it.  Would take ‚Äö.j  cid:0  i   additions.  Instead:  Table w≈í1 : : n C 1; 0 : : n¬ç  w≈íi; i  cid:0  1¬ç D 0 for 1  i  n  w≈íi; j ¬ç D w≈íi; j  cid:0  1¬ç C pj for 1  i  j  n Can compute all ‚Äö.n2  values in O.1  time each.  OPTIMAL-BST.p; q; n  let e≈í1 : : n C 1; 0 : : n¬ç, w≈í1 : : n C 1; 0 : : n¬ç, and root≈í1 : : n; 1 : : n¬ç be new tables for i D 1 to n C 1 e≈íi; i  cid:0  1¬ç D 0 w≈íi; i  cid:0  1¬ç D 0 for i D 1 to n  cid:0  l C 1  for l D 1 to n  j D i C l  cid:0  1 e≈íi; j ¬ç D 1 w≈íi; j ¬ç D w≈íi; j  cid:0  1¬ç C pj for r D i to j  t D e≈íi; r  cid:0  1¬ç C e≈ír C 1; j ¬ç C w≈íi; j ¬ç if t < e≈íi; j ¬ç  e≈íi; j ¬ç D t root≈íi; j ¬ç D r  return e and root  First for loop initializes e; w entries for subtrees with 0 keys. Main for loop:      Iteration for l works on subtrees with l keys. Idea: compute in order of subtree sizes, smaller  1 key  to larger  n keys .  For example at beginning:  0 0  e 1 2 3 4 5 6  i  j  2 .65 .2 0  3 .8 .3 .05 0  1 .25 0  pi  4  1.25 .75 .3 .2 0  5  2.10 1.35 .85 .7 .3 0   Lecture Notes for Chapter 15: Dynamic Programming  15-15  0 0  1 1  w 1 2 3 4 5 6  i  root 1 2 3 4 5  i  1 .25 0  j  2 .45 .2 0  4 3 .5 .7 .25 .45 .05 .25 0 .2 0  5 1.0 .75 .55 .5 .3 0  2 1 2  j 3 1 2 3  4 2 2 4 4  5 2 4 5 5 5  Time O.n3 : for loops nested 3 deep, each loop index takes on  n values. Can also show .n3 . Therefore, ‚Äö.n3 .  Construct an optimal solution  CONSTRUCT-OPTIMAL-BST.root  r D root≈í1; n¬ç print ‚Äúk‚Äùr ‚Äúis the root‚Äù CONSTRUCT-OPT-SUBTREE.1; r  cid:0  1; r; ‚Äúleft‚Äù; root  CONSTRUCT-OPT-SUBTREE.r C 1; n; r; ‚Äúright‚Äù; root  CONSTRUCT-OPT-SUBTREE .i; j; r; dir; root  if i  j  t D root≈íi; j ¬ç print ‚Äúk‚Äùt ‚Äúis‚Äù dir ‚Äúchild of k‚Äùr CONSTRUCT-OPT-SUBTREE.i; t  cid:0  1; t; ‚Äúleft‚Äù; root  CONSTRUCT-OPT-SUBTREE.t C 1; j; t; ‚Äúright‚Äù; root   Elements of dynamic programming  Mentioned already:      optimal substructure overlapping subproblems  Optimal substructure   Show that a solution to a problem consists of making a choice, which leaves  one or subproblems to solve.   15-16  Lecture Notes for Chapter 15: Dynamic Programming   Suppose that you are given this last choice that leads to an optimal solution. [We Ô¨Ånd that students often have trouble understanding the relationship be- tween optimal substructure and determining which choice is made in an opti- mal solution. One way that helps them understand optimal substructure is to imaginethatthedynamic-programming godstellyouwhatwasthelastchoice madeinanoptimalsolution.]   Given this choice, determine which subproblems arise and how to characterize  the resulting space of subproblems.   Show that the solutions to the subproblems used within the optimal solution  must themselves be optimal. Usually use cut-and-paste:   Suppose that one of the subproblem solutions is not optimal.  Cut it out.  Paste in an optimal solution.  Get a better solution to the original problem. Contradicts optimality of prob-  lem solution.  That was optimal substructure. Need to ensure that you consider a wide enough range of choices and subprob- lems that you get them all. [The dynamic-programming gods are too busy to tell you what that last choice really was.] Try all the choices, solve all the subprob- lems resulting from each choice, and pick the choice whose solution, along with subproblem solutions, is best. How to characterize the space of subproblems?   Keep the space as simple as possible.  Expand it as necessary.  Examples Rod cutting   Space of subproblems was rods of length n  cid:0  i, for 1  i  n.  No need to try a more general space of subproblems.  Optimal binary search trees  keys k1; k2; : : : ; kj .   Suppose we had tried to constrain space of subproblems to subtrees with   An optimal BST would have root kr, for some 1  r  j .  Get subproblems k1; : : : ; kr cid:0 1 and krC1; : : : ; kj .  Unless we could guarantee that r D j , so that subproblem with krC1; : : : ; kj  Thus, needed to allow the subproblems to vary at ‚Äúboth ends,‚Äù i.e., allow  is empty, then this subproblem is not of the form k1; k2; : : : ; kj .  both i and j to vary.  Optimal substructure varies across problem domains:  1. How many subproblems are used in an optimal solution. 2. How many choices in determining which subproblem s  to use.   Lecture Notes for Chapter 15: Dynamic Programming  15-17   Rod cutting:   1 subproblem  of size n  cid:0  i   n choices   Longest common subsequence:   1 subproblem  Either   1 choice  if xi D yj , LCS of Xi cid:0 1 and Yj cid:0 1 , or  2 choices  if xi ¬§ yj , LCS of Xi cid:0 1 and Y , and LCS of X and Yj cid:0 1    Optimal binary search tree:   2 subproblems  ki ; : : : ; kr cid:0 1 and krC1; : : : ; kj    j  cid:0  i C 1 choices for kr in ki ; : : : ; kj . Once we determine optimal solutions  to subproblems, we choose from among the j  cid:0  i C 1 candidates for kr. Informally, running time depends on   of subproblems overall     of choices .  Rod cutting: ‚Äö.n  subproblems,  n choices for each  Longest common subsequence: ‚Äö.mn  subproblems,  2 choices for each  Optimal binary search tree: ‚Äö.n2  subproblems, O.n  choices for each    O.n2  running time.   ‚Äö.mn  running time.   O.n3  running time.  Can use the subproblem graph to get the same analysis: count the number of edges.   Each vertex corresponds to a subproblem.  Choices for a subproblem are vertices that the subproblem has edges going to.  For rod cutting, subproblem graph has n vertices and  n edges per vertex   O.n2  running time. In fact, can get an exact count of the edges: for i D 0; 1; : : : ; n, vertex for subproblem size i has out-degree i    of edges DPn  Subproblem graph for matrix-chain multiplication would have ‚Äö.n2  vertices, each with degree  n  cid:0  1   O.n3  running time.  iD0 i D n.n C 1 =2.  Dynamic programming uses optimal substructure bottom up.   First Ô¨Ånd optimal solutions to subproblems.  Then choose which to use in optimal solution to the problem.  When we look at greedy algorithms, we‚Äôll see that they work top down: Ô¨Årst make a choice that looks best, then solve the resulting subproblem. Don‚Äôt be fooled into thinking optimal substructure applies to all optimization prob- lems. It doesn‚Äôt. Here are two problems that look similar. directed graph G D .V; E .  In both, we‚Äôre given an unweighted,   15-18  Lecture Notes for Chapter 15: Dynamic Programming   V is a set of vertices.  E is a set of edges.  And we ask about Ô¨Ånding a path  sequence of connected edges  from vertex u to vertex .   Shortest path: Ô¨Ånd path u ;  with fewest edges. Must be simple  no cycles ,  since removing a cycle from a path gives a path with fewer edges.   Longest simple path: Ô¨Ånd simple path u ;  with most edges. If didn‚Äôt require  simple, could repeatedly traverse a cycle to make an arbitrarily long path.  Shortest path has optimal substructure.  p1  p2  u  v   Suppose p is shortest path u ; .  Let w be any vertex on p.  Let p1 be the portion of p going u ; w.  Then p1 is a shortest path u ; w.  w  p  r  t      It seems like it should. It does not.  q  s  Proof Suppose there exists a shorter path p01 going u ; w. Cut out p1, replace it with p01, get path u  p2 ;  with fewer edges than p.  p0 1 ; w  Therefore, can Ô¨Ånd shortest path u ;  by considering all intermediate vertices w, then Ô¨Ånding shortest paths u ; w and w ; . Same argument applies to p2. Does longest path have optimal substructure?  Consider q ! r ! t D longest path q ; t. Are its subpaths longest paths? No!  Subpath q ; r is q ! r.  Longest simple path q ; r is q ! s ! t ! r.  Subpath r ; t is r ! t.  Longest simple path r ; t is r ! q ! s ! t.   Lecture Notes for Chapter 15: Dynamic Programming  15-19  Not only isn‚Äôt there optimal substructure, but we can‚Äôt even assemble a legal solu- tion from solutions to subproblems. Combine longest simple paths:  q ! s ! t ! r ! q ! s ! t Not simple! In fact, this problem is NP-complete  so it probably has no optimal substructure to Ô¨Ånd.  What‚Äôs the big difference between shortest path and longest path?   Shortest path has independent subproblems.  Solution to one subproblem does not affect solution to another subproblem of  the same problem.   Longest simple path: subproblems are not independent.  Consider subproblems of longest simple paths q ; r and r ; t.  Longest simple path q ; r uses s and t.  Cannot use s and t to solve longest simple path r ; t, since if we do, the path  isn‚Äôt simple.   But we have to use t to Ô¨Ånd longest simple path r ; t!  Using resources  vertices  to solve one subproblem renders them unavailable to  solve the other subproblem.  [Forshortest paths, ifwelookatashortest path u than w canappearin p1 and p2. Otherwise,wehaveacycle.]  p1 ; w  p2 ; ,novertex other  Independent subproblems in our examples:   Rod cutting and longest common subsequence  1 subproblem   automatically independent.   Optimal binary search tree   ki ; : : : ; kr cid:0 1 and krC1; : : : ; kj   independent.  Overlapping subproblems  These occur when a recursive algorithm revisits the same problem over and over. Good divide-and-conquer algorithms usually generate a brand new problem at each stage of recursion. Example: merge sort  1..8  1..4  5..8  1..2  3..4  5..6  7..8  1..1  2..2  3..3  4..4  5..5  6..6  7..7  8..8   15-20  Lecture Notes for Chapter 15: Dynamic Programming  Won‚Äôt go through exercise of showing repeated subproblems. Book has a good example for matrix-chain multiplication. Alternative approach to dynamic programming: memoization    ‚ÄúStore, don‚Äôt recompute.‚Äù   Make a table indexed by subproblem.  When solving a subproblem:   Lookup in table.    If answer is there, use it.   Else, compute answer, then store it.    In bottom-up dynamic programming, we go one step further. We determine in what order we‚Äôd want to access the table, and Ô¨Åll it in that way.   Solutions for Chapter 15: Dynamic Programming  We can verify that T .n  D 2n is a solution to the given recurrence by the substitu- tion method. We note that for n D 0, the formula is true since 20 D 1. For n > 0, substituting into the recurrence and using the formula for summing a geometric series yields  Solution to Exercise 15.1-1  T .n  D 1 C  2j  n cid:0 1  XjD0  D 1 C .2n  cid:0  1  D 2n :  Solution to Exercise 15.1-2  Here is a counterexample for the ‚Äúgreedy‚Äù strategy:  length i price pi  pi = i  1 1 1  2 20 10  3 33 11  4 36 1  Let the given rod length be 4. According to a greedy strategy, we Ô¨Årst cut out a rod of length 3 for a price of 33, which leaves us with a rod of length 1 of price 1. The total price for the rod is 34. The optimal way is to cut it into two rods of length 2 each fetching us 40 dollars.   15-22  Solutions for Chapter 15: Dynamic Programming  Solution to Exercise 15.1-3  MODIFIED-CUT-ROD .p; n; c   let r≈í0 : : n¬ç be a new array r≈í0¬ç D 0 for j D 1 to n q D p≈íj ¬ç for i D 1 to j  cid:0  1 r≈íj ¬ç D q  return r≈ín¬ç  q D max.q; p≈íi ¬ç C r≈íj  cid:0  i ¬ç  cid:0  c   The major modiÔ¨Åcation required is in the body of the inner for loop, which now reads q D max.q; p≈íi ¬ç C r≈íj  cid:0  i ¬ç  cid:0  c . This change reÔ¨Çects the Ô¨Åxed cost of making the cut, which is deducted from the revenue. We also have to handle the case in which we make no cuts  when i equals j  ; the total revenue in this case is simply p≈íj ¬ç. Thus, we modify the inner for loop to run from i to j  cid:0  1 instead of to j . The assignment q D p≈íj ¬ç takes care of the case of no cuts. If we did not make these modiÔ¨Åcations, then even in the case of no cuts, we would be deducting c from the total revenue.  Solution to Exercise 15.1-4  MEMOIZED-CUT-ROD.p; n   let r≈í0 : : n¬ç and s≈í0 : : n¬ç be new arrays for i D 0 to n r≈íi ¬ç D  cid:0 1 .al; s  D MEMOIZED-CUT-ROD-AUX .p; n; r; s  print ‚ÄúThe optimal value is ‚Äù al ‚Äú and the cuts are at ‚Äù j D n while j > 0  print s≈íj ¬ç j D j  cid:0  s≈íj ¬ç   Solutions for Chapter 15: Dynamic Programming  15-23  MEMOIZED-CUT-ROD-AUX.p; n; r; s  if r≈ín¬ç  0 if n == 0  return r≈ín¬ç  else q D  cid:0 1  q D 0 for i D 1 to n  q D p≈íi ¬ç C al s≈ín¬ç D i  r≈ín¬ç D q return .q; s   .al; s  D MEMOIZED-CUT-ROD-AUX .p; n  cid:0  i; r; s  if q < p≈íi ¬ç C al  PRINT-CUT-ROD-SOLUTION constructs the actual lengths where a cut should hap- pen. Array entry s≈íi ¬ç contains the value j indicating that an optimal cut for a rod of length i is j inches. The next cut is given by s≈íi  cid:0  j ¬ç, and so on.  Solution to Exercise 15.1-5  FIBONACCI.n   let Ô¨Åb≈í0 : : n¬ç be a new array Ô¨Åb≈í0¬ç D Ô¨Åb≈í1¬ç D 1 for i D 2 to n return Ô¨Åb≈ín¬ç  Ô¨Åb≈íi ¬ç D Ô¨Åb≈íi  cid:0  1¬ç C Ô¨Åb≈íi  cid:0  2¬ç  FIBONACCI directly implements the recurrence relation of the Fibonacci sequence. Each number in the sequence is the sum of the two previous numbers in the se- quence. The running time is clearly O.n . The subproblem graph consists of n C 1 vertices, 0; 1; : : : ; n. For i D 2; 3; : : : ; n, vertex i has two leaving edges: to vertex i cid:0 1 and to vertex i cid:0 2. No edges leave vertices 0 or 1. Thus, the subproblem graph has 2n  cid:0  2 edges.  Solution to Exercise 15.2-4  The vertices of the subproblem graph are the ordered pairs ij , where i  j . If i D j , then there are no edges out of ij . If i < j , then for every k such that i  k < j , the subproblem graph contains edges .ij ; i k  and .ij ; kC1;j  . These edges indicate that to solve the subproblem of optimally parenthesizing the product Ai  Aj , we need to solve subproblems of optimally parenthesizing the products Ai  Ak and AkC1  Aj . The number of vertices is XiD1 XjDi  n.n C 1   1 D  2  ;  n  n   15-24  Solutions for Chapter 15: Dynamic Programming  and the number of edges is n  n  n  XiD1  n  XjDi  .j  cid:0  i   D  XiD1 XiD1 .n  cid:0  i  .n  cid:0  i C 1   D   substituting t D j  cid:0  i   n cid:0 i  t  XtD0 .n  cid:0  i  .n  cid:0  i C 1   :  2  n  Substituting r D n  cid:0  i and reversing the order of summation, we obtain XiD1 D  n cid:0 1  1  2  2  1  .r 2 C r   XrD0 2 .n  cid:0  1 n.2n  cid:0  1  .n  cid:0  1 n.n C 1   6  :  6  D  D  .n  cid:0  1 n  2  C    Thus, the subproblem graph has ‚Äö.n2  vertices and ‚Äö.n3  edges.   by equations  A.3  and  A.1    Solution to Exercise 15.2-5 This solution is also posted publicly  Each time the l-loop executes, the i-loop executes n  cid:0  l C 1 times. Each time the i-loop executes, the k-loop executes j  cid:0  i D l  cid:0  1 times, each time referencing m twice. Thus the total number of times that an entry of m is referenced while computing other entries isPn XiD1 XjDi  lD2.n  cid:0  l C 1 .l  cid:0  1 2. Thus,  R.i; j   D  n  n  n  n cid:0 1  XlD2 .n  cid:0  l C 1 .l  cid:0  1 2 XlD1 .n  cid:0  l l D 2 XlD1 XlD1 nl  cid:0  2 n.n  cid:0  1 n  cid:0  2 6 2n3  cid:0  3n2 C n  D 2  n cid:0 1  n cid:0 1  l 2  2  D 2 D n3  cid:0  n2  cid:0  D :  n3  cid:0  n  3  3  .n  cid:0  1 n.2n  cid:0  1    Solutions for Chapter 15: Dynamic Programming  15-25  Solution to Exercise 15.3-1 This solution is also posted publicly  Running RECURSIVE-MATRIX-CHAIN is asymptotically more efÔ¨Åcient than enu- merating all the ways of parenthesizing the product and computing the number of multiplications for each. Consider the treatment of subproblems by the two approaches.   For each possible place to split the matrix chain, the enumeration approach Ô¨Ånds all ways to parenthesize the left half, Ô¨Ånds all ways to parenthesize the right half, and looks at all possible combinations of the left half with the right half. The amount of work to look at each combination of left- and right-half subproblem results is thus the product of the number of ways to do the left half and the number of ways to do the right half.   For each possible place to split the matrix chain, RECURSIVE-MATRIX-CHAIN Ô¨Ånds the best way to parenthesize the left half, Ô¨Ånds the best way to parenthesize the right half, and combines just those two results. Thus the amount of work to combine the left- and right-half subproblem results is O.1 .  Section 15.2 argued that the running time for enumeration is .4n=n3=2 . We will show that the running time for RECURSIVE-MATRIX-CHAIN is O.n3n cid:0 1 . To get an upper bound on the running time of RECURSIVE-MATRIX-CHAIN, we‚Äôll use the same approach used in Section 15.2 to get a lower bound: Derive a recur- rence of the form T .n   : : : and solve it by substitution. For the lower-bound recurrence, the book assumed that the execution of lines 1‚Äì2 and 6‚Äì7 each take at least unit time. For the upper-bound recurrence, we‚Äôll assume those pairs of lines each take at most constant time c. Thus, we have the recurrence  n cid:0 1  XkD1 .T .k  C T .n  cid:0  k  C c   if n D 1 ; if n  2 :  c C  T .n  ¬Ä c XiD1  T .n   2  n cid:0 1  T .i   C cn :  This is just like the book‚Äôs  recurrence except that it has c instead of 1, and so we can be rewrite it as  We shall prove that T .n  D O.n3n cid:0 1  using the substitution method.  Note: Any upper bound on T .n  that is o.4n=n3=2  will sufÔ¨Åce. You might prefer to prove one that is easier to think up, such as T .n  D O.3:5n .  SpeciÔ¨Åcally, we shall show that T .n   cn3n cid:0 1 for all n  1. The basis is easy, since T .1   c D c  1  31 cid:0 1. Inductively, for n  2 we have   15-26  Solutions for Chapter 15: Dynamic Programming  T .n   2  n cid:0 1  n cid:0 1  T .i   C cn  XiD1 XiD1 ci 3i cid:0 1 C cn  2  c  2 i 3i cid:0 1 C n! n cid:0 1 XiD1 .3  cid:0  1 2 C n D c 2  n3n cid:0 1 1  cid:0  3n 3  cid:0  1 C D cn3n cid:0 1 C c  1  cid:0  3n 2 C n D cn3n cid:0 1 C .2n C 1  cid:0  3n   cn3n cid:0 1 for all c > 0, n  1 .  2  c   see below   Running RECURSIVE-MATRIX-CHAIN takes O.n3n cid:0 1  time, and enumerating all parenthesizations takes .4n=n3=2  time, and so RECURSIVE-MATRIX-CHAIN is more efÔ¨Åcient than enumeration. Note: The above substitution uses the following fact:  This equation can be derived from equation  A.5  by taking the derivative. Let  n cid:0 1  XiD1  ixi cid:0 1 D  nxn cid:0 1  x  cid:0  1 C  1  cid:0  xn .x  cid:0  1 2  :  n cid:0 1  XiD1  xi D  xn  cid:0  1 x  cid:0  1  cid:0  1 :  f .x  D Then n cid:0 1  XiD1  ixi cid:0 1 D f 0.x  D  nxn cid:0 1  x  cid:0  1 C  1  cid:0  xn .x  cid:0  1 2  :  Solution to Exercise 15.3-5  We say that a problem exhibits the optimal substructure property when optimal solutions to a problem incorporate optimal solutions to related subproblems, which we may solve independently  i.e., they do not share resources . When we impose a limit li on the number of pieces of size i that we are permitted to produce, the subproblems can no longer be solved independently. For example, consider a rod of length 4 with the following prices and limits:  length i price pi limit li  1 15 2  2 20 1  3 33 1  4 36 1  This instance has only three solutions that do not violate the limits: length 4 with price 36; lengths 1 and 3 with price 48; and lengths 1, 1, and 2 with price 50. The   Solution to Exercise 15.3-6  Solutions for Chapter 15: Dynamic Programming  15-27  optimal solution, therefore is to cut into lengths 1, 1, and 2. When we look at the subproblem for length 2, it has two solutions that do not violate the limits: length 2 with price 20, and lengths 1 and 1 with price 30. The optimal solution for length 2, therefore, is to cut into lengths 1 and 1. But we cannot use this optimal solution for the subproblem in the optimal solution for the original problem, because it would result in using four rods of length 1 to solve the original problem, violating the limit of two length-1 rods.  Any solution must add the additional assumption that no currency can be repeated in a sequence of trades. Without this assumption, if rij > 1=rj i for some currencies i and j , we could repeatedly exchange i ! j ! i ! j !  and make an unbounded proÔ¨Åt. To see that this problem has optimal substructure when ck D 0 for all k, observe that the problem of exchanging currency a for currency b is equivalent to Ô¨Ånding a sequence of currencies k1; k2; : : : ; km such that k1 D a, km D b, and the product rk1k2rk2k3  rkm cid:0 1km is maximized. We use the usual cut-and-paste argument. Suppose that an optimal solution con- tains a sequence hki ; kiC1; : : : ; kji of currencies, and suppose that there exists a sequence hk0i ; k0iC1; : : : ; k0ji, such that k0i D ki , k0j D kj , and rk0 i C1  rk0 > i k0 rki ki C1  rkj  cid:0 1kj . Then we could substitute the sequence hk0i ; k0iC1; : : : ; k0ji for the sequence hki ; kiC1; : : : ; kji in the optimal solution to create an even better solution. We show that optimal substructure does not hold when the ck are arbitrary values by means of an example. Suppose we have four currencies, with the following exchange rates:  j  cid:0 1k0  j  rij 1 2 3 4  1 1 1 2 2 5 1 6  i  j 2 2 1 2 3 1 3  3 5 2 3 2 1 1 3  4 6 3 3 1  Let c1 D 2 and c2 D c3 D 3. Note that this example is not too badly contrived, in that rj i D 1=rij for all i and j . To see how this example does not exhibit optimal substructure, let‚Äôs examine an optimal solution for exchanging currency 1 for currency 4. There are Ô¨Åve possible exchange sequences, with the following costs: h1; 4i h1; 2; 4i h1; 3; 4i h1; 2; 3; 4i h1; 3; 2; 4i The optimal exchange sequence, h1; 2; 3; 4i, appears in boldface.  D 4 ; W 6  cid:0  2 D 3 ; W 2  3  cid:0  3 D 9=2 ; W 5=2  3  cid:0  3 D 6 W 2  3=2  3  cid:0  3 W 5=2  2=3  3  cid:0  3 D 2   15-28  Solutions for Chapter 15: Dynamic Programming  Let‚Äôs examine the subproblem of exchanging currency 1 for currency 3. Allow- ing currency 4 to be part of the exchange sequence, there are again Ô¨Åve possible exchange sequences with the following costs and the optimal one in boldface: h1; 3i h1; 2; 3i h1; 4; 3i h1; 2; 4; 3i h1; 4; 2; 3i We see that the solution to the original problem includes the subproblem of ex- changing currency 1 for currency 3, yet the solution h1; 2; 3i to the subproblem used in the optimal solution to the original problem is not the optimal solution h1; 3i to the subproblem on its own.  D 1=2 W 5=2  cid:0  2 D 0 W 2  3=2  cid:0  3 D  cid:0 1 W 6  1=3  cid:0  3 D  cid:0 1 W 2  3  1=3  cid:0  3 W 6  1=3  3=2 D 3 D 0  Solution to Exercise 15.4-4 This solution is also posted publicly  When computing a particular row of the c table, no rows before the previous row are needed. Thus only two rows‚Äî2  Y:length entries‚Äîneed to be kept in memory at a time.  Note: Each row of c actually has Y:lengthC 1 entries, but we don‚Äôt need to store the column of 0‚Äôs‚Äîinstead we can make the program ‚Äúknow‚Äù that those entries are 0.  With this idea, we need only 2  min.m; n  entries if we always call LCS-LENGTH with the shorter sequence as the Y argument. We can thus do away with the c table as follows:   Use two arrays of length min.m; n , preious-row and current-row, to hold the  appropriate rows of c. Initialize preious-row to all 0 and compute current-row from left to right.     When current-row is Ô¨Ålled, if there are still more rows to compute, copy  current-row into preious-row and compute the new current-row.  Actually only a little more than one row‚Äôs worth of c entries‚Äîmin.m; n  C 1 en- tries‚Äîare needed during the computation. The only entries needed in the table when it is time to compute c≈íi; j ¬ç are c≈íi; k¬ç for k  j  cid:0  1  i.e., earlier entries in the current row, which will be needed to compute the next row ; and c≈íi  cid:0  1; k¬ç for k  j  cid:0  1  i.e., entries in the previous row that are still needed to compute the rest of the current row . This is one entry for each k from 1 to min.m; n  except that there are two entries with k D j  cid:0  1, hence the additional entry needed besides the one row‚Äôs worth of entries. We can thus do away with the c table as follows:  Use an array a of length min.m; n  C 1 to hold the appropriate entries of c. At  the time c≈íi; j ¬ç is to be computed, a will hold the following entries:  a≈ík¬ç D c≈íi; k¬ç for 1  k < j  cid:0  1  i.e., earlier entries in the current ‚Äúrow‚Äù ,  a≈ík¬ç D c≈íi  cid:0  1; k¬ç for k  j  cid:0  1  i.e., entries in the previous ‚Äúrow‚Äù ,   Solution to Problem 15-1  Solutions for Chapter 15: Dynamic Programming  15-29     a≈í0¬ç D c≈íi; j  cid:0  1¬ç  i.e., the previous entry computed, which couldn‚Äôt be put into the ‚Äúright‚Äù place in a without erasing the still-needed c≈íi  cid:0  1; j  cid:0  1¬ç .  Initialize a to all 0 and compute the entries from left to right.  Note that the 3 values needed to compute c≈íi; j ¬ç for j > 1 are in a≈í0¬ç D  When c≈íi; j ¬ç has been computed, move a≈í0¬ç  c≈íi; j  cid:0  1¬ç  to its ‚Äúcorrect‚Äù  c≈íi; j  cid:0  1¬ç, a≈íj  cid:0  1¬ç D c≈íi  cid:0  1; j  cid:0  1¬ç, and a≈íj ¬ç D c≈íi  cid:0  1; j ¬ç. place, a≈íj  cid:0  1¬ç, and put c≈íi; j ¬ç in a≈í0¬ç.  p0 ; t.  We will make use of the optimal substructure property of longest paths in acyclic graphs. Let u be some vertex of the graph. If u D t, then the longest path from u to t has zero weight. If u ¬§ t, let p be a longest path from u to t. Path p has at least two vertices. Let  be the second vertex on the path. Let p0 be the subpath of p from  to t  p0 might be a zero-length path . That is, the path p looks like u !  We claim that p0 is a longest path from  to t. To prove the claim, we use a cut-and-paste argument. If p0 were not a longest path, then there exists a longer path p00 from  to t. We could cut out p0 and paste p00 ; t which is longer than p, thus contradicting the in p00 to produce a path u !  assumption that p is a longest path from u to t. It is important to note that the graph is acyclic. Because the graph is acyclic, path p00 cannot include the vertex u, for otherwise there would be a cycle of the form u !  ; u in the graph. Thus, we can indeed use p00 to construct a longer path. The acyclicity requirement ensures that by pasting in path p00, the overall path is still a simple path  there is no cycle in the path . This difference between the cyclic and the acyclic case allows us to use dynamic programming to solve the acyclic case. Let dist≈íu¬ç denote the weight of a longest path from u to t. The optimal substructure property allows us to write a recurrence for dist≈íu¬ç as  dist≈íu¬ç D  0  max  if u D t ; .u; 2EÀöw.u;   C dist≈í¬ç cid:9  otherwise :  This recurrence allows us to construct the following procedure:   15-30  Solutions for Chapter 15: Dynamic Programming  LONGEST-PATH-AUX.G; u; t; dist; next   if u == t  dist≈íu¬ç D 0 return .dist; next  elseif next≈íu¬ç  0 else next≈íu¬ç D 0  return .dist; next   for each vertex  2 G:Adj≈íu¬ç  .dist; next  D LONGEST-PATH-AUX.G; ; t; dist; next  if w.u;   C dist≈í¬ç > dist≈íu¬ç  dist≈íu¬ç D w.u;   C dist≈í¬ç next≈íu¬ç D   return .dist; next    See Section 22.1 for an explanation of the notation G:Adj≈íu¬ç.  LONGEST-PATH-AUX is a memoized, recursive procedure, which returns the tuple .dist; next . The array dist is the memoized array that holds the solution to sub- problems. That is, after the procedure returns, dist≈íu¬ç will hold the weight of a longest path from u to t. The array next serves two purposes:    It holds information necessary for printing out an actual path. SpeciÔ¨Åcally, if u is a vertex on the longest path that the procedure found, then next≈íu¬ç is the next vertex on the path.   The value in next≈íu¬ç is used to check whether the current subproblem has been solved earlier. A value of at least zero indicates that this subproblem has been solved earlier.  The Ô¨Årst if condition checks for the base case u D t. The second if condition checks whether the current subproblem has already been solved. The for loop iterates over each adjacent edge .u;   and updates the longest distance in dist≈íu¬ç. What is the running time of LONGEST-PATH-AUX? Each subproblem represented by a vertex u is solved at most once due to the memoization. For each vertex, we examine its adjacent edges. Thus, each edge is examined at most once, and the overall running time is O.E .  Section 22.1 discusses how we achieve O.E  time by representing the graph with adjacency lists.  The PRINT-PATH procedure prints out the path using information stored in the next array:  PRINT-PATH.s; t; next  u D s print u while u ¬§ t  print ‚Äú!‚Äù next[u] u D next≈íu¬ç  The LONGEST-PATH-MAIN procedure is the main driver. It creates and initializes the dist and the next arrays. It then calls LONGEST-PATH-AUX to Ô¨Ånd a path and PRINT-PATH to print out the actual path.   Solutions for Chapter 15: Dynamic Programming  15-31  LONGEST-PATH-MAIN.G; s; t   n D jG:Vj let dist≈í1 : : n¬ç and next≈í1 : : n¬ç be new arrays for i D 1 to n  dist≈íi ¬ç D  cid:0 1 next≈íi ¬ç D  cid:0 1 .dist; next  D LONGEST-PATH-AUX.G; s; t; dist; next  if dist≈ís¬ç ==  cid:0 1 else print ‚ÄúThe weight of the longest path is ‚Äù dist≈ís¬ç  print ‚ÄúNo path exists‚Äù  PRINT-PATH.s; t; next   Initializating the dist and next arrays takes O.V   time. Thus the overall running time of LONGEST-PATH-MAIN is O.V C E .  Alternative solution  We can also solve the problem using a bottom-up aproach. To do so, we need to ensure that we solve ‚Äúsmaller‚Äù subproblems before we solve ‚Äúlarger‚Äù ones. In our case, we can use a topological sort  see Section 22.4  to obtain a bottom-up procedure, imposing the required ordering on the vertices in ‚Äö.V C E  time. LONGEST-PATH2.G; s; t    let dist≈í1 : : n¬ç and next≈í1 : : n¬ç be new arrays topologically sort the vertices of G for i D 1 to jG:Vj dist≈íi ¬ç D  cid:0 1 dist≈ís¬ç D 0 for each u in topological order, starting from s  for each edge .u;   2 G:Adj≈íu¬ç if dist≈íu¬ç C w.u;   > dist≈í¬ç  dist≈í¬ç D dist≈íu¬ç C w.u;   next≈íu¬ç D   print ‚ÄúThe longest distance is ‚Äù dist≈ít ¬ç PRINT-PATH.s; t; next  The running time of LONGEST-PATH2 is ‚Äö.V C E .  Solution to Problem 15-2  We solve the longest palindrome subsequence  LPS  problem in a manner similar to how we compute the longest common subsequence in Section 15.4.  Step 1: Characterizing a longest palindrome subsequence  The LPS problem has an optimal-substructure property, where the subproblems correspond to pairs of indices, starting and ending, of the input sequence.   15-32  Solutions for Chapter 15: Dynamic Programming  of X2;n cid:0 1.  For a sequence X D hx1; x2; : : : ; xni, we denote the subsequence starting at xi and ending at xj by Xij D hxi ; xiC1; : : : ; xji. Theorem  Optimal substructure of an LPS  Let X D hx1; x2; : : : ; xni be the input sequence, and let Z D h¬¥1; ¬¥2; : : : ; ¬¥mi be any LPS of X. 1. If n D 1, then m D 1 and ¬¥1 D x1. 2. If n D 2 and x1 D x2, then m D 2 and ¬¥1 D ¬¥2 D x1 D x2. 3. If n D 2 and x1 ¬§ x2, then m D 1 and ¬¥1 is equal to either x1 or xn. 4. If n > 2 and x1 D xn, then m > 2, ¬¥1 D ¬¥m D x1 D xn, and Z2;m cid:0 1 is an LPS 5. If n > 2 and x1 ¬§ xn, then ¬¥1 ¬§ x1 implies that Z1;m is an LPS of X2;n. 6. If n > 2 and x1 ¬§ xn, then ¬¥m ¬§ xn implies that Z1;m is an LPS of X1;n cid:0 1. Proof Properties  1 ,  2 , and  3  follow trivially from the deÔ¨Ånition of LPS.  4  If n > 2 and x1 D xn, then we can choose x1 and xn as the ends of Z and at least one more element of X as part of Z. Thus, it follows that m > 2. If ¬¥1 ¬§ x1, then we could append x1 D xn to the ends of Z to obtain a palindrome subsequence of X with length m C 2, contradicting the supposition that Z is a longest palindrome subsequence of X. Thus, we must have ¬¥1 D x1 .D xn D ¬¥m . Now, Z2;m cid:0 1 is a length-.m  cid:0  2  palindrome subsequence of X2;n cid:0 1. We wish to show that it is an LPS. Suppose for the purpose of contradiction that there exists a palindrome subsequence W of X2;n cid:0 1 with length greater than m  cid:0  2. Then, appending x1 D xn to the ends of W produces a palindrome subsequence of X whose length is greater than m, which is a contradiction.  5  If ¬¥1 ¬§ x1, then Z is a palindrome subsequence of X2;n. If there were a palindrome subsequence W of X2;n with length greater than m, then W would also be a palindrome subsequence of X, contradicting the assumption that Z is an LPS of X.  6  The proof is symmetric to  2 .  The way that the theorem characterizes longest palindrome subsequences tells us that an LPS of a sequence contains within it an LPS of a subsequence of the se- quence. Thus, the LPS problem has an optimal-substructure property.  Step 2: A recursive solution  The theorem implies that we should examine either one or two subproblems when Ô¨Ånding an LPS of X D hx1; x2; : : : ; xni, depending on whether x1 D xn. Let us deÔ¨Åne p≈íi; j ¬ç to be the length of an LPS of the subsequence Xij . If i D j , the LPS has length 1. If j D i C1, then the LPS has length either 1 or 2, depending on whether xi D xj . The optimal substructure of the LPS problem gives the following recursive formula:   Solutions for Chapter 15: Dynamic Programming  15-33  1 p≈íi C 1; j  cid:0  1¬ç C 2 max.p≈íi; j  cid:0  1¬ç; p≈íi C 1; j ¬ç   if i D j ; if j D i C 1 and xi D xj ; if j D i C 1 and xi ¬§ xj ; if j > i C 1 and xi D xj ; if j > i C 1 and xi ¬§ xj :  2  p≈íi; j ¬ç DÀö 1  Step 3: Computing the length of an LPS  Procedure LONGEST-PALINDROME takes a sequence X D hx1; x2; : : : ; xni as input. The procedure Ô¨Ålls cells p≈íi; i ¬ç, where 1  i  n, and p≈íi; i C 1¬ç, where 1  i  n cid:0 1, as the base cases. It then starts Ô¨Ålling cells p≈íi; j ¬ç, where j > i C 1. The procedure Ô¨Ålls the p table row by row, starting with row n  cid:0  2 and moving to- ward row 1.  Rows n  cid:0  1 and n are already Ô¨Ålled as part of the base cases.  Within each row, the procedure Ô¨Ålls the entries from left to right. The procedure also main- tains the table b≈í1 : : n; 1 : : n¬ç to help us construct an optimal solution. Intuitively, b≈íi; j ¬ç points to the table entry corresponding to the optimal subproblem solution chosen when computing p≈íi; j ¬ç. The procedure returns the b and p tables; p≈í1; n¬ç contains the length of an LPS of X. The running time of LONGEST-PALINDROME is clearly ‚Äö.n2 .  LONGEST-PALINDROME.X   n D X:length let b≈í1 : : n; 1 : : n¬ç and p≈í0 : : n; 0 : : n¬ç be new tables for i D 1 to n  cid:0  1  p≈íi; i ¬ç D 1 j D i C 1 if xi == xj  p≈íi; j ¬ç D 2 b≈íi; j ¬ç D ‚Äú.‚Äù else p≈íi; j ¬ç D 1 b≈íi; j ¬ç D ‚Äú‚Äù p≈ín; n¬ç D 1 for i D n  cid:0  2 downto 1 for j D i C 2 to n  if xi == xj  elseif p≈íi C 1; j ¬ç  p≈íi; j  cid:0  1¬ç  p≈íi; j ¬ç D p≈íi C 1; j  cid:0  1¬ç C 2 b≈íi; j ¬ç D ‚Äú.‚Äù p≈íi; j ¬ç D p≈íi C 1; j ¬ç b≈íi; j ¬ç D ‚Äú‚Äù else p≈íi; j ¬ç D p≈íi; j  cid:0  1¬ç b≈íi; j ¬ç D ‚Äú ‚Äù  return p and b   15-34  Solutions for Chapter 15: Dynamic Programming  Step 4: Constructing an LPS  Solution to Problem 15-3  The b table returned by LONGEST-PALINDROME enables us to quickly construct an LPS of X D hx1; x2; : : : ; xmi. We simply begin at b≈í1; n¬ç and trace through the table by following the arrows. Whenever we encounter a ‚Äú.‚Äù in entry b≈íi; j ¬ç, it implies that xi D yj are the Ô¨Årst and last elements of the LPS that LONGEST- PALINDROME found. The following recursive procedure returns a sequence S that contains an LPS of X. The initial call is GENERATE-LPS.b; X; 1; X:length;hi , where hi denotes an empty sequence. Within the procedure, the symbol jj denotes concatenation of a symbol and a sequence.  GENERATE-LPS.b; X; i; j; S    if i > j  return S elseif i == j  return S jj xi elseif b≈íi; j ¬ç == ‚Äú.‚Äù return xi jj GENERATE-LPS.b; X; i C 1; j  cid:0  1; S  jj xi elseif b≈íi; j ¬ç == ‚Äú‚Äù return GENERATE-LPS.b; X; i C 1; j; S   else return GENERATE-LPS.b; X; i; j  cid:0  1; S    Taking the book‚Äôs hint, we sort the points by x-coordinate, left to right, in O.n lg n  time. Let the sorted points be, left to right, hp1; p2; p3; : : : ; pni. Therefore, p1 is the leftmost point, and pn is the rightmost. We deÔ¨Åne as our subproblems paths of the following form, which we call bitonic paths. A bitonic path Pi;j , where i  j , includes all points p1; p2; : : : ; pj ; it starts at some point pi, goes strictly left to point p1, and then goes strictly right to point pj . By ‚Äúgoing strictly left,‚Äù we mean that each point in the path has a lower x- coordinate than the previous point. Looked at another way, the indices of the sorted points form a strictly decreasing sequence. Likewise, ‚Äúgoing strictly right‚Äù means that the indices of the sorted points form a strictly increasing sequence. Moreover, Pi;j contains all the points p1; p2; p3; : : : ; pj . Note that pj is the rightmost point in Pi;j and is on the rightgoing subpath. The leftgoing subpath may be degenerate, consisting of just p1. Let us denote the euclidean distance between any two points pi and pj by jpi pjj. And let us denote by b≈íi; j ¬ç, for 1  i  j  n, the length of the shortest bitonic path Pi;j . Since the leftgoing subpath may be degenerate, we can easily compute all values b≈í1; j ¬ç. The only value of b≈íi; i ¬ç that we will need is b≈ín; n¬ç, which is the length of the shortest bitonic tour. We have the following formulation of b≈íi; j ¬ç for 1  i  j  n:  b≈í1; 2¬ç D jp1p2j ; b≈íi; j ¬ç D b≈íi; j  cid:0  1¬ç C jpj cid:0 1pjj  b≈íj  cid:0  1; j ¬ç D  min 1k<j cid:0 1fb≈ík; j  cid:0  1¬ç C jpkpjjg :  for i < j  cid:0  1 ;   Solutions for Chapter 15: Dynamic Programming  15-35  Why are these formulas correct? Any bitonic path ending at p2 has p2 as its right- most point, so it consists only of p1 and p2. Its length, therefore, is jp1p2j. Now consider a shortest bitonic path Pi;j . The point pj cid:0 1 is somewhere on this path. If it is on the rightgoing subpath, then it immediately preceeds pj on this subpath. Otherwise, it is on the leftgoing subpath, and it must be the rightmost point on this subpath, so i D j  cid:0  1. In the Ô¨Årst case, the subpath from pi to pj cid:0 1 must be a shortest bitonic path Pi;j cid:0 1, for otherwise we could use a cut-and-paste argument to come up with a shorter bitonic path than Pi;j .  This is part of our opti- mal substructure.  The length of Pi;j , therefore, is given by b≈íi; j  cid:0  1¬ç C jpj cid:0 1pjj. In the second case, pj has an immediate predecessor pk, where k < j  cid:0  1, on the rightgoing subpath. Optimal substructure again applies: the subpath from pk to pj cid:0 1 must be a shortest bitonic path Pk;j cid:0 1, for otherwise we could use cut-and- paste to come up with a shorter bitonic path than Pi;j .  We have implicitly relied on paths having the same length regardless of which direction we traverse them.  The length of Pi;j , therefore, is given by min1kj cid:0 1 fb≈ík; j  cid:0  1¬ç C jpkpjjg. We need to compute b≈ín; n¬ç. In an optimal bitonic tour, one of the points adjacent to pn must be pn cid:0 1, and so we have b≈ín; n¬ç D b≈ín  cid:0  1; n¬ç C jpn cid:0 1pnj : To reconstruct the points on the shortest bitonic tour, we deÔ¨Åne r≈íi; j ¬ç to be the index of the immediate predecessor of pj on the shortest bitonic path Pi;j . Because the immediate predecessor of p2 on P1;2 is p1, we know that r≈í1; 2¬ç must be 1. The pseudocode below shows how we compute b≈íi; j ¬ç and r≈íi; j ¬ç. It Ô¨Ålls in only entries b≈íi; j ¬ç where 1  i  n  cid:0  1 and i C 1  j  n, or where i D j D n, and only entries r≈íi; j ¬ç where 1  i  n  cid:0  2 and i C 2  j  n. EUCLIDEAN-TSP.p  sort the points so that hp1; p2; p3; : : : ; pni are in order of increasing x-coordinate let b≈í1 : : n; 2 : : n¬ç and r≈í1 : : n  cid:0  2; 3 : : n¬ç be new arrays b≈í1; 2¬ç D jp1p2j for j D 3 to n  for i D 1 to j  cid:0  2 b≈íi; j ¬ç D b≈íi; j  cid:0  1¬ç C jpj cid:0 1pjj r≈íi; j ¬ç D j  cid:0  1 b≈íj  cid:0  1; j ¬ç D 1 for k D 1 to j  cid:0  2  q D b≈ík; j  cid:0  1¬ç C jpkpjj if q < b≈íj  cid:0  1; j ¬ç  b≈íj  cid:0  1; j ¬ç D q r≈íj  cid:0  1; j ¬ç D k  b≈ín; n¬ç D b≈ín  cid:0  1; n¬ç C jpn cid:0 1pnj return b and r  We print out the tour we found by starting at pn, then a leftgoing subpath that includes pn cid:0 1, from right to left, until we hit p1. Then we print right-to-left the re- maining subpath, which does not include pn cid:0 1. For the example in Figure 15.11 b  on page 405, we wish to print the sequence p7; p6; p4; p3; p1; p2; p5. Our code is recursive. The right-to-left subpath is printed as we go deeper into the recursion, and the left-to-right subpath is printed as we back out.   15-36  Solutions for Chapter 15: Dynamic Programming  PRINT-TOUR.r; n   print pn print pn cid:0 1 k D r≈ín  cid:0  1; n¬ç PRINT-PATH r; k; n  cid:0  1  print pk  PRINT-PATH.r; i; j    if i < j  k D r≈íi; j ¬ç if k ¬§ i print pk if k > 1  PRINT-PATH r; i; k   else k D r≈íj; i ¬ç  if k > 1  PRINT-PATH r; k; j   print pk  The relative values of the parameters i and j in each call of PRINT-PATH indicate which subpath we‚Äôre working on. If i < j , we‚Äôre on the right-to-left subpath, and if i > j , we‚Äôre on the left-to-right subpath. The test for k ¬§ i prevents us from printing p1 an extra time, which could occur when we call PRINT-PATH.r; 1; 2 . The time to run EUCLIDEAN-TSP is O.n2  since the outer loop on j iterates n cid:0  2 times and the inner loops on i and k each run at most n cid:0  2 times. The sorting step at the beginning takes O.n lg n  time, which the loop times dominate. The time to run PRINT-TOUR is O.n , since each point is printed just once.  Solution to Problem 15-4 This solution is also posted publicly  Note: We assume that no word is longer than will Ô¨Åt into a line, i.e., li  M for all i. First, we‚Äôll make some deÔ¨Ånitions so that we can state the problem more uniformly. Special cases about the last line and worries about whether a sequence of words Ô¨Åts in a line will be handled in these deÔ¨Ånitions, so that we can forget about them when framing our overall strategy.   DeÔ¨Åne extras≈íi; j ¬ç D M  cid:0  j C i  cid:0 Pj  kDi lk to be the number of extra spaces at the end of a line containing words i through j . Note that extras may be negative.   Now deÔ¨Åne the cost of including a line containing words i through j in the sum  we want to minimize:  lc≈íi; j ¬ç D¬Ä 1  if extras≈íi; j ¬ç < 0  i.e., words i; : : : ; j don‚Äôt Ô¨Åt  ; if j D n and extras≈íi; j ¬ç  0  last line costs 0  ;  0 .extras≈íi; j ¬ç 3 otherwise :   Solutions for Chapter 15: Dynamic Programming  15-37  By making the line cost inÔ¨Ånite when the words don‚Äôt Ô¨Åt on it, we prevent such an arrangement from being part of a minimal sum, and by making the cost 0 for the last line  if the words Ô¨Åt , we prevent the arrangement of the last line from inÔ¨Çuencing the sum being minimized.  We want to minimize the sum of lc over all lines of the paragraph. Our subproblems are how to optimally arrange words 1; : : : ; j , where j D 1; : : : ; n. Consider an optimal arrangement of words 1; : : : ; j . Suppose we know that the last line, which ends in word j , begins with word i. The preceding lines, therefore, contain words 1; : : : ; i  cid:0  1. In fact, they must contain an optimal arrangement of words 1; : : : ; i  cid:0  1.  The usual type of cut-and-paste argument applies.  Let c≈íj ¬ç be the cost of an optimal arrangement of words 1; : : : ; j . If we know that the last line contains words i; : : : ; j , then c≈íj ¬ç D c≈íi  cid:0  1¬çClc≈íi; j ¬ç. As a base case, when we‚Äôre computing c≈í1¬ç, we need c≈í0¬ç. If we set c≈í0¬ç D 0, then c≈í1¬ç D lc≈í1; 1¬ç, which is what we want. But of course we have to Ô¨Ågure out which word begins the last line for the sub- problem of words 1; : : : ; j . So we try all possibilities for word i, and we pick the one that gives the lowest cost. Here, i ranges from 1 to j . Thus, we can deÔ¨Åne c≈íj ¬ç recursively by  c≈íj ¬ç D  0  min 1ij  .c≈íi  cid:0  1¬ç C lc≈íi; j ¬ç   if j D 0 ; if j > 0 :  Note that the way we deÔ¨Åned lc ensures that      all choices made will Ô¨Åt on the line  since an arrangement with lc D 1 cannot be chosen as the minimum , and the cost of putting words i; : : : ; j on the last line will not be 0 unless this really is the last line of the paragraph  j D n  or words i : : : j Ô¨Åll the entire line.  We can compute a table of c values from left to right, since each value depends only on earlier values. To keep track of what words go on what lines, we can keep a parallel p table that points to where each c value came from. When c≈íj ¬ç is computed, if c≈íj ¬ç is based on the value of c≈ík  cid:0  1¬ç, set p≈íj ¬ç D k. Then after c≈ín¬ç is computed, we can trace the pointers to see where to break the lines. The last line starts at word p≈ín¬ç and goes through word n. The previous line starts at word p≈íp≈ín¬ç¬ç and goes through word p≈ín¬ç  cid:0  1, etc. In pseudocode, here‚Äôs how we construct the tables:   15-38  Solutions for Chapter 15: Dynamic Programming  PRINT-NEATLY.l; n; M    let extras≈í1 : : n; 1 : : n¬ç, lc≈í1 : : n; 1 : : n¬ç, and c≈í0 : : n¬ç be new arrays    Compute extras≈íi; j ¬ç for 1  i  j  n. for i D 1 to n  extras≈íi; i ¬ç D M  cid:0  li for j D i C 1 to n  extras≈íi; j ¬ç D extras≈íi; j  cid:0  1¬ç  cid:0  lj  cid:0  1     Compute lc≈íi; j ¬ç for 1  i  j  n. for i D 1 to n  for j D i to n  if extras≈íi; j ¬ç < 0 lc≈íi; j ¬ç D 1 lc≈íi; j ¬ç D 0  elseif j == n and extras≈íi; j ¬ç  0 else lc≈íi; j ¬ç D .extras≈íi; j ¬ç 3     Compute c≈íj ¬ç and p≈íj ¬ç for 1  j  n. c≈í0¬ç D 0 for j D 1 to n c≈íj ¬ç D 1 for i D 1 to j  if c≈íi  cid:0  1¬ç C lc≈íi; j ¬ç < c≈íj ¬ç  c≈íj ¬ç D c≈íi  cid:0  1¬ç C lc≈íi; j ¬ç p≈íj ¬ç D i  return c and p  Quite clearly, both the time and space are ‚Äö.n2 . In fact, we can do a bit better: we can get both the time and space down to ‚Äö.nM  . The key observation is that at most dM=2e words can Ô¨Åt on a line.  Each word is at least one character long, and there‚Äôs a space between words.  Since a line with words i; : : : ; j contains j  cid:0  i C 1 words, if j  cid:0  i C 1 > dM=2e then we know that lc≈íi; j ¬ç D 1. We need only compute and store extras≈íi; j ¬ç and lc≈íi; j ¬ç for j  cid:0  i C 1  dM=2e. And the inner for loop header in the computation of c≈íj ¬ç and p≈íj ¬ç can run from max.1; j  cid:0  dM=2e C 1  to j . We can reduce the space even further to ‚Äö.n . We do so by not storing the lc and extras tables, and instead computing the value of lc≈íi; j ¬ç as needed in the last loop. The idea is that we could compute lc≈íi; j ¬ç in O.1  time if we knew the value of extras≈íi; j ¬ç. And if we scan for the minimum value in descending order of i, we can compute that as extras≈íi; j ¬ç D extras≈íi C 1; j ¬ç  cid:0  li  cid:0  1.  Initially, extras≈íj; j ¬ç D M  cid:0  lj .  This improvement reduces the space to ‚Äö.n , since now the only tables we store are c and p. Here‚Äôs how we print which words are on which line. The printed output of GIVE-LINES.p; j   is a sequence of triples .k; i; j  , indicating that words i; : : : ; j are printed on line k. The return value is the line number k.   Solutions for Chapter 15: Dynamic Programming  15-39  GIVE-LINES.p; j   i D p≈íj ¬ç if i == 1 k D 1 else k D GIVE-LINES.p; i  cid:0  1  C 1 print .k; i; j   return k  The initial call is GIVE-LINES.p; n . Since the value of j decreases in each recur- sive call, GIVE-LINES takes a total of O.n  time.  Solution to Problem 15-5  a. Dynamic programming is the ticket. This problem is slightly similar to the longest-common-subsequence problem. In fact, we‚Äôll deÔ¨Åne the notational con- veniences Xi and Yj in the similar manner as we did for the LCS problem: Xi D x≈í1 : : i ¬ç and Yj D y≈í1 : : j ¬ç. Our subproblems will be determining an optimal sequence of operations that converts Xi to Yj , for 0  i  m and 0  j  n. We‚Äôll call this the ‚ÄúXi ! Yj problem.‚Äù The original problem is the Xm ! Yn problem. Let‚Äôs suppose for the moment that we know what was the last operation used to convert Xi to Yj . There are six possibilities. We denote by c≈íi; j ¬ç the cost of an optimal solution to the Xi ! Yj problem.            If the last operation was a copy, then we must have had x≈íi ¬ç D y≈íj ¬ç. The sub- problem that remains is converting Xi cid:0 1 to Yj cid:0 1. And an optimal solution to the Xi ! Yj problem must include an optimal solution to the Xi cid:0 1 ! Yj cid:0 1 problem. The cut-and-paste argument applies. Thus, assuming that the last operation was a copy, we have c≈íi; j ¬ç D c≈íi  cid:0  1; j  cid:0  1¬ç C cost.copy . If it was a replace, then we must have had x≈íi ¬ç ¬§ y≈íj ¬ç.  Here, we assume that we cannot replace a character with itself. It is a straightforward mod- iÔ¨Åcation if we allow replacement of a character with itself.  We have the same optimal substructure argument as for copy, and assuming that the last operation was a replace, we have c≈íi; j ¬ç D c≈íi  cid:0  1; j  cid:0  1¬ç C cost.replace . If it was a twiddle, then we must have had both x≈íi ¬ç D y≈íj  cid:0  1¬ç and x≈íi  cid:0  1¬ç D y≈íj ¬ç, along with the implicit assumption that i; j  2. Now our subproblem is Xi cid:0 2 ! Yj cid:0 2 and, assuming that the last operation was a twiddle, we have c≈íi; j ¬ç D c≈íi  cid:0  2; j  cid:0  2¬ç C cost.twiddle . If it was a delete, then we have no restrictions on x or y. Since we can view delete as removing a character from Xi and leaving Yj alone, our subprob- lem is Xi cid:0 1 ! Yj . Assuming that the last operation was a delete, we have c≈íi; j ¬ç D c≈íi  cid:0  1; j ¬ç C cost.delete . If it was an insert, then we have no restrictions on x or y. Our subproblem is Xi ! Yj cid:0 1. Assuming that the last operation was an insert, we have c≈íi; j ¬ç D c≈íi; j  cid:0  1¬ç C cost.insert .   15-40  Solutions for Chapter 15: Dynamic Programming    If it was a kill, then we had to have completed converting Xm to Yn, so that the current problem must be the Xm ! Yn problem. In other words, we must have i D m and j D n. If we think of a kill as a multiple delete, we can get any Xi ! Yn, where 0  i < m, as a subproblem. We pick the best one, and so assuming that the last operation was a kill, we have c≈ím; n¬ç D min  0i <mfc≈íi; n¬çg C cost.kill  :  We have not handled the base cases, in which i D 0 or j D 0. These are easy. X0 and Y0 are the empty strings. We convert an empty string into Yj by a sequence of j inserts, so that c≈í0; j ¬ç D j  cost.insert . Similarly, we convert Xi into Y0 by a sequence of i deletes, so that c≈íi; 0¬ç D i  cost.delete . When i D j D 0, either formula gives us c≈í0; 0¬ç D 0, which makes sense, since there‚Äôs no cost to convert the empty string to the empty string. For i; j > 0, our recursive formulation for c≈íi; j ¬ç applies the above formulas in the situations in which they hold:  c≈íi  cid:0  1; j  cid:0  1¬ç C cost.replace  c≈íi  cid:0  2; j  cid:0  2¬ç C cost.twiddle   c≈íi; j ¬ç D min‚Ä† c≈íi  cid:0  1; j  cid:0  1¬ç C cost.copy   c≈íi  cid:0  1; j ¬ç C cost.delete  c≈íi; j ¬ç D c≈íi; j  cid:0  1¬ç C cost.insert  0i <mfc≈íi; n¬çg C cost.kill  min  if x≈íi ¬ç D y≈íj ¬ç ; if x≈íi ¬ç ¬§ y≈íj ¬ç ; if i; j  2; x≈íi ¬ç D y≈íj  cid:0  1¬ç; and x≈íi  cid:0  1¬ç D y≈íj ¬ç ; always ; always ; if i D m and j D n :  Like we did for LCS, our pseudocode Ô¨Ålls in the table in row-major order, i.e., row-by-row from top to bottom, and left to right within each row. Column- major order  column-by-column from left to right, and top to bottom within each column  would also work. Along with the c≈íi; j ¬ç table, we Ô¨Åll in the table op≈íi; j ¬ç, holding which operation was used.   Solutions for Chapter 15: Dynamic Programming  15-41  EDIT-DISTANCE.x; y; m; n   let c≈í0 : : m; 0 : : n¬ç and op≈í0 : : m; 0 : : n¬ç be new arrays for i D 0 to m  for j D 0 to n  c≈íi; 0¬ç D i  cost.delete  op≈íi; 0¬ç D DELETE c≈í0; j ¬ç D j  cost.insert  op≈í0; j ¬ç D INSERT for j D 1 to n c≈íi; j ¬ç D 1 if x≈íi ¬ç == y≈íj ¬ç  for i D 1 to m  if x≈íi ¬ç ¬§ y≈íj ¬ç and c≈íi  cid:0  1; j  cid:0  1¬ç C cost.replace  < c≈íi; j ¬ç  c≈íi; j ¬ç D c≈íi  cid:0  1; j  cid:0  1¬ç C cost.copy  op≈íi; j ¬ç D COPY c≈íi; j ¬ç D c≈íi  cid:0  1; j  cid:0  1¬ç C cost.replace  op≈íi; j ¬ç D REPLACE by y≈íj ¬ç  if i  2 and j  2 and x≈íi ¬ç == y≈íj  cid:0  1¬ç and x≈íi  cid:0  1¬ç == y≈íj ¬ç and c≈íi  cid:0  2; j  cid:0  2¬ç C cost.twiddle  < c≈íi; j ¬ç c≈íi; j ¬ç D c≈íi  cid:0  2; j  cid:0  2¬ç C cost.twiddle  op≈íi; j ¬ç D TWIDDLE c≈íi; j ¬ç D c≈íi  cid:0  1; j ¬ç C cost.delete  op≈íi; j ¬ç D DELETE c≈íi; j ¬ç D c≈íi; j  cid:0  1¬ç C cost.insert  op≈íi; j ¬ç D INSERT y≈íj ¬ç   if c≈íi  cid:0  1; j ¬ç C cost.delete  < c≈íi; j ¬ç  if c≈íi; j  cid:0  1¬ç C cost.insert  < c≈íi; j ¬ç  for i D 0 to m  cid:0  1  if c≈íi; n¬ç C cost.kill  < c≈ím; n¬ç  c≈ím; n¬ç D c≈íi; n¬ç C cost.kill  op≈ím; n¬ç D KILL i  return c and op  The time and space are both ‚Äö.mn . If we store a KILL operation in op≈ím; n¬ç, we also include the index i after which we killed, to help us reconstruct the optimal sequence of operations.  We don‚Äôt need to store y≈íi ¬ç in the op table for replace or insert operations.  To reconstruct this sequence, we use the op table returned by EDIT-DISTANCE. The procedure OP-SEQUENCE.op; i; j   reconstructs the optimal operation se- quence that we found to transform Xi into Yj . The base case is when i D j D 0. The Ô¨Årst call is OP-SEQUENCE.op; m; n .   15-42  Solutions for Chapter 15: Dynamic Programming  OP-SEQUENCE.op; i; j   if i == 0 and j D 0 if op≈íi; j ¬ç == COPY or op≈íi; j ¬ç D REPLACE  return  elseif op≈íi; j ¬ç == TWIDDLE  elseif op≈íi; j ¬ç == INSERT  elseif op≈íi; j ¬ç == DELETE  i0 D i  cid:0  1 j 0 D j  cid:0  1 i0 D i  cid:0  2 j 0 D j  cid:0  2 i0 D i  cid:0  1 j 0 D j i0 D i j 0 D j  cid:0  1 let op≈íi; j ¬ç == KILL k i0 D k j 0 D j OP-SEQUENCE.op; i0; j 0  print op≈íi; j ¬ç  else     don‚Äôt care yet what character is inserted     must be KILL, and must have i D m and j D n  This procedure determines which subproblem we used, recurses on it, and then prints its own last operation.  b. The DNA-alignment problem is just the edit-distance problem, with  cost.copy  D  cid:0 1 ; cost.replace  D C1 ; cost.delete  D C2 ; cost.insert  D C2 ; and the twiddle and kill operations are not permitted. The score that we are trying to maximize in the DNA-alignment problem is precisely the negative of the cost we are trying to minimize in the edit-distance problem. The negative cost of copy is not an impediment, since we can only apply the copy operation when the characters are equal.  Solution to Problem 15-8  a. Let us set up a recurrence for the number of valid seams as a function of m. Suppose we are in the process of carving out a seam row by row, starting from the Ô¨Årst row. Let the last pixel carved out be A≈íi; j ¬ç. How many choices do we have for the pixel in row i C 1 such that the pixel continues the seam? If the last pixel A≈íi; j ¬ç were on the column boundary  i D 1 or i D n , then there would be two choices for the next pixel. For example, when i D 1, the two choices for the next pixel are A≈íi C 1; j ¬ç and A≈íi C 1; j C 1¬ç. Otherwise, there would   Solutions for Chapter 15: Dynamic Programming  15-43  be three choices for the next pixel: A≈íi C 1; j  cid:0  1¬ç; A≈íi C 1; j ¬ç; A≈íi C 1; j C 1¬ç. Thus, for a general pixel A≈íi; j ¬ç, there are at least two possible choices for a pixel p in the next row such that p continues a seam ending in A≈íi; j ¬ç. Let T .i   denote the number of possible seams from row 1 to row i. Then, for i D 1, we have T .i   D n, and for i > 1, T .i    2T .i  cid:0  1  : It is easy to guess that T .i    n2i cid:0 1, which we verify by direct substitution. For i D 1, we have T .1  D n  n  20. For i > 1, we have T .i    2T .i  cid:0  1   2  n2i cid:0 2 D n2i cid:0 1 :  Thus, the total number T .m  of seams is at least n2m cid:0 1. We conclude that the number of seams grows at least exponentially in m.  b. As proved in the previous part, it is infeasible to systematically check every  seam, since the number of possible seams grows exponentially. The structure of the problem allows us to build the solution row by row. Con- sider a pixel A≈íi; j ¬ç. We ask the question: ‚ÄúIf i were the Ô¨Årst row of the picture, what is the minimum disruptive measure of seams that start with the pixel A≈íi; j ¬ç?‚Äù Let S be a seam of minimum disruptive measure among all seams that start with pixel A≈íi; j ¬ç. Let A≈íi C 1; p¬ç, where p 2 fj  cid:0  1; j; j C 1g, be the pixel of S in the next row. Let S0 be the sub-seam of S that starts with A≈íi C 1; p¬ç. We claim that S0 has the minimum disruptive measure among seams that start with A≈íi C 1; p¬ç. Why? Suppose there exists another seam S00 that starts with A≈íi C 1; p¬ç and has disruptive measure less than that of S0. By using S00 as the sub-seam instead of S0, we can obtain another seam that starts with A≈íi; j ¬ç and has a disruptive measure which is less than that of S. Thus, we obtain a contradiction to our assumption that S is a seam of minimum disruptive mea- sure. Let disr≈íi; j ¬ç be the value of the minimum disruptive measure among all seams that start with pixel A≈íi; j ¬ç. For row m, the seam with the minimum disruptive measure consists of just one point. We can now state a recurrence for disr≈íi; j ¬ç as follows. In the base case, disr≈ím; j ¬ç D d ≈ím; j ¬ç for j D 1; 2; : : : ; n. In the recursive case, for j D 1; 2; : : : ; n, disr≈íi; j ¬ç D d ≈íi; j ¬ç C min where the set K of index offsets is  k2K fdisr≈íi C i; j C k¬çg ;  K D¬Ä f0; 1g  f cid:0 1; 0; 1g f cid:0 1; 0g  if j D 1 ; if 1 < j < m ; if j D n :  Since every seam has to start with a pixel of the Ô¨Årst row, we simply Ô¨Ånd the minimum disr≈í1; j ¬ç for pixels in the Ô¨Årst row to obtain the minimum disruptive measure.   15-44  Solutions for Chapter 15: Dynamic Programming  COMPRESS-IMAGE .d   m D d:rows n D d:columns let disr≈í1 : : m; 1 : : n¬ç and next≈í1 : : m; 1 : : n¬ç be new tables for j D 1 to n disr≈ím; j ¬ç D d ≈ím; j ¬ç for i D m  cid:0  1 downto 1 for j D 1 to n  low D max. cid:0 1; 1  cid:0  j   high D min.1; n  cid:0  j   disr≈íi; j ¬ç D 1 for k D low to high  if disr≈íi C 1; j C k¬ç < disr≈íi; j ¬ç  disr≈íi; j ¬ç D disr≈íi C 1; j C k¬ç next≈íi; j ¬ç D j C k  disr≈íi; j ¬ç D disr≈íi; j ¬ç C d ≈íi; j ¬ç  al D 1 start D 1 for j D 1 to n  if disr≈í1; j ¬ç < al  al D disr≈í1; j ¬ç start D j  print ‚ÄúThe minimum value of the disruptive measure is ‚Äù al for i D 1 to m  print ‚Äúcut point at ‚Äù .i; start  start D next≈íi; start¬ç  The procedure COMPRESS-IMAGE is simply an implementation of this recur- rence in a bottom-up fashion. We Ô¨Årst carry out the initialization of the base cases, which are the cases when row i D m. The minimum disruptive measure for the base cases is sim- ply d ≈ím; j ¬ç. The next for loop runs down from m  cid:0  1 to 1. Thus, disr≈íi C 1; j ¬ç is already available before computing disr≈íi; j ¬ç for pixels of row i. The assignments to low and high allow the index offset k to range over the correct set K from above. We set low to 0 when j D 1 and to  cid:0 1 when j > 1, and we set high to 0 when j D n and to 1 when j < n. The innermost for loop sets disr≈íi; j ¬ç to the minimum value of disr≈íi C 1; j C k¬ç for all k 2 K, and the line that follows this loop adds in d ≈íi; j ¬ç. We use the next table to reconstruct the actual seam. For a given pixel, it records which pixel was used as the next pixel. SpeciÔ¨Åcally, for a pixel A≈íi; j ¬ç, if next≈íi; j ¬ç D p, where p 2 fj  cid:0  1; j; j C 1g, then the next pixel of the seam is A≈íi C 1; p¬ç. The last line of the for loop adds the disruptive measure of the current pixel to the disruptive measure of the seam. The next for loop Ô¨Ånds the minimum disruptive measure of pixels in the Ô¨Årst row. We print the minimum disruptive measure as the answer.   Solution to Problem 15-9  Solutions for Chapter 15: Dynamic Programming  15-45  The rest of the code reconstructs the actual seam, using the information stored in the next array. Noting that the innermost for loop runs over at most three values of k, we see that the running time of COMPRESS-IMAGE is O.mn . The space requirement is also O.mn . We can improve upon the space requirement by observing that row i of the disr table depends on only row i C 1. Therefore, we can store just two rows at any time. Thus, we can improve the space requirement of COMPRESS-IMAGE to O.n .  Our Ô¨Årst step will be to identify the subproblems that satisfy the optimal- substructure property. Before we frame the subproblem, we make two simplifying modiÔ¨Åcations to the input:   We sort L so that the indices in L are in ascending order.  We prepend the index 0 to the beginning of L and append n to the end of L.  Let L≈íi : : j ¬ç denote a subarray of L that starts from index i and ends at index j . DeÔ¨Åne the subproblem denoted by .i; j   as ‚ÄúWhat is the cheapest sequence of breaks to break the substring S ≈í L≈íi ¬ç C 1 : : L≈íj ¬ç ¬ç?‚Äù Note that the Ô¨Årst and last elements of the subarray L≈íi : : j ¬ç deÔ¨Åne the ends of the substring, and we have to worry about only the indices of the subarray L≈íi C 1 : : j  cid:0  1¬ç. For example, let L D h20; 17; 14; 11; 25i and n D 30. First, we sort L. Then, we prepend 0 and append n as explained to get L D h0; 11; 14; 17; 20; 25; 30i. Now, what is the subproblem .2; 6 ? We obtain a substring by breaking S after character L≈í2¬ç D 11 and character L≈í6¬ç D 25. We ask ‚ÄúWhat is the cheapest sequence of breaks to break the substring S ≈í12 : : 25¬ç?‚Äù We have to worry about only indices in the subarray L≈í3 : : 5¬ç D h14; 17; 20i, since the other indices are not present in the substring. At this point, the problem looks similar to matrix-chain multiplication  see Sec- tion 15.2 . We can make the Ô¨Årst break at any element of L≈íi C 1 : : j  cid:0  1¬ç. Suppose that an optimal sequence of breaks  for subproblem .i; j   makes the Ô¨Årst break at L≈ík¬ç, where i < k < j . This break gives rise to two subproblems:  The ‚ÄúpreÔ¨Åx‚Äù subproblem .i; k , covering the subarray L≈íi C 1 : : k  cid:0  1¬ç,  The ‚ÄúsufÔ¨Åx‚Äù subproblem .k; j  , covering the subarray L≈ík C 1 : : j  cid:0  1¬ç. The overall cost can be expressed as the sum of the length of the substring, the preÔ¨Åx cost, and the sufÔ¨Åx cost. We show optimal substructure by claiming that the sequence of breaks in  for the preÔ¨Åx subproblem .i; k  must be an optimal one. Why? If there were a less costly way to break the substring S ≈í L≈íi ¬çC1 : : L≈ík¬ç ¬ç represented by the subproblem .i; k , then substituting that sequence of breaks in  would produce another sequence of breaks whose cost is lower than that of , which would be a contradiction. A sim- ilar observation holds for the sequence of breaks for the sufÔ¨Åx subproblem .k; j  : it must be an optimal sequence of breaks.   15-46  Solutions for Chapter 15: Dynamic Programming  Let cost≈íi; j ¬ç denote the cost of the cheapest solution to subproblem .i; j  . We write the recurrence relation for cost as  cost≈íi; j ¬ç DÀö 0  if j  cid:0  i  1 ; i   1 :  min  Thus, our approach to solving the subproblem .i; j   will be to try to split the re- spective substring at all possible values of k and then choosing a break that results in the minimum cost. We need to be careful to solve smaller subproblems before we solve larger subproblems. In particular, we solve subproblems in increasing order of the length j  cid:0  i. BREAK-STRING.n; L   prepend 0 to the start of L and append n to the end of L m D L:length sort L into increasing order let cost≈í1 : : m; 1 : : m¬ç and break≈í1 : : m; 1 : : m¬ç be new tables for i D 1 to m  cid:0  1 cost≈ím; m¬ç D 0 for len D 3 to m  cost≈íi; i ¬ç D cost≈íi; i C 1¬ç D 0  for i D 1 to m  cid:0  len C 1  j D i C len  cid:0  1 cost≈íi; j ¬ç D 1 for k D i C 1 to j  cid:0  1  if cost≈íi; k¬ç C cost≈ík; j ¬ç < cost≈íi; j ¬ç  cost≈íi; j ¬ç D cost≈íi; k¬ç C cost≈ík; j ¬ç break≈íi; j ¬ç D k  cost≈íi; j ¬ç D cost≈íi; j ¬ç C L≈íj ¬ç  cid:0  L≈íi ¬ç  print ‚ÄúThe minimum cost of breaking the string is ‚Äù cost≈í1; m¬ç PRINT-BREAKS.L; break; 1; m   After sorting L, we initialize the base cases, in which i D j or j D i C 1. The nested for loops represent the main computation. The outermost for loop runs for len D 3 to m, which means that we need to consider subarrays of L with length at least 3, since the Ô¨Årst and the last element deÔ¨Åne the substring, and we need at least one more element to specify a break. The increasing values of len also ensures that we solve subproblems with smaller length before we solve subproblems with greater length. The inner for loop on i runs from 1 to m cid:0  lenC 1. The upper bound of m cid:0  lenC 1 is the largest value that the start index i can take such that i C len  cid:0  1  m. In the innermost for loop, we try each possible location k as the place to make the Ô¨Årst break for subproblem .i; j  . The Ô¨Årst such place is L≈íiC1¬ç, and not L≈íi ¬ç, since L≈íi ¬ç represents the start of the substring  and thus not a valid place for a break . Similarly, the last valid place is L≈íj  cid:0  1¬ç, because L≈íj ¬ç represents the end of the substring. The if condition tests whether k is the best place for a break found so far, and it updates the best value in cost≈íi; j ¬ç if so. We use break≈íi; j ¬ç to record that the   Solutions for Chapter 15: Dynamic Programming  15-47  best place for the Ô¨Årst break is k. SpeciÔ¨Åcally, if break≈íi; j ¬ç D k, then an optimal sequence of breaks for .i; j   makes the Ô¨Årst break at L≈ík¬ç. Finally, we add the length of the substring L≈íj ¬ç  cid:0  L≈íi ¬ç to cost≈íi; j ¬ç because, irre- spective of what we choose as the Ô¨Årst break, it costs us a price equal to the length of the substring to make a break. The lowest cost for the original problem ends up in cost≈í1; m¬ç. By our initialization, L≈í1¬ç D 0 and L≈ím¬ç D n. Thus, cost≈í1; m¬ç will hold the optimum price of cutting the substring from L≈í1¬ç C 1 D 1 to L≈ím¬ç D n, which is the entire string. The running time is ‚Äö.m3 , and it is dictated by the three nested for loops. They Ô¨Åll in the entries above the main diagonal of the two tables, except for entries in which j D i C 1. That is, they Ô¨Åll in rows i D 1; 2; : : : ; m  cid:0  2, entries j D i C 2; i C 3; : : : ; m. When Ô¨Ålling in entry ≈íi; j ¬ç, we check values of k running from i C 1 to j  cid:0  1, or j  cid:0  i  cid:0  1 entries. Thus, the total number of iterations of the innermost for loop is m cid:0 2 XiD1 XjDiC2   d D j  cid:0  i  cid:0  1   .j  cid:0  i  cid:0  1  D  m cid:0 i cid:0 1  m cid:0 2  d  m  XdD1 ‚Äö..m  cid:0  i  2   ‚Äö.h2   D  m cid:0 2  XiD1 XiD1 XhD2 D D ‚Äö.m3   m cid:0 1   equation  A.2     h D m  cid:0  i   equation  A.3   .  Since each iteration of the innermost for loop takes constant time, the total running time is ‚Äö.m3 . Note in particular that the running time is independent of the length of the string n.  PRINT-BREAKS.L; break; i; j   if j  cid:0  i  2  k D break≈íi; j ¬ç print ‚ÄúBreak at ‚Äù L≈ík¬ç PRINT-BREAKS.L; break; i; k  PRINT-BREAKS.L; break; k; j    PRINT-BREAKS uses the information stored in break to print out the actual se- quence of breaks.  Solution to Problem 15-11  We state the subproblem .k; s  as ‚ÄúWhat is the cheapest way to satisfy all the de- mands of months k; : : : ; n when we start with a surplus of s before the kth month?‚Äù A plan for the subproblem .k; s  would specify the number of machines to manu- facture for each month k; : : : ; n such that demands are satisÔ¨Åed. In some optimal plan P to .k; s , let f  machines be maufactured in month k. Thus, the surplus s0 in month k C 1 is s C f   cid:0  dk. Let P 0 be the part of the   15-48  Solutions for Chapter 15: Dynamic Programming  plan P for months k C 1; : : : ; n. We claim that P 0 is an optimal plan for the subproblem .k C 1; s0 . Why? Suppose P 0 were not an optimal plan and let P 00 be an optimal plan for .k C 1; s0 . If we modify plan P by cutting out P 0 and pasting in P 00  i.e., by using plan P 00 for months k C 1; : : : ; n , we obtain another plan for .k; s  which is cheaper than plan P . Thus, we obtain a contradiction to the assumption that plan P was optimal. Let cost≈ík; s¬ç denote the cost of an optimal plan for .k; s , and let f denote the number of machines that can be manufactured in month k. The bounds for f are as follows:   At least the number of machines so that  along with surplus s  there are enough machines to satisfy the current month‚Äôs demand. Let us denote this lower bound by L.k; s . We have L.k; s  D max.dk  cid:0  s; 0  :   At most the number of machines such that there are enough machines to sat- isfy the demands of all the following months. Let us denote this upper bound by U.k; s . We have  U.k; s  D  n XiDk  di!  cid:0  s :  For the last month, we need only manufacture the minimum required number of machines, given by L.n; s . For other months, we examine the costs of manufac- turing all feasible numbers of machines and see which choice gives us the cheapest plan. We can now write the recurrence for cost as the following:  cost≈ík; s¬ç D‚Äö c  max.L.n; s   cid:0  m; 0   C h.s C L.n; s   cid:0  dn  min C c  max.f  cid:0  m; 0  C h.s C f  cid:0  dk o  L.k;s f U.k;s ncost≈ík C 1; s C f  cid:0  dk¬ç  if k D n ;  if 0 < k < n :  The recurrence suggests how to build an optimal plan in a bottom-up fashion. We now present the algorithm for constructing an optimal plan.   Solutions for Chapter 15: Dynamic Programming  15-49  INVENTORY-PLANNING.n; m; c; D; d; h   let cost≈í1 : : n; 0 : : D¬ç and make≈í1 : : n; 0 : : D¬ç be new tables    Compute cost≈ín; 0 : : D¬ç and make≈ín; 0 : : D¬ç. for s D 0 to D  f D max.dn  cid:0  s; 0  cost≈ín; s¬ç D c  max.f  cid:0  m; 0  C h.s C f  cid:0  dn  make≈ín; s¬ç D f     Compute cost≈í1 : : n  cid:0  1; 0 : : D¬ç and make≈í1 : : n  cid:0  1; 0 : : D¬ç. U D dn for k D n  cid:0  1 downto 1  U D U C dk for s D 0 to D  cost≈ík; s¬ç D 1 for f D max.dk  cid:0  s; 0  to U  cid:0  s al D cost≈ík C 1; s C f  cid:0  dk¬ç if al < cost≈ík; s¬ç  C c  max.f  cid:0  m; 0  C h.s C f  cid:0  dk   cost≈ík; s¬ç D al make≈ík; s¬ç D f  print cost[1,0] PRINT-PLAN.make; n; d    PRINT-PLAN.make; n; d   s D 0 for k D 1 to n  print ‚ÄúFor month ‚Äù k ‚Äú manufacture ‚Äù make≈ík; s¬ç ‚Äú machines‚Äù s D s C make≈ík; s¬ç  cid:0  dk  In INVENTORY-PLANNING, we build the solution month by month, starting from month n, moving backward toward month 1. First, we solve the subproblem for the last month, for all surpluses. Then, for each month and for each surplus entering that month, we calculate the cheapest way to satisfy demand for that month based on the solved subproblems of the next month.   f is the number of machines that we try to manufacture in month k.    cost≈ík; s¬ç holds the cheapest way to satisfy demands of months k; : : : ; n, with a net surplus of s left over at the beginning of month k.   make≈ík; s¬ç holds the number of machines to manufacture in month k and the surplus s of an optimal plan. We will use this table to reconstruct the optimal plan.  We Ô¨Årst initialize the base cases, which are the cases for month n starting with surplus s, for s D 0; : : : ; D. If dn > s, it sufÔ¨Åces to manufacture dn  cid:0  s ma- chines, since we need not keep any surplus after month n. If dn  s, we need not manufacture any machines at all. We then calculate the total cost for month n as the sum of hiring extra labor c  max.f  cid:0  m; 0  and the inventory costs for leftover surplus h.sCf  cid:0 dn , which can be nonzero if we had started out with a large surplus.   15-50  Solutions for Chapter 15: Dynamic Programming  Solution to Problem 15-12  The outer for loop of the next block of code runs down from month n cid:0  1 to 1, thus ensuring that when we consider month k, we have already solved the subproblems of month k C 1. The next inner for loop iterates through all possible values of f as described. For every choice of f for a given month k, the total cost of .k; s  is given by the cost of extra labor  if any  plus the cost of inventory  if there is a surplus  plus the cost of the subproblem .k C 1; s C f  cid:0  dk . This value is checked and updated. Finally, the required answer is the answer to the subproblem .1; 0 , which ap- pears in cost≈í1; 0¬ç. That is, it is the cheapest way to satisfy all the demands of months 1; : : : ; n when we start with a surplus of 0. The running time of INVENTORY-PLANNING is clearly O.nD2 . The space re- quirement is O.nD . We can improve upon the space requirement by noting that we need only store the solution to subproblems of the next month. With this obser- vation, we can construct an algorithm that uses O.n C D  space.  Let p:cost denote the cost and p:orp denote the VORP of player p. We shall assume that all dollar amounts are expressed in units of $100,000. Since the order of choosing players for the positions does not matter, we may assume that we make our decisions starting from position 1, moving toward posi- tion N . For each position, we decide to either sign one player or sign no players. Suppose we decide to sign player p, who plays position 1. Then, we are left with an amount of X  cid:0  p:cost dollars to sign players at positions 2; : : : ; N . This obser- vation guides us in how to frame the subproblems. We deÔ¨Åne the cost and VORP of a set of players as the sum of costs and the sum of VORPs of all players in that set. Let .i; x  denote the following subproblem: ‚ÄúSuppose we consider only positions i; i C 1; : : : ; N and we can spend at most x dollars. What set of players  with at most one player for each position under con- sideration  has the maximum VORP?‚Äù A valid set of players for .i; x  is one in which each player in the set plays one of the positions i; i C 1; : : : ; n, each position has at most one player, and the cost of the players in the set is at most x dollars. An optimal set of players for .i; x  is a valid set with the maximum VORP. We now show that the problem exhibits optimal substructure.  Theorem  Optimal substructure of the VORP maximization problem  Let L D fp1; p2; : : : ; pkg be a set of players, possibly empty, with maximum VORP for the subproblem .i; x . 1. If i D N , then L has at most one player. If all players in position N have cost more than x, then L has no players. Otherwise, L D fp1g, where p1 has the maximum VORP among players for position N with cost at most x. 2. If i < N and L includes player p for position i, then L0 D L  cid:0  fpg is an 3. If i < N and L does not include a player for position i, then L is an optimal  optimal set for the subproblem .i C 1; x  cid:0  p:cost . set for the subproblem .i C 1; x .   Solutions for Chapter 15: Dynamic Programming  15-51  Proof Property  1  follows trivially from the problem statement.  2  Suppose that L0 is not an optimal set for the subproblem .i C 1; x  cid:0  p:cost . Then, there exists another valid set L00 for .i C 1; x  cid:0  p:cost  that has VORP more than L0. Let L000 D L00 [ fpg. The cost of L000 is at most x, since L00 has a cost at most x  cid:0  p:cost. Moreover, L000 has at most one player for each position i; i C 1; : : : ; N . Thus, L000 is a valid set for .i; x . But L000 has VORP more than L, thus contradicting the assumption that L had the maximum VORP for .i; x .  3  Clearly, any valid set for .i C 1; x  is also a valid set for .i; x . If L were not an optimal set for .i C 1; x , then there exists another valid set L0 for .i C 1; x  with VORP more than L. The set L0 would also be a valid set for .i; x , which contradicts the assumption that L had the maximum VORP for .i; x .  The theorem suggests that when i < N , we examine two subproblems and choose the better of the two. Let ≈íi; x¬ç denote the maximum VORP for .i; x . Let S.i; x  be the set of players who play position i and cost at most x. In the following recurrence for ≈íi; x¬ç, we assume that the max function returns  cid:0 1 when invoked over an empty set:  if i D N ;  p2S.N;x Àöp:orp cid:9  max≈íi C 1; x¬ç;  ≈íi; x¬ç DÀö max  max  p2S.i;x Àöp:orp C ≈íi C 1; x  cid:0  p:cost¬ç cid:9  if i < N :  This recurrence lends itself to implementation in a straightforward way. Let pij denote the j th player who plays position i.   15-52  Solutions for Chapter 15: Dynamic Programming  FREE-AGENT-VORP.p; N; P; X    let ≈í1 : : N ¬ç≈í0 : : X ¬ç and who≈í1 : : N ¬ç≈í0 : : X ¬ç be new tables for x D 0 to X  ≈íN; x¬ç D  cid:0 1 who≈íN; x¬ç D 0 for k D 1 to P  if pN k:cost  x and pN k:orp > ≈íN; x¬ç  ≈íN; x¬ç D pN k:orp who≈íN; x¬ç D k  for i D N  cid:0  1 downto 1  for x D 0 to X  ≈íi; x¬ç D ≈íi C 1; x¬ç who≈íi; x¬ç D 0 for k D 1 to P  if pi k:cost  x and ≈íi C 1; x  cid:0  pi k:cost¬ç C pi k:orp > ≈íi; x¬ç  ≈íi; x¬ç D ≈íi C 1; x  cid:0  pi k:cost¬ç C pi k:orp who≈íi; x¬ç D k  print ‚ÄúThe maximum value of VORP is ‚Äù ≈í1; X ¬ç amt D X for i D 1 to N  k D who≈íi; amt¬ç if k ¬§ 0  print ‚Äúsign player ‚Äù pi k amt D amt  cid:0  pi k:cost  print ‚ÄúThe total money spent is ‚Äù X  cid:0  amt The input to FREE-AGENT-VORP is the list of players p and N , P , and X, as given in the problem. The table ≈íi; x¬ç holds the maximum VORP for the sub- problem .i; x . The table who≈íi; x¬ç holds information necessary to reconstruct the actual solution. SpeciÔ¨Åcally, who≈íi; x¬ç holds the index of player to sign for posi- tion i, or 0 if no player should be signed for position i. The Ô¨Årst set of nested for loops initializes the base cases, in which i D N . For every amount x, the inner loop simply picks the player with the highest VORP who plays position N and whose cost is at most x. The next set of three nested for loops represents the main computation. The outer- most for loop runs down from position N  cid:0  1 to 1. This order ensures that smaller subproblems are solved before larger ones. We initialize ≈íi; x¬ç as ≈íi C 1; x¬ç. This way, we already take care of the case in which we decide not to sign any player who plays position i. The innermost for loop tries to sign each player  if we have enough money  in turn, and it keeps track of the maximum VORP possible. The maximum VORP for the entire problem ends up in ≈í1; X ¬ç. The Ô¨Ånal for loop uses the information in who table to print out which players to sign. The running time of FREE-AGENT-VORP is clearly ‚Äö.NPX  , and it uses ‚Äö.NX   space.   Lecture Notes for Chapter 16: Greedy Algorithms  Chapter 16 Introduction  Similar to dynamic programming. Used for optimization problems.  Activity selection  Idea When we have a choice to make, make the one that looks best right now. Make a locally optimal choice in hope of getting a globally optimal solution. Greedy algorithms don‚Äôt always yield an optimal solution. But sometimes they do. We‚Äôll see a problem for which they do. Then we‚Äôll look at some general characteristics of when greedy algorithms give optimal solutions. [WedonotcoverHuffmancodesormatroidsinthesenotes.]  n activities require exclusive use of a common resource. For example, scheduling the use of a classroom. Set of activities S D fa1; : : : ; ang. ai needs resource during period ≈ísi ; fi  , which is a half-open interval, where si D start time and fi D Ô¨Ånish time.  Goal Select the largest possible set of nonoverlapping  mutually compatible  activities.  Note Could have many other objectives:   Schedule room for longest time.  Maximize income rental fees. Assume that activities are sorted by Ô¨Ånish time: f1  f2  f3    fn cid:0 1  fn.   16-2  Lecture Notes for Chapter 16: Greedy Algorithms  Example S sorted by Ô¨Ånish time: [Leaveonboard]  i si fi  8 9 1 2 3 4 5 11 13 1 2 4 1 5 3 5 7 8 9 10 11 14 16  7 9  6 8  a5  a4  a2  a1  a3  a6  a8  a7  a9  0  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  Maximum-size mutually compatible set: fa1; a3; a6; a8g. Not unique: also fa2; a5; a7; a9g.  Optimal substructure of activity selection  Sij D fak 2 S W fi  sk < fk  sjg  D activities that start after ai Ô¨Ånishes and Ô¨Ånish before aj starts :  [Leaveonboard]  . . .  fi  sk  fk  sj  ai  ak  aj  . . .  Activities in Sij are compatible with all activities that Ô¨Ånish by fi, and all activities that start no earlier than sj .      Let Aij be a maximum-size set of mutually compatible activities in Sij . Let ak 2 Aij be some activity in Aij . Then we have two subproblems:  Find mutually compatible activities in Si k  activities that start after ai Ô¨Ånishes   Find mutually compatible activities in Skj  activities that start after ak Ô¨Ånishes  and that Ô¨Ånish before ak starts .  and that Ô¨Ånish before aj starts .  Let Ai k D Aij \ Si k D activities in Aij that Ô¨Ånish before ak starts ; Akj D Aij \ Skj D activities in Aij that start afer ak Ô¨Ånishes : Then Aij D Ai k [ fakg [ Akj   jAijj D jAi kj C jAkjj C 1. Claim Optimal solution Aij must include optimal solutions for the two subproblems for Si k and Skj .   Lecture Notes for Chapter 16: Greedy Algorithms  16-3  Proof Use the usual cut-and-paste argument. Will show the claim for Skj ; proof for Si k is symmetric. Suppose we could Ô¨Ånd a set A0kj of mutually compatible activities in Skj , where  ÀáÀáA0kjÀáÀá > jAkjj. Then use A0kj instead of Akj when solving the subproblem for Sij . Size of resulting set of mutually compatible activities would be jAi kjCÀáÀáA0kjÀáÀáC1 >  jAi kj C jAkjj C 1 D jAj. Contradicts assumption that Aij is optimal.   claim   One recursive solution  Since optimal solution Aij must include optimal solutions to the subproblems for Si k and Skj , could solve by dynamic programming. Let c≈íi; j ¬ç D size of optimal solution for Sij . Then c≈íi; j ¬ç D c≈íi; k¬ç C c≈ík; j ¬ç C 1 : But we don‚Äôt know which activity ak to choose, so we have to try them all: c≈íi; j ¬ç D  0  if Sij D ; ; if Sij ¬§ ; :  max ak2Sij fc≈íi; k¬ç C c≈ík; j ¬ç C 1g  Could then develop a recursive algorithm and memoize it. Or could develop a bottom-up algorithm and Ô¨Åll in table entries. Instead, we will look at a greedy approach.  Making the greedy choice  Choose an activity to add to optimal solution before solving subproblems. For activity-selection problem, we can get away with considering only the greedy choice: the activity that leaves the resource available for as many other activities as possible. Question: Which activity leaves the resource available for the most other activities? Answer: The Ô¨Årst activity to Ô¨Ånish.  If more than one activity has earliest Ô¨Ånish time, can choose any such activity.  Since activities are sorted by Ô¨Ånish time, just choose activity a1. That leaves only one subproblem to solve: Ô¨Ånding a maximum size set of mutually compatible activities that start after a1 Ô¨Ånishes.  Don‚Äôt have to worry about activ- ities that Ô¨Ånish before a1 starts, because s1 < f1 and no activity ai has Ô¨Ånish time fi < f1   no activity ai has fi  s1.  Since have only subproblem to solve, simplify notation: Sk D fai 2 S W si  fkg D activities that start after ak Ô¨Ånishes : Making greedy choice of a1   S1 remains as only subproblem to solve. [Slight abuseofnotation: referringtoSk notonlyasasetofactivitiesbutasasubproblem consistingoftheseactivities.] By optimal substructure, if a1 is in an optimal solution, then an optimal solution to the original problem consists of a1 plus all activities in an optimal solution to S1. But need to prove that a1 is always part of some optimal solution.   16-4  Lecture Notes for Chapter 16: Greedy Algorithms  Theorem If Sk is nonempty and am has the earliest Ô¨Ånish time in Sk, then am is included in some optimal solution.  Proof Let Ak be an optimal solution to Sk, and let aj have the earliest Ô¨Ånish time of any activity in Ak. If aj D am, done. Otherwise, let A0k D Ak  cid:0  fajg [ famg be Ak but with am substituted for aj .  Claim Activities in A0k are disjoint. Proof Activities in Ak are disjoint, aj fm  fj . Since jA0kj D jAkj, conclude that A0k is an optimal solution to Sk, and it in- cludes am.  theorem   is Ô¨Årst activity in Ak to Ô¨Ånish, and  claim   So, don‚Äôt need full power of dynamic programming. Don‚Äôt need to work bottom- up. Instead, can just repeatedly choose the activity that Ô¨Ånishes Ô¨Årst, keep only the activities that are compatible with that one, and repeat until no activities remain. Can work top-down: make a choice, then solve a subproblem. Don‚Äôt have to solve subproblems before making a choice.  Recursive greedy algorithm  Start and Ô¨Ånish times are represented by arrays s and f , where f is assumed to be already sorted in monotonically increasing order. To start, add Ô¨Åctitious activity a0 with f0 D 0, so that S0 D S, the entire set of activities. Procedure REC-ACTIVITY-SELECTOR takes as parameters the arrays s and f , in- dex k of current subproblem, and number n of activities in the original problem.     Ô¨Ånd the Ô¨Årst activity in Sk to Ô¨Ånish  REC-ACTIVITY-SELECTOR.s; f; k; n  m D k C 1 while m  n and s≈ím¬ç < f ≈ík¬ç if m  n else return ;  m D m C 1 return famg [ REC-ACTIVITY-SELECTOR .s; f; m; n   Initial call REC-ACTIVITY-SELECTOR .s; f; 0; n .   Lecture Notes for Chapter 16: Greedy Algorithms  16-5  Idea The while loop checks akC1; akC2; : : : ; an until it Ô¨Ånds an activity am that is com- patible with ak  need sm  fk .      If the loop terminates because am is found  m  n , then recursively solve Sm, and return this solution, along with am. If the loop never Ô¨Ånds a compatible am  m > n , then just return empty set.  Go through example given earlier. Should get fa1; a3; a6; a8g.  Time ‚Äö.n ‚Äîeach activity examined exactly once, assuming that activities are already sorted by Ô¨Ånish times.  Iterative greedy algorithm  Can convert the recursive algorithm to an iterative one. recursive.  It‚Äôs already almost tail  GREEDY-ACTIVITY-SELECTOR .s; f   n D s:length A D fa1g k D 1 for m D 2 to n  if s≈ím¬ç  f ≈ík¬ç  A D A [ famg k D m  return A  Go through example given earlier. Should again get fa1; a3; a6; a8g.  Time ‚Äö.n , if activities are already sorted by Ô¨Ånish times. For both the recursive and iterative algorithms, add O.n lg n  time if activities need to be sorted.  Greedy strategy  The choice that seems best at the moment is the one we go with. What did we do for activity selection?  1. Determine the optimal substructure. 2. Develop a recursive solution. 3. Show that if we make the greedy choice, only one subproblem remains.   16-6  Lecture Notes for Chapter 16: Greedy Algorithms  4. Prove that it‚Äôs always safe to make the greedy choice. 5. Develop a recursive greedy algorithm. 6. Convert it to an iterative algorithm.  At Ô¨Årst, it looked like dynamic programming. In the activity-selection problem, we started out by deÔ¨Åning subproblems Sij , where both i and j varied. But then found that making the greedy choice allowed us to restrict the subproblems to be of the form Sk. Could instead have gone straight for the greedy approach: in our Ô¨Årst crack at deÔ¨Åning subproblems, use the Sk form. Could then have proven that the greedy choice am  the Ô¨Årst activity to Ô¨Ånish , combined with optimal solution to the re- maining compatible activities Sm, gives an optimal solution to Sk. Typically, we streamline these steps:  1. Cast the optimization problem as one in which we make a choice and are left  with one subproblem to solve.  2. Prove that there‚Äôs always an optimal solution that makes the greedy choice, so  that the greedy choice is always safe.  3. Demonstrate optimal substructure by showing that, having made the greedy choice, combining an optimal solution to the remaining subproblem with the greedy choice gives an optimal solution to the original problem.  No general way to tell whether a greedy algorithm is optimal, but two key ingredi- ents are  1. greedy-choice property and 2. optimal substructure.  Greedy-choice property  Can assemble a globally optimal solution by making locally optimal  greedy  choices.  Dynamic programming  Make a choice at each step.  Choice depends on knowing optimal solutions to subproblems. Solve subprob-  lems Ô¨Årst.   Solve bottom-up.  Greedy  Make a choice at each step.  Make the choice before solving the subproblems.  Solve top-down.  Typically show the greedy-choice property by what we did for activity selection:   Look at an optimal solution.   Lecture Notes for Chapter 16: Greedy Algorithms  16-7    If it includes the greedy choice, done.   Otherwise, modify the optimal solution to include the greedy choice, yielding  another solution that‚Äôs just as good.  Can get efÔ¨Åciency gains from greedy-choice property.   Preprocess input to put it into greedy order.  Or, if dynamic data, use a priority queue.  Optimal substructure  Just show that optimal solution to subproblem and greedy choice   optimal solu- tion to problem.  Greedy vs. dynamic programming  The knapsack problem is a good example of the difference.  0-1 knapsack problem  n items.    Item i is worth $i, weighs wi pounds.   Find a most valuable subset of items with total weight  W .  Have to either take an item or not take it‚Äîcan‚Äôt take part of it.  Fractional knapsack problem Like the 0-1 knapsack problem, but can take fraction of an item. Both have optimal substructure. But the fractional knapsack problem has the greedy-choice property, and the 0-1 knapsack problem does not. To solve the fractional problem, rank items by value weight: i =wi . Let i =wi  iC1=wiC1 for all i. Take items in decreasing order of value weight. Will take all of the items with the greatest value weight, and possibly a fraction of the next item.  FRACTIONAL-KNAPSACK.; w; W   load D 0 i D 1 while load < W and i  n take all of item i  if wi  W  cid:0  load else take .W  cid:0  load =wi of item i add what was taken to load i D i C 1   16-8  Lecture Notes for Chapter 16: Greedy Algorithms  Time: O.n lg n  to sort, O.n  thereafter. Greedy doesn‚Äôt work for the 0-1 knapsack problem. Might get empty space, which lowers the average value per pound of the items taken.  2  3  1 i i 60 100 120 30 10 wi i =wi 6 4 W D 50. Greedy solution:  20 5   Take items 1 and 2.      value D 160, weight D 30.  Have 20 pounds of capacity left over. Optimal solution:   Take items 2 and 3.  value D 220, weight D 50.  No leftover capacity.   Solutions for Chapter 16: Greedy Algorithms  Solution to Exercise 16.1-1  The tricky part is determining which activities are in the set Sij . If activity k is in Sij , then we must have i < k < j , which means that j  cid:0  i  2, but we must also have that fi  sk and fk  sj . If we start k at j  cid:0  1 and decrement k, we can stop once k reaches i, but we can also stop once we Ô¨Ånd that fk  fi, since then activities i C 1 through k cannot be compatible with activity i. We create two Ô¨Åctitious activities, a0 with f0 D 0 and anC1 with snC1 D 1. We are interested in a maximum-size set A0;nC1 of mutually compatible activities in S0;nC1. We‚Äôll use tables c≈í0 : : n C 1; 0 : : n C 1¬ç, as in recurrence  16.2   so that c≈íi; j ¬ç D jAijj , and act≈í0 : : n C 1; 0 : : n C 1¬ç, where act≈íi; j ¬ç is the activity k that we choose to put into Aij . We Ô¨Åll the tables in according to increasing difference j  cid:0  i, which we denote by l in the pseudocode. Since Sij D ; if j  cid:0  i < 2, we initialize c≈íi; i ¬ç D 0 for all i and c≈íi; i C 1¬ç D 0 for 0  i  n. As in RECURSIVE-ACTIVITY-SELECTOR and GREEDY-ACTIVITY-SELECTOR, the start and Ô¨Ånish times are given as arrays s and f , where we assume that the arrays already include the two Ô¨Åctitious activities and that the activities are sorted by monotonically increasing Ô¨Ånish time.   16-10  Solutions for Chapter 16: Greedy Algorithms  DYNAMIC-ACTIVITY-SELECTOR .s; f; n  let c≈í0 : : n C 1; 0 : : n C 1¬ç and act≈í0 : : n C 1; 0 : : n C 1¬ç be new tables for i D 0 to n c≈íi; i ¬ç D 0 c≈íi; i C 1¬ç D 0 c≈ín C 1; n C 1¬ç D 0 for l D 2 to n C 1 j D i C l c≈íi; j ¬ç D 0 k D j  cid:0  1 while f ≈íi ¬ç < f ≈ík¬ç  for i D 0 to n  cid:0  l C 1  if f ≈íi ¬ç  s≈ík¬ç and f ≈ík¬ç  s≈íj ¬ç and c≈íi; k¬ç C c≈ík; j ¬ç C 1 > c≈íi; j ¬ç  c≈íi; j ¬ç D c≈íi; k¬ç C c≈ík; j ¬ç C 1 act≈íi; j ¬ç D k  k D k  cid:0  1 print ‚ÄúA maximum size set of mutually compatible activities has size ‚Äù c≈í0; n C 1¬ç print ‚ÄúThe set contains ‚Äù PRINT-ACTIVITIES.c; act; 0; n C 1  PRINT-ACTIVITIES .c; act; i; j    if c≈íi; j ¬ç > 0  k D act≈íi; j ¬ç print k PRINT-ACTIVITIES.c; act; i; k  PRINT-ACTIVITIES.c; act; k; j    The PRINT-ACTIVITIES procedure recursively prints the set of activities placed into the optimal solution Aij . It Ô¨Årst prints the activity k that achieved the maxi- mum value of c≈íi; j ¬ç, and then it recurses to print the activities in Ai k and Akj . The recursion bottoms out when c≈íi; j ¬ç D 0, so that Aij D ;. Whereas GREEDY-ACTIVITY-SELECTOR runs in ‚Äö.n  time, ACTIVITY-SELECTOR procedure runs in O.n3  time.  the DYNAMIC-  Solution to Exercise 16.1-2  The proposed approach‚Äîselecting the last activity to start that is compatible with all previously selected activities‚Äîis really the greedy algorithm but starting from the end rather than the beginning. Another way to look at it is as follows. We are given a set S D fa1; a2; : : : ; ang of activities, where ai D ≈ísi ; fi  , and we propose to Ô¨Ånd an optimal solution by selecting the last activity to start that is compatible with all previously selected activities. Instead, let us create a set S0 D fa01; a02; : : : ; a0ng, where a0i D ≈ífi ; si  . That is, a0i is ai in reverse. Clearly, a subset of fai1; ai2; : : : ; aikg  S is mutually ; : : : ; a0ik cid:9   S0 is also compatible if and only if the corresponding subsetÀöa0i1  ; a0i2   Solutions for Chapter 16: Greedy Algorithms  16-11  mutually compatible. Thus, an optimal solution for S maps directly to an optimal solution for S0 and vice versa. The proposed approach of selecting the last activity to start that is compatible with all previously selected activities, when run on S, gives the same answer as the greedy algorithm from the text‚Äîselecting the Ô¨Årst activity to Ô¨Ånish that is compat- ible with all previously selected activities‚Äîwhen run on S0. The solution that the proposed approach Ô¨Ånds for S corresponds to the solution that the text‚Äôs greedy algorithm Ô¨Ånds for S0, and so it is optimal.   For the approach of selecting the activity of least duration from those that are  compatible with previously selected activities:  Solution to Exercise 16.1-3  i si fi duration  1 0 3 3  2 2 4 2  3 3 6 3   For the approach of always selecting the compatible activity that overlaps the  This approach selects just fa2g, but the optimal solution selects fa1; a3g. fewest other remaining activities:  i si fi  of overlapping activities  1 0 2 3  2 1 3 4  3 1 3 4  4 1 3 4  5 2 4 4  6 3 5 2  7 4 6 4  8 5 7 4  9 5 7 4  10 5 7 4  11 6 8 3  This approach Ô¨Årst selects a6, and after that choice it can select only two other activities  one of a1; a2; a3; a4 and one of a8; a9; a10; a11 . An optimal solution is fa1; a5; a7; a11g.  For the approach of always selecting the compatible remaining activity with the earliest start time, just add one more activity with the interval ≈í0; 14  to the example in Section 16.1. It will be the Ô¨Årst activity selected, and no other activities are compatible with it.  Solution to Exercise 16.1-4 This solution is also posted publicly  Let S be the set of n activities. The ‚Äúobvious‚Äù solution of using GREEDY-ACTIVITY-SELECTOR to Ô¨Ånd a maxi- mum-size set S1 of compatible activities from S for the Ô¨Årst lecture hall, then using it again to Ô¨Ånd a maximum-size set S2 of compatible activities from S  cid:0  S1 for the second hall,  and so on until all the activities are assigned , requires ‚Äö.n2  time in the worst case. Moreover, it can produce a result that uses more lecture halls   16-12  Solutions for Chapter 16: Greedy Algorithms  than necessary. Consider activities with the intervals f≈í1; 4 ; ≈í2; 5 ; ≈í6; 7 ; ≈í4; 8 g. GREEDY-ACTIVITY-SELECTOR would choose the activities with intervals ≈í1; 4  and ≈í6; 7  for the Ô¨Årst lecture hall, and then each of the activities with intervals ≈í2; 5  and ≈í4; 8  would have to go into its own hall, for a total of three halls used. An optimal solution would put the activities with intervals ≈í1; 4  and ≈í4; 8  into one hall and the activities with intervals ≈í2; 5  and ≈í6; 7  into another hall, for only two halls used. There is a correct algorithm, however, whose asymptotic time is just the time needed to sort the activities by time‚ÄîO.n lg n  time for arbitrary times, or pos- sibly as fast as O.n  if the times are small integers. The general idea is to go through the activities in order of start time, assigning each to any hall that is available at that time. To do this, move through the set of events consisting of activities starting and activities Ô¨Ånishing, in order of event time. Maintain two lists of lecture halls: Halls that are busy at the current event- time t  because they have been assigned an activity i that started at si  t but won‚Äôt Ô¨Ånish until fi > t  and halls that are free at time t.  As in the activity- selection problem in Section 16.1, we are assuming that activity time intervals are half open‚Äîi.e., that if si  fj , then activities i and j are compatible.  When t is the start time of some activity, assign that activity to a free hall and move the hall from the free list to the busy list. When t is the Ô¨Ånish time of some activity, move the activity‚Äôs hall from the busy list to the free list.  The activity is certainly in some hall, because the event times are processed in order and the activity must have started before its Ô¨Ånish time t, hence must have been assigned to a hall.  To avoid using more halls than necessary, always pick a hall that has already had an activity assigned to it, if possible, before picking a never-used hall.  This can be done by always working at the front of the free-halls list‚Äîputting freed halls onto the front of the list and taking halls from the front of the list‚Äîso that a new hall doesn‚Äôt come to the front and get chosen if there are previously-used halls.  This guarantees that the algorithm uses as few lecture halls as possible: The algo- rithm will terminate with a schedule requiring m  n lecture halls. Let activity i be the Ô¨Årst activity scheduled in lecture hall m. The reason that i was put in the mth lecture hall is that the Ô¨Årst m  cid:0  1 lecture halls were busy at time si . So at this time there are m activities occurring simultaneously. Therefore any schedule must use at least m lecture halls, so the schedule returned by the algorithm is optimal. Run time:   Sort the 2n activity-starts activity-ends events.  In the sorted order, an activity- ending event should precede an activity-starting event that is at the same time.  O.n lg n  time for arbitrary times, possibly O.n  if the times are restricted  e.g., to small integers .   Process the events in O.n  time: Scan the 2n events, doing O.1  work for each  moving a hall from one list to the other and possibly associating an activity with it .  Total: O.n C time to sort  [The idea of this algorithm is related to the rectangle-overlap algorithm in Exer- cise14.3-7.]   Solutions for Chapter 16: Greedy Algorithms  16-13  Solution to Exercise 16.1-5  We can no longer use the greedy algorithm to solve this problem. However, as we show, the problem still has an optimal substructure which allows us to formulate a dynamic programming solution. The analysis here follows closely the analysis of Section 16.1 in the book. We deÔ¨Åne the value of a set of compatible events as the sum of values of events in that set. Let Sij be deÔ¨Åned as in Section 16.1. An optimal solution to Sij is a subset of mutually compatible events of Sij that has maximum value. Let Aij be an optimal solution to Sij . Suppose Aij includes an event ak. Let Ai k and Akj be deÔ¨Åned as in Section 16.1. Thus, we have Aij D Ai k [fakg[ Akj , and so the value of maximum-value set Aij is equal to the value of Ai k plus the value of Akj plus k. The usual cut-and-paste argument shows that the optimal solution Aij must also include optimal solutions to the two subproblems for Si k and Skj . If we could Ô¨Ånd a set A0kj of mutually compatible activities in Skj where the value of A0kj is greater than the value of Akj , then we could use A0kj , rather than Akj , in a solution to the subproblem for Sij . We would have constructed a set of mutually compatible activities with greater value than that of Aij , which contradicts the assumption that Aij is an optimal solution. A symmetric argument applies to the activities in Si k. Let us denote the value of an optimal solution for the set Sij by al≈íi; j ¬ç. Then, we would have the recurrence al≈íi; j ¬ç D al≈íi; k¬ç C al≈ík; j ¬ç C k : Of course, since we do not know that an optimal solution for the set Sij includes activity ak, we would have to examine all activities in Sij to Ô¨Ånd which one to choose, so that  al≈íi; j ¬ç D  0  max ak2Sij fal≈íi; k¬ç C al≈ík; j ¬ç C kg  if Sij D ; ; if Sij ¬§ ; :  While implementing the recurrence, the tricky part is determining which activities are in the set Sij . If activity k is in Sij , then we must have i < k < j , which means that j  cid:0  i  2, but we must also have that fi  sk and fk  sj . If we start k at j  cid:0  1 and decrement k, we can stop once k reaches i, but we can also stop once we Ô¨Ånd that fk  fi, since then activities i C 1 through k cannot be compatible with activity i. We create two Ô¨Åctitious activities, a0 with f0 D 0 and anC1 with snC1 D 1. We are interested in a maximum-size set A0;nC1 of mutually compatible activities in S0;nC1. We‚Äôll use tables al≈í0 : : n C 1; 0 : : n C 1¬ç, as in the recurrence, and act≈í0 : : n C 1; 0 : : n C 1¬ç, where act≈íi; j ¬ç is the activity k that we choose to put into Aij . We Ô¨Åll the tables in according to increasing difference j  cid:0  i, which we denote by l in the pseudocode. Since Sij D ; if j  cid:0  i < 2, we initialize al≈íi; i ¬ç D 0 for all i and al≈íi; i C 1¬ç D 0 for 0  i  n. As in RECURSIVE-ACTIVITY-SELECTOR and GREEDY-ACTIVITY-SELECTOR, the start and Ô¨Ånish times are given as arrays s and f , where we assume that the arrays already include the two Ô¨Åctitious activities   16-14  Solutions for Chapter 16: Greedy Algorithms  and that the activities are sorted by monotonically increasing Ô¨Ånish time. The array  speciÔ¨Åes the value of each activity.  MAX-VALUE-ACTIVITY-SELECTOR .s; f; ; n  let al≈í0 : : n C 1; 0 : : n C 1¬ç and act≈í0 : : n C 1; 0 : : n C 1¬ç be new tables for i D 0 to n  al≈íi; i ¬ç D 0 al≈íi; i C 1¬ç D 0 al≈ín C 1; n C 1¬ç D 0 for l D 2 to n C 1 j D i C l al≈íi; j ¬ç D 0 k D j  cid:0  1 while f ≈íi ¬ç < f ≈ík¬ç  for i D 0 to n  cid:0  l C 1  if f ≈íi ¬ç  s≈ík¬ç and f ≈ík¬ç  s≈íj ¬ç and  al≈íi; k¬ç C al≈ík; j ¬ç C k > al≈íi; j ¬ç  al≈íi; j ¬ç D al≈íi; k¬ç C al≈ík; j ¬ç C k act≈íi; j ¬ç D k  print ‚ÄúA maximum-value set of mutually compatible activities has value ‚Äù  k D k  cid:0  1  al≈í0; n C 1¬ç  print ‚ÄúThe set contains ‚Äù PRINT-ACTIVITIES.al; act; 0; n C 1  PRINT-ACTIVITIES .al; act; i; j    if al≈íi; j ¬ç > 0  k D act≈íi; j ¬ç print k PRINT-ACTIVITIES.al; act; i; k  PRINT-ACTIVITIES.al; act; k; j    The PRINT-ACTIVITIES procedure recursively prints the set of activities placed into the optimal solution Aij . It Ô¨Årst prints the activity k that achieved the maxi- mum value of al≈íi; j ¬ç, and then it recurses to print the activities in Ai k and Akj . The recursion bottoms out when al≈íi; j ¬ç D 0, so that Aij D ;. Whereas GREEDY-ACTIVITY-SELECTOR runs in ‚Äö.n  time, the MAX-VALUE- ACTIVITY-SELECTOR procedure runs in O.n3  time.  Solution to Exercise 16.2-2 This solution is also posted publicly  The solution is based on the optimal-substructure observation in the text: Let i be the highest-numbered item in an optimal solution S for W pounds and items 1; : : : ; n. Then S0 D S  cid:0  fig must be an optimal solution for W  cid:0  wi pounds and items 1; : : : ; i  cid:0  1, and the value of the solution S is i plus the value of the subproblem solution S0.   Solutions for Chapter 16: Greedy Algorithms  16-15  We can express this relationship in the following formula: DeÔ¨Åne c≈íi; w¬ç to be the value of the solution for items 1; : : : ; i and maximum weight w. Then  c≈íi; w¬ç D¬Ä 0  c≈íi  cid:0  1; w¬ç max.i C c≈íi  cid:0  1; w  cid:0  wi ¬ç; c≈íi  cid:0  1; w¬ç   if i D 0 or w D 0 ; if wi > w ; if i > 0 and w  wi :  The last case says that the value of a solution for i items either includes item i, in which case it is i plus a subproblem solution for i  cid:0  1 items and the weight excluding wi , or doesn‚Äôt include item i, in which case it is a subproblem solution for i  cid:0  1 items and the same weight. That is, if the thief picks item i, he takes i value, and he can choose from items 1; : : : ; i  cid:0  1 up to the weight limit w  cid:0  wi , and get c≈íi  cid:0  1; w  cid:0  wi ¬ç additional value. On the other hand, if he decides not to take item i, he can choose from items 1; : : : ; i  cid:0  1 up to the weight limit w, and get c≈íi  cid:0  1; w¬ç value. The better of these two choices should be made. The algorithm takes as inputs the maximum weight W , the number of items n, and the two sequences  D h1; 2; : : : ; ni and w D hw1; w2; : : : ; wni. It stores the c≈íi; j ¬ç values in a table c≈í0 : : n; 0 : : W ¬ç whose entries are computed in row-major order.  That is, the Ô¨Årst row of c is Ô¨Ålled in from left to right, then the second row, and so on.  At the end of the computation, c≈ín; W ¬ç contains the maximum value the thief can take.  DYNAMIC-0-1-KNAPSACK .; w; n; W    let c≈í0 : : n; 0 : : W ¬ç be a new array for w D 0 to W c≈í0; w¬ç D 0 for i D 1 to n c≈íi; 0¬ç D 0 for w D 1 to W if wi  w  if i C c≈íi  cid:0  1; w  cid:0  wi ¬ç > c≈íi  cid:0  1; w¬ç c≈íi; w¬ç D i C c≈íi  cid:0  1; w  cid:0  wi ¬ç else c≈íi; w¬ç D c≈íi  cid:0  1; w¬ç  else c≈íi; w¬ç D c≈íi  cid:0  1; w¬ç  We can use the c table to deduce the set of items to take by starting at c≈ín; W ¬ç and tracing where the optimal values came from. If c≈íi; w¬ç D c≈íi  cid:0  1; w¬ç, then item i is not part of the solution, and we continue tracing with c≈íi  cid:0  1; w¬ç. Otherwise item i is part of the solution, and we continue tracing with c≈íi  cid:0  1; w  cid:0  wi ¬ç. The above algorithm takes ‚Äö.nW   time total:  ‚Äö.nW   to Ô¨Åll in the c table: .nC 1  .W C 1  entries, each requiring ‚Äö.1  time  O.n  time to trace the solution  since it starts in row n of the table and moves  to compute.  up one row at each step .   16-16  Solutions for Chapter 16: Greedy Algorithms  Solution to Exercise 16.2-4  The optimal strategy is the obvious greedy one. Starting with both bottles full, Professor Gekko should go to the westernmost place that he can reÔ¨Åll his bottles within m miles of Grand Forks. Fill up there. Then go to the westernmost reÔ¨Ålling location he can get to within m miles of where he Ô¨Ålled up, Ô¨Åll up there, and so on. Looked at another way, at each reÔ¨Ålling location, Professor Gekko should check whether he can make it to the next reÔ¨Ålling location without stopping at this one. If he can, skip this one. If he cannot, then Ô¨Åll up. Professor Gekko doesn‚Äôt need to know how much water he has or how far the next reÔ¨Ålling location is to implement this approach, since at each Ô¨Ållup, he can determine which is the next location at which he‚Äôll need to stop. This problem has optimal substructure. Suppose there are n possible reÔ¨Ålling loca- tions. Consider an optimal solution with s reÔ¨Ålling locations and whose Ô¨Årst stop is at the kth location. Then the rest of the optimal solution must be an optimal solution to the subproblem of the remaining n  cid:0  k stations. Otherwise, if there were a better solution to the subproblem, i.e., one with fewer than s  cid:0  1 stops, we could use it to come up with a solution with fewer than s stops for the full problem, contradicting our supposition of optimality. This problem also has the greedy-choice property. Suppose there are k reÔ¨Ålling locations beyond the start that are within m miles of the start. The greedy solution chooses the kth location as its Ô¨Årst stop. No station beyond the kth works as a Ô¨Årst stop, since Professor Gekko would run out of water Ô¨Årst. If a solution chooses a location j < k as its Ô¨Årst stop, then Professor Gekko could choose the kth location instead, having at least as much water when he leaves the kth location as if he‚Äôd chosen the j th location. Therefore, he would get at least as far without Ô¨Ålling up again if he had chosen the kth location. If there are n reÔ¨Ålling locations on the map, Professor Gekko needs to inspect each one just once. The running time is O.n .  Use a linear-time median algorithm to calculate the median m of the i =wi ra- tios. Next, partition the items into three sets: G D fi W i =wi > mg, E D fi W i =wi D mg, and L D fi W i =wi < mg; this step takes linear time. Com- pute WG DPi2G wi and WE DPi2E wi, the total weight of the items in sets G and E, respectively.  If WG > W , then do not yet take any items in set G, and instead recurse on the set of items G and knapsack capacity W .     Otherwise  WG  W  , take all items in set G, and take as much of the items in set E as will Ô¨Åt in the remaining capacity W  cid:0  WG. If WG C WE  W  i.e., there is no capacity left after taking all the items in set G and all the items in set E that Ô¨Åt in the remaining capacity W  cid:0  WG , then we are done.    Solution to Exercise 16.2-6   Solutions for Chapter 16: Greedy Algorithms  16-17   Otherwise  WG C WE < W  , then after taking all the items in sets G and E,  recurse on the set of items L and knapsack capacity W  cid:0  WG  cid:0  WE .  To analyze this algorithm, note that each recursive call takes linear time, exclusive of the time for a recursive call that it may make. When there is a recursive call, there is just one, and it‚Äôs for a problem of at most half the size. Thus, the running time is given by the recurrence T .n   T .n=2  C ‚Äö.n , whose solution is T .n  D O.n .  Solution to Exercise 16.2-7 This solution is also posted publicly  Solution to Exercise 16.3-1  bi aj  Sort A and B into monotonically decreasing order. Here‚Äôs a proof that this method yields an optimal solution. Consider any indices i bj . We want to show that and j such that i < j , and consider the terms ai it is no worse to include these terms in the payoff than to include ai bi , i.e., that ai bi . Since A and B are sorted into monotonically decreasing order and i < j , we have ai  aj and bi  bj . Since ai and aj are positive bi cid:0 bj . Multiplying both sides by and bi  cid:0  bj is nonnegative, we have ai bj aj ai Since the order of multiplication doesn‚Äôt matter, sorting A and B into monotoni- cally increasing order works as well.  bi cid:0 bj  aj  bj  ai  bj yields ai  bj  ai  bj and aj  bi and aj  bj aj  bj aj  bi aj  bi .  We are given that x:freq  y:freq are the two lowest frequencies in order, and that a:freq  b:freq. Now, b:freq D x:freq   a:freq  x:freq   a:freq D x:freq  since x:freq is the lowest frequency  , and since y:freq  b:freq,  b:freq D x:freq   y:freq  x:freq   y:freq D x:freq  since x:freq is the lowest frequency  . Thus, if we assume that x:freq D b:freq, then we have that each of a:freq, b:freq, and y:freq equals x:freq, and so a:freq D b:freq D x:freq D y:freq.  Solution to Exercise 16.4-2  We need to show three things to prove that .S;  cid:9    is a matroid:  1. S is Ô¨Ånite. That‚Äôs because S is the set of of m columns of matrix T .   16-18  Solutions for Chapter 16: Greedy Algorithms  2.  cid:9  is hereditary. That‚Äôs because if B 2  cid:9  , then the columns in B are linearly in- dependent. If A  B, then the columns of A must also be linearly independent, and so A 2  cid:9  . 3. .S;  cid:9    satisÔ¨Åes the exchange property. To see why, let us suppose that A; B 2  cid:9  and jAj < jBj. We will use the following properties of matrices:     The rank of a matrix is the number of columns in a maximal set of linearly independent columns  see page 1223 of the text . The rank is also equal to the dimension of the column space of the matrix. If the column space of matrix B is a subspace of the column space of ma- trix A, then rank.B   rank.A .  Because the columns in A are linearly independent, if we take just these columns as a matrix A, we have that rank.A  D jAj. Similarly, if we take the columns of B as a matrix B, we have rank.B  D jBj. Since jAj < jBj, we have rank.A  < rank.B . We shall show that there is some column b 2 B that is not a linear combination of the columns in A, and so A[fbg is linearly independent. The proof proceeds by contradiction. Assume that each column in B is a linear combination of the columns of A. That means that any vector that is a linear combination of the columns of B is also a linear combination of the columns of A, and so, treating the columns of A and B as matrices, the column space of B is a subspace of the column space of A. By the second property above, we have rank.B   rank.A . But we have already shown that rank.A  < rank.B , a contradiction. Therefore, some column in B is not a linear combination of the columns of A, and .S;  cid:9    satisÔ¨Åes the exchange property.  [This exercise deÔ¨Ånes what is commonly known as the dual of a matroid, and it askstoprovethatthedualofamatroidisitself amatroid. Theliterature contains simpler proofs of this fact, but they depend on other  equivalent  deÔ¨Ånitions of a matroid. The proof given here is more complicated, but it relies only on the deÔ¨Ånitiongiveninthetext.] We need to show three things to prove that .S;  cid:9  0  is a matroid:  1. S is Ô¨Ånite. We are given that. 2.  cid:9  0 is hereditary. Suppose that B0 2  cid:9  0 and A0  B0. Since B0 2  cid:9  0, there is some maximal set B 2  cid:9  such that B  S  cid:0  B0. But A0  B0 implies that S  cid:0  B0  S  cid:0  A0, and so B  S  cid:0  B0  S  cid:0  A0. Thus, there exists a maximal set B 2  cid:9  such that B  S  cid:0  A0, proving that A0 2  cid:9  0. 3. .S;  cid:9  0  satisÔ¨Åes the exchange property. We start with two preliminary facts about sets. The proofs of these facts are omitted. Fact 1: jX  cid:0  Y j D jXj  cid:0  jX \ Y j.  Solution to Exercise 16.4-3   Solutions for Chapter 16: Greedy Algorithms  16-19  Fact 2: Let S be the universe of elements. If X  cid:0  Y  Z and Z  S  cid:0  Y , then  jX \ Zj D jXj  cid:0  jX \ Y j.  To show that .S;  cid:9  0  satisÔ¨Åes the exchange property, let us assume that A0 2  cid:9  0, B0 2  cid:9  0, and that jA0j < jB0j. We need to show that there exists some x 2 B0  cid:0  A0 such that A0 [ fxg 2  cid:9  0. Because A0 2  cid:9  0 and B0 2  cid:9  0, there are maximal sets A  S  cid:0  A0 and B  S  cid:0  B0 such that A 2  cid:9  and B 2  cid:9  . DeÔ¨Åne the set X D B0  cid:0  A0  cid:0  A, so that X consists of elements in B0 but not in A0 or A. If X is nonempty, then let x be any element of X. By how we deÔ¨Åned set X, we know that x 2 B0 and x 62 A0, so that x 2 B0  cid:0  A0. Since x 62 A, we also have that A  S  cid:0  A0  cid:0  fxg D S  cid:0  .A0 [ fxg , and so A0 [ fxg 2  cid:9  0. If X is empty, the situation is more complicated. Because jA0j < jB0j, we have that B0  cid:0  A0 ¬§ ;, and so X being empty means that B0  cid:0  A0  A. Claim There is an element y 2 B  cid:0  A0 such that .A  cid:0  B0  [ fyg 2  cid:9  . Proof First, observe that because A cid:0 B0  A and A 2  cid:9  , we have that A cid:0 B0 2  cid:9  . Similarly, B  cid:0  A0  B and B 2  cid:9  , and so B  cid:0  A0 2  cid:9  . If we show that jA  cid:0  B0j < jB  cid:0  A0j, the assumption that .S;  cid:9    is a matroid proves the existence of y. Because B0  cid:0  A0  A and A  S  cid:0  A0, we can apply Fact 2 to conclude that jB0 \ Aj D jB0j  cid:0  jB0 \ A0j. We claim that jB \ A0j  jA0  cid:0  B0j. To see why, observe that A0  cid:0  B0 D A0 \ .S  cid:0  B0  and B  S  cid:0  B0, and so B \ A0  .S  cid:0  B0  \ A0 D A0 \ .S  cid:0  B0  D A0  cid:0  B0. Applying Fact 1, we see that jA0  cid:0  B0j D jA0j cid:0 jA0 \ B0j D jA0j cid:0 jB0 \ A0j, and hence jB \ A0j  jA0j  cid:0  jB0 \ A0j. Now, we have  jA0j < jB0j  jA0j  cid:0  jB0 \ A0j < jB0j  cid:0  jB0 \ A0j jB \ A0j < jB0j  cid:0  jB0 \ A0j jB \ A0j < jB0 \ Aj  jBj  cid:0  jB \ A0j > jAj  cid:0  jB0 \ Aj  jB  cid:0  A0j > jA  cid:0  B0j   by assumption   subtracting same quantity   jB \ A0j  jA0j  cid:0  jB0 \ A0j   jB0 \ Aj D jB0j  cid:0  jB0 \ A0j   jAj D jBj   Fact 1    claim   Now we know there is an element y 2 B  cid:0  A0 such that .A  cid:0  B0  [ fyg 2  cid:9  . Moreover, we claim that y 62 A. To see why, we know that by the exchange property, we can, without loss of generality, choose y so that y 62 A  cid:0  B0. In order for y to be in A, it would have to be in A \ B0. But y 2 B, which means that y 62 B0, and hence y 62 A \ B0. Therefore y 62 A. Applying the exchange property, we add such an element y in B  cid:0  A0 to A cid:0  B0, maintaining that the set we get, say C , is in  cid:9  . Then we keep applying the exchange property, adding a new element in A  cid:0  C to C , maintaining that C is in  cid:9  , until jCj D jAj. Once jCj D jAj, there must exist some element x 2 A   16-20  Solutions for Chapter 16: Greedy Algorithms  that we have not added into C . We know that such an element exists because the element y that we Ô¨Årst added into C was not in A, and so some element x in A must be left over. Also, we must have x 2 B0 because all the elements in A  cid:0  B0 are initially in C . Therefore, we have x 2 B0  cid:0  A0. The set C so constructed is maximal, because it has the same cardinality as A, which is maximal, and C 2  cid:9  . All the elements but one in C are also in A; the one exception is in B  cid:0  A0, and so C contains no elements in A0. Because we never added x to C , we have that C  S  cid:0  A0  cid:0  fxg D S  cid:0  .A0 [ fxg . Therefore, A0 [ fxg 2  cid:9  0, as we needed to show.  Solution to Problem 16-1  Before we go into the various parts of this problem, let us Ô¨Årst prove once and for all that the coin-changing problem has optimal substructure. Suppose we have an optimal solution for a problem of making change for n cents, and we know that this optimal solution uses a coin whose value is c cents; let this optimal solution use k coins. We claim that this optimal solution for the problem of n cents must contain within it an optimal solution for the problem of n cid:0  c cents. We use the usual cut-and-paste argument. Clearly, there are k  cid:0  1 coins in the solution to the n  cid:0  c cents problem used within our optimal solution to the n cents problem. If we had a solution to the n cid:0  c cents problem that used fewer than k  cid:0  1 coins, then we could use this solution to produce a solution to the n cents problem that uses fewer than k coins, which contradicts the optimality of our solution.  a. A greedy algorithm to make change using quarters, dimes, nickels, and pennies  change.  change.  change.  works as follows:  Give q D bn=25c quarters. That leaves nq D n mod 25 cents to make  Then give d D bnq=10c dimes. That leaves nd D nq mod 10 cents to make  Then give k D bnd =5c nickels. That leaves nk D nd mod 5 cents to make  Finally, give p D nk pennies. An equivalent formulation is the following. The problem we wish to solve is making change for n cents. If n D 0, the optimal solution is to give no coins. If n > 0, determine the largest coin whose value is less than or equal to n. Let this coin have value c. Give one such coin, and then recursively solve the subproblem of making change for n  cid:0  c cents. To prove that this algorithm yields an optimal solution, we Ô¨Årst need to show that the greedy-choice property holds, that is, that some optimal solution to making change for n cents includes one coin of value c, where c is the largest coin value such that c  n. Consider some optimal solution. If this optimal solution includes a coin of value c, then we are done. Otherwise, this optimal solution does not include a coin of value c. We have four cases to consider:   Solutions for Chapter 16: Greedy Algorithms  16-21          If 1  n < 5, then c D 1. A solution may consist only of pennies, and so it must contain the greedy choice. If 5  n < 10, then c D 5. By supposition, this optimal solution does not contain a nickel, and so it consists of only pennies. Replace Ô¨Åve pennies by one nickel to give a solution with four fewer coins. If 10  n < 25, then c D 10. By supposition, this optimal solution does not contain a dime, and so it contains only nickels and pennies. Some subset of the nickels and pennies in this solution adds up to 10 cents, and so we can replace these nickels and pennies by a dime to give a solution with  between 1 and 9  fewer coins. If 25  n, then c D 25. By supposition, this optimal solution does not If contain a quarter, and so it contains only dimes, nickels, and pennies. it contains three dimes, we can replace these three dimes by a quarter and a nickel, giving a solution with one fewer coin. If it contains at most two dimes, then some subset of the dimes, nickels, and pennies adds up to 25 cents, and so we can replace these coins by one quarter to give a solution with fewer coins.  Thus, we have shown that there is always an optimal solution that includes the greedy choice, and that we can combine the greedy choice with an optimal solu- tion to the remaining subproblem to produce an optimal solution to our original problem. Therefore, the greedy algorithm produces an optimal solution. For the algorithm that chooses one coin at a time and then recurses on sub- problems, the running time is ‚Äö.k , where k is the number of coins used in an optimal solution. Since k  n, the running time is O.n . For our Ô¨Årst de- scription of the algorithm, we perform a constant number of calculations  since there are only 4 coin types , and the running time is O.1 .  b. When the coin denominations are c0; c1; : : : ; ck, the greedy algorithm to make change for n cents works by Ô¨Ånding the denomination cj such that j D max f0  i  k W ci  ng, giving one coin of denomination cj , and recurs-  An equivalent, ing on the subproblem of making change for n  cid:0  cj cents. but more efÔ¨Åcient, algorithm is to give n=ckÀò coins of denomination ck and b.n mod ciC1 =cic coins of denomination ci for i D 0; 1; : : : ; k  cid:0  1.  To show that the greedy algorithm produces an optimal solution, we start by proving the following lemma:  Lemma For i D 0; 1; : : : ; k, let ai be the number of coins of denomination ci used in an optimal solution to the problem of making change for n cents. Then for i D 0; 1; : : : ; k  cid:0  1, we have ai < c. Proof If ai  c for some 0  i < k, then we can improve the solution by using one more coin of denomination ciC1 and c fewer coins of denomination ci. The amount for which we make change remains the same, but we use c  cid:0  1 > 0  lemma  fewer coins.  To show that the greedy solution is optimal, we show that any non-greedy so- lution is not optimal. As above, let j D max f0  i  k W ci  ng, so that the   16-22  Solutions for Chapter 16: Greedy Algorithms  greedy solution uses at least one coin of denomination cj . Consider a non- greedy solution, which must use no coins of denomination cj or higher. Let the non-greedy solution use ai coins of denomination ci, for i D 0; 1; : : : ; j  cid:0  1; iD0 ai ci D n. Since n  cj , we have that Pj cid:0 1 thus we have Pj cid:0 1 iD0 ai ci  cj . Now suppose that the non-greedy solution is optimal. By the above lemma, ai  c  cid:0  1 for i D 0; 1; : : : ; j  cid:0  1. Thus, j cid:0 1 XiD0 XiD0 ai ci  .c  cid:0  1 ci j cid:0 1 XiD0 D .c  cid:0  1  cj  cid:0  1 c  cid:0  1  j cid:0 1  ci  D .c  cid:0  1  D cj  cid:0  1 < cj ;  which contradicts our earlier assertion thatPj cid:0 1  iD0 ai ci  cj . We conclude that the non-greedy solution is not optimal. Since any algorithm that does not produce the greedy solution fails to be opti- mal, only the greedy algorithm produces the optimal solution. The problem did not ask for the running time, but for the more efÔ¨Åcient greedy- algorithm formulation, it is easy to see that the running time is O.k , since we have to perform at most k each of the division, Ô¨Çoor, and mod operations.  c. With actual U.S. coins, we can use coins of denomination 1, 10, and 25. When n D 30 cents, the greedy solution gives one quarter and Ô¨Åve pennies, for a total of six coins. The non-greedy solution of three dimes is better. The smallest integer numbers we can use are 1, 3, and 4. When n D 6 cents, the greedy solution gives one 4-cent coin and two 1-cent coins, for a total of three coins. The non-greedy solution of two 3-cent coins is better.  d. Since we have optimal substructure, dynamic programming might apply. And  indeed it does. Let us deÔ¨Åne c≈íj ¬ç to be the minimum number of coins we need to make change for j cents. Let the coin denominations be d1; d2; : : : ; dk. Since one of the coins is a penny, there is a way to make change for any amount j  1. Because of the optimal substructure, if we knew that an optimal solution for the problem of making change for j cents used a coin of denomination di , we would have c≈íj ¬ç D 1 C c≈íj  cid:0  di ¬ç. As base cases, we have that c≈íj ¬ç D 0 for all j  0. To develop a recursive formulation, we have to check all denominations, giving c≈íj ¬ç D  0  if j  0 ; if j > 1 :  1 C min  1ik fc≈íj  cid:0  di ¬çg  We can compute the c≈íj ¬ç values in order of increasing j by using a table. The following procedure does so, producing a table c≈í1 : : n¬ç. It avoids even exam- ining c≈íj ¬ç for j  0 by ensuring that j  di before looking up c≈íj  cid:0  di ¬ç. The   Solutions for Chapter 16: Greedy Algorithms  16-23  procedure also produces a table denom≈í1 : : n¬ç, where denom≈íj ¬ç is the denomi- nation of a coin used in an optimal solution to the problem of making change for j cents.  COMPUTE-CHANGE.n; d; k   let c≈í1 : : n¬ç and denom≈í1 : : n¬ç be new arrays for j D 1 to n c≈íj ¬ç D 1 for i D 1 to k  if j  di and 1 C c≈íj  cid:0  di ¬ç < c≈íj ¬ç  c≈íj ¬ç D 1 C c≈íj  cid:0  di ¬ç denom≈íj ¬ç D di  return c and denom  This procedure obviously runs in O.nk  time. We use the following procedure to output the coins used in the optimal solution computed by COMPUTE-CHANGE:  GIVE-CHANGE.j; denom   if j > 0  give one coin of denomination denom≈íj ¬ç GIVE-CHANGE.j  cid:0  denom≈íj ¬ç; denom   The initial call is GIVE-CHANGE.n; denom . Since the value of the Ô¨Årst pa- rameter decreases in each recursive call, this procedure runs in O.n  time.  a. The procedure CACHE-MANAGER is a generic procedure, which initializes a cache by calling INITIALIZE-CACHE and then calls ACCESS with each data element in turn. The inputs are a sequence R D hr1; r2; : : : ; rni of memory requests and a cache size k.  CACHE-MANAGER.R; k   INITIALIZE-CACHE .R; k  for i D 1 to n ACCESS.ri    The running time of CACHE-MANAGER of course depends heavily on how ACCESS is implemented. We have several choices for how to implement the greedy strategy outlined in the problem. A straightforward way of implement- ing the greedy strategy is that when processing request ri , for each of the at most k elements currently in the cache, scan through requests riC1; : : : ; rn to Ô¨Ånd which of the elements in the cache and ri has its next access furthest in the future, and evict this element. Because each scan takes O.n  time, each request entails O.k  scans, and there are n requests, the running time of this straightforward approach is O.k n2 .  Solution to Problem 16-5   16-24  Solutions for Chapter 16: Greedy Algorithms  Instead, we describe an asymptotically faster algorithm, which uses a red-black tree to check whether a given element is currently in the cache, a max-priority queue to retrieve the data element with the furthest access time, and a hash table  resolving collisions by chaining  to map data elements to integer indices. We assume that the data elements can be linearly ordered, so that it makes sense to put them into a red-black tree and a max-priority queue. The following pro- cedure INITIALIZE-CACHE creates and initializes some global data structures that are used by ACCESS.  INITIALIZE-CACHE.R; k   let T be a new red-black tree let P be a new max-priority queue let H be a new hash table ind D 1 for i D 1 to n j D HASH-SEARCH.ri   if j == NIL HASH-INSERT.ri ; ind  let Sind be a new linked list j D ind ind D ind C 1 append i to Sj  In the above procedure, here is the meaning of various variables:   The red-black tree T has at most k nodes and holds the distinct data elements that are currently in the cache. We assume that the red-black tree procedures are modiÔ¨Åed to keep track of the number of nodes currently in the tree, and that the procedure TREE-SIZE returns this value. Because red-black tree T has at most k nodes, we can insert into, delete from, or search in it in O.lg k  worst-case time.   The max-priority queue P contains elements with two attributes: key is the next access time of a data element, and alue is the actual data element for each data element in the cache. key gives the key and alue is satellite data in the priority queue. Like the red-black tree T , the max-priority queue contains only elements currently in the cache. We need to maintain T and P separately, however, because T is keyed on the data elements and P is keyed on access times. Using a max-heap to implement P , we can extract the maximum element or insert a new element in O.lg k  time, and we can Ô¨Ånd the maximum element in ‚Äö.1  time.   The hash table H is a dictionary or a map, which maps each data element to a unique integer. This integer is used to index linked lists, which are described next. We assume that the HASH-INSERT procedure uses the table-expansion technique of Section 17.4.1 to keep the hash table‚Äôs load factor to be at most some constant Àõ. In this way, the amortized cost per insertion is ‚Äö.1  and, under the assumption of simple uniform hashing, then by Theorems 11.1 and 11.2, the average-case search time is also ‚Äö.1 .   For every distinct data element ri, we create a linked list Sind  where ind is obtained through the hash table  holding the indices in the in-   Solutions for Chapter 16: Greedy Algorithms  16-25  For example,  put array where ri occurs. if the input sequence is hd; b; d; b; d; a; c; d; b; a; c; bi, then we create four linked lists: S1 for a, S2 for b, S3 for c, and S4 for d . S1 holds the indices where a is accessed, and so S1 D h6; 10i. Similarly, S2 D h2; 4; 9; 12i, S3 D h7; 11i and S4 D h1; 3; 5; 8i.  For each data element ri, we Ô¨Årst check whether there is already a linked list associated with ri and create a new linked list if not. We retrieve the linked list associated with ri and append i to it, indicating that an access to ri occurs at access i.  ACCESS.ri       Compute the next access time for ri. ind D HASH-SEARCH.ri   time D 1 delete the head of Sind if Sind is not empty  time D head of Sind     Check to see whether ri is currently in the cache. if TREE-SEARCH.T:root; ri   ¬§ NIL elseif TREE-SIZE.T   < k  print ‚Äúcache hit‚Äù     Insert in an empty slot in the cache. let ¬¥ be a new node for T ¬¥:key D ri RB-INSERT.T; ¬¥  let eent be a new object for P eent :key D time eent :alue D ri INSERT.P; eent  print ‚Äúcache miss, inserted ‚Äù ri ‚Äú in empty slot‚Äù  else eent D MAXIMUM.P    if eent :key  time else    evict the element with furthest access time  print ‚Äúcache miss, no data element evicted‚Äù     ri has the furthest access time  print ‚Äúcache miss, evict data element ‚Äù eent :alue eent D EXTRACT-MAX.P   RB-DELETE.T; TREE-SEARCH.T:root; eent :alue   eent :key D time eent :alue D ri INSERT.P; eent  let ¬¥ be a new node for T ¬¥:key D ri RB-INSERT.T; ¬¥   The procedure ACCESS takes an input ri and decides which element to evict, if any, from the cache. The Ô¨Årst if condition properly sets time to the next access time of ri . The head of the linked list associated with ri contains i; we remove this element from the list, and the new head contains the next access   16-26  Solutions for Chapter 16: Greedy Algorithms  time for ri. Then, we check to see whether ri is already present in the cache. If ri is not present in the cache, we check to see whether we can store ri in an empty slot. If there are no empty slots, we have to evict the element with the furthest access time. We retrieve the element with the furthest access time from the max-priority queue and compare it with that of ri . If ri‚Äôs next access is sooner, we evict the element with the furthest access time from the cache  deleting the element from the tree and from the priority queue  and insert ri into the tree and priority queue. Under the assumption of simple uniform hashing, the average-case running time of ACCESS is O.lg k , since it performs a constant number of operations on the red-black tree, priority queue, and hash table. Thus, the average-case running time of CACHE-MANAGER is O.n lg k .  b. To show that the problem exhibits optimal substructure, we deÔ¨Åne the subprob- lem .C; i   as the contents of the cache just before the ith request, where C is a subset of the set of input data elements containing at most k of them. A solution to .C; i   is a sequence of decisions that speciÔ¨Åes which element to evict  if any  for each request i; i C 1; : : : ; n. An optimal solution to .C; i   is a solution that minimizes the number of cache misses. Let S be an optimal solution to .C; i  . Let S0 be the subsolution of S for requests i C 1; i C 2; : : : ; n. If a cache hit occurs on the ith request, then the cache remains unchanged. If a cache miss occurs, then the ith request results in the contents of the cache changing to C 0  possibly with C 0 D C if no element was evicted . We claim that S0 is an optimal solution to .C 0; i C 1 . Why? If S0 were not an optimal solution to .C 0; i C1 , then there exists another solution S00 to .C 0; i C1  that makes fewer cache misses than S0. By combining S00 with the decision of S at the ith request, we obtain another solution that makes fewer cache misses than S, which contradicts our assumption that S is an optimal solution to .C; i  . Suppose the ith request results in a cache miss. Let PC be the set of all cache states that can be reached from C through a single decision of the cache man- ager. The set PC contains up to k C 1 states: k of them arising from different elements of the cache being evicted and one arising from the decision of evict- ing no element. For example, if C D fr1; r2; r3g and the requested data element is r4, then PC D ffr1; r2; r3g ;fr1; r2; r4g ;fr1; r3; r4g ;fr2; r3; r4gg. Let miss.C; i   denote the minimum number of cache misses for .C; i  . We can state a recurrence for miss.C; i   as  miss.C; i   D‚Ä† 0  if i D n and rn 2 C ; if i D n and rn 62 C ; if i < n and ri 2 C ; if i < n and ri 62 C : Thus, we conclude that the problem exhibits optimal substructure.  1 miss.C; i C 1  1 C min  C 02PC fmiss.C 0; i C 1 g  c. To prove that the furthest-in-future strategy yields an optimal solution, we show that the problem exhibits the greedy-choice property. Combined with the optimal-substructure property from part  b , the greedy-choice property will   Solutions for Chapter 16: Greedy Algorithms  16-27  prove that furthest-in-future produces the minimum possible number of cache misses. We use the deÔ¨Ånitions of subproblem, solution, and optimal solution from part  b . Since we will be comparing different solutions, let us deÔ¨Åne CAi as the state of the cache for solution A just before the ith request. The following theorem is the key.  Theorem  Greedy-choice property  Let A be some optimal solution to .C; i  . Let b be the element in CAi [ frig whose next access at the time of the ith request is furthest in the future, at time m. Then, we can construct another solution A0 to .C; i   that has the fol- lowing properties:  1. On the ith request, A0 evicts b. 2. For i C 1  j  m, the caches CAj and CA0j differ by at most one element. If they differ, then b 2 CAj is always the element in CAj that is not in CA0j . Equivalently, if CAj and CA0j differ, we can write CAj D Dj [ fbg and CA0j D Dj [ fxg, where Dj is a size- k  cid:0  1  set and x ¬§ b is some data element.  3. For requests i; : : : ; m  cid:0  1, if A has a cache hit, then A0 has a cache hit. 4. CAj D CA0j for j > m. 5. For requests i; : : : ; m, the number of cache misses produced by A0 is at most  the number of cache misses produced by A.  Proof If A evicts b at request i, then the proof of the theorem is trivial. There- fore, suppose A evicts data element a on request i, where a ¬§ b. We will prove the theorem by constructing A0 inductively for each request.  1  At request i, A0 evicts b instead of a.  2  We proceed with induction on j , where i C 1  j  m. The construction for property 1 establishes the base case because CA;iC1 and CA0;iC1 differ by just one element and b is the element in CA;iC1 that is not in CA0;iC1. For the induction step, suppose property 2 is true for some request j , where i C 1  j < m. If A does not evict any element or evicts an element in Dj , then construct A0 to make the same decision on request j as A makes. If A evicts b on request j , then construct A0 to evict x and keep the same element as A keeps, namely rj . This construction conserves property 2 for j C 1. Note that this construction might sometimes insert duplicate elements in the cache. This situation can easily be dealt with by introducing a dummy element for x.  3  Suppose A has a cache hit for request j , where i  j  m  cid:0  1. Then, rj 2 Dj since rj ¬§ b. Thus, rj 2 CA0j and A0 has a cache hit, too.  4  By property 2, the cache CAm differs from CA0m by at most one element, with b being the element in CAm that might not be in CA0m. If CAm D CA0m, then construct A0 to make the same decision on request m as A. Otherwise, CAm ¬§ CA0m and b 2 CAm. Construct A0 to evict x and keep b on request m. Since the mth request is for element b and b 2 CAm, A has a cache hit so that it does not evict any element. Thus, we can ensure that CA;mC1 D CA0;mC1. From the .m C 1 st request on, A0 simply makes the same decisions as A.   16-28  Solutions for Chapter 16: Greedy Algorithms   5  By property 3, for requests i; : : : ; m  cid:0  1, whenever we have a cache hit for A, we also have a cache hit for A0. Thus, we have to concern ourselves with only the mth request. If A has a cache miss on the mth request, we are done. Otherwise, A has a cache hit on the mth request, and we will prove that there exists at least one request j , where i C1  j  m cid:0 1, such that the j th request results in a cache miss for A and a cache hit for A0. Because A evicts data element a in request i, then, by our construction of A0, CA0;iC1 D DiC1 [ fag. If A has a cache hit, then because The mth request is for data element b. none of the requests i C 1; : : : ; m  cid:0  1 were for b, A could not have evicted b and brought it back. Moreover, because A has a cache hit on the mth request, b 2 CAm. Therefore, A did not evict b in any of requests i; : : : ; m  cid:0  1. By our construction, A0 did not evict a. But a request for a occurs at least once before the mth request. Consider the Ô¨Årst such instance. At this instance, A has a cache miss and A0 has a cache hit.  The above theorem and the optimal-substructure property proved in part  b  imply that furthest-in-future produces the minimum number of cache misses.   Lecture Notes for Chapter 17: Amortized Analysis  Chapter 17 overview  Amortized analysis   Analyze a sequence of operations on a data structure.  Goal: Show that although some individual operations may be expensive, on  average the cost per operation is small.  Average in this context does not mean that we‚Äôre averaging over a distribution of inputs.   No probability is involved.  We‚Äôre talking about average cost in the worst case.  Organization  We‚Äôll look at 3 methods:  aggregate analysis accounting method potential method  Using 3 examples:              stack with multipop operation binary counter dynamic tables  later on   Aggregate analysis  Stack operations   PUSH.S; x : O.1  each   O.n  for any sequence of n operations.  POP.S  : O.1  each   O.n  for any sequence of n operations.   17-2  Lecture Notes for Chapter 17: Amortized Analysis   MULTIPOP.S; k   while S is not empty and k > 0  POP.S   k D k  cid:0  1  Running time of MULTIPOP:  Linear in  of POP operations.  Let each PUSH POP cost 1.     of iterations of while loop is min.s; k , where s D  of objects on stack.   Therefore, total cost D min.s; k . Sequence of n PUSH, POP, MULTIPOP operations:  Worst-case cost of MULTIPOP is O.n .  Have n operations.  Therefore, worst-case cost of sequence is O.n2 .  Observation   Each object can be popped only once per time that it‚Äôs pushed.  Have  n PUSHes    n POPs, including those in MULTIPOP.  Therefore, total cost D O.n .  Average over the n operations   O.1  per operation on average. Again, notice no probability.  Showed worst-case O.n  cost for sequence.  Therefore, O.1  per operation on average. This technique is called aggregate analysis.  Binary counter  and A≈ík  cid:0  1¬ç is the most signiÔ¨Åcant bit.   k-bit binary counter A≈í0 : : k  cid:0  1¬ç of bits, where A≈í0¬ç is the least signiÔ¨Åcant bit  Counts upward from 0. k cid:0 1   Value of counter is  A≈íi ¬ç  2i .  XiD0    Initially, counter value is 0, so A≈í0 : : k  cid:0  1¬ç D 0.   To increment, add 1 .mod 2k :  INCREMENT.A; k  i D 0 while i < k and A≈íi ¬ç == 1  if i < k  A≈íi ¬ç D 0 i D i C 1 A≈íi ¬ç D 1   Lecture Notes for Chapter 17: Amortized Analysis  17-3  Example: k D 3 [UnderlinedbitsÔ¨Çip. Showcostslater.]  counter value  0 1 2 3 4 5 6 7 0 :::  A  2 1 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 0 0  :::  cost 0 1 3 4 7 8 10 11 14  15  Cost of INCREMENT D ‚Äö  of bits Ô¨Çipped  .  Analysis Each call could Ô¨Çip k bits, so n INCREMENTs takes O.nk  time.  Observation  Not every bit Ô¨Çips every time. [Showcostsfromabove.] bit 0 1 2  every time 1=2 the time 1=4 the time  Ô¨Çips how often  :::  :::  i  1=2i the time  i  k  never  Therefore, total  of Ô¨Çips D  times in n INCREMENTs  n  bn=2c bn=4c  bn=2ic  0  k cid:0 1  1=2i  < n  XiD0n=2iÀò 1XiD0 1  cid:0  1=2 D n D 2n :  1  Therefore, n INCREMENTs costs O.n . Average cost per operation D O.1 .   17-4  Lecture Notes for Chapter 17: Amortized Analysis  Accounting method  Assign different charges to different operations.   Some are charged more than actual cost.  Some are charged less. Amortized cost D amount we charge. When amortized cost > actual cost, store the difference on speciÔ¨Åc objects in the data structure as credit. Use credit later to pay for operations whose actual cost > amortized cost. Differs from aggregate analysis:      In the accounting method, different operations can have different costs. In aggregate analysis, all operations have same cost.   Otherwise, have a sequence of operations for which the amortized cost is not  ci for all sequences of n operations.  Need credit to never go negative.  an upper bound on actual cost.   Amortized cost would tell us nothing.  Let ci D actual cost of ith operation ;  yci D amortized cost of ith operation :  n  n  Then require  XiD1 Total credit stored D  XiD1 yci  XiD1 yci  cid:0   n  n  XiD1  0 .  ci  ‚Äû∆í‚Äö‚Ä¶ had better be  Stack  actual cost  operation PUSH POP MULTIPOP min.k; s   1 1  2 0 0  amortized cost  Intuition When pushing an object, pay $2.      $1 pays for the PUSH. $1 is prepayment for it being popped by either POP or MULTIPOP.   Since each object has $1, which is credit, the credit can never go negative.  Therefore, total amortized cost, D O.n , is an upper bound on total actual cost.   Lecture Notes for Chapter 17: Amortized Analysis  17-5  Binary counter  Charge $2 to set a bit to 1.      $1 pays for setting a bit to 1. $1 is prepayment for Ô¨Çipping it back to 0.   Have $1 of credit for every 1 in the counter.  Therefore, credit  0. Amortized cost of INCREMENT:  Cost of resetting bits to 0 is paid by credit.  At most 1 bit is set to 1.  Therefore, amortized cost  $2.  For n operations, amortized cost D O.n .  Potential method  Like the accounting method, but think of the credit as potential stored with the entire data structure.  Accounting method stores credit with speciÔ¨Åc objects.  Potential method stores potential in the data structure as a whole.  Can release potential to pay for future operations.  Most Ô¨Çexible of the amortized analysis methods. Let Di D data structure after ith operation ;  D0 D initial data structure ; ci D actual cost of ith operation ; yci D amortized cost of ith operation :  Potential function ÀÜ W Di ! R ÀÜ.Di   is the potential associated with data structure Di . yci D ci C ÀÜ.Di    cid:0  ÀÜ.Di cid:0 1   increase in potential due to ith operation  :  D ci C ¬ÅÀÜ.Di   ‚Äû ∆í‚Äö ‚Ä¶ Total amortized cost D  n  n  yci  XiD1 XiD1 .ci C ÀÜ.Di    cid:0  ÀÜ.Di cid:0 1    ci C ÀÜ.Dn   cid:0  ÀÜ.D0  :  D  D  n  XiD1   telescoping sum: every term other than D0 and Dn  is added once and subtracted once    17-6  Lecture Notes for Chapter 17: Amortized Analysis  If we require that ÀÜ.Di    ÀÜ.D0  for all i, then the amortized cost is always an upper bound on actual cost. In practice: ÀÜ.D0  D 0, ÀÜ.Di    0 for all i.  Stack ÀÜ D  of objects in stack  .D  of $1 bills in accounting method   D0 D empty stack   ÀÜ.D0  D 0. Since  of objects in stack is always  0, ÀÜ.Di    0 D ÀÜ.D0  for all i. operation PUSH  actual cost  1  ¬ÅÀÜ .s C 1   cid:0  s D 1 where s D  of objects initially .s  cid:0  1   cid:0  s D  cid:0 1 .s  cid:0  k0   cid:0  s D  cid:0 k0  amortized cost 1 C 1 D 2 1  cid:0  1 D 0 k0  cid:0  k0 D 0  POP MULTIPOP  1  k0 D min.k; s   Therefore, amortized cost of a sequence of n operations D O.n .  Binary counter  ÀÜ D bi D  of 1‚Äôs after ith INCREMENT Suppose ith operation resets ti bits to 0. ci  ti C 1  resets ti bits, sets  1 bit to 1       If bi D 0, the ith operation reset all k bits and didn‚Äôt set one, so bi cid:0 1 D ti D k   bi D bi cid:0 1  cid:0  ti . If bi > 0, the ith operation reset ti bits, set one, so bi D bi cid:0 1  cid:0  ti C 1.   Either way, bi  bi cid:0 1  cid:0  ti C 1.  Therefore,  ¬ÅÀÜ.Di    .bi cid:0 1  cid:0  ti C 1   cid:0  bi cid:0 1  D 1  cid:0  ti : yci D ci C ¬ÅÀÜ.Di     .ti C 1  C .1  cid:0  ti   D 2 :  If counter starts at 0, ÀÜ.D0  D 0. Therefore, amortized cost of n operations D O.n .  Dynamic tables  A nice use of amortized analysis.   Lecture Notes for Chapter 17: Amortized Analysis  17-7   Have a table‚Äîmaybe a hash table.  Don‚Äôt know in advance how many objects will be stored in it.  When it Ô¨Ålls, must reallocate with a larger size, copying all objects into the new,  larger table.   When it gets sufÔ¨Åciently small, might want to reallocate with a smaller size.  Details of table organization not important.  Scenario  Goals  1. O.1  amortized time per operation. 2. Unused space always  constant fraction of allocated space. Load factor Àõ D num=size, where num D  items stored, size D allocated size. If size D 0, then num D 0. Call Àõ D 1. Never allow Àõ > 1. Keep Àõ > a constant fraction   goal  2 .  Table expansion  Consider only insertion.   When the table becomes full, double its size and reinsert all existing items.  Guarantees that Àõ  1=2.  Each time we actually insert an item into the table, it‚Äôs an elementary insertion.  TABLE-INSERT.T; x   if T:size == 0  if T:num == T:size  allocate T:table with 1 slot T:size D 1 allocate new-table with 2  T:size slots insert all items in T:table into new-table free T:table T:table D new-table T:size D 2  T:size insert x into T:table T:num D T:num C 1 Initially, T:num D T:size D 0.     expand?     T:num elem insertions     1 elem insertion   17-8  Lecture Notes for Chapter 17: Amortized Analysis  Running time Charge 1 per elementary insertion. Count only elementary insertions, since all other costs together are constant per call. ci D actual cost of ith operation  If not full, ci D 1. If full, have i  cid:0  1 items in the table at the start of the ith operation. Have to copy all i  cid:0  1 existing items, then insert ith item   ci D i.      n operations   ci D O.n    O.n2  time for n operations. Of course, we don‚Äôt always expand: ci D  i  if i  cid:0  1 is exact power of 2 ;  1 otherwise :  Total cost D  ci  n  XiD1  n C  D n C < n C 2n D 3n  2j  blg ncXjD0  2blg ncC1  cid:0  1  2  cid:0  1  Therefore, aggregate analysis says amortized cost per operation D 3.  Accounting method  Charge $3 per insertion of x.        $1 pays for x‚Äôs insertion. $1 pays for x to be moved in the future. $1 pays for some other item to be moved.  Suppose we‚Äôve just expanded, size D m before next expansion, size D 2m after next expansion.   Assume that the expansion used up all the credit, so that there‚Äôs no credit stored  after the expansion.   Will expand again after another m insertions.  Each insertion will put $1 on one of the m items that were in the table just after  expansion and will put $1 on the item inserted.   Have $2m of credit by next expansion, when there are 2m items to move. Just  enough to pay for the expansion, with no credit left over!   Lecture Notes for Chapter 17: Amortized Analysis  17-9  Potential method  ÀÜ.T   D 2  T:num  cid:0  T:size        Initially, num D size D 0   ÀÜ D 0. Just after expansion, size D 2  num   ÀÜ D 0. Just before expansion, size D num   ÀÜ D num   have enough potential to pay for moving all items.   Need ÀÜ  0, always.  Always have size  num   size=2   2  num  size   ÀÜ   0 .  Amortized cost of i th operation numi D num after ith operation , sizei D size after ith operation , ÀÜi D ÀÜ after ith operation . If no expansion: sizei D sizei cid:0 1 ; numi D numi cid:0 1 C 1 ;    ci D 1 : Then we have yci D ci C ÀÜi  cid:0  ÀÜi cid:0 1  D 1 C .2  numi  cid:0  sizei    cid:0  .2  numi cid:0 1  cid:0  sizei cid:0 1  D 1 C .2  numi  cid:0  sizei    cid:0  .2.numi  cid:0  1   cid:0  sizei   D 1 C 2 D 3 : If expansion: sizei D 2  sizei cid:0 1 ; sizei cid:0 1 D numi cid:0 1 D numi  cid:0  1 ; ci D numi cid:0 1 C 1 D numi :    Then we have yci D ci C ÀÜi C ÀÜi cid:0 1  D numi C .2  numi  cid:0  sizei    cid:0  .2  numi cid:0 1  cid:0  sizei cid:0 1  D numi C .2  numi  cid:0  2.numi  cid:0  1    cid:0  .2.numi  cid:0  1   cid:0  .numi  cid:0  1   D numi C 2  cid:0  .numi  cid:0  1  D 3 :   17-10  Lecture Notes for Chapter 17: Amortized Analysis  sizei  numi  i  32  24  16  8  0  0  8  16  24  i  32  Expansion and contraction  When Àõ drops too low, contract the table.   Allocate a new, smaller one.  Copy all items.  Still want   Àõ bounded from below by a constant, amortized cost per operation D O.1 .    Measure cost in terms of elementary insertions and deletions.  Àõ would become > 1 .  ‚ÄúObvious strategy‚Äù  Double size when inserting into a full table  when Àõ D 1, so that after insertion  Halve size when deletion would make table less than half full  when Àõ D 1=2,  Then always have 1=2  Àõ  1.  Suppose we Ô¨Åll table.  so that after deletion Àõ would become < 1=2 .  Then insert   double 2 deletes   halve 2 inserts   double 2 deletes   halve    Not performing enough operations after expansion or contraction to pay for the next one.  F  Lecture Notes for Chapter 17: Amortized Analysis  17-11  Simple solution  Double as before: when inserting with Àõ D 1   after doubling, Àõ D 1=2.  Halve size when deleting with Àõ D 1=4   after halving, Àõ D 1=2.  Thus, immediately after either expansion or contraction, have Àõ D 1=2.  Always have 1=4  Àõ  1.  Intuition  Want to make sure that we perform enough operations between consecutive  expansions contractions to pay for the change in table size.   Need to delete half the items before contraction.  Need to double number of items before expansion.  Either way, number of operations between expansions contractions is at least a  constant fraction of number of items copied.  if Àõ  1=2 ; T:size=2  cid:0  T:num if Àõ < 1=2 :  ÀÜ.T   D  2  T:num  cid:0  T:size T empty   ÀÜ D 0. Àõ  1=2   num  size=2   2  num  size   ÀÜ  0. Àõ < 1=2   num < size=2   ÀÜ  0.  Further intuition ÀÜ measures how far from Àõ D 1=2 we are.  Àõ D 1=2   ÀÜ D 2  num  cid:0  2  num D 0.  Àõ D 1   ÀÜ D 2  num  cid:0  num D num.  Àõ D 1=4   ÀÜ D size=2  cid:0  num D 4  num=2  cid:0  num D num.  Therefore, when we double or halve, have enough potential to pay for moving  all num items.   Potential increases linearly between Àõ D 1=2 and Àõ D 1, and it also increases  Since Àõ has different distances to go to get to 1 or 1=4, starting from 1=2, rate  linearly between Àõ D 1=2 and Àõ D 1=4. of increase of ÀÜ differs.  For Àõ to go from 1=2 to 1, num increases from size=2 to size, for a total increase of size=2. ÀÜ increases from 0 to size. Thus, ÀÜ needs to increase by 2 for each item inserted. That‚Äôs why there‚Äôs a coefÔ¨Åcient of 2 on the T:num term in the formula for ÀÜ when Àõ  1=2.  For Àõ to go from 1=2 to 1=4, num decreases from size=2 to size=4, for a total decrease of size=4. ÀÜ increases from 0 to size=4. Thus, ÀÜ needs to increase by 1 for each item deleted. That‚Äôs why there‚Äôs a coefÔ¨Åcient of  cid:0 1 on the T:num term in the formula for ÀÜ when Àõ < 1=2.  Amortized costs: more cases  insert, delete       Àõ  1=2, Àõ < 1=2  use Àõi , since Àõ can vary a lot   size does doesn‚Äôt change   17-12  Lecture Notes for Chapter 17: Amortized Analysis  Insert  Àõi cid:0 1  1=2, same analysis as before   yci D 3.  Àõi cid:0 1 < 1=2   no expansion  only occurs when Àõi cid:0 1 D 1 .  If Àõi cid:0 1 < 1=2 and Àõi < 1=2: yci D ci C ÀÜi C ÀÜi cid:0 1  D 1 C .sizei =2  cid:0  numi    cid:0  .sizei cid:0 1=2  cid:0  numi cid:0 1  D 1 C .sizei =2  cid:0  numi    cid:0  .sizei =2  cid:0  .numi  cid:0  1   D 0 :  If Àõi cid:0 1 < 1=2 and Àõi  1=2: yci D 1 C .2  numi  cid:0  sizei    cid:0  .sizei cid:0 1=2  cid:0  numi cid:0 1   3 2  sizei cid:0 1 C 3  D 1 C .2.numi cid:0 1 C 1   cid:0  sizei cid:0 1   cid:0  .sizei cid:0 1=2  cid:0  numi cid:0 1  D 3  numi cid:0 1  cid:0  D 3  Àõi cid:0 1sizei cid:0 1  cid:0  3 2  sizei cid:0 1 C 3 < D 3 :  3 2  sizei cid:0 1 C 3  3 2  sizei cid:0 1  cid:0   Therefore, amortized cost of insert is < 3.  Delete    If Àõi cid:0 1 < 1=2, then Àõi < 1=2.  If no contraction: yci D 1 C .sizei =2  cid:0  numi    cid:0  .sizei cid:0 1=2  cid:0  numi cid:0 1  D 1 C .sizei =2  cid:0  numi    cid:0  .sizei =2  cid:0  .numi C 1   D 2 :  If contraction: yci D .numi C 1 ‚Äû ∆í‚Äö ‚Ä¶ move + delete ≈ísizei =2 D sizei cid:0 1=4 D numi cid:0 1 D numi C 1¬ç    C .sizei =2  cid:0  numi    cid:0  .sizei cid:0 1=2  cid:0  numi cid:0 1   D .numi C 1  C ..numi C 1   cid:0  numi    cid:0  ..2  numi C 2   cid:0  .numi C 1   D 1 :    If Àõi cid:0 1  1=2, then no contraction.  If Àõi  1=2: yci D 1 C .2  numi  cid:0  sizei    cid:0  .2  numi cid:0 1  cid:0  sizei cid:0 1  D 1 C .2  numi  cid:0  sizei    cid:0  .2  numi C 2  cid:0  sizei   D  cid:0 1 :             Lecture Notes for Chapter 17: Amortized Analysis  17-13    1 2  sizei  cid:0  1 :  1 2  sizei cid:0 1  cid:0  1 D  If Àõi < 1=2, since Àõi cid:0 1  1=2, have numi D numi cid:0 1  cid:0  1  Thus, yci D 1 C .sizei =2  cid:0  numi    cid:0  .2  numi cid:0 1  cid:0  sizei cid:0 1  D 1 C .sizei =2  cid:0  numi    cid:0  .2  numi C 2  cid:0  sizei   D  cid:0 1 C   cid:0 1 C D 2 :  3 2  sizei  cid:0  3  numi 2  sizei  cid:0  3 1 3  2  sizei  cid:0  1  Therefore, amortized cost of delete is  2.   Solutions for Chapter 17: Amortized Analysis  Solution to Exercise 17.1-3 This solution is also posted publicly  Let ci D cost of ith operation. ci D  i  1 otherwise :  if i is an exact power of 2 ;  Operation Cost  1 2 3 4 5 6 7 8 9 10 :::  1 2 1 4 1 1 1 8 1 1 :::  n operations cost  n  lg n  XjD0  XiD1 2j D n C .2n  cid:0  1  < 3n : ci  n C  Note: Ignoring Ô¨Çoor in upper bound ofP 2j .  Average cost of operation D By aggregate analysis, the amortized cost per operation D O.1 .  Total cost  operations  < 3 .   Solutions for Chapter 17: Amortized Analysis  17-15  Solution to Exercise 17.2-1  [We assume that the only way in which COPY is invoked is automatically, after everysequenceof k PUSH and POP operations.] Charge $2 for each PUSH and POP operation and $0 for each COPY. When we call PUSH, we use $1 to pay for the operation, and we store the other $1 on the item pushed. When we call POP, we again use $1 to pay for the operation, and we store the other $1 in the stack itself. Because the stack size never exceeds k, the actual cost of a COPY operation is at most $k, which is paid by the $k found in the items in the stack and the stack itself. Since k PUSH and POP operations occur between two consecutive COPY operations, $k of credit are stored, either on individual items  from PUSH operations  or in the stack itself  from POP operations  by the time a COPY occurs. Since the amortized cost of each operation is O.1  and the amount of credit never goes negative, the total cost of n operations is O.n .  Solution to Exercise 17.2-2 This solution is also posted publicly  if i is an exact power of 2 ;  Let ci D cost of ith operation. ci D  i Charge each operation $3  amortized cost yci .  1 otherwise :      If i is not an exact power of 2, pay $1, and store $2 as credit. If i is an exact power of 2, pay $i, using stored credit.  Operation Cost Actual cost Credit remaining  1 2 3 4 5 6 7 8 9 10 :::  3 3 3 3 3 3 3 3 3 3 :::  1 2 1 4 1 1 1 8 1 1 :::  2 3 5 4 6 8 10 5 7 9 :::  Since the amortized cost is $3 per operation,  n  XiD1  yci D 3n.   17-16  Solutions for Chapter 17: Amortized Analysis  We know from Exercise 17.1-3 that  ci < 3n.  n  XiD1  n  XiD1  n  XiD1  Then we have  yci   ci   credit D amortized cost  cid:0  actual cost  0.  Since the amortized cost of each operation is O.1 , and the amount of credit never goes negative, the total cost of n operations is O.n .  Solution to Exercise 17.2-3 This solution is also posted publicly  We introduce a new Ô¨Åeld A:max to hold the index of the high-order 1 in A. Initially, A:max is set to  cid:0 1, since the low-order bit of A is at index 0, and there are initially no 1‚Äôs in A. The value of A:max is updated as appropriate when the counter is incremented or reset, and we use this value to limit how much of A must be looked at to reset it. By controlling the cost of RESET in this way, we can limit it to an amount that can be covered by credit from earlier INCREMENTs.  INCREMENT.A  i D 0 while i < A:length and A≈íi ¬ç == 1  A≈íi ¬ç D 0 i D i C 1 if i < A:length A≈íi ¬ç D 1    Additions to book‚Äôs INCREMENT start here. if i > A:max  A:max D i else A:max D  cid:0 1 RESET.A  for i D 0 to A:max A≈íi ¬ç D 0 A:max D  cid:0 1 As for the counter in the book, we assume that it costs $1 to Ô¨Çip a bit. In addition, we assume it costs $1 to update A:max. Setting and resetting of bits by INCREMENT will work exactly as for the original counter in the book: $1 will pay to set one bit to 1; $1 will be placed on the bit that is set to 1 as credit; the credit on each 1 bit will pay to reset the bit during incrementing. In addition, we‚Äôll use $1 to pay to update max, and if max increases, we‚Äôll place an additional $1 of credit on the new high-order 1.  If max doesn‚Äôt increase, we can just waste that $1‚Äîit won‚Äôt be needed.  Since RESET manipulates bits at positions only up to A:max, and since each bit up to there must have become the high-order 1   Solution to Exercise 17.3-3  Solutions for Chapter 17: Amortized Analysis  17-17  at some time before the high-order 1 got up to A:max, every bit seen by RESET has $1 of credit on it. So the zeroing of bits of A by RESET can be completely paid for by the credit stored on the bits. We just need $1 to pay for resetting max. Thus charging $4 for each INCREMENT and $1 for each RESET is sufÔ¨Åcient, so the sequence of n INCREMENT and RESET operations takes O.n  time.  Let Di be the heap after the ith operation, and let Di consist of ni elements. Also, let k be a constant such that each INSERT or EXTRACT-MIN operation takes at most k ln n time, where n D max.ni cid:0 1; ni  .  We don‚Äôt want to worry about taking the log of 0, and at least one of ni cid:0 1 and ni is at least 1. We‚Äôll see later why we use the natural log.  DeÔ¨Åne  This function exhibits the characteristics we like in a potential function: if we start with an empty heap, then ÀÜ.D0  D 0, and we always maintain that ÀÜ.Di    0. Before proving that we achieve the desired amortized times, we show that if n  2, then n ln n  ÀÜ.Di   D  0  k ni ln ni  if ni D 0 ; if ni > 0 :  n ln  1  1  n  n cid:0 1  2. We have n  cid:0  1 D n ln1 C n  cid:0  1 n  cid:0  1n D ln1 C n cid:0 1n  lne D ln e n D n  cid:0  1  2 ;  n cid:0 1  n  1   since 1 C x  ex for all real x   n  assuming that n  2.  The equation ln e n cid:0 1 is why we use the natural log.  If the ith operation is an INSERT, then ni D ni cid:0 1 C 1. If the ith operation inserts into an empty heap, then ni D 1, ni cid:0 1 D 0, and the amortized cost is yci D ci C ÀÜ.Di    cid:0  ÀÜ.Di cid:0 1   n cid:0 1 D n   k ln 1 C k  1 ln 1  cid:0  0 D 0 :  If the ith operation inserts into a nonempty heap, then ni D ni cid:0 1 C 1, and the amortized cost is yci D ci C ÀÜ.Di    cid:0  ÀÜ.Di cid:0 1    k ln ni C k ni ln ni  cid:0  k ni cid:0 1 ln ni cid:0 1 D k ln ni C k ni ln ni  cid:0  k.ni  cid:0  1  ln.ni  cid:0  1    17-18  Solutions for Chapter 17: Amortized Analysis  D k ln ni C k ni ln ni  cid:0  k ni ln.ni  cid:0  1  C k ln.ni  cid:0  1  < 2k ln ni C k ni ln  2k ln ni C 2k D O.lg ni   :  ni ni  cid:0  1  If the ith operation is an EXTRACT-MIN, then ni D ni cid:0 1  cid:0  1. If the ith operation extracts the one and only heap item, then ni D 0, ni cid:0 1 D 1, and the amortized cost is yci D ci C ÀÜ.Di    cid:0  ÀÜ.Di cid:0 1    k ln 1 C 0  cid:0  k  1 ln 1 D 0 :  If the ith operation extracts from a heap with more than 1 item, then ni D ni cid:0 1  cid:0  1 and ni cid:0 1  2, and the amortized cost is yci D ci C ÀÜ.Di    cid:0  ÀÜ.Di cid:0 1    k ln ni cid:0 1 C k ni ln ni  cid:0  k ni cid:0 1 ln ni cid:0 1 D k ln ni cid:0 1 C k.ni cid:0 1  cid:0  1  ln.ni cid:0 1  cid:0  1   cid:0  k ni cid:0 1 ln ni cid:0 1 D k ln ni cid:0 1 C k ni cid:0 1 ln.ni cid:0 1  cid:0  1   cid:0  k ln.ni cid:0 1  cid:0  1   cid:0  k ni cid:0 1 ln ni cid:0 1 D k ln < k ln  ni cid:0 1  cid:0  1 ni cid:0 1  A slightly different potential function‚Äîwhich may be easier to work with‚Äîis as follows. For each node x in the heap, let di .x  be the depth of x in Di . DeÔ¨Åne  ni cid:0 1 ni cid:0 1  cid:0  1 C k ni cid:0 1 ln ni cid:0 1 ni cid:0 1  cid:0  1 C k ni cid:0 1 ln 1 ni cid:0 1 ni cid:0 1  cid:0  1   since ni cid:0 1  2   D k ln  k ln 2 D O.1  :  ÀÜ.Di   D Xx2Di  k.di .x  C 1   D k ni C Xx2Di  di .x ! ;  where k is deÔ¨Åned as before. Initially, the heap has no items, which means that the sum is over an empty set, and so ÀÜ.D0  D 0. We always have ÀÜ.Di    0, as required. Observe that after an INSERT, the sum changes only by an amount equal to the depth of the new last node of the heap, which is blg nic. Thus, the change in potential due to an INSERT is k.1 C blg nic , and so the amortized cost is O.lg ni   C O.lg ni   D O.lg ni   D O.lg n . After an EXTRACT-MIN, the sum changes by the negative of the depth of the old last node in the heap, and so the potential decreases by k.1 C blg ni cid:0 1c . The amortized cost is at most k lg ni cid:0 1  cid:0  k.1 C blg ni cid:0 1c  D O.1 .   Solutions for Chapter 17: Amortized Analysis  17-19  Solution to Problem 17-2  a. The SEARCH operation can be performed by searching each of the individually sorted arrays. Since all the individual arrays are sorted, searching one of them using a binary search algorithm takes O.lg m  time, where m is the size of the array. In an unsuccessful search, the time is ‚Äö.lg m . In the worst case, we may assume that all the arrays A0; A1; : : : ; Ak cid:0 1 are full, k D dlg.n C 1 e, and we perform an unsuccessful search. The total time taken is T .n  D ‚Äö.lg 2k cid:0 1 C lg 2k cid:0 2 C  C lg 21 C lg 20   D ‚Äö..k  cid:0  1  C .k  cid:0  2  C  C 1 C 0  D ‚Äö.k.k  cid:0  1 =2  D ‚Äö.dlg.n C 1 e .dlg.n C 1 e  cid:0  1 =2  D ‚Äö cid:0 lg2 n :  Thus, the worst-case running time is ‚Äö.lg2 n .  b. We create a new sorted array of size 1 containing the new element to be inserted. If array A0  which has size 1  is empty, then we replace A0 with the new sorted array. Otherwise, we merge sort the two arrays into another sorted array of size 2. If A1 is empty, then we replace A1 with the new array; otherwise we merge sort the arrays as before and continue. Since array Ai is of size 2i , if we merge sort two arrays of size 2i each, we obtain one of size 2iC1, which is the size of AiC1. Thus, this method will result in another list of arrays in the same structure that we had before. Let us analyze its worst-case running time. We will assume that merge sort takes 2m time to merge two sorted lists of size m each. If all the arrays A0; A1; : : : ; Ak cid:0 2 are full, then the running time to Ô¨Åll array Ak cid:0 1 would be T .n  D 2 cid:0 20 C 21 C  C 2k cid:0 2  D 2.2k cid:0 1  cid:0  1  D 2k  cid:0  2 D ‚Äö.n  :  Therefore, the worst-case time to insert an element into this data structure is ‚Äö.n . However, let us now analyze the amortized running time. Using the aggregate method, we compute the total cost of a sequence of n inserts, starting with the empty data structure. Let r be the position of the rightmost 0 in the binary representation hnk cid:0 1; nk cid:0 2; : : : ; n0i of n, so that nj D 1 for j D 0; 1; : : : ; r  cid:0  1. The cost of an insertion when n items have already been inserted is  r cid:0 1  2  2j D O.2r   :  XjD0 Furthermore, r D 0 half the time, r D 1 a quarter of the time, and so on. There are at most dn=2re insertions for each value of r. The total cost of the n operations is therefore bounded by   17-20  Solutions for Chapter 17: Amortized Analysis  O dlg.nC1 eXrD0 l n  2rm 2r! D O.n lg n  :  The amortized cost per INSERT operation, therefore is O.lg n . We can also use the accounting method to analyze the running time. We can charge $k to insert an element. $1 pays for the insertion, and we put $.k  cid:0  1  on the inserted item to pay for it being involved in merges later on. Each time it is merged, it moves to a higher-indexed array, i.e., from Ai to AiC1. It can move to a higher-indexed array at most k  cid:0  1 times, and so the $.k  cid:0  1  on the item sufÔ¨Åces to pay for all the times it will ever be involved in merges. Since k D ‚Äö.lg n , we have an amortized cost of ‚Äö.lg n  per insertion.  c. DELETE.x  will be implemented as follows:  1. Find the smallest j for which the array Aj with 2j elements is full. Let y be  the last element of Aj .  search procedure.  2. Let x be in the array Ai . If necessary, Ô¨Ånd which array this is by using the  3. Remove x from Ai and put y into Ai. Then move y to its correct place in Ai. 4. Divide Aj  which now has 2j  cid:0  1 elements left : The Ô¨Årst element goes into array A0, the next 2 elements go into array A1, the next 4 elements go into array A2, and so forth. Mark array Aj as empty. The new arrays are created already sorted.  The cost of DELETE is ‚Äö.n  in the worst case, where i D k  cid:0  1 and j D k  cid:0  2: ‚Äö.lg n  to Ô¨Ånd Aj , ‚Äö.lg2 n  to Ô¨Ånd Ai, ‚Äö.2i   D ‚Äö.n  to put y in its correct place in array Ai, and ‚Äö.2j   D ‚Äö.n  to divide array Aj . The following sequence of n operations, where n=3 is a power of 2, yields an amortized cost that is no better: perform n=3 INSERT operations, followed by n=3 pairs of DELETE and INSERT. It costs O.n lg n  to do the Ô¨Årst n=3 INSERT operations. This creates a single full array. Each subsequent DELETE INSERT pair costs ‚Äö.n  for the DELETE to divide the full array and another ‚Äö.n  for the INSERT to recombine it. The total is then ‚Äö.n2 , or ‚Äö.n  per operation.  Solution to Problem 17-4  a. For RB-INSERT, consider a complete red-black tree in which the colors alter- nate between levels. That is, the root is black, the children of the root are red, the grandchildren of the root are black, the great-grandchildren of the root are red, and so on. When a node is inserted as a red child of one of the red leaves, then case 1 of RB-INSERT-FIXUP occurs .lg.n C 1  =2 times, so that there are .lg n  color changes to Ô¨Åx the colors of nodes on the path from the inserted node to the root. For RB-DELETE, consider a complete red-black tree in which all nodes are black. If a leaf is deleted, then the double blackness will be pushed all the way up to the root, with a color change at each level  case 2 of RB-DELETE-FIXUP , for a total of .lg n  color changes.   Solutions for Chapter 17: Amortized Analysis  17-21  b. All cases except for case 1 of RB-INSERT-FIXUP and case 2 of RB-DELETE-  FIXUP are terminating.  c. Case 1 of RB-INSERT-FIXUP reduces the number of red nodes by 1. As Fig- ure 13.5 shows, node ¬¥‚Äôs parent and uncle change from red to black, and ¬¥‚Äôs grandparent changes from black to red. Hence, ÀÜ.T 0  D ÀÜ.T    cid:0  1.  d. Lines 1‚Äì16 of RB-INSERT cause one node insertion and a unit increase in po- tential. The nonterminating case of RB-INSERT-FIXUP  Case 1  makes three color changes and decreases the potential by 1. The terminating cases of RB- INSERT-FIXUP  cases 2 and 3  cause one rotation each and do not affect the potential.  Although case 3 makes color changes, the potential does not change. As Figure 13.6 shows, node ¬¥‚Äôs parent changes from red to black, and ¬¥‚Äôs grand- parent changes from black to red.   e. The number of structural modiÔ¨Åcations and amount of potential change result- ing from lines 1‚Äì16 of RB-INSERT and from the terminating cases of RB- INSERT-FIXUP are O.1 , and so the amortized number of structural modiÔ¨Åca- tions of these parts is O.1 . The nonterminating case of RB-INSERT-FIXUP may repeat O.lg n  times, but its amortized number of structural modiÔ¨Åcations is 0, since by our assumption the unit decrease in the potential pays for the structural modiÔ¨Åcations needed. Therefore, the amortized number of structural modiÔ¨Åcations performed by RB-INSERT is O.1 .  f. From Figure 13.5, we see that case 1 of RB-INSERT-FIXUP makes the follow-  ing changes to the tree:   Changes a black node with two red children  node C   to a red node, resulting   Changes a red node  node A in part  a  and node B in part  b   to a black  in a potential change of  cid:0 2. node with one red child, resulting in no potential change.   Changes a red node  node D  to a black node with no red children, resulting  in a potential change of 1.  The total change in potential is  cid:0 1, which pays for the structural modiÔ¨Åcations performed, and thus the amortized number of structural modiÔ¨Åcations in case 1  the nonterminating case  is 0. The terminating cases of RB-INSERT-FIXUP cause O.1  structural changes. Because w.  is based solely on node col- ors and the number of color changes caused by terminating cases is O.1 , the change in potential in terminating cases is O.1 . Hence, the amortized number of structural modiÔ¨Åcations in the terminating cases is O.1 . The overall amor- tized number of structural modiÔ¨Åcations in RB-INSERT, therefore, is O.1 .  g. Figure 13.7 shows that case 2 of RB-DELETE-FIXUP makes the following  changes to the tree:   Changes a black node with no red children  node D  to a red node, resulting      in a potential change of  cid:0 1. If B is red, then it loses a black child, with no effect on potential. If B is black, then it goes from having no red children to having one red child, resulting in a potential change of  cid:0 1.   17-22  Solutions for Chapter 17: Amortized Analysis  The total change in potential is either  cid:0 1 or  cid:0 2, depending on the color of B. In either case, one unit of potential pays for the structural modiÔ¨Åcations per- formed, and thus the amortized number of structural modiÔ¨Åcations in case 2  the nonterminating case  is at most 0. The terminating cases of RB-DELETE cause O.1  structural changes. Because w.  is based solely on node col- ors and the number of color changes caused by terminating cases is O.1 , the change in potential in terminating cases is O.1 . Hence, the amortized number of structural changes in the terminating cases is O.1 . The overall amortized number of structural modiÔ¨Åcations in RB-DELETE-FIXUP, therefore, is O.1 .  h. Since the amortized number structural modiÔ¨Åcation in each operation is O.1 , the actual number of structural modiÔ¨Åcations for any sequence of m RB- INSERT and RB-DELETE operations on an initially empty red-black tree is O.m  in the worst case.   Lecture Notes for Chapter 21: Data Structures for Disjoint Sets  Chapter 21 overview  Disjoint-set data structures   Also known as ‚Äúunion Ô¨Ånd.‚Äù  Maintain collection S D fS1; : : : ; Skg of disjoint dynamic  changing over time   Each set is identiÔ¨Åed by a representative, which is some member of the set.  sets.  Doesn‚Äôt matter which member is the representative, as long as if we ask for the representative twice without modifying the set, we get the same answer both times.  [We do not include notes for the proof of running time of the disjoint-set forest implementation,whichiscoveredinSection21.4.]  Operations   MAKE-SET.x : make a new set Si D fxg, and add Si to S.  UNION.x; y : if x 2 Sx; y 2 Sy, then S D S  cid:0  Sx  cid:0  Sy [ fSx [ Syg.   Representative of new set is any member of Sx [ Sy, often the representative  Destroys Sx and Sy  since sets must be disjoint .  of one of Sx and Sy.   FIND-SET.x : return representative of set containing x.  Analysis in terms of:  n D  of elements D  of MAKE-SET operations,  m D total  of operations.   21-2  Lecture Notes for Chapter 21: Data Structures for Disjoint Sets  Analysis  Since MAKE-SET counts toward total  of operations, m  n.  Can have at most n  cid:0  1 UNION operations, since after n  cid:0  1 UNIONs, only 1  Assume that the Ô¨Årst n operations are MAKE-SET  helpful for analysis, usually  set remains.  not really necessary .  Application Dynamic connected components. For a graph G D .V; E , vertices u;  are in same connected component if and only if there‚Äôs a path between them.   Connected components partition vertices into equivalence classes.  CONNECTED-COMPONENTS .G  for each vertex  2 G:V for each edge .u;   2 G:E  MAKE-SET.   if FIND-SET.u  ¬§ FIND-SET.   UNION.u;    SAME-COMPONENT.u;    if FIND-SET.u  == FIND-SET.   return TRUE else return FALSE  Note If actually implementing connected components,      each vertex needs a handle to its object in the disjoint-set data structure, each object in the disjoint-set data structure needs a handle to its vertex.  Linked list representation   Each set is a singly linked list, represented by an object with attributes   head: the Ô¨Årst element in the list, assumed to be the set‚Äôs representative, and    tail: the last element in the list.  Objects may appear within the list in any order.   Each object in the list has attributes for    the set member,   pointer to the set object, and  next.   Lecture Notes for Chapter 21: Data Structures for Disjoint Sets  21-3  MAKE-SET: create a singleton list. FIND-SET: follow the pointer back to the list object, and then follow the head pointer to the representative. UNION: a couple of ways to do it.  1. UNION.x; y : append y‚Äôs list onto end of x‚Äôs list. Use x‚Äôs tail pointer to Ô¨Ånd  the end.   Need to update the pointer back to the set object for every node on y‚Äôs list.    If appending a large list onto a small list, it can take a while.  Operation UNION.x2; x1  UNION.x3; x2  UNION.x4; x3  UNION.x5; x4   :::  UNION.xn; xn cid:0 1    objects updated  1 2 3 4 :::  n  cid:0  1 ‚Äö.n2  total  2. Weighted-union heuristic: Always append the smaller list to the larger list.  Amortized time per operation D ‚Äö.n .  Break ties arbitrarily.  A single union can still take .n  time, e.g., if both sets have n=2 members.  Theorem With weighted union, a sequence of m operations on n elements takes O.m C n lg n  time. Sketch of proof Each MAKE-SET and FIND-SET still takes O.1 . How many times can each object‚Äôs representative pointer be updated? It must be in the smaller set each time. times updated  size of resulting set  1 2 3 ::: k ::: lg n   2  4  8 :::  2k :::  n  Therefore, each representative is updated  lg n times. Seems pretty good, but we can do much better.   theorem    21-4  Lecture Notes for Chapter 21: Data Structures for Disjoint Sets  Disjoint-set forest  Forest of trees.    1 tree per set. Root is representative.  Each node points only to its parent.  c  e  h  b  f  d  g  UNION e,g   c  f  e  d  g  h  b   MAKE-SET: make a single-node tree.  UNION: make one root a child of the other.  FIND-SET: follow pointers to the root.  Not so good‚Äîcould get a linear chain of nodes.  Great heuristics  root of the larger tree.   Union by rank: make the root of the smaller tree  fewer nodes  a child of the   Don‚Äôt actually use size.  Use rank, which is an upper bound on height of node.  Make the root with the smaller rank into a child of the root with the larger   Path compression: Find path D nodes visited during FIND-SET on the trip to  the root. Make all nodes on the Ô¨Ånd path direct children of root.  rank.  a  d  c  b  d  a  b  c  Each node has two attributes, p  parent  and rank.   Lecture Notes for Chapter 21: Data Structures for Disjoint Sets  21-5  MAKE-SET.x  x:p D x x:rank D 0 UNION.x; y   LINK.x; y   if x:rank > y:rank  LINK.FIND-SET.x ; FIND-SET.y    y:rank D y:rank C 1  FIND-SET.x  if x ¬§ x:p return x:p  x:p D FIND-SET.x:p   y:p D x else x:p D y    If equal ranks, choose y as parent and increment its rank. if x:rank == y:rank  FIND-SET makes a pass up to Ô¨Ånd the root, and a pass down as recursion unwinds to update each node on Ô¨Ånd path to point directly to root.  If use both union by rank and path compression, O.m Àõ.n  .  Running time  n 0‚Äì2 3 4‚Äì7  8‚Äì2047  2048‚ÄìA4.1   Àõ.n   0 1 2 3 4  What‚Äôs A4.1 ? See Section 21.4, if you dare. It‚Äôs  1080   of atoms in observ- able universe. This bound is tight‚Äîthere exists a sequence of operations that takes .m Àõ.n   time.   Solutions for Chapter 21: Data Structures for Disjoint Sets  Solution to Exercise 21.2-3 This solution is also posted publicly  We want to show that we can assign O.1  charges to MAKE-SET and FIND-SET and an O.lg n  charge to UNION such that the charges for a sequence of these operations are enough to cover the cost of the sequence‚ÄîO.mC n lg n , according to the theorem. When talking about the charge for each kind of operation, it is helpful to also be able to talk about the number of each kind of operation. Consider the usual sequence of m MAKE-SET, UNION, and FIND-SET operations, n of which are MAKE-SET operations, and let l < n be the number of UNION operations.  Recall the discussion in Section 21.1 about there being at most n  cid:0  1 UNION operations.  Then there are n MAKE-SET operations, l UNION operations, and m  cid:0  n  cid:0  l FIND-SET operations. The theorem didn‚Äôt separately name the number l of UNIONs; rather, it bounded the number by n. If you go through the proof of the theorem with l UNIONs, you get the time bound O.m cid:0 lCl lg l  D O.mCl lg l  for the sequence of operations. That is, the actual time taken by the sequence of operations is at most c.mC l lg l , for some constant c. Thus, we want to assign operation charges such that  MAKE-SET charge  C FIND-SET charge  C UNION charge   c.m C l lg l  ; so that the amortized costs give an upper bound on the actual costs. The following assignments work, where c0 is some constant  c:  MAKE-SET: c0  FIND-SET: c0  UNION: c0.lg n C 1  Substituting into the above sum, we get c0n C c0.m  cid:0  n  cid:0  l  C c0.lg n C 1 l D c0m C c0l lg n D c0.m C l lg n  > c.m C l lg l  :   n .m  cid:0  n  cid:0  l    l   Solutions for Chapter 21: Data Structures for Disjoint Sets  21-7  Solution to Exercise 21.2-5  As the hint suggests, make the representative of each set be the tail of its linked list. Except for the tail element, each element‚Äôs representative pointer points to the tail. The tail‚Äôs representative pointer points to the head. An element is the tail if its next pointer is NIL. Now we can get to the tail in O.1  time: if x:next == NIL, then tail D x, else tail D x:rep. We can get to the head in O.1  time as well: if x:next == NIL, then head D x:rep, else head D x:rep:rep. The set object needs only to store a pointer to the tail, though a pointer to any list element would sufÔ¨Åce.  Solution to Exercise 21.2-6 This solution is also posted publicly  Let‚Äôs call the two lists A and B, and suppose that the representative of the new list will be the representative of A. Rather than appending B to the end of A, instead splice B into A right after the Ô¨Årst element of A. We have to traverse B to update pointers to the set object anyway, so we can just make the last element of B point to the second element of A.  Solution to Exercise 21.3-3  You need to Ô¨Ånd a sequence of m operations on n elements that takes .m lg n  time. Start with n MAKE-SETs to create singleton sets fx1g ;fx2g ; : : : ;fxng. Next perform the n cid:0  1 UNION operations shown below to create a single set whose tree has depth lg n.   21-8  Solutions for Chapter 21: Data Structures for Disjoint Sets  UNION.x1; x2  UNION.x3; x4  UNION.x5; x6  ::: UNION.xn cid:0 1; xn  UNION.x2; x4  UNION.x6; x8  UNION.x10; x12  ::: UNION.xn cid:0 2; xn  UNION.x4; x8  UNION.x12; x16  UNION.x20; x24  ::: UNION.xn cid:0 4; xn  ::: UNION.xn=2; xn   n=2 of these  n=4 of these  n=8 of these  1 of these  Finally, perform m  cid:0  2n C 1 FIND-SET operations on the deepest element in the tree. Each of these FIND-SET operations takes .lg n  time. Letting m  3n, we have more than m=3 FIND-SET operations, so that the total cost is .m lg n .  Solution to Exercise 21.3-4  Solution to Exercise 21.3-5  Maintain a circular, singly linked list of the nodes of each set. To print, just follow the list until you get back to node x, printing each member of the list. The only other operations that change are FIND-SET, which sets x:next D x, and LINK, which exchanges the pointers x:next and y:next.  With the path-compression heuristic, the sequence of m MAKE-SET, FIND-SET, and LINK operations, where all the LINK operations take place before any of the FIND-SET operations, runs in O.m  time. The key observation is that once a node x appears on a Ô¨Ånd path, x will be either a root or a child of a root at all times thereafter. We use the accounting method to obtain the O.m  time bound. We charge a MAKE-SET operation two dollars. One dollar pays for the MAKE-SET, and one dollar remains on the node x that is created. The latter pays for the Ô¨Årst time that x appears on a Ô¨Ånd path and is turned into a child of a root. We charge one dollar for a LINK operation. This dollar pays for the actual linking of one node to another.   Solutions for Chapter 21: Data Structures for Disjoint Sets  21-9  We charge one dollar for a FIND-SET. This dollar pays for visiting the root and its child, and for the path compression of these two nodes, during the FIND-SET. All other nodes on the Ô¨Ånd path use their stored dollar to pay for their visitation and path compression. As mentioned, after the FIND-SET, all nodes on the Ô¨Ånd path become children of a root  except for the root itself , and so whenever they are visited during a subsequent FIND-SET, the FIND-SET operation itself will pay for them. Since we charge each operation either one or two dollars, a sequence of m opera- tions is charged at most 2m dollars, and so the total time is O.m . Observe that nothing in the above argument requires union by rank. Therefore, we get an O.m  time bound regardless of whether we use union by rank.  Solution to Exercise 21.4-4  Clearly, each MAKE-SET and LINK operation takes O.1  time. Because the rank of a node is an upper bound on its height, each Ô¨Ånd path has length O.lg n , which in turn implies that each FIND-SET takes O.lg n  time. Thus, any sequence of m MAKE-SET, LINK, and FIND-SET operations on n elements takes O.m lg n  time. It is easy to prove an analogue of Lemma 21.7 to show that if we convert a sequence of m0 MAKE-SET, UNION, and FIND-SET operations into a sequence of m MAKE-SET, LINK, and FIND-SET operations that take O.m lg n  time, then the sequence of m0 MAKE-SET, UNION, and FIND-SET operations takes O.m0 lg n  time.  Solution to Exercise 21.4-5  Professor Dante is mistaken. Take the following scenario. Let n D 16, and make 16 separate singleton sets using MAKE-SET. Then do 8 UNION operations to link the sets into 8 pairs, where each pair has a root with rank 0 and a child with rank 1. Now do 4 UNIONs to link pairs of these trees, so that there are 4 trees, each with a root of rank 2, children of the root of ranks 1 and 0, and a node of rank 0 that is the child of the rank-1 node. Now link pairs of these trees together, so that there are two resulting trees, each with a root of rank 3 and each containing a path from a leaf to the root with ranks 0, 1, and 3. Finally, link these two trees together, so that there is a path from a leaf to the root with ranks 0, 1, 3, and 4. Let x and y be the nodes on this path with ranks 1 and 3, respectively. Since A1.1  D 3, level.x  D 1, and since A0.3  D 4, level.y  D 0. Yet y follows x on the Ô¨Ånd path.  Solution to Exercise 21.4-6  First, Àõ0.22047  cid:0  1  D minfk W Ak.1   2047g D 3, and 22047  cid:0  1  1080.   21-10  Solutions for Chapter 21: Data Structures for Disjoint Sets  Second, we need that 0  level.x   Àõ0.n  for all nonroots x with x:rank  1. With this deÔ¨Ånition of Àõ0.n , we have AÀõ0.n .x:rank   AÀõ0.n .1   lg.n C 1  > lg n  x:p:rank. The rest of the proof goes through with Àõ0.n  replacing Àõ.n .  Solution to Problem 21-1  a. For the input sequence  4; 8; E; 3; E; 9; 2; 6; E; E; E; 1; 7; E; 5 ;  the values in the extracted array would be 4; 3; 2; 6; 8; 1. The following table shows the situation after the ith iteration of the for loop when we use OFF-LINE-MINIMUM on the same input.  For this input, n D 9 and m‚Äîthe number of extractions‚Äîis 6 .  K1 K2  K3  K5  K6  K7  f9; 2; 6g f9; 2; 6g  f3g f3g f3g  f4; 8g f4; 8g f4; 8g f4; 8g  K4  fg fg  f9; 2; 6g f9; 2; 6; 3g  f9; 2; 6; 3; 4; 8g f9; 2; 6; 3; 4; 8g  i  0 1 2 3 4 5 6 7 8  f1; 7g  fg fg fg fg fg fg  f9; 2; 6; 3; 4; 8g f9; 2; 6; 3; 4; 8g  f5g  f5; 1; 7g f5; 1; 7g f5; 1; 7g f5; 1; 7g f5; 1; 7g f5; 1; 7g f5; 1; 7g  f5; 1; 7; 9; 2; 6; 3; 4; 8g  extracted  1 2 3 4 5 6  1 2 1 3 2 1 4 3 2 1 4 3 2 1 4 3 2 6 1 4 3 2 6 1 4 3 2 6 8 1  Because j D m C 1 in the iterations for i D 5 and i D 7, no changes occur in these iterations.  b. We want to show that the array extracted returned by OFF-LINE-MINIMUM is correct, meaning that for i D 1; 2; : : : ; m, extracted≈íj ¬ç is the key returned by the j th EXTRACT-MIN call. We start with n INSERT operations and m EXTRACT-MIN operations. The smallest of all the elements will be extracted in the Ô¨Årst EXTRACT-MIN after its insertion. So we Ô¨Ånd j such that the minimum element is in Kj , and put the minimum element in extracted≈íj ¬ç, which corresponds to the EXTRACT-MIN after the minimum element insertion. Now we reduce to a similar problem with n  cid:0  1 INSERT operations and m  cid:0  1 EXTRACT-MIN operations in the following way: the INSERT operations are the same but without the insertion of the smallest that was extracted, and the EXTRACT-MIN operations are the same but without the extraction that ex- tracted the smallest element. Conceptually, we unite Ij and IjC1, removing the extraction between them and also removing the insertion of the minimum element from Ij [ IjC1. Uniting Ij and IjC1 is accomplished by line 6. We need to determine which set is Kl, rather than just using KjC1 unconditionally, because KjC1 may have been destroyed when it was united into a higher-indexed set by a previous execution of line 6.   Solutions for Chapter 21: Data Structures for Disjoint Sets  21-11  Because we process extractions in increasing order of the minimum value found, the remaining iterations of the for loop correspond to solving the re- duced problem. There are two other points worth making. First, if the smallest remaining ele- ment had been inserted after the last EXTRACT-MIN  i.e., j D m C 1 , then no changes occur, because this element is not extracted. Second, there may be smaller elements within the Kj sets than the the one we are currently looking for. These elements do not affect the result, because they correspond to ele- ments that were already extracted, and their effect on the algorithm‚Äôs execution is over.  c. To implement this algorithm, we place each element in a disjoint-set forest. Each root has a pointer to its Ki set, and each Ki set has a pointer to the root of the tree representing it. All the valid sets Ki are in a linked list. Before OFF-LINE-MINIMUM, there is initialization that builds the initial sets Ki according to the Ii sequences.  Line 2  ‚Äúdetermine j such that i 2 Kj ‚Äù  turns into j D FIND-SET.i  .  Line 5  ‚Äúlet l be the smallest value greater than j for which set Kl exists‚Äù   turns into Kl D Kj :next. remove Kj from the linked list.   Line 6  ‚ÄúKl D Kj [ Kl, destroying Kj ‚Äù  turns into l D LINK.j; l  and  To analyze the running time, we note that there are n elements and that we have the following disjoint-set operations:   n MAKE-SET operations  at most n  cid:0  1 UNION operations before starting  n FIND-SET operations  at most n LINK operations  Thus the number m of overall operations is O.n . The total running time is O.m Àõ.n   D O.n Àõ.n  . [The‚Äútightbound‚Äùwordingthatthisquestionusesdoesnotrefertoan‚Äúasymp- toticallytight‚Äùbound. Instead,thequestionismerelyaskingforaboundthatis nottoo‚Äúloose.‚Äù]  Solution to Problem 21-2  a. Denote the number of nodes by n, and let n D .m C 1 =3, so that m D 3n  cid:0  1. First, perform the n operations MAKE-TREE.1 , MAKE-TREE.2 , . . . , MAKE-TREE.n . Then perform the sequence of n  cid:0  1 GRAFT operations GRAFT.1; 2 , GRAFT.2; 3 , . . . , GRAFT.n cid:0 1; n ; this sequence produces a single disjoint-set tree that is a linear chain of n nodes with n at the root and 1 as the only leaf. Then perform FIND-DEPTH.1  repeatedly, n times. The total number of operations is n C .n  cid:0  1  C n D 3n  cid:0  1 D m.   21-12  Solutions for Chapter 21: Data Structures for Disjoint Sets  Each MAKE-TREE and GRAFT operation takes O.1  time. Each FIND-DEPTH operation has to follow an n-node Ô¨Ånd path, and so each of the n FIND-DEPTH operations takes ‚Äö.n  time. The total time is n  ‚Äö.n  C .2n  cid:0  1   O.1  D ‚Äö.n2  D ‚Äö.m2 .  b. MAKE-TREE is like MAKE-SET, except that it also sets the d value to 0:  MAKE-TREE.  :p D  :rank D 0 :d D 0 It is correct to set :d to 0, because the depth of the node in the single-node disjoint-set tree is 0, and the sum of the depths on the Ô¨Ånd path for  consists only of :d.  c. FIND-DEPTH will call a procedure FIND-ROOT:  FIND-ROOT.  if :p ¬§ :p:p y D :p :p D FIND-ROOT.y  :d D :d C y:d  return :p  FIND-DEPTH.  FIND-ROOT.  if  == :p  return :d  else return :d C :p:d     no need to save the return value  FIND-ROOT performs path compression and updates pseudodistances along the Ô¨Ånd path from . It is similar to FIND-SET on page 571, but with three changes. First, when  is either the root or a child of a root  one of these conditions holds if and only if :p D :p:p  in the disjoint-set forest, we don‚Äôt have to recurse; instead, we just return :p. Second, when we do recurse, we save the pointer :p into a new variable y. Third, when we recurse, we update :d by adding into it the d values of all nodes on the Ô¨Ånd path that are no longer proper ancestors of  after path compression; these nodes are precisely the proper ancestors of  other than the root. Thus, as long as  does not start out the FIND-ROOT call as either the root or a child of the root, we add y:d into :d. Note that y:d has been updated prior to updating :d, if y is also neither the root nor a child of the root. FIND-DEPTH Ô¨Årst calls FIND-ROOT to perform path compression and update pseudodistances. Afterward, the Ô¨Ånd path from  consists of either just   if  is a root  or just  and :p  if  is not a root, in which case it is a child of the root after path compression . In the former case, the depth of  is just :d, and in the latter case, the depth is :d C :p:d.   Solutions for Chapter 21: Data Structures for Disjoint Sets  21-13  d. Our procedure for GRAFT is a combination of UNION and LINK:  GRAFT.r;   r0 D FIND-ROOT.r  0 D FIND-ROOT.  ¬¥ D FIND-DEPTH.  if r0:rank > 0:rank  else r0:p D 0  0:p D r0 r0:d D r0:d C ¬¥ C 1 0:d D 0:d  cid:0  r0:d r0:d D r0:d C ¬¥ C 1  cid:0  0:d if r0:rank == 0:rank 0:rank D 0:rank C 1  This procedure works as follows. First, we call FIND-ROOT on r and  in order to Ô¨Ånd the roots r0 and 0, respectively, of their trees in the disjoint-set forest. As we saw in part  c , these FIND-ROOT calls also perform path com- pression and update pseudodistances on the Ô¨Ånd paths from r and . We then call FIND-DEPTH. , saving the depth of  in the variable ¬¥.  Since we have just compressed ‚Äôs Ô¨Ånd path, this call of FIND-DEPTH takes O.1  time.  Next, we emulate the action of LINK, by making the root  r0 or 0  of smaller rank a child of the root of larger rank; in case of a tie, we make r0 a child of 0. If 0 has the smaller rank, then all nodes in r‚Äôs tree will have their depths in- creased by the depth of  plus 1  because r is to become a child of  . Altering the psuedodistance of the root of a disjoint-set tree changes the computed depth of all nodes in that tree, and so adding ¬¥ C 1 to r0:d accomplishes this update for all nodes in r‚Äôs disjoint-set tree. Since 0 will become a child of r0 in the disjoint-set forest, we have just increased the computed depth of all nodes in the disjoint-set tree rooted at 0 by r0:d. These computed depths should not have changed, however. Thus, we subtract off r0:d from 0:d, so that the sum 0:d C r0:d after making 0 a child of r0 equals 0:d before making 0 a child of r0. On the other hand, if r0 has the smaller rank, or if the ranks are equal, then r0 becomes a child of 0 in the disjoint-set forest. In this case, 0 remains a root in the disjoint-set forest afterward, and we can leave 0:d alone. We have to update r0:d, however, so that after making r0 a child of 0, the depth of each node in r‚Äôs disjoint-set tree is increased by ¬¥ C 1. We add ¬¥ C 1 to r0:d, but we also subtract out 0:d, since we have just made r0 a child of 0. Finally, if the ranks of r0 and 0 are equal, we increment the rank of 0, as is done in the LINK procedure.  e. The asymptotic running times of MAKE-TREE, FIND-DEPTH, and GRAFT are equivalent to those of MAKE-SET, FIND-SET, and UNION, respectively. Thus, a sequence of m operations, n of which are MAKE-TREE operations, takes ‚Äö.m Àõ.n   time in the worst case.   Lecture Notes for Chapter 22: Elementary Graph Algorithms  Graph representation  Given graph G D .V; E . In pseudocode, represent vertex set by G:V and edge set by G:E.   G may be either directed or undirected.  Two common ways to represent graphs for algorithms:  1. Adjacency lists. 2. Adjacency matrix.  When expressing the running time of an algorithm, it‚Äôs often in terms of both jV j and jEj. In asymptotic notation‚Äîand only in asymptotic notation‚Äîwe‚Äôll drop the cardinality. Example: O.V C E . [TheintroductiontoPartVItalksmoreaboutthis.]  Adjacency lists  Array Adj of jV j lists, one per vertex. Vertex u‚Äôs list has all vertices  such that .u;   2 E.  Works for both directed and undirected graphs.  In pseudocode, denote the array as attribute G:Adj, so will see notation such as G:Adj≈íu¬ç.  Example For an undirected graph:  1  5  2  4  3  Adj  1 2 3 4 5  2 1 2 2 4  5 5 4 5 1  3  4  3 2   22-2  Lecture Notes for Chapter 22: Elementary Graph Algorithms  If edges have weights, can put the weights in the lists. Weight: w W E ! R We‚Äôll use weights later on for spanning trees and shortest paths. Space: ‚Äö.V C E . Time: to list all vertices adjacent to u: ‚Äö.degree.u  . Time: to determine whether .u;   2 E: O.degree.u  .  Example For a directed graph:  1  3  2  4  Adj  1 2 3 4  2 4 1  4  2  3  Same asymptotic space and time.  Adjacency matrix  jV j  jV j matrix A D .aij   aij D  1 if .i; j   2 E ;  0 otherwise :  1 0 1 0 0 1  2 1 0 1 1 1  3 0 1 0 1 0  4 0 1 1 0 1  5 1 1 0 1 0  1 2 3 4 5  1 0 0 1 0  2 1 0 1 0  3 0 0 0 1  4 0 1 0 1  1 2 3 4  Space: ‚Äö.V 2 . Time: to list all vertices adjacent to u: ‚Äö.V  . Time: to determine whether .u;   2 E: ‚Äö.1 . Can store weights instead of bits for weighted graph. We‚Äôll use both representations in these lecture notes.  Representing graph attributes  Graph algorithms usually need to maintain attributes for vertices and or edges. Use the usual dot-notation: denote attribute d of vertex  by :d. Use the dot-notation for edges, too: denote attribute f of edge .u;   by .u;  :f .   Breadth-Ô¨Årst search  Lecture Notes for Chapter 22: Elementary Graph Algorithms  22-3  Implementing graph attributes No one best way to implement. Depends on the programming language, the algo- rithm, and how the rest of the program interacts with the graph. If representing the graph with adjacency lists, can represent vertex attributes in additional arrays that parallel the Adj array, e.g., d ≈í1 : :jV j¬ç, so that if vertices adjacent to u are in Adj≈íu¬ç, store u:d in array entry d ≈íu¬ç. But can represent attributes in other ways. Example: represent vertex attributes as instance variables within a subclass of a Vertex class.  Input: Graph G D .V; E , either directed or undirected, and source vertex s 2 V . Output: :d D distance  smallest  of edges  from s to , for all  2 V . In book, also : such that .u;   is last edge on shortest path s ; .  u is ‚Äôs predecessor.    set of edges f.:;   W  ¬§ sg forms a tree.  Later, we‚Äôll see a generalization of breadth-Ô¨Årst search, with edge weights. For now, we‚Äôll keep it simple.  Compute only :d, not :. [Seebookfor :.]  Omitting colors of vertices. [Usedinbooktoreasonaboutthealgorithm. We‚Äôll  skipthemhere.]  Idea Send a wave out from s.  First hits all vertices 1 edge from s.  From there, hits all vertices 2 edges from s.  Etc. Use FIFO queue Q to maintain wavefront.     2 Q if and only if wave has hit  but has not come out of  yet.  u:d D 1  BFS.V; E; s  for each u 2 V  cid:0  fsg s:d D 0 Q D ; ENQUEUE.Q; s  while Q ¬§ ;  u D DEQUEUE.Q  for each  2 G:Adj≈íu¬ç if :d == 1:d D u:d C 1  ENQUEUE.Q;     22-4  Lecture Notes for Chapter 22: Elementary Graph Algorithms  Example directed graph [undirectedexampleinbook] .  s  a  b  0  1  3  1 c  e  2  f  g  h  3  2  3  i  3  Can show that Q consists of vertices with d values.  i    i  i  : : :  i  i C 1 i C 1 : : :  i C 1   Only 1 or 2 values.  If 2, differ by 1 and all smallest are Ô¨Årst.  Since each vertex gets a Ô¨Ånite d value at most once, values assigned to vertices are monotonically increasing over time. Actual proof of correctness is a bit trickier. See book. BFS may not reach all vertices. Time D O.V C E .  O.V   because every vertex enqueued at most once.  O.E  because every vertex dequeued at most once and we examine .u;   only when u is dequeued. Therefore, every edge examined at most once if directed, at most twice if undirected.  Depth-Ô¨Årst search  Input: G D .V; E , directed or undirected. No source vertex given! Output: 2 timestamps on each vertex:   :d D discovery time  :f D Ô¨Ånishing time These will be useful for other algorithms later on. Can also compute :. [Seebook.]  Will methodically explore every edge.   Start over from different vertices as necessary.  As soon as we discover a vertex, explore from it.   Unlike BFS, which puts a vertex on a queue so that we explore from it later.   Lecture Notes for Chapter 22: Elementary Graph Algorithms  22-5  As DFS progresses, every vertex has a color:  WHITE D undiscovered  GRAY D discovered, but not Ô¨Ånished  not done exploring from it   BLACK D Ô¨Ånished  have found everything reachable from it  Discovery and Ô¨Ånishing times:  Unique integers from 1 to 2jV j.  For all , :d < :f . In other words, 1  :d < :f  2jV j.  Pseudocode Uses a global timestamp time.  DFS.G  for each u 2 G:V time D 0 for each u 2 G:V  u:color D WHITE  if u:color == WHITE  DFS-VISIT.G; u   DFS-VISIT.G; u  time D time C 1 u:d D time u:color D GRAY for each  2 G:Adj≈íu¬ç if :color == WHITE DFS-VISIT.   u:color D BLACK time D time C 1 u:f D time     discover u    explore .u;       Ô¨Ånish u  Example [Gothroughthisexample,addinginthed andf valuesasthey‚Äôrecomputed. Show colorsastheychange. Don‚Äôtputintheedgetypesyet.]  2  7  T  T  1  12  B  3  4  T  d  f  T  C  9  10  C  C  8  11  C  65  C  13  16  T  14  15  T  F  C   22-6  Lecture Notes for Chapter 22: Elementary Graph Algorithms  Time D ‚Äö.V C E .  Similar to BFS analysis.  ‚Äö, not just O, since guaranteed to examine every vertex and edge. DFS forms a depth-Ô¨Årst forest comprised of > 1 depth-Ô¨Årst trees. Each tree is made of edges .u;   such that u is gray and  is white when .u;   is explored.  Theorem  Parenthesis theorem  [Proofomitted.] For all u; , exactly one of the following holds:  1. u:d < u:f < :d < :f or :d < :f < u:d < u:f  i.e., the intervals ≈íu:d; u:f ¬ç and ≈í:d; :f ¬ç are disjoint  and neither of u and  is a descendant of the other.  2. u:d < :d < :f < u:f and  is a descendant of u. 3. :d < u:d < u:f < :f and u is a descendant of .  So u:d < :d < u:f < :f cannot happen. Like parentheses:   OK:  Not OK:      [ ]   [   ]    [ ]   [   ]    [     ]  Corollary  is a proper descendant of u if and only if u:d < :d < :f < u:f .  Theorem  White-path theorem  [Proofomitted.]  is a descendant of u if and only if at time u:d, there is a path u ;  consisting of only white vertices.  Except for u, which was just colored gray.   ClassiÔ¨Åcation of edges   Tree edge: in the depth-Ô¨Årst forest. Found by exploring .u;  .  Back edge: .u;  , where u is a descendant of .  Forward edge: .u;  , where  is a descendant of u, but not a tree edge.  Cross edge: any other edge. Can go between vertices in same depth-Ô¨Årst tree  or in different depth-Ô¨Årst trees.  [Nowlabeltheexamplefromabovewithedgetypes.] In an undirected graph, there may be some ambiguity since .u;   and .; u  are the same edge. Classify by the Ô¨Årst type above that matches.  Theorem [Proofomitted.] In DFS of an undirected graph, we get only tree and back edges. No forward or cross edges.   Lecture Notes for Chapter 22: Elementary Graph Algorithms  22-7  Topological sort  Directed acyclic graph  dag   A directed graph with no cycles. Good for modeling processes and structures that have a partial order:  a > b and b > c   a > c.  But may have a and b such that neither a > b nor b > c. Can always make a total order  either a > b or b > a for all a ¬§ b  from a partial order. In fact, that‚Äôs what a topological sort will do.  Example Dag of dependencies for putting on goalie equipment: [Leaveonboard, butshow withoutdiscoveryandÔ¨Ånishingtimes. Willputtheminlater.]  25 26  socks  15 24  shorts  7 14  T-shirt  1 6  batting glove  16 23  hose  8 13  chest pad  17 22  pants  18 21  skates  19 20  leg pads  9 12  sweater  10 11  mask  catch glove  2 5  3 4  blocker  Lemma A directed graph G is acyclic if and only if a DFS of G yields no back edges. Proof   : Show that back edge   cycle. Suppose there is a back edge .u;  . Then  is ancestor of u in depth-Ô¨Årst forest.  B  v  T  T  T  u   22-8  Lecture Notes for Chapter 22: Elementary Graph Algorithms  Therefore, there is a path  ; u, so  ; u !  is a cycle.   : Show that cycle   back edge. Suppose G contains cycle c. Let  be the Ô¨Årst vertex discovered in c, and let .u;   be the preceding edge in c. At time :d, vertices of c form a white path  ; u  since  is the Ô¨Årst vertex discovered in c . By white-path theorem, u is descendant  lemma  of  in depth-Ô¨Årst forest. Therefore, .u;   is a back edge.  Topological sort of a dag: a linear ordering of vertices such that if .u;   2 E, then u appears somewhere before .  Not like sorting numbers.   TOPOLOGICAL-SORT.G  call DFS.G  to compute Ô¨Ånishing times :f for all  2 G:V output vertices in order of decreasing Ô¨Ånishing times  Don‚Äôt need to sort by Ô¨Ånishing times.   Can just output vertices as they‚Äôre Ô¨Ånished and understand that we want the  reverse of this list.   Or put them onto the front of a linked list as they‚Äôre Ô¨Ånished. When done, the  list contains vertices in topologically sorted order.  Time ‚Äö.V C E . Do example. [NowwritediscoveryandÔ¨Ånishingtimesingoalieequipmentexam- ple.] Order: 26 24 23 22 21 20 14 13 12 11 mask 6 5 4  socks shorts hose pants skates leg pads t-shirt chest pad sweater  batting glove catch glove blocker  Correctness Just need to show if .u;   2 E, then :f < u:f . When we explore .u;  , what are the colors of u and ?   u is gray.   Lecture Notes for Chapter 22: Elementary Graph Algorithms  22-9        Is  gray, too?  No, because then  would be ancestor of u.    .u;   is a back edge.   contradiction of previous lemma  dag has no back edges .  Is  white?  Then becomes descendant of u.  By parenthesis theorem, u:d < :d <:f < u:f .  Is  black?  Then  is already Ô¨Ånished.  Since we‚Äôre exploring .u;  , we have not yet Ô¨Ånished u. Therefore, :f < u:f .  Strongly connected components  Given directed graph G D .V; E . A strongly connected component  SCC  of G is a maximal set of vertices C  V such that for all u;  2 C , both u ;  and  ; u.  Example [JustshowSCC‚ÄôsatÔ¨Årst. DoDFSalittlelater.]  14 19  15 16  17 18  13 20  3 4  2 5  1 12  10 11  6 9  7 8  Algorithm uses GT D transpose of G.  GT D .V; ET , ET D f.u;   W .; u  2 Eg.  GT is G with all edges reversed. Can create GT in ‚Äö.V C E  time if using adjacency lists.  Observation G and GT have the same SCC‚Äôs.  u and  are reachable from each other in G if and only if reachable from each other in GT.   Component graph   GSCC D .V SCC; ESCC .  V SCC has one vertex for each SCC in G.  ESCC has an edge if there‚Äôs an edge between the corresponding SCC‚Äôs in G.   22-10  Lecture Notes for Chapter 22: Elementary Graph Algorithms  For our example:  Lemma GSCC is a dag. More formally, let C and C 0 be distinct SCC‚Äôs in G, let u;  2 C , u0; 0 2 C 0, and suppose there is a path u ; u0 in G. Then there cannot also be a path 0 ;  in G.  Proof Suppose there is a path 0 ;  in G. Then there are paths u ; u0 ; 0 and 0 ;  ; u in G. Therefore, u and 0 are reachable from each other, so they are not in separate SCC‚Äôs.  lemma   SCC.G   call DFS.G  to compute Ô¨Ånishing times u:f for all u compute GT call DFS.GT , but in the main loop, consider vertices in order of decreasing u:f   as computed in Ô¨Årst DFS   output the vertices in each tree of the depth-Ô¨Årst forest formed in second DFS  as a separate SCC  Example:  1. Do DFS 2. GT 3. DFS  roots blackened   Time: ‚Äö.V C E . How can this possibly work?  Idea By considering vertices in second DFS in decreasing order of Ô¨Ånishing times from Ô¨Årst DFS, we are visiting vertices of the component graph in topological sort order. To prove that it works, Ô¨Årst deal with 2 notational issues:   Will be discussing u:d and u:f . These always refer to Ô¨Årst DFS.  Extend notation for d and f to sets of vertices U  V :   d.U   D minu2U fu:dg  earliest discovery time   f .U   D maxu2U fu:fg  latest Ô¨Ånishing time    Lecture Notes for Chapter 22: Elementary Graph Algorithms  22-11  Lemma Let C and C 0 be distinct SCC‚Äôs in G D .V; E . Suppose there is an edge .u;   2 E such that u 2 C and  2 C 0. C¬¢  C  u  v  Then f .C   > f .C 0 .  Proof Two cases, depending on which SCC had the Ô¨Årst discovered vertex during the Ô¨Årst DFS.      If d.C   < d.C 0 , let x be the Ô¨Årst vertex discovered in C . At time x:d, all vertices in C and C 0 are white. Thus, there exist paths of white vertices from x to all vertices in C and C 0. By the white-path theorem, all vertices in C and C 0 are descendants of x in depth-Ô¨Årst tree. By the parenthesis theorem, x:f D f .C   > f .C 0 . If d.C   > d.C 0 , let y be the Ô¨Årst vertex discovered in C 0. At time y:d, all vertices in C 0 are white and there is a white path from y to each vertex in C 0   all vertices in C 0 become descendants of y. Again, y:f D f .C 0 . At time y:d, all vertices in C are white. By earlier lemma, since there is an edge .u;  , we cannot have a path from C 0 to C . So no vertex in C is reachable from y. Therefore, at time y:f , all vertices in C are still white. Therefore, for all w 2 C , w:f > y:f , which implies that f .C   > f .C 0 .   lemma   Corollary Let C and C 0 be distinct SCC‚Äôs in G D .V; E . Suppose there is an edge .u;   2 ET, where u 2 C and  2 C 0. Then f .C   < f .C 0 . Proof .u;   2 ET   .; u  2 E. Since SCC‚Äôs of G and GT are the same, f .C 0  > f .C  .  corollary   Corollary Let C and C 0 be distinct SCC‚Äôs in G D .V; E , and suppose that f .C   > f .C 0 . Then there cannot be an edge from C to C 0 in GT.  Proof It‚Äôs the contrapositive of the previous corollary.  Now we have the intuition to understand why the SCC procedure works. When we do the second DFS, on GT, start with SCC C such that f .C   is max- imum. The second DFS starts from some x 2 C , and it visits all vertices in C .   22-12  Lecture Notes for Chapter 22: Elementary Graph Algorithms  Corollary says that since f .C   > f .C 0  for all C 0 ¬§ C , there are no edges from C to C 0 in GT. Therefore, DFS will visit only vertices in C . Which means that the depth-Ô¨Årst tree rooted at x contains exactly the vertices of C . The next root chosen in the second DFS is in SCC C 0 such that f .C 0  is maximum over all SCC‚Äôs other than C . DFS visits all vertices in C 0, but the only edges out of C 0 go to C , which we‚Äôve already visited. Therefore, the only tree edges will be to vertices in C 0. We can continue the process. Each time we choose a root for the second DFS, it can reach only      vertices in its SCC‚Äîget tree edges to these, vertices in SCC‚Äôs already visited in second DFS‚Äîget no tree edges to these.  We are visiting vertices of .GT SCC in reverse of topologically sorted order. [Thebookhasaformalproof.]   Solutions for Chapter 22: Elementary Graph Algorithms  Solution to Exercise 22.1-6  We start by observing that if aij D 1, so that .i; j   2 E, then vertex i cannot be a universal sink, for it has an outgoing edge. Thus, if row i contains a 1, then vertex i cannot be a universal sink. This observation also means that if there is a self-loop .i; i  , then vertex i is not a universal sink. Now suppose that aij D 0, so that .i; j   62 E, and also that i ¬§ j . Then vertex j cannot be a universal sink, for either its in-degree must be strictly less than jV j  cid:0  1 or it has a self-loop. Thus if column j contains a 0 in any position other than the diagonal entry .j; j  , then vertex j cannot be a universal sink. Using the above observations, the following procedure returns TRUE if vertex k is a universal sink, and FALSE otherwise. It takes as input a jV j  jV j adjacency matrix A D .aij  . IS-SINK.A; k  let A be jV j  jV j for j D 1 to jV j if akj == 1     check for a 1 in row k     check for an off-diagonal 0 in column k  return FALSE  for i D 1 to jV j  if ai k == 0 and i ¬§ k  return FALSE  return TRUE  Because this procedure runs in O.V   time, we may call it only O.1  times in order to achieve our O.V  -time bound for determining whether directed graph G contains a universal sink. Observe also that a directed graph can have at most one universal sink. This prop- erty holds because if vertex j is a universal sink, then we would have .i; j   2 E for all i ¬§ j and so no other vertex i could be a universal sink. The following procedure takes an adjacency matrix A as input and returns either a message that there is no universal sink or a message containing the identity of the universal sink. It works by eliminating all but one vertex as a potential universal sink and then checking the remaining candidate vertex by a single call to IS-SINK.   22-14  Solutions for Chapter 22: Elementary Graph Algorithms  UNIVERSAL-SINK.A  let A be jV j  jV j i D j D 1 while i  jV j and j  jV j  if aij == 1  i D i C 1 else j D j C 1 return ‚Äúthere is no universal sink‚Äù  if i > jV j elseif IS-SINK.A; i   == FALSE  return ‚Äúthere is no universal sink‚Äù  else return i ‚Äúis a universal sink‚Äù  UNIVERSAL-SINK walks through the adjacency matrix, starting at the upper left corner and always moving either right or down by one position, depending on whether the current entry aij it is examining is 0 or 1. It stops once either i or j exceeds jV j. To understand why UNIVERSAL-SINK works, we need to show that after the while loop terminates, the only vertex that might be a universal sink is vertex i. The call to IS-SINK then determines whether vertex i is indeed a universal sink. Let us Ô¨Åx i and j to be values of these variables at the termination of the while loop. We claim that every vertex k such that 1  k < i cannot be a universal sink. That is because the way that i achieved its Ô¨Ånal value at loop termination was by Ô¨Ånding a 1 in each row k for which 1  k < i. As we observed above, any vertex k whose row contains a 1 cannot be a universal sink. If i > jV j at loop termination, then we have eliminated all vertices from consid- eration, and so there is no universal sink. If, on the other hand, i  jV j at loop termination, we need to show that every vertex k such that i < k  jV j cannot be a universal sink. If i  jV j at loop termination, then the while loop terminated because j > jV j. That means that we found a 0 in every column. Recall our earlier observation that if column k contains a 0 in an off-diagonal position, then vertex k cannot be a universal sink. Since we found a 0 in every column, we found a 0 in every column k such that i < k  jV j. Moreover, we never examined any matrix entries in rows greater than i, and so we never examined the diagonal entry in any column k such that i < k  jV j. Therefore, all the 0s that we found in columns k such that i < k  jV j were off-diagonal. We conclude that every vertex k such that i < k  jV j cannot be a universal sink. Thus, we have shown that every vertex less than i and every vertex greater than i cannot be a universal sink. The only remaining possibility is that vertex i might be a universal sink, and the call to IS-SINK checks whether it is. To see that UNIVERSAL-SINK runs in O.V   time, observe that either i or j is incremented in each iteration of the while loop. Thus, the while loop makes at most 2jV j  cid:0  1 iterations. Each iteration takes O.1  time, for a total while loop time of O.V   and, combined with the O.V  -time call to IS-SINK, we get a total running time of O.V  .   Solutions for Chapter 22: Elementary Graph Algorithms  22-15  Solution to Exercise 22.1-7 This solution is also posted publicly      bi ebje  bi ebT  ej DXe2E  BB T .i; j   DXe2E If i D j , then bi ebje D 1  it is 1  1 or . cid:0 1   . cid:0 1   whenever e enters or leaves vertex i, and 0 otherwise. If i ¬§ j , then bi ebje D  cid:0 1 when e D .i; j   or e D .j; i  , and 0 otherwise. BB T .i; j   D  degree of i D in-degree C out-degree   cid:0 . of edges connecting i and j    if i D j ; if i ¬§ j :  Thus,  Solution to Exercise 22.2-3  Note: This exercise changed in the third printing. This solution reÔ¨Çects the change. The BFS procedure cares only whether a vertex is white or not. A vertex  must become non-white at the same time that :d is assigned a Ô¨Ånite value so that we do not attempt to assign to :d again, and so we need to change vertex colors in lines 5 and 14. Once we have changed a vertex‚Äôs color to non-white, we do not need to change it again.  Solution to Exercise 22.2-5 This solution is also posted publicly  The correctness proof for the BFS algorithm shows that u:d D ƒ±.s; u , and the algorithm doesn‚Äôt assume that the adjacency lists are in any particular order. In Figure 22.3, if t precedes x in Adj≈íw¬ç, we can get the breadth-Ô¨Årst tree shown in the Ô¨Ågure. But if x precedes t in Adj≈íw¬ç and u precedes y in Adj≈íx¬ç, we can get edge .x; u  in the breadth-Ô¨Årst tree.  Solution to Exercise 22.2-6  The edges in E are shaded in the following graph:  s  u  v  w  x   22-16  Solutions for Chapter 22: Elementary Graph Algorithms  To see that E cannot be a breadth-Ô¨Årst tree, let‚Äôs suppose that Adj≈ís¬ç contains u before . BFS adds edges .s; u  and .s;   to the breadth-Ô¨Årst tree. Since u is enqueued before , BFS then adds edges .u; w  and .u; x .  The order of w and x in Adj≈íu¬ç doesn‚Äôt matter.  Symmetrically, if Adj≈ís¬ç contains  before u, then BFS adds edges .s;   and .s; u  to the breadth-Ô¨Årst tree,  is enqueued before u, and BFS adds edges .; w  and .; x .  Again, the order of w and x in Adj≈í¬ç doesn‚Äôt matter.  BFS will never put both edges .u; w  and .; x  into the breadth-Ô¨Årst tree. In fact, it will also never put both edges .u; x  and .; w  into the breadth-Ô¨Årst tree.  Solution to Exercise 22.2-7  Create a graph G where each vertex represents a wrestler and each edge represents a rivalry. The graph will contain n vertices and r edges. Perform as many BFS‚Äôs as needed to visit all vertices. Assign all wrestlers whose distance is even to be babyfaces and all wrestlers whose distance is odd to be heels. Then check each edge to verify that it goes between a babyface and a heel. This solution would take O.n C r  time for the BFS, O.n  time to designate each wrestler as a babyface or heel, and O.r  time to check edges, which is O.n C r  time overall.  Solution to Exercise 22.3-4  Solution to Exercise 22.3-5  Note: This exercise changed in the third printing. This solution reÔ¨Çects the change. The DFS and DFS-VISIT procedures care only whether a vertex is white or not. By coloring vertex u gray when it is Ô¨Årst visited, in line 3 of DFS-VISIT, we ensure that u will not be visited again. Once we have changed a vertex‚Äôs color to non-white, we do not need to change it again.  a. Edge .u;   is a tree edge or forward edge if and only if  is a descendant of u in the depth-Ô¨Årst forest.  If .u;   is a back edge, then u is a descendant of , and if .u;   is a cross edge, then neither of u or  is a descendant of the other.  By Corollary 22.8, therefore, .u;   is a tree edge or forward edge if and only if u:d < :d < :f < u:f .  b. First, suppose that .u;   is a back edge. A self-loop is by deÔ¨Ånition a back edge. If .u;   is a self-loop, then clearly :d D u:d < u:f D :f . If .u;   is not a self-loop, then u is a descendant of  in the depth-Ô¨Årst forest, and by Corollary 22.8, :d < u:d < u:f < :f . Now, suppose that :d  u:d < u:f  :f . If u and  are the same vertex, then :d D u:d < u:f D :f , and .u;   is a self-loop and hence a back edge. If u   Solutions for Chapter 22: Elementary Graph Algorithms  22-17  and  are distinct, then :d < u:d < u:f < :f . By the parenthesis theorem, interval ≈íu:d; u:f ¬ç is contained entirely within the interval ≈í:d; :f ¬ç, and u is a descendant of  in a depth-Ô¨Årst tree. Thus, .u;   is a back edge.  c. First, suppose that .u;   is a cross edge. Since neither u nor  is an ancestor of the other, the parenthesis theorem says that the intervals ≈íu:d; u:f ¬ç and ≈í:d; :f ¬ç are entirely disjoint. Thus, we must have either u:d < u:f < :d < :f or :d < :f < u:d < u:f . We claim that we cannot have u:d < :d if .u;   is a cross edge. Why? If u:d < :d, then  is white at time u:d. By the white-path theorem,  is a descendant of u, which contradicts .u;   being a cross edge. Thus, we must have :d < :f < u:d < u:f . Now suppose that :d < :f < u:d < u:f . By the parenthesis theorem, neither u nor  is a descendant of the other, which means that .u;   must be a cross edge.  w  w  Solution to Exercise 22.3-8  Let us consider the example graph and depth-Ô¨Årst search below.  d w 1 2 u 4   f 6 3 5  u  v  Clearly, there is a path from u to  in G. The bold edges are in the depth-Ô¨Årst forest produced. We can see that u:d < :d in the depth-Ô¨Årst search but  is not a descendant of u in the forest.  Solution to Exercise 22.3-9  Let us consider the example graph and depth-Ô¨Årst search below.  d w 1 2 u 4   f 6 3 5  u  v  Clearly, there is a path from u to  in G. The bold edges of G are in the depth-Ô¨Årst forest produced by the search. However, :d > u:f and the conjecture is false.  Solution to Exercise 22.3-11  Let us consider the example graph and depth-Ô¨Årst search below.   22-18  Solutions for Chapter 22: Elementary Graph Algorithms  d w 1 3 u 5   f 2 4 6  w  u  v  Clearly u has both incoming and outgoing edges in G but a depth-Ô¨Årst search of G produced a depth-Ô¨Årst forest where u is in a tree by itself.  Solution to Exercise 22.3-12 This solution is also posted publicly  The following pseudocode modiÔ¨Åes the DFS and DFS-VISIT procedures to assign values to the cc attributes of vertices.  DFS.G  for each vertex u 2 G:V u:color D WHITE u: D NIL  time D 0 counter D 0 for each vertex u 2 G:V if u:color == WHITE  DFS-VISIT.G; u; counter  u:cc D counter time D time C 1 u:d D time u:color D GRAY for each  2 G:Adj≈íu¬ç  if :color == WHITE  counter D counter C 1 DFS-VISIT.G; u; counter      label the vertex  : D u DFS-VISIT.G; ; counter  u:color D BLACK time D time C 1 u:f D time This DFS increments a counter each time DFS-VISIT is called to grow a new tree in the DFS forest. Every vertex visited  and added to the tree  by DFS-VISIT is labeled with that same counter value. Thus u:cc D :cc if and only if u and  are visited in the same call to DFS-VISIT from DFS, and the Ô¨Ånal value of the counter is the number of calls that were made to DFS-VISIT by DFS. Also, since every vertex is visited eventually, every vertex is labeled. Thus all we need to show is that the vertices visited by each call to DFS-VISIT from DFS are exactly the vertices in one connected component of G.   Solutions for Chapter 22: Elementary Graph Algorithms  22-19   All vertices in a connected component are visited by one call to DFS-VISIT  from DFS: Let u be the Ô¨Årst vertex in component C visited by DFS-VISIT. Since a vertex becomes non-white only when it is visited, all vertices in C are white when DFS-VISIT is called for u. Thus, by the white-path theorem, all vertices in C become descendants of u in the forest, which means that all vertices in C are visited  by recursive calls to DFS-VISIT  before DFS-VISIT returns to DFS.  All vertices visited by one call to DFS-VISIT from DFS are in the same con-  nected component: If two vertices are visited in the same call to DFS-VISIT from DFS, they are in the same connected component, because vertices are visited only by following paths in G  by following edges found in adjacency lists, starting from some vertex .  Solution to Exercise 22.4-3 This solution is also posted publicly  An undirected graph is acyclic  i.e., a forest  if and only if a DFS yields no back edges.      If there‚Äôs a back edge, there‚Äôs a cycle. If there‚Äôs no back edge, then by Theorem 22.10, there are only tree edges. Hence, the graph is acyclic.  Thus, we can run DFS: if we Ô¨Ånd a back edge, there‚Äôs a cycle.  Time: O.V  .  Not O.V C E !   If we ever see jV j distinct edges, we must have seen a back edge because  by Theorem B.2 on p. 1174  in an acyclic  undirected  forest, jEj  jV j  cid:0  1.   22-20  Solutions for Chapter 22: Elementary Graph Algorithms  Solution to Exercise 22.4-5  TOPOLOGICAL-SORT.G      Initialize in-degree, ‚Äö.V   time. for each vertex u 2 G:V    Compute in-degree, ‚Äö.V C E  time. for each vertex u 2 G:V  u:in-degree D 0  for each  2 G:Adj≈íu¬ç  :in-degree D :in-degree C 1     Initialize Queue, ‚Äö.V   time. Q D ; for each vertex u 2 G:V if u:in-degree == 0 ENQUEUE.Q; u      while loop takes O.V C E  time. while Q ¬§ ; u D DEQUEUE.Q  output u    for loop executes O.E  times total. for each  2 G:Adj≈íu¬ç :in-degree D :in-degree  cid:0  1 if :in-degree == 0  ENQUEUE.Q;       Check for cycles, O.V   time. for each vertex u 2 G:V if u:in-degree ¬§ 0  report that there‚Äôs a cycle     Another way to check for cycles would be to count the vertices    that are output and report a cycle if that number is < jV j. To Ô¨Ånd and output vertices of in-degree 0, we Ô¨Årst compute all vertices‚Äô in-degrees by making a pass through all the edges  by scanning the adjacency lists of all the vertices  and incrementing the in-degree of each vertex an edge enters.  Computing all in-degrees takes ‚Äö.V C E  time  jV j adjacency lists accessed,  jEj edges total found in those lists, ‚Äö.1  work for each edge .  We keep the vertices with in-degree 0 in a FIFO queue, so that they can be en- queued and dequeued in O.1  time.  The order in which vertices in the queue are processed doesn‚Äôt matter, so any kind of FIFO queue works.     Initializing the queue takes one pass over the vertices doing ‚Äö.1  work, for total time ‚Äö.V  .  As we process each vertex from the queue, we effectively remove its outgoing edges from the graph by decrementing the in-degree of each vertex one of those edges enters, and we enqueue any vertex whose in-degree goes to 0. We do not need to actually remove the edges from the adjacency list, because that adjacency list   Solutions for Chapter 22: Elementary Graph Algorithms  22-21  will never be processed again by the algorithm: Each vertex is enqueued dequeued at most once because it is enqueued only if it starts out with in-degree 0 or if its in- degree becomes 0 after being decremented  and never incremented  some number of times.   The processing of a vertex from the queue happens O.V   times because no vertex can be enqueued more than once. The per-vertex work  dequeue and output  takes O.1  time, for a total of O.V   time.   Because the adjacency list of each vertex is scanned only when the vertex is dequeued, the adjacency list of each vertex is scanned at most once. Since the sum of the lengths of all the adjacency lists is ‚Äö.E , at most O.E  time is spent in total scanning adjacency lists. For each edge in an adjacency list, ‚Äö.1  work is done, for a total of O.E  time.  Thus the total time taken by the algorithm is O.V C E . The algorithm outputs vertices in the right order  u before  for every edge .u;    because  will not be output until its in-degree becomes 0, which happens only when every edge .u;   leading into  has been ‚Äúremoved‚Äù due to the processing  including output  of u. If there are no cycles, all vertices are output.   Proof: Assume that some vertex 0 is not output. Vertex 0 cannot start out with in-degree 0  or it would be output , so there are edges into 0. Since 0‚Äôs in-degree never becomes 0, at least one edge .1; 0  is never removed, which means that at least one other vertex 1 was not output. Similarly, 1 not output means that some vertex 2 such that .2; 1  2 E was not output, and so on. Since the number of vertices is Ô¨Ånite, this path   ! 2 ! 1 ! 0  is Ô¨Ånite, so we must have i D j for some i and j in this sequence, which means there is a cycle.  If there are cycles, not all vertices will be output, because some in-degrees never become 0.   Proof: Assume that a vertex in a cycle is output  its in-degree becomes 0 . Let  be the Ô¨Årst vertex in its cycle to be output, and let u be ‚Äôs predecessor in the cycle. In order for ‚Äôs in-degree to become 0, the edge .u;   must have been ‚Äúremoved,‚Äù which happens only when u is processed. But this cannot have happened, because  is the Ô¨Årst vertex in its cycle to be processed. Thus no vertices in cycles are output.  Solution to Exercise 22.5-5  We have at our disposal an O.V C E -time algorithm that computes strongly con- nected components. Let us assume that the output of this algorithm is a map- ping u:scc, giving the number of the strongly connected component containing vertex u, for each vertex u. Without loss of generality, assume that u:scc is an integer in the set f1; 2; : : : ;jV jg.   22-22  Solutions for Chapter 22: Elementary Graph Algorithms  Construct the multiset  a set that can contain the same object more than once  T D fu:scc W u 2 V g, and sort it by using counting sort. Since the values we are sorting are integers in the range 1 to jV j, the time to sort is O.V  . Go through the sorted multiset T and every time we Ô¨Ånd an element x that is distinct from the one before it, add x to V SCC.  Consider the Ô¨Årst element of the sorted set as ‚Äúdistinct from the one before it.‚Äù  It takes O.V   time to construct V SCC. Construct the set of ordered pairs S D f.x; y  W there is an edge .u;   2 E; x D u:scc; and y D :sccg : We can easily construct this set in ‚Äö.E  time by going through all edges in E and looking up u:scc and :scc for each edge .u;   2 E. Having constructed S, remove all elements of the form .x; x . Alternatively, when we construct S, do not put an element in S when we Ô¨Ånd an edge .u;   for which u:scc D :scc. S now has at most jEj elements. Now sort the elements of S using radix sort. Sort on one component at a time. The order does not matter. In other words, we are performing two passes of counting sort. The time to do so is O.V C E , since the values we are sorting on are integers in the range 1 to jV j. Finally, go through the sorted set S, and every time we Ô¨Ånd an element .x; y  that is distinct from the element before it  again considering the Ô¨Årst element of the sorted set as distinct from the one before it , add .x; y  to ESCC. Sorting and then adding .x; y  only if it is distinct from the element before it ensures that we add .x; y  at most once. It takes O.E  time to go through S in this way, once S has been sorted. The total time is O.V C E .  The basic idea is to replace the edges within each SCC by one simple, directed cycle and then remove redundant edges between SCC‚Äôs. Since there must be at least k edges within an SCC that has k vertices, a single directed cycle of k edges gives the k-vertex SCC with the fewest possible edges. The algorithm works as follows: 1. Identify all SCC‚Äôs of G. Time: ‚Äö.V C E , using the SCC algorithm in Sec- 2. Form the component graph GSCC. Time: O.V C E , by Exercise 22.5-5. 3. Start with E0 D ;. Time: O.1 . 4. For each SCC of G, let the vertices in the SCC be 1; 2; : : : ; k, and add to E0 the directed edges .1; 2 ; .2; 3 ; : : : ; .k cid:0 1; k ; .k; 1 . These edges form a simple, directed cycle that includes all vertices of the SCC. Time for all SCC‚Äôs: O.V  .  tion 22.5.  5. For each edge .u;   in the component graph GSCC, select any vertex x in u‚Äôs SCC and any vertex y in ‚Äôs SCC, and add the directed edge .x; y  to E0. Time: O.E .  Thus, the total time is ‚Äö.V C E .  Solution to Exercise 22.5-6   Solutions for Chapter 22: Elementary Graph Algorithms  22-23  Solution to Exercise 22.5-7  To determine whether G D .V; E  is semiconnected, do the following: 1. Call STRONGLY-CONNECTED-COMPONENTS. 2. Form the component graph.  By Exercise 22.5-5, you may assume that this  takes O.V C E  time.  3. Topologically sort the component graph.  Recall that it‚Äôs a dag.  Assuming that G contains k SCC‚Äôs, the topological sort gives a linear ordering h1; 2; : : : ; ki of the vertices. 4. Verify that the sequence of vertices h1; 2; : : : ; ki given by topological sort forms a linear chain in the component graph. That is, verify that the edges .1; 2 ; .2; 3 ; : : : ; .k cid:0 1; k  exist in the component graph. If the vertices form a linear chain, then the original graph is semiconnected; otherwise it is not.  Because we know that all vertices in each SCC are mutually reachable from each other, it sufÔ¨Åces to show that the component graph is semiconnected if and only if it contains a linear chain. We must also show that if there‚Äôs a linear chain in the component graph, it‚Äôs the one returned by topological sort. We‚Äôll Ô¨Årst show that if there‚Äôs a linear chain in the component graph, then it‚Äôs the one returned by topological sort. In fact, this is trivial. A topological sort has to respect every edge in the graph. So if there‚Äôs a linear chain, a topological sort must give us the vertices in order. Now we‚Äôll show that the component graph is semiconnected if and only if it con- tains a linear chain. First, suppose that the component graph contains a linear chain. Then for every pair of vertices u;  in the component graph, there is a path between them. If u precedes  in the linear chain, then there‚Äôs a path u ; . Otherwise,  precedes u, and there‚Äôs a path  ; u. Conversely, suppose that the component graph does not contain a linear chain. Then in the list returned by topological sort, there are two consecutive vertices i and iC1, but the edge .i ; iC1  is not in the component graph. Any edges out of i are to vertices j , where j > i C 1, and so there is no path from i to iC1 in the component graph. And since iC1 follows i in the topological sort, there cannot be any paths at all from iC1 to i. Thus, the component graph is not semiconnected. Running time of each step: 1. ‚Äö.V C E . 2. O.V C E . 3. Since the component graph has at most jV j vertices and at most jEj edges, O.V C E . 4. Also O.V C E . We just check the adjacency list of each vertex i in the component graph to verify that there‚Äôs an edge .i ; iC1 . We‚Äôll go through each adjacency list once.  Thus, the total running time is ‚Äö.V C E .   22-24  Solutions for Chapter 22: Elementary Graph Algorithms  Solution to Problem 22-1 This solution is also posted publicly  a. 1. Suppose .u;   is a back edge or a forward edge in a BFS of an undirected graph. Then one of u and , say u, is a proper ancestor of the other    in the breadth-Ô¨Årst tree. Since we explore all edges of u before exploring any edges of any of u‚Äôs descendants, we must explore the edge .u;   at the time we explore u. But then .u;   must be a tree edge.  2. In BFS, an edge .u;   is a tree edge when we set : D u. But we only do so when we set :d D u:d C 1. Since neither u:d nor :d ever changes thereafter, we have :d D u:d C 1 when BFS completes. 3. Consider a cross edge .u;   where, without loss of generality, u is visited before . At the time we visit u, vertex  must already be on the queue, for otherwise .u;   would be a tree edge. Because  is on the queue, we have :d  u:d C 1 by Lemma 22.3. By Corollary 22.4, we have :d  u:d. Thus, either :d D u:d or :d D u:d C 1.  b. 1. Suppose .u;   is a forward edge. Then we would have explored it while  visiting u, and it would have been a tree edge.  2. Same as for undirected graphs. 3. For any edge .u;  , whether or not it‚Äôs a cross edge, we cannot have :d > u:d C 1, since we visit  at the latest when we explore edge .u;  . Thus, :d  u:d C 1. 4. Clearly, :d  0 for all vertices . For a back edge .u;  ,  is an ancestor of u in the breadth-Ô¨Årst tree, which means that :d  u:d.  Note that since self-loops are considered to be back edges, we could have u D .   a. An Euler tour is a single cycle that traverses each edge of G exactly once, but it might not be a simple cycle. An Euler tour can be decomposed into a set of edge-disjoint simple cycles, however. If G has an Euler tour, therefore, we can look at the simple cycles that, together, form the tour. In each simple cycle, each vertex in the cycle has one entering edge and one leaving edge. In each simple cycle, therefore, each vertex  has in-degree.  D out-degree. , where the degrees are either 1  if  is on the simple cycle  or 0  if  is not on the simple cycle . Adding the in- and out- degrees over all edges proves that if G has an Euler tour, then in-degree.  D out-degree.  for all vertices . We prove the converse‚Äîthat if in-degree.  D out-degree.  for all vertices , then G has an Euler tour‚Äîin two different ways. One proof is nonconstructive, and the other proof will help us design the algorithm for part  b . First, we claim that if in-degree.  D out-degree.  for all vertices , then we can pick any vertex u for which in-degree.u  D out-degree.u   1 and create  Solution to Problem 22-3   Solutions for Chapter 22: Elementary Graph Algorithms  22-25  a cycle  not necessarily simple  that contains u. To prove this claim, let us start by placing vertex u on the cycle, and choose any leaving edge of u, say .u;  . Now we put  on the cycle. Since in-degree.  D out-degree.   1, we can pick some leaving edge of  and continue visiting edges and vertices. Each time we pick an edge, we can remove it from further consideration. At each vertex other than u, at the time we visit an entering edge, there must be an unvisited leaving edge, since in-degree.  D out-degree.  for all vertices . The only vertex for which there might not be an unvisited leaving edge is u, since we started the cycle by visiting one of u‚Äôs leaving edges. Since there‚Äôs always a leaving edge we can visit from all vertices other than u, eventually the cycle must return to u, thus proving the claim. The nonconstructive proof proves the contrapositive‚Äîthat if G does not have an Euler tour, then in-degree.  ¬§ out-degree.  for some vertex ‚Äîby con- tradiction. Choose a graph G D .V; E  that does not have an Euler tour but has at least one edge and for which in-degree.  D out-degree.  for all ver- tices , and let G have the fewest edges of any such graph. By the above claim, G contains a cycle. Let C be a cycle of G with the greatest number of edges, and let VC be the set of vertices visited by cycle C . By our assumption, C is not an Euler tour, and so the set of edges E0 D E  cid:0  C is nonempty. If we use the set V of vertices and the set E0 of edges, we get the graph G0 D .V; E0 ; this graph has in-degree.  D out-degree.  for all vertices , since we have removed one entering edge and one leaving edge for each vertex on cycle C . Consider any component G00 D .V 00; E00  of G0, and observe that G00 also has in-degree.  D out-degree.  for all vertices . Since E00  E0 ¬® E, it fol- lows from how we chose G that G00 must have an Euler tour, say C 0. Because the original graph G is connected, there must be some vertex x 2 V 00 [ VC and, without loss of generality, consider x to be the Ô¨Årst and last vertex on both C and C 0. But then the cycle C 00 formed by Ô¨Årst traversing C and then travers- ing C 0 is a cycle of G with more edges than C , contradicting our choice of C . We conclude that C must have been an Euler tour. The constructive proof uses the same ideas. Let us start at a vertex u and, via random traversal of edges, create a cycle. We know that once we take any edge entering a vertex  ¬§ u, we can Ô¨Ånd an edge leaving  that we have not yet taken. Eventually, we get back to vertex u, and if there are still edges leaving u that we have not taken, we can continue the cycle. Eventually, we get back to vertex u and there are no untaken edges leaving u. If we have visited every edge in the graph G, we are done. Otherwise, since G is connected, there must be some unvisited edge leaving a vertex, say , on the cycle. We can traverse a new cycle starting at , visiting only previously unvisited edges, and we can splice this cycle into the cycle we already know. That is, if the original cycle is hu; : : : ; ; w; : : : ; ui, and the new cycle is h; x; : : : ; i, then we can create the cycle hu; : : : ; ; x; : : : ; ; w; : : : ; ui. We continue this process of Ô¨Ånding a vertex with an unvisited leaving edge on a visited cycle, visiting a cycle starting and ending at this vertex, and splicing in the newly visited cycle, until we have visited every edge.  b. The algorithm is based on the idea in the constructive proof above.   22-26  Solutions for Chapter 22: Elementary Graph Algorithms  We assume that G is represented by adjacency lists, and we work with a copy of the adjacency lists, so that as we visit each edge, we can remove it from its adjacency list. The singly linked form of adjacency list will sufÔ¨Åce. The output of this algorithm is a doubly linked list T of vertices which, read in list order, will give an Euler tour. The algorithm constructs T by Ô¨Ånding cycles  also represented by doubly linked lists  and splicing them into T . By using doubly linked lists for cycles and the Euler tour, splicing a cycle into the Euler tour takes constant time. We also maintain a singly linked list L, in which each list element consists of two parts:  1. a vertex , and 2. a pointer to some appearance of  in T .  Initially, L contains one vertex, which may be any vertex of G. Here is the algorithm:  EULER-TOUR.G  T D empty list L D .any vertex  2 G:V; NIL  while L is not empty  remove .; location-in-T   from L C D VISIT.G; L;   if location-in-T == NIL  T D C  return T  else splice C into T just before location-in-T  VISIT.G; L;   C D empty sequence of vertices u D  while out-degree.u  > 0  let w be the Ô¨Årst vertex in G:Adj≈íu¬ç remove w from G:Adj≈íu¬ç, decrementing out-degree.u  add u onto the end of C if out-degree.u  > 0  add .u; u‚Äôs location in C   to L  u D w  return C  The use of NIL in the initial assignment to L ensures that the Ô¨Årst cycle C returned by VISIT becomes the current version of the Euler tour T . All cycles returned by VISIT thereafter are spliced into T . We assume that whenever an empty cycle is returned by VISIT, splicing it into T leaves T unchanged. Each time that EULER-TOUR removes a vertex  from the list L, it calls VISIT.G; L;   to Ô¨Ånd a cycle C , possibly empty and possibly not simple, that starts and ends at ; the cycle C is represented by a list that starts with  and ends with the last vertex on the cycle before the cycle ends at . EULER-TOUR   Solutions for Chapter 22: Elementary Graph Algorithms  22-27  then splices this cycle C into the Euler tour T just before some appearance of  in T . When VISIT is at a vertex u, it looks for some vertex w such that the edge .u; w  has not yet been visited. Removing w from Adj≈íu¬ç ensures that we will never visit .u; w  again. VISIT adds u onto the cycle C that it constructs. If, after removing edge .u; w , vertex u still has any leaving edges, then u, along with its location in C , is added to L. The cycle construction continues from w, and it ceases once a vertex with no unvisited leaving edges is found. Using the argument from part  a , at that point, this vertex must close up a cycle. At that point, therefore, the cycle C is returned. It is possible that a vertex u has unvisited leaving edges at the time it is added to list L in VISIT, but that by the time that u is removed from L in EULER-TOUR, all of its leaving edges have been visited. In this case, the while loop of VISIT executes 0 iterations, and VISIT returns an empty cycle. Once the list L is empty, every edge has been visited. The resulting cycle T is then an Euler tour. To see that EULER-TOUR takes O.E  time, observe that because we remove each edge from its adjacency list as it is visited, no edge is visited more than once. Since each edge is visited at some time, the number of times that a vertex is added to L, and thus removed from L, is at most jEj. Thus, the while loop in EULER-TOUR executes at most E iterations. The while loop in VISIT executes one iteration per edge in the graph, and so it executes at most E iterations as well. Since adding vertex u to the doubly linked list C takes constant time and splicing C into T takes constant time, the entire algorithm takes O.E  time.  Compute GT in the usual way, so that GT is G with its edges reversed. Then do a depth-Ô¨Årst search on GT, but in the main loop of DFS, consider the vertices in order of increasing values of L. . If vertex u is in the depth-Ô¨Årst tree with root , then min.u  D . Clearly, this algorithm takes O.V C E  time. To show correctness, Ô¨Årst note that if u is in the depth-Ô¨Årst tree rooted at  in GT, then there is a path  ; u in GT, and so there is a path u ;  in G. Thus, the minimum vertex label of all vertices reachable from u is at most L. , or in other words, L.   min fL.w  W w 2 R.u g. Now suppose that L.  > min fL.w  W w 2 R.u g, so that there is a vertex w 2 R.u  such that L.w  < L. . At the time :d that we started the depth- Ô¨Årst search from , we would have already discovered w, so that w:d < :d. By the parenthesis theorem, either the intervals ≈í:d; :f ¬ç, and ≈íw:d; w:f ¬ç are dis- joint and neither  nor w is a descendant of the other, or we have the ordering w:d < :d < :f < w:f and  is a descendant of w. The latter case cannot occur, since  is a root in the depth-Ô¨Årst forest  which means that  cannot be a de- scendant of any other vertex . In the former case, since w:d < :d, we must have w:d < w:f < :d < :f . In this case, since u is reachable from w in GT, we would  Solution to Problem 22-4   22-28  Solutions for Chapter 22: Elementary Graph Algorithms  have discovered u by the time w:f , so that u:d < w:f . Since we discovered u dur- ing a search that started at , we have :d  u:d. Thus, :d  u:d < w:f < :d, which is a contradiction. We conclude that no such vertex w can exist.   Lecture Notes for Chapter 23: Minimum Spanning Trees  Chapter 23 overview  Problem   A town has a set of houses and a set of roads.  A road connects 2 and only 2 houses.  A road connecting houses u and  has a repair cost w.u;  .  Goal: Repair enough  and no more  roads such that  1. everyone stays connected: can reach every house from all other houses, and 2. total repair cost is minimum.  Model as a graph:  Undirected graph G D .V; E .  Weight w.u;   on each edge .u;   2 E.  Find T  E such that 2. w.T   D X.u; 2T  w.u;   is minimized.  1. T connects all vertices  T is a spanning tree , and  A spanning tree whose weight is minimum over all spanning trees is called a min- imum spanning tree, or MST. Example of such a graph [edgesinMSTareshaded] :  a  10  12  b  9  c  3  8  e  1  7  3  d  f  8  5  6  g  h  9  2  i  11  In this example, there is more than one MST. Replace edge .e; f   in the MST by .c; e . Get a different spanning tree with the same weight.   23-2  Lecture Notes for Chapter 23: Minimum Spanning Trees  Growing a minimum spanning tree  Some properties of an MST:        It has jV j  cid:0  1 edges. It has no cycles. It might not be unique.  Building up the solution   We will build a set A of edges.    Initially, A has no edges.   As we add edges to A, maintain a loop invariant:  Loop invariant: A is a subset of some MST.   Add only edges that maintain the invariant. If A is a subset of some MST, an edge .u;   is safe for A if and only if A [ f.u;  g is also a subset of some MST. So we will add only safe edges.  Generic MST algorithm  GENERIC-MST.G; w  A D ; while A is not a spanning tree  Ô¨Ånd an edge .u;   that is safe for A A D A [ f.u;  g  return A  Use the loop invariant to show that this generic algorithm works.  Initialization: The empty set trivially satisÔ¨Åes the loop invariant. Maintenance: Since we add only safe edges, A remains a subset of some MST. Termination: All edges added to A are in an MST, so when we stop, A is a span-  ning tree that is also an MST.  Finding a safe edge  How do we Ô¨Ånd safe edges? Let‚Äôs look at the example. Edge .c; f   has the lowest weight of any edge in the graph. Is it safe for A D ;? Intuitively: Let S  V be any set of vertices that includes c but not f  so that f is in V  cid:0  S . In any MST, there has to be one edge  at least  that connects S with V  cid:0  S. Why not choose the edge with minimum weight?  Which would be .c; f   in this case.  Some deÔ¨Ånitions: Let S  V and A  E.   Lecture Notes for Chapter 23: Minimum Spanning Trees  23-3  in V  cid:0  S.   A cut .S; V  cid:0  S   is a partition of vertices into disjoint sets V and S  cid:0  V .  Edge .u;   2 E crosses cut .S; V  cid:0  S   if one endpoint is in S and the other is  A cut respects A if and only if no edge in A crosses the cut.  An edge is a light edge crossing a cut if and only if its weight is minimum over all edges crossing the cut. For a given cut, there can be > 1 light edge crossing it.  Theorem Let A be a subset of some MST, .S; V  cid:0  S   be a cut that respects A, and .u;   be a light edge crossing .S; V  cid:0  S  . Then .u;   is safe for A. Proof Let T be an MST that includes A. If T contains .u;  , done. So now assume that T does not contain .u;  . We‚Äôll construct a different MST T 0 that includes A [ f.u;  g. Recall: a tree has unique path between each pair of vertices. Since T is an MST, it contains a unique path p between u and . Path p must cross the cut .S; V  cid:0  S   at least once. Let .x; y  be an edge of p that crosses the cut. From how we chose .u;  , must have w.u;    w.x; y .  u  v  S  x  y  V‚ÄìS  [Exceptforthedashededge .u;  ,alledgesshownarein T . A issomesubsetof theedgesof T ,but A cannotcontainanyedgesthatcrossthecut .S; V  cid:0  S  ,since thiscutrespects A. Shadededgesarethepath p.] Since the cut respects A, edge .x; y  is not in A. To form T 0 from T :   Remove .x; y . Breaks T into two components.  Add .u;  . Reconnects.   23-4  Lecture Notes for Chapter 23: Minimum Spanning Trees  So T 0 D T  cid:0  f.x; y g [ f.u;  g. T 0 is a spanning tree. w.T 0  D w.T    cid:0  w.x; y  C w.u;     w.T   ;  since w.u;    w.x; y . Since T 0 is a spanning tree, w.T 0   w.T  , and T is an MST, then T 0 must be an MST. Need to show that .u;   is safe for A:  A  T and .x; y  62 A   A  T 0.  A [ f.u;  g  T 0.  Since T 0 is an MST, .u;   is safe for A.   theorem   So, in GENERIC-MST:  single vertex.  tree.   A is a forest containing connected components. Initially, each component is a   Any safe edge merges two of these components into one. Each component is a   Since an MST has exactly jV j  cid:0  1 edges, the for loop iterates jV j  cid:0  1 times. Equivalently, after adding jV j cid:0 1 safe edges, we‚Äôre down to just one component.  Corollary If C D .VC ; EC   is a connected component in the forest GA D .V; A  and .u;   is a light edge connecting C to some other component in GA  i.e., .u;   is a light edge crossing the cut .VC ; V  cid:0  VC   , then .u;   is safe for A. Proof Set S D VC in the theorem. This idea naturally leads to the algorithm known as Kruskal‚Äôs algorithm to solve the minimum-spanning-tree problem.   corollary   Kruskal‚Äôs algorithm  G D .V; E  is a connected, undirected, weighted graph. w W E ! R.  Starts with each vertex being its own component.  Repeatedly merges two components into one by choosing the light edge that  connects them  i.e., the light edge crossing the cut between them .   Scans the set of edges in monotonically increasing order by weight.  Uses a disjoint-set data structure to determine whether an edge connects ver-  tices in different components.   Lecture Notes for Chapter 23: Minimum Spanning Trees  23-5  KRUSKAL.G; w  A D ; for each vertex  2 G:V sort the edges of G:E into nondecreasing order by weight w for each .u;   taken from the sorted list  MAKE-SET.   if FIND-SET.u  ¬§ FIND-SET.   A D A [ f.u;  g UNION.u;    return A  W W W W W W W W W W W W  safe safe safe reject safe safe reject safe safe reject reject safe  Run through the above example to see how Kruskal‚Äôs algorithm works on it: .c; f   .g; i   .e; f   .c; e  .d; h  .f; h  .e; d   .b; d   .d; g  .b; c  .g; h  .a; b  At this point, we have only one component, so all other edges will be rejected. [We could add a test to the main loop of KRUSKAL to stop once jV j  cid:0  1 edges have beenaddedto A.] Get the shaded edges shown in the Ô¨Ågure. Suppose we had examined .c; e  before .e; f  . Then would have found .c; e  safe and would have rejected .e; f  .  Analysis  O.1   Initialize A: First for loop: Sort E: Second for loop: O.E  FIND-SETs and UNIONs  jV j MAKE-SETs O.E lg E    Assuming the implementation of disjoint-set data structure, already seen in  Chapter 21, that uses union by rank and path compression: O..V C E  Àõ.V    C O.E lg E  :   Since G is connected, jEj  jV j  cid:0  1   O.E Àõ.V    C O.E lg E .  Àõ.jV j  D O.lg V   D O.lg E .  Therefore, total time is O.E lg E .    jEj  jV j2   lg jEj D O.2 lg V   D O.lg V  .   23-6  Lecture Notes for Chapter 23: Minimum Spanning Trees   Therefore, O.E lg V   time.  If edges are already sorted, O.E Àõ.V   , which is  almost linear.   Prim‚Äôs algorithm   Builds one tree, so A is always a tree.  Starts from an arbitrary ‚Äúroot‚Äù r.  At each step, Ô¨Ånd a light edge crossing cut .VA; V  cid:0  VA , where VA D vertices  that A is incident on. Add this edge to A.  VA  light edge  [Edgesof A areshaded.]  How to Ô¨Ånd the light edge quickly? Use a priority queue Q:  Each object is a vertex in V  cid:0  VA.  Key of  is minimum weight of any edge .u;  , where u 2 VA.  Then the vertex returned by EXTRACT-MIN is  such that there exists u 2 VA  Key of  is 1 if  is not adjacent to any vertices in VA. The edges of A will form a rooted tree with root r:  and .u;   is light edge crossing .VA; V  cid:0  VA .    r is given as an input to the algorithm, but it can be any vertex.  : D NIL if  D r or  has no parent.   Each vertex knows its parent in the tree by the attribute : D parent of .  As algorithm progresses, A D f.; :  W  2 V  cid:0  frg  cid:0  Qg.  At termination, VA D V   Q D ;, so MST is A D f.; :  W  2 V  cid:0  frgg. [ThepseudocodethatfollowsdiffersfromthebookinthatitexplicitlycallsINSERT and DECREASE-KEY tooperateon Q.]   Lecture Notes for Chapter 23: Minimum Spanning Trees  23-7  PRIM.G; w; r  Q D ; for each u 2 G:V u:key D 1 u: D NIL INSERT.Q; u   DECREASE-KEY .Q; r; 0  while Q ¬§ ;  u D EXTRACT-MIN.Q  for each  2 G:Adj≈íu¬ç     r:key D 0  if  2 Q and w.u;   < :key  : D u DECREASE-KEY .Q; ; w.u;     Do example from previous graph. [Letastudentpicktheroot.]  Depends on how the priority queue is implemented:   Suppose Q is a binary heap.  Initialize Q and Ô¨Årst for loop: O.V lg V   Decrease key of r: while loop:  Analysis  Total:  O.lg V   jV j EXTRACT-MIN calls   O.V lg V    jEj DECREASE-KEY calls   O.E lg V   O.E lg V     Suppose we could do DECREASE-KEY in O.1  amortized time.  Then  jEj DECREASE-KEY calls take O.E  time altogether   total time becomes O.V lg V C E . In fact, there is a way to do DECREASE-KEY in O.1  amortized time: Fi- bonacci heaps, in Chapter 19.   Solutions for Chapter 23: Minimum Spanning Trees  Solution to Exercise 23.1-1 This solution is also posted publicly  Theorem 23.1 shows this. Let A be the empty set and S be any set containing u but not .  Solution to Exercise 23.1-4 This solution is also posted publicly  Solution to Exercise 23.1-6 This solution is also posted publicly  A triangle whose edge weights are all equal is a graph in which every edge is a light edge crossing some cut. But the triangle is cyclic, so it is not a minimum spanning tree.  Suppose that for every cut of G, there is a unique light edge crossing the cut. Let us consider two distinct minimum spanning trees, T and T 0, of G. Because T and T 0 are distinct, T contains some edge .u;   that is not in T 0. If we remove .u;   from T , then T becomes disconnected, resulting in a cut .S; V  cid:0  S  . The edge .u;   is a light edge crossing the cut .S; V  cid:0  S    by Exercise 23.1-3  and, by our assumption, it‚Äôs the only light edge crossing this cut. Because .u;   is the only light edge crossing .S; V  cid:0  S   and .u;   is not in T 0, each edge in T 0 that crosses .S; V  cid:0  S   must have weight strictly greater than w.u;  . As in the proof of Theorem 23.1, we can identify the unique edge .x; y  in T 0 that crosses .S; V  cid:0  S   and lies on the cycle that results if we add .u;   to T 0. By our assumption, we know that w.u;   < w.x; y . Then, we can then remove .x; y  from T 0 and replace it by .u;  , giving a spanning tree with weight strictly less than w.T 0 . Thus, T 0 was not a minimum spanning tree, contradicting the assumption that the graph had two unique minimum spanning trees.   Solutions for Chapter 23: Minimum Spanning Trees  23-9  Here‚Äôs a counterexample for the converse:  x  1  1  y  z  Here, the graph is its own minimum spanning tree, and so the minimum spanning tree is unique. Consider the cut .fxg ;fy; ¬¥g . Both of the edges .x; y  and .x; ¬¥  are light edges crossing the cut, and they are both light edges.  Solution to Exercise 23.1-10  Let w.T   DP.x;y 2T w.x; y . We have w0.T   D w.T    cid:0  k. Consider any other spanning tree T 0, so that w.T    w.T 0 . If .x; y  62 T 0, then w0.T 0  D w.T 0   w.T   > w0.T  . If .x; y  2 T 0, then w0.T 0  D w.T 0   cid:0  k  w.T    cid:0  k D w0.T  . Either way, w0.T    w0.T 0 , and so T is a minimum spanning tree for weight function w0.  Solution to Exercise 23.2-4  We know that Kruskal‚Äôs algorithm takes O.V   time for initialization, O.E lg E  time to sort the edges, and O.E Àõ.V    time for the disjoint-set operations, for a total running time of O.V C E lg E C E Àõ.V    D O.E lg E . If we knew that all of the edge weights in the graph were integers in the range from 1 to jV j, then we could sort the edges in O.V C E  time using counting sort. Since the graph is connected, V D O.E , and so the sorting time is reduced to O.E . This would yield a total running time of O.V C E C E Àõ.V    D O.E Àõ.V   , again since V D O.E , and since E D O.E Àõ.V   . The time to process the edges, not the time to sort them, is now the dominant term. Knowledge about the weights won‚Äôt help speed up any other part of the algorithm, since nothing besides the sort uses the weight values. If the edge weights were integers in the range from 1 to W for some constant W , then we could again use counting sort to sort the edges more quickly. This time, sorting would take O.E C W   D O.E  time, since W is a constant. As in the Ô¨Årst part, we get a total running time of O.E Àõ.V   .   23-10  Solutions for Chapter 23: Minimum Spanning Trees  Solution to Exercise 23.2-5  The time taken by Prim‚Äôs algorithm is determined by the speed of the queue oper- ations. With the queue implemented as a Fibonacci heap, it takes O.E C V lg V   time. Since the keys in the priority queue are edge weights, it might be possible to im- plement the queue even more efÔ¨Åciently when there are restrictions on the possible edge weights. We can improve the running time of Prim‚Äôs algorithm if W is a constant by imple- menting the queue as an array Q≈í0 : : W C 1¬ç  using the W C 1 slot for keyD 1 , where each slot holds a doubly linked list of vertices with that weight as their key. Then EXTRACT-MIN takes only O.W   D O.1  time  just scan for the Ô¨Årst nonempty slot , and DECREASE-KEY takes only O.1  time  just remove the ver- tex from the list it‚Äôs in and insert it at the front of the list indexed by the new key . This gives a total running time of O.E , which is the best possible asymptotic time  since .E  edges must be processed . However, if the range of edge weights is 1 to jV j, then EXTRACT-MIN takes ‚Äö.V   time with this data structure. So the total time spent doing EXTRACT-MIN is ‚Äö.V 2 , slowing the algorithm to ‚Äö.E C V 2  D ‚Äö.V 2 . In this case, it is better to keep the Fibonacci-heap priority queue, which gave the ‚Äö.E C V lg V   time. Other data structures yield better running times:    van Emde Boas trees  see Chapter 20  give an upper bound of O.ECV lg lg V   time for Prim‚Äôs algorithm.  A redistributive heap  used in the single-source shortest-paths algorithm of Ahuja, Mehlhorn, Orlin, and Tarjan, and mentioned in the chapter notes for  Chapter 24  gives an upper bound of O cid:0 E C Vplg V for Prim‚Äôs algorithm.  Solution to Exercise 23.2-7  We start with the following lemma.  Lemma Let T be a minimum spanning tree of G D .V; E , and consider a graph G0 D .V 0; E0  for which G is a subgraph, i.e., V  V 0 and E  E0. Let T D E  cid:0  T be the edges of G that are not in T . Then there is a minimum spanning tree of G0 that includes no edges in T .  Proof By Exercise 23.2-1, there is a way to order the edges of E so that Kruskal‚Äôs algorithm, when run on G, produces the minimum spanning tree T . We will show that Kruskal‚Äôs algorithm, run on G0, produces a minimum spanning tree T 0 that includes no edges in T . We assume that the edges in E are considered in the same relative order when Kruskal‚Äôs algorithm is run on G and on G0. We Ô¨Årst state and prove the following claim.   Solutions for Chapter 23: Minimum Spanning Trees  23-11  Claim For any pair of vertices u;  2 V , if these vertices are in the same set after Kruskal‚Äôs algorithm run on G considers any edge .x; y  2 E, then they are in the same set after Kruskal‚Äôs algorithm run on G0 considers .x; y . Proof of claim Let us order the edges of E by nondecreasing weight as h.x1; y1 ; .x2; y2 ; : : : ; .xk; yk i, where k D jEj. This sequence gives the order in which the edges of E are considered by Kruskal‚Äôs algorithm, whether it is run on G or on G0. We will use induction, with the inductive hypothesis that if u and  are in the same set after Kruskal‚Äôs algorithm run on G considers an edge .xi ; yi  , then they are in the same set after Kruskal‚Äôs algorithm run on G0 considers the same edge. We use induction on i. Basis: For the basis, i D 0. Kruskal‚Äôs algorithm run on G has not considered any edges, and so all vertices are in different sets. The inductive hypothesis holds trivially. Inductive step: We assume that any vertices that are in the same set after Kruskal‚Äôs algorithm run on G has considered edges h.x1; y1 ; .x2; y2 ; : : : ; .xi cid:0 1; yi cid:0 1 i are in the same set after Kruskal‚Äôs algorithm run on G0 has considered the same edges. When Kruskal‚Äôs algorithm runs on G0, after it considers .xi cid:0 1; yi cid:0 1 , it may consider some edges in E0 cid:0 E before considering .xi ; yi  . The edges in E0 cid:0 E may cause UNION operations to occur, but sets are never divided. Hence, any vertices that are in the same set after Kruskal‚Äôs algorithm run on G0 considers .xi cid:0 1; yi cid:0 1  are still in the same set when .xi ; yi   is considered. When Kruskal‚Äôs algorithm run on G considers .xi ; yi  , either xi and yi are found to be in the same set or they are not.      If Kruskal‚Äôs algorithm run on G Ô¨Ånds xi and yi to be in the same set, then no UNION operation occurs. The sets of vertices remain the same, and so the inductive hypothesis continues to hold after considering .xi ; yi  . If Kruskal‚Äôs algorithm run on G Ô¨Ånds xi and yi to be in different sets, then the operation UNION.xi ; yi   will occur. Kruskal‚Äôs algorithm run on G0 will Ô¨Ånd that either xi and yi are in the same set or they are not. By the induc- tive hypothesis, when edge .xi ; yi   is considered, all vertices in xi‚Äôs set when Kruskal‚Äôs algorithm runs on G are in xi ‚Äôs set when Kruskal‚Äôs algorithm runs on G0, and the same holds for yi. Regardless of whether Kruskal‚Äôs algorithm run on G0 Ô¨Ånds xi and yi to already be in the same set, their sets are united af- ter considering .xi ; yi  , and so the inductive hypothesis continues to hold after  claim  considering .xi ; yi  .  With the claim in hand, we suppose that some edge .u;   2 T is placed into T 0. That means that Kruskal‚Äôs algorithm run on G found u and  to be in the same set  since .u;   2 T   but Kruskal‚Äôs algorithm run on G0 found u and  to be in different sets  since .u;   is placed into T 0 . This fact contradicts the claim, and we conclude that no edge in T is placed into T 0. Thus, by running Kruskal‚Äôs algorithm on G and G0, we demonstrate that there exists a minimum spanning tree of G0 that includes no edges in T .  lemma   We use this lemma as follows. Let G0 D .V 0; E0  be the graph G D .V; E  with the one new vertex and its incident edges added. Suppose that we have a minimum   23-12  Solutions for Chapter 23: Minimum Spanning Trees  Solution to Problem 23-1  spanning tree T for G. We compute a minimum spanning tree for G0 by creating the graph G00 D .V 0; E00 , where E00 consists of the edges of T and the edges in E0  cid:0  E  i.e., the edges added to G that made G0 , and then Ô¨Ånding a minimum spanning tree T 0 for G00. By the lemma, there is a minimum spanning tree for G0 that includes no edges of E  cid:0  T . In other words, G0 has a minimum spanning tree that includes only edges in T and E0  cid:0  E; these edges comprise exactly the set E00. Thus, the the minimum spanning tree T 0 of G00 is also a minimum spanning tree of G0. Even though the proof of the lemma uses Kruskal‚Äôs algorithm, we are not required to use this algorithm to Ô¨Ånd T 0. We can Ô¨Ånd a minimum spanning tree by any means we choose. Let us use Prim‚Äôs algorithm with a Fibonacci-heap priority queue. Since jV 0j D jV j C 1 and jE00j  2jV j  cid:0  1  E00 contains the jV j  cid:0  1 edges of T and at most jV j edges in E0  cid:0  E , it takes O.V   time to construct G00, and the run of Prim‚Äôs algorithm with a Fibonacci-heap priority queue takes time O.E00 C V 0 lg V 0  D O.V lg V  . Thus, if we are given a minimum spanning tree of G, we can compute a minimum spanning tree of G0 in O.V lg V   time.  a. To see that the minimum spanning tree is unique, observe that since the graph is connected and all edge weights are distinct, then there is a unique light edge crossing every cut. By Exercise 23.1-6, the minimum spanning tree is unique. To see that the second-best minimum spanning tree need not be unique, here is a weighted, undirected graph with a unique minimum spanning tree of weight 7 and two second-best minimum spanning trees of weight 8:  1  1  1  3  5  3  5  3  5  2  4  minimum  spanning tree  2  4  second-best minimum  spanning tree  2  4  second-best minimum  spanning tree  b. Since any spanning tree has exactly jV j  cid:0  1 edges, any second-best minimum spanning tree must have at least one edge that is not in the  best  minimum spanning tree. If a second-best minimum spanning tree has exactly one edge, say .x; y , that is not in the minimum spanning tree, then it has the same set of edges as the minimum spanning tree, except that .x; y  replaces some edge, say .u;  , of the minimum spanning tree. In this case, T 0 D T  cid:0 f.u;  g[f.x; y g, as we wished to show. Thus, all we need to show is that by replacing two or more edges of the min- imum spanning tree, we cannot obtain a second-best minimum spanning tree. Let T be the minimum spanning tree of G, and suppose that there exists a second-best minimum spanning tree T 0 that differs from T by two or more   Solutions for Chapter 23: Minimum Spanning Trees  23-13  edges. There are at least two edges in T  cid:0  T 0, and let .u;   be the edge in T  cid:0  T 0 with minimum weight. If we were to add .u;   to T 0, we would get a cycle c. This cycle contains some edge .x; y  in T 0  cid:0  T  since otherwise, T would contain a cycle . We claim that w.x; y  > w.u;  . We prove this claim by contradiction, so let us assume that w.x; y  < w.u;  .  Recall the assumption that edge weights are distinct, so that we do not have to concern ourselves with w.x; y  D w.u;  .  If we add .x; y  to T , we get a cycle c0, which contains some edge .u0; 0  in T  cid:0 T 0  since otherwise, T 0 would contain a cycle . There- fore, the set of edges T 00 D T  cid:0 f.u0; 0 g[f.x; y g forms a spanning tree, and we must also have w.u0; 0  < w.x; y , since otherwise T 00 would be a span- ning tree with weight less than w.T  . Thus, w.u0; 0  < w.x; y  < w.u;  , which contradicts our choice of .u;   as the edge in T  cid:0 T 0 of minimum weight. Since the edges .u;   and .x; y  would be on a common cycle c if we were to add .u;   to T 0, the set of edges T 0  cid:0  f.x; y g [ f.u;  g is a spanning tree, and its weight is less than w.T 0 . Moreover, it differs from T  because it differs from T 0 by only one edge . Thus, we have formed a spanning tree whose weight is less than w.T 0  but is not T . Hence, T 0 was not a second-best minimum spanning tree.  c. We can Ô¨Åll in max≈íu; ¬ç for all u;  2 V in O.V 2  time by simply doing a search from each vertex u, having restricted the edges visited to those of the spanning tree T . It doesn‚Äôt matter what kind of search we do: breadth-Ô¨Årst, depth-Ô¨Årst, or any other kind. We‚Äôll give pseudocode for both breadth-Ô¨Årst and depth-Ô¨Årst approaches. Each approach differs from the pseudocode given in Chapter 22 in that we don‚Äôt need to compute d or f values, and we‚Äôll use the max table itself to record whether a vertex has been visited in a given search. In particular, max≈íu; ¬ç D NIL if and only if u D  or we have not yet visited vertex  in a search from vertex u. Note also that since we‚Äôre visiting via edges in a spanning tree of an undirected graph, we are guaranteed that the search from each vertex u‚Äîwhether breadth-Ô¨Årst or depth-Ô¨Årst‚Äîwill visit all vertices. There will be no need to ‚Äúrestart‚Äù the search as is done in the DFS procedure of Section 22.3. Our pseudocode assumes that the adjacency list of each vertex consists only of edges in the spanning tree T . Here‚Äôs the breadth-Ô¨Årst search approach:   23-14  Solutions for Chapter 23: Minimum Spanning Trees  BFS-FILL-MAX.G; T; w  let max be a new table with an entry max≈íu; ¬ç for each u;  2 G:V for each vertex u 2 G:V max≈íu; ¬ç D NIL  for each vertex  2 G:V Q D ; ENQUEUE.Q; u  while Q ¬§ ;  x D DEQUEUE.Q  for each  2 G:Adj≈íx¬ç  if x == u or w.x;   > max≈íu; x¬ç  if max≈íu; ¬ç == NIL and  ¬§ u max≈íu; ¬ç D .x;    else max≈íu; ¬ç D max≈íu; x¬ç ENQUEUE.Q;    return max  Here‚Äôs the depth-Ô¨Årst search approach:  DFS-FILL-MAX.G; T; w  let max be a new table with an entry max≈íu; ¬ç for each u;  2 G:V for each vertex u 2 G:V max≈íu; ¬ç D NIL  for each vertex  2 G:V DFS-FILL-MAX-VISIT.G; u; u; max   return max  DFS-FILL-MAX-VISIT .G; u; x; max  for each vertex  2 G:Adj≈íx¬ç  if x == u or w.x;   > max≈íu; x¬ç  if max≈íu; ¬ç == NIL and  ¬§ u max≈íu; ¬ç D .x;    else max≈íu; ¬ç D max≈íu; x¬ç DFS-FILL-MAX-VISIT.G; u; ; max   For either approach, we are Ô¨Ålling in jV j rows of the max table. Since the number of edges in the spanning tree is jV j  cid:0  1, each row takes O.V   time to Ô¨Åll in. Thus, the total time to Ô¨Åll in the max table is O.V 2 .  d. In part  b , we established that we can Ô¨Ånd a second-best minimum spanning tree by replacing just one edge of the minimum spanning tree T by some edge .u;   not in T . As we know, if we create spanning tree T 0 by replacing edge .x; y  2 T by edge .u;   62 T , then w.T 0  D w.T   cid:0  w.x; y C w.u;  . For a given edge .u;  , the edge .x; y  2 T that minimizes w.T 0  is the edge of maximum weight on the unique path between u and  in T . If we have al- ready computed the max table from part  c  based on T , then the identity of this edge is precisely what is stored in max≈íu; ¬ç. All we have to do is determine an edge .u;   62 T for which w.max≈íu; ¬ç   cid:0  w.u;   is minimum.   Solutions for Chapter 23: Minimum Spanning Trees  23-15  Thus, our algorithm to Ô¨Ånd a second-best minimum spanning tree goes as fol- lows: 1. Compute the minimum spanning tree T . Time: O.ECV lg V  , using Prim‚Äôs algorithm with a Fibonacci-heap implementation of the priority queue. Since jEj < jV j2, this running time is O.V 2 . Time: O.V 2 .  2. Given the minimum spanning tree T , compute the max table, as in part  c .  O.E , which is O.V 2 .  3. Find an edge .u;   62 T that minimizes w.max≈íu; ¬ç   cid:0  w.u;  . Time: 4. Having found an edge .u;   in step 3, return T 0 D T  cid:0 fmax≈íu; ¬çg[f.u;  g  as a second-best minimum spanning tree.  The total time is O.V 2 .   Lecture Notes for Chapter 24: Single-Source Shortest Paths  Shortest paths  Input:  How to Ô¨Ånd the shortest route between two points on a map.  k  XiD1  w.i cid:0 1; i     Directed graph G D .V; E   Weight function w W E ! R Weight of path p D h0; 1; : : : ; ki D D sum of edge weights on path p : Shortest-path weight u to : ƒ±.u;   D  minnw.p  W u Shortest path u to  is any path p such that w.p  D ƒ±.u;  .  ; o if there exists a path u ;  ;  otherwise :  1  p  Example shortest paths from s [ƒ± valuesappearinsidevertices. Shadededgesshowshortestpaths.]  s  0  2  1  2  7  s  0  2  1  2  7  3  5  t 3  5 y  6  4  3  6  x 9  11 z  3  5  t 3  5 y  6  4  3  6  x 9  11 z  This example shows that the shortest path might not be unique. It also shows that when we look at shortest paths from one vertex to all other vertices, the shortest paths are organized as a tree.   24-2  Lecture Notes for Chapter 24: Single-Source Shortest Paths  Can think of weights as representing any measure that    accumulates linearly along a path, and   we want to minimize.  Examples: time, cost, penalties, loss. Generalization of breadth-Ô¨Årst search to weighted graphs.  Variants  vertex  2 V .   Single-source: Find shortest paths from a given source vertex s 2 V to every  Single-destination: Find shortest paths to a given destination vertex.  Single-pair: Find shortest path from u to . No way known that‚Äôs better in  worst case than solving single-source.   All-pairs: Find shortest path from u to  for all u;  2 V . We‚Äôll see algorithms  for all-pairs in the next chapter.  Negative-weight edges  OK, as long as no negative-weight cycles are reachable from the source.    If we have a negative-weight cycle, we can just keep going around it, and get w.s;   D  cid:0 1 for all  on the cycle.   But OK if the negative-weight cycle is not reachable from the source.  Some algorithms work only if there are no negative-weight edges in the graph.  We‚Äôll be clear when they‚Äôre allowed and not allowed.  Optimal substructure  Lemma Any subpath of a shortest path is a shortest path.  Proof Cut-and-paste.  pux  u  pxy  y  x  pyv  v  Suppose this path p is a shortest path from u to . Then ƒ±.u;   D w.p  D w.pux  C w.pxy  C w.py . Now suppose there exists a shorter path x Then w.p0xy  < w.pxy . Construct p0:  p0 ; y.  xy  pux  u  p'xy  y  x  pyv  v   Lecture Notes for Chapter 24: Single-Source Shortest Paths  24-3  Then w.p0  D w.pux  C w.p0xy  C w.py  < w.pux  C w.pxy  C w.py  D w.p  :  Cycles  Shortest paths can‚Äôt contain cycles:  Contradicts the assumption that p is a shortest path.   lemma    Already ruled out negative-weight cycles.  Positive-weight   we can get a shorter path by omitting the cycle.  Zero-weight: no reason to use them   assume that our solutions won‚Äôt use  them.  Output of single-source shortest-path algorithm  For each vertex  2 V :  :d D ƒ±.s;  .  Initially, :d D 1.           Reduces as algorithms progress. But always maintain :d  ƒ±.s;  .  Call :d a shortest-path estimate. : D predecessor of  on a shortest path from s.  If no predecessor, : D NIL.    induces a tree‚Äîshortest-path tree.  We won‚Äôt prove properties of  in lecture‚Äîsee text.  All the shortest-paths algorithms start with INIT-SINGLE-SOURCE.  Initialization  INIT-SINGLE-SOURCE .G; s  for each  2 G:V :d D 1 : D NIL  s:d D 0  Relaxing an edge .u;    Can we improve the shortest-path estimate for  by going through u and taking .u;  ?   24-4  Lecture Notes for Chapter 24: Single-Source Shortest Paths  RELAX.u; ; w  if :d > u:d C w.u;    :d D u:d C w.u;   : D u v 10  3  4  u 4  4  RELAX  7  3  6 RELAX  6  4  For all the single-source shortest-paths algorithms we‚Äôll look at,      start by calling INIT-SINGLE-SOURCE, then relax edges.  The algorithms differ in the order and how many times they relax each edge.  Shortest-paths properties  Based on calling INIT-SINGLE-SOURCE once and then calling RELAX zero or more times.  Triangle inequality  For all .u;   2 E, we have ƒ±.s;    ƒ±.s; u  C w.u;  . Proof Weight of shortest path s ;  is  weight of any path s ; . Path s ; u !  is a path s ; , and if we use a shortest path s ; u, its weight is ƒ±.s; u  C w.u;  .  Upper-bound property  Always have :d  ƒ±.s;   for all . Once :d D ƒ±.s;  , it never changes. Proof Initially true. Suppose there exists a vertex such that :d < ƒ±.s;  . Without loss of generality,  is Ô¨Årst vertex for which this happens. Let u be the vertex that causes :d to change. Then :d D u:d C w.u;  . So,  :d < ƒ±.s;     ƒ±.s; u  C w.u;    u:d C w.u;     :d < u:d C w.u;   :   triangle inequality    is Ô¨Årst violation    Lecture Notes for Chapter 24: Single-Source Shortest Paths  24-5  Contradicts :d D u:d C w.u;  . Once :d reaches ƒ±.s;  , it never goes lower. It never goes up, since relaxations only lower shortest-path estimates.  No-path property  If ƒ±.s;   D 1, then :d D 1 always. Proof :d  ƒ±.s;   D 1   :d D 1.  Convergence property  If s ; u !  is a shortest path, u:d D ƒ±.s; u , and we call RELAX.u; ; w , then :d D ƒ±.s;   afterward. Proof After relaxation: :d  u:d C w.u;     RELAX code   D ƒ±.s; u  C w.u;   D ƒ±.s;     lemma‚Äîoptimal substructure   Since :d  ƒ±.s;  , must have :d D ƒ±.s;  .  Path relaxation property  Let p D h0; 1; : : : ; ki be a shortest path from s D 0 to k. If we relax, in order, .0; 1 ; .1; 2 ; : : : ; .k cid:0 1; k , even intermixed with other relaxations, then k:d D ƒ±.s; k  . Proof Induction to show that i :d D ƒ±.s; i   after .i cid:0 1; i   is relaxed. Basis: i D 0. Initially, 0:d D 0 D ƒ±.s; 0  D ƒ±.s; s . Inductive step: Assume i cid:0 1:d D ƒ±.s; i cid:0 1 . Relax .i cid:0 1; i  . By convergence property, i :d D ƒ±.s; i   afterward and i :d never changes.  The Bellman-Ford algorithm   Allows negative-weight edges.  Computes :d and : for all  2 V .  Returns TRUE if no negative-weight cycles reachable from s, FALSE otherwise.   24-6  Lecture Notes for Chapter 24: Single-Source Shortest Paths  BELLMAN-FORD.G; w; s   for each edge .u;   2 G:E  INIT-SINGLE-SOURCE .G; s  for i D 1 to jG:Vj  cid:0  1 RELAX.u; ; w  for each edge .u;   2 G:E if :d > u:d C w.u;    return FALSE  return TRUE Core: The nested for loops relax all edges jV j  cid:0  1 times. Time: ‚Äö.VE .  Example  s  0  4  r ‚Äì1  ‚Äì1  2  3  1  2  2 z  5  ‚Äì2 y  1  x  ‚Äì3  Values you get on each pass and how quickly it converges depends on order of relaxation. But guaranteed to converge after jV j  cid:0  1 passes, assuming no negative-weight cycles.  Proof Use path-relaxation property. Let  be reachable from s, and let p D h0; 1; : : : ; ki be a shortest path from s to , where 0 D s and k D . Since p is acyclic, it has  jV j  cid:0  1 edges, so k  jV j  cid:0  1. Each iteration of the for loop relaxes all edges:  First iteration relaxes .0; 1 .  Second iteration relaxes .1; 2 .  kth iteration relaxes .k cid:0 1; k . By the path-relaxation property, :d D k:d D ƒ±.s; k  D ƒ±.s;  . How about the TRUE FALSE return value?  Suppose there is no negative-weight cycle reachable from s.  At termination, for all .u;   2 E, :d D ƒ±.s;     ƒ±.s; u  C w.u;   D u:d C w.u;   :  So BELLMAN-FORD returns TRUE.   triangle inequality    Lecture Notes for Chapter 24: Single-Source Shortest Paths  24-7   Now suppose there exists negative-weight cycle c D h0; 1; : : : ; ki, where  0 D k, reachable from s. Then  .i cid:0 1; i   < 0 :  k  XiD1  Suppose  for contradiction  that BELLMAN-FORD returns TRUE. Then i :d  i cid:0 1:d C w.i cid:0 1; i   for i D 1; 2; : : : ; k. Sum around c: k XiD1  .i cid:0 1:d C w.i cid:0 1; i     i :d   k  D  i cid:0 1:d C  w.i cid:0 1; i    k  XiD1  k  Each vertex appears once in each summation Pk 0  Contradicts c being a negative-weight cycle.  w.i cid:0 1; i   :  XiD1  XiD1 XiD1  k  iD1 i :d andPk  iD1 i cid:0 1:d    Single-source shortest paths in a directed acyclic graph  Since a dag, we‚Äôre guaranteed no negative-weight cycles.  DAG-SHORTEST-PATHS .G; w; s   topologically sort the vertices INIT-SINGLE-SOURCE .G; s  for each vertex u, taken in topologically sorted order  for each vertex  2 G:Adj≈íu¬ç  RELAX.u; ; w   1  y 5  ‚Äì2  z 3  x  6  4  ‚Äì1  2  Example  s 0  2  7  6  t 2  Time ‚Äö.V C E .   24-8  Lecture Notes for Chapter 24: Single-Source Shortest Paths  Correctness Because we process vertices in topologically sorted order, edges of any path must be relaxed in order of appearance in the path.   Edges on any shortest path are relaxed in order.   By path-relaxation property, correct.  Dijkstra‚Äôs algorithm  No negative-weight edges. Essentially a weighted version of breadth-Ô¨Årst search.    Instead of a FIFO queue, uses a priority queue.   Keys are shortest-path weights  :d .  Have two sets of vertices:  S D vertices whose Ô¨Ånal shortest-path weights are determined,  Q D priority queue D V  cid:0  S. DIJKSTRA.G; w; s   INIT-SINGLE-SOURCE .G; s      i.e., insert all vertices into Q  S D ;Q D G:V while Q ¬§ ;  u D EXTRACT-MIN.Q  S D S [ fug for each vertex  2 G:Adj≈íu¬ç  RELAX.u; ; w    Looks a lot like Prim‚Äôs algorithm, but computing :d, and using shortest-path  weights as keys.   Dijkstra‚Äôs algorithm can be viewed as greedy, since it always chooses the ‚Äúlight-  est‚Äù  ‚Äúclosest‚Äù?  vertex in V  cid:0  S to add to S.  Example  10  5  x 8  5 y  2  1  s  0  3  4  6  z  Order of adding to S: s; y; ¬¥; x.   Lecture Notes for Chapter 24: Single-Source Shortest Paths  24-9  Correctness  Loop invariant: At the start of each iteration of the while loop, :d D ƒ±.s;   for all  2 S.  Initialization: Initially, S D ;, so trivially true. Termination: At end, Q D ;   S D V   :d D ƒ±.s;   for all  2 V . Maintenance: Need to show that u:d D ƒ±.s; u  when u is added to S in each iteration. Suppose there exists u such that u:d ¬§ ƒ±.s; u . Without loss of generality, let u be the Ô¨Årst vertex for which u:d ¬§ ƒ±.s; u  when u is added to S. Observations:  u ¬§ s, since s:d D ƒ±.s; s  D 0.  Therefore, s 2 S, so S ¬§ ;.  There must be some path s ; u, since otherwise u:d D ƒ±.s; u  D 1 by  no-path property.  So, there‚Äôs a path s ; u.  p ; u.  Then there‚Äôs a shortest path s Just before u is added to S, path p connects a vertex in S  i.e., s  to a vertex in V  cid:0  S  i.e., u . Let y be Ô¨Årst vertex along p that‚Äôs in V  cid:0  S, and let x 2 S be y‚Äôs predecessor.  S  s  p1  x  y  p2  u  Decompose p into s or p2 may have no edges.   ; x ! y  p1  p2  ; u.  Could have x D s or y D u, so that p1  Claim y:d D ƒ±.s; y  when u is added to S. Proof x 2 S and u is the Ô¨Årst vertex such that u:d ¬§ ƒ±.s; u  when u is added to S   x:d D ƒ±.s; x  when x is added to S. Relaxed .x; y  at that time, so by the convergence property, y:d D ƒ±.s; y .  claim  Now can get a contradiction to u:d ¬§ ƒ±.s; u : y is on shortest path s ; u, and all edge weights are nonnegative   ƒ±.s; y   ƒ±.s; u    y:d D ƒ±.s; y   ƒ±.s; u   u:d   upper-bound property  .   24-10  Lecture Notes for Chapter 24: Single-Source Shortest Paths  Also, both y and u were in Q when we chose u, so u:d  y:d   u:d D y:d : Therefore, y:d D ƒ±.s; y  D ƒ±.s; u  D u:d. Contradicts assumption that u:d ¬§ ƒ±.s; u . Hence, Dijkstra‚Äôs algorithm is cor- rect.  Analysis Like Prim‚Äôs algorithm, depends on implementation of priority queue. If binary heap, each operation takes O.lg V   time   O.E lg V  . If a Fibonacci heap:       Each EXTRACT-MIN takes O.1  amortized time.  There are O.V   other operations, taking O.lg V   amortized time each.  Therefore, time is O.V lg V C E .  Difference constraints  Given a set of inequalities of the form xj  cid:0  xi  bk.  x‚Äôs are variables, 1  i; j  n, b‚Äôs are constants, 1  k  m.    Want to Ô¨Ånd a set of values for the x‚Äôs that satisfy all m inequalities, or determine that no such values exist. Call such a set of values a feasible solution.  Example  x1  cid:0  x2  5 x1  cid:0  x3  6 x2  cid:0  x4   cid:0 1 x3  cid:0  x4   cid:0 2 x4  cid:0  x1   cid:0 3 Solution: x D .0; cid:0 4; cid:0 5; cid:0 3  Also: x D .5; 1; 0; 2  D [above solution] C 5 Lemma If x is a feasible solution, then so is x C d for any constant d . Proof x is a feasible solution   xj  cid:0  xi  bk for all i; j; k   .xj C d    cid:0  .xi C d    bk.   lemma    Lecture Notes for Chapter 24: Single-Source Shortest Paths  24-11  Constraint graph  G D .V; E , weighted, directed.  V D f0; 1; 2; : : : ; ng: one vertex per variable C 0  E D f.i ; j   W xj  cid:0  xi  bk is a constraintg [ f.0; 1 ; .0; 2 ; : : : ; .0; n g  w.0; j   D 0 for all j  w.i ; j   D bk if xj  cid:0  xi  bk  v0  0  0  0  0  0  ‚Äì3  v1 0  ‚Äì3 v4  5  ‚Äì1  6  ‚Äì2  v2 ‚Äì4  ‚Äì5 v3  Theorem Given a system of difference constraints, let G D .V; E  be the corresponding constraint graph. 1. If G has no negative-weight cycles, then x D .ƒ±.0; 1 ; ƒ±.0; 2 ; : : : ; ƒ±.0; n   is a feasible solution.  2. If G has a negative-weight cycle, then there is no feasible solution.  Proof 1. Show no negative-weight cycles   feasible solution. Need to show that xj  cid:0  xi  bk for all constraints. Use xj D ƒ±.0; j   xi D ƒ±.0; i   bk D w.i ; j   : By the triangle inequality, ƒ±.0; j    ƒ±.0; i   C w.i ; j    xj  xi C bk  xj  cid:0  xi  bk : Therefore, feasible.  2. Show negative-weight cycles   no feasible solution.  Without loss of generality, let a negative-weight cycle be c D h1; 2; : : : ; ki, where 1 D k.  0 can‚Äôt be on c, since 0 has no entering edges.  c corresponds to the constraints x2  cid:0  x1  w.1; 2  ; x3  cid:0  x2  w.2; 3  ;  :::  xk cid:0 1  cid:0  xk cid:0 2  w.k cid:0 2; k cid:0 1  ;  xk  cid:0  xk cid:0 1  w.k cid:0 1; k  :   24-12  Lecture Notes for Chapter 24: Single-Source Shortest Paths  If x is a solution satisfying these inequalities, it must satisfy their sum. So add them up. Each xi is added once and subtracted once.  1 D k   x1 D xk.  We get 0  w.c . But w.c  < 0, since c is a negative-weight cycle. Contradiction   no such feasible solution x exists.   theorem   How to Ô¨Ånd a feasible solution  1. Form constraint graph.   n C 1 vertices.  m C n edges.  ‚Äö.m C n  time.  2. Run BELLMAN-FORD from 0.   O..n C 1 .m C n   D O.n2 C nm  time.  3. If BELLMAN-FORD returns FALSE   no feasible solution.  If BELLMAN-FORD returns TRUE   set xi D ƒ±.0; i   for all i.   Solutions for Chapter 24: Single-Source Shortest Paths  Solution to Exercise 24.1-3 This solution is also posted publicly  If the greatest number of edges on any shortest path from the source is m, then the path-relaxation property tells us that after m iterations of BELLMAN-FORD, every vertex  has achieved its shortest-path weight in :d. By the upper-bound property, after m iterations, no d values will ever change. Therefore, no d values will change in the .mC 1 st iteration. Because we do not know m in advance, we cannot make the algorithm iterate exactly m times and then terminate. But if we just make the algorithm stop when nothing changes any more, it will stop after m C 1 iterations. BELLMAN-FORD- M+1  .G; w; s   INITIALIZE-SINGLE-SOURCE .G; s  changes D TRUE while changes == TRUE changes D FALSE for each edge .u;   2 G:E  RELAX-M.u; ; w   RELAX-M.u; ; w  if :d > u:d C w.u;    :d D u:d C w.u;   : D u changes D TRUE  The test for a negative-weight cycle  based on there being a d value that would change if another relaxation step was done  has been removed above, because this version of the algorithm will never get out of the while loop unless all d values stop changing.  Solution to Exercise 24.2-3  Instead of modifying the DAG-SHORTEST-PATHS procedure, we‚Äôll modify the structure of the graph so that we can run DAG-SHORTEST-PATHS on it. In fact,   24-14  Solutions for Chapter 24: Single-Source Shortest Paths  we‚Äôll give two ways to transform a PERT chart G D .V; E  with weights on ver- tices to a PERT chart G0 D .V 0; E0  with weights on edges. In each way, we‚Äôll have that jV 0j  2jV j and jE0j  jV j C jEj. We can then run on G0 the same algorithm to Ô¨Ånd a longest path through a dag as is given in Section 24.2 of the text. In the Ô¨Årst way, we transform each vertex  2 V into two vertices 0 and 00 in V 0. All edges in E that enter  will enter 0 in E0, and all edges in E that leave  will leave 00 in E0. In other words, if .u;   2 E, then .u00; 0  2 E0. All such edges have weight 0. We also put edges .0; 00  into E0 for all vertices  2 V , and these edges are given the weight of the corresponding vertex  in G. Thus, jV 0j D 2jV j, jE0j D jV j C jEj, and the edge weight of each path in G0 equals the vertex weight of the corresponding path in G. In the second way, we leave vertices in V alone, but we add one new source vertex s to V 0, so that V 0 D V [ fsg. All edges of E are in E0, and E0 also includes an edge .s;   for every vertex  2 V that has in-degree 0 in G. Thus, the only vertex with in-degree 0 in G0 is the new source s. The weight of edge .u;   2 E0 is the weight of vertex  in G. In other words, the weight of each entering edge in G0 is the weight of the vertex it enters in G. In effect, we have ‚Äúpushed back‚Äù the weight of each vertex onto the edges that enter it. Here, jV 0j D jV j C 1, jE0j  jV j C jEj  since no more than jV j vertices have in-degree 0 in G , and again the edge weight of each path in G0 equals the vertex weight of the corresponding path in G.  Solution to Exercise 24.3-3 This solution is also posted publicly  Yes, the algorithm still works. Let u be the leftover vertex that does not get extracted from the priority queue Q. If u is not reachable from s, then u:d D ƒ±.s; u  D 1. then there is a shortest path p D s ; x ! u. When the vertex x was extracted, x:d D ƒ±.s; x  and then the edge .x; u  was relaxed; thus, u:d D ƒ±.s; u .  If u is reachable from s,  Solution to Exercise 24.3-4  1. Verify that s:d D 0 and s: D NIL. 2. Verify that :d D ::Cw.:;   for all  ¬§ s. 3. Verify that :d D 1 if and only if :√ü D NIL for all  ¬§ s. 4. If any of the above veriÔ¨Åcation tests fail, declare the output to be incorrect. Otherwise, run one pass of Bellman-Ford, i.e., relax each edge .u;   2 E one time. If any values of :d change, then declare the output to be incorrect; otherwise, declare the output to be correct.   Solutions for Chapter 24: Single-Source Shortest Paths  24-15  Solution to Exercise 24.3-5  Let the graph have vertices s; x; y; ¬¥ and edges .s; x ; .x; y ; .y; ¬¥ ; .s; y , and let every edge have weight 0. Dijkstra‚Äôs algorithm could relax edges in the or- der .s; y ; .s; x ; .y; ¬¥ ; .x; y . The graph has two shortest paths from s to ¬¥: hs; x; y; ¬¥i and hs; y; ¬¥i, both with weight 0. The edges on the shortest path hs; x; y; ¬¥i are relaxed out of order, because .x; y  is relaxed after .y; ¬¥ .  Solution to Exercise 24.3-6 This solution is also posted publicly  To Ô¨Ånd the most reliable path between s and t, run Dijkstra‚Äôs algorithm with edge weights w.u;   D  cid:0  lg r.u;   to Ô¨Ånd shortest paths from s in O.ECV lg V   time. The most reliable path is the shortest path from s to t, and that path‚Äôs reliability is the product of the reliabilities of its edges. Here‚Äôs why this method works. Because the probabilities are independent, the probability that a path will not fail is the product of the probabilities that its edges will not fail. We want to Ô¨Ånd a path s  ; t such thatQ.u; 2p r.u;   is maximized. This is equivalent to maximizing lg.Q.u; 2p r.u;    DP.u; 2p lg r.u;  , which is in turn equivalent to minimizingP.u; 2p  cid:0  lg r.u;  .  Note: r.u;   can be 0, and lg 0 is undeÔ¨Åned. So in this algorithm, deÔ¨Åne lg 0 D  cid:0 1.  Thus if we assign weights w.u;   D  cid:0  lg r.u;  , we have a shortest-path problem. Since lg 1 = 0, lg x < 0 for 0 < x < 1, and we have deÔ¨Åned lg 0 D  cid:0 1, all the weights w are nonnegative, and we can use Dijkstra‚Äôs algorithm to Ô¨Ånd the shortest paths from s in O.E C V lg V   time.  p  Alternative solution  You can also work with the original probabilities by running a modiÔ¨Åed version of Dijkstra‚Äôs algorithm that maximizes the product of reliabilities along a path instead of minimizing the sum of weights along a path. In Dijkstra‚Äôs algorithm, use the reliabilities as edge weights and substitute  max  and EXTRACT-MAX  for min  and EXTRACT-MIN  in relaxation and the      queue,  for C in relaxation, 1  identity for   for 0  identity for C  and  cid:0 1  identity for min  for 1  identity for max .  For example, we would use the following instead of the usual RELAX procedure:  RELAX-RELIABILITY.u; ; r  if :d < u:d  r.u;    :d D u:d  r.u;   : D u   24-16  Solutions for Chapter 24: Single-Source Shortest Paths  This algorithm is isomorphic to the one above: it performs the same operations except that it is working with the original probabilities instead of the transformed ones.  Solution to Exercise 24.3-8  Observe that if a shortest-path estimate is not 1, then it‚Äôs at most .jV j  cid:0  1 W . Why? In order to have :d < 1, we must have relaxed an edge .u;   with u:d < 1. By induction, we can show that if we relax .u;  , then :d is at most the number of edges on a path from s to  times the maximum edge weight. Since any acyclic path has at most jV j  cid:0  1 edges and the maximum edge weight is W , we see that :d  .jV j  cid:0  1 W . Note also that :d must also be an integer, unless it is 1. We also observe that in Dijkstra‚Äôs algorithm, the values returned by the EXTRACT- MIN calls are monotonically increasing over time. Why? After we do our initial jV j INSERT operations, we never do another. The only other way that a key value can change is by a DECREASE-KEY operation. Since edge weights are nonneg- ative, when we relax an edge .u;  , we have that u:d  :d. Since u is the minimum vertex that we just extracted, we know that any other vertex we extract later has a key value that is at least u:d. When keys are known to be integers in the range 0 to k and the key values extracted are monotonically increasing over time, we can implement a min-priority queue so that any sequence of m INSERT, EXTRACT-MIN, and DECREASE-KEY operations takes O.m C k  time. Here‚Äôs how. We use an array, say A≈í0 : : k¬ç, where A≈íj ¬ç is a linked list of each element whose key is j . Think of A≈íj ¬ç as a bucket for all elements with key j . We implement each bucket by a circular, doubly linked list with a sentinel, so that we can insert into or delete from each bucket in O.1  time. We perform the min-priority queue operations as follows:    INSERT: To insert an element with key j , just insert it into the linked list in A≈íj ¬ç. Time: O.1  per INSERT.   EXTRACT-MIN: We maintain an index min of the value of the smallest key extracted. Initially, min is 0. To Ô¨Ånd the smallest key, look in A≈ímin¬ç and, if this list is nonempty, use any element in it, removing the element from the list and returning it to the caller. Otherwise, we rely on the monotonicity property and increment min until we either Ô¨Ånd a list A≈ímin¬ç that is nonempty  using any element in A≈ímin¬ç as before  or we run off the end of the array A  in which case the min-priority queue is empty . Since there are at most m INSERT operations, there are at most m elements in the min-priority queue. We increment min at most k times, and we remove and return some element at most m times. Thus, the total time over all EXTRACT- MIN operations is O.m C k .  DECREASE-KEY: To decrease the key of an element from j to i, Ô¨Årst check whether i  j , Ô¨Çagging an error if not. Otherwise, we remove the element from its list A≈íj ¬ç in O.1  time and insert it into the list A≈íi ¬ç in O.1  time. Time: O.1  per DECREASE-KEY.   Solution to Exercise 24.3-9  Solutions for Chapter 24: Single-Source Shortest Paths  24-17  To apply this kind of min-priority queue to Dijkstra‚Äôs algorithm, we need to let k D .jV j  cid:0  1 W , and we also need a separate list for keys with value 1. The num- ber of operations m is O.V C E   since there are jV j INSERT and jV j EXTRACT- MIN operations and at most jEj DECREASE-KEY operations , and so the total time is O.V C E C V W   D O.V W C E .  First, observe that at any time, there are at most W C 2 distinct key values in the priority queue. Why? A key value is either 1 or it is not. Consider what happens whenever a key value :d becomes Ô¨Ånite. It must have occurred due to the relax- ation of an edge .u;  . At that time, u was being placed into S, and u:d  y:d for all vertices y 2 V  cid:0  S. After relaxing edge .u;  , we have :d  u:d C W . Since any other vertex y 2 V  cid:0 S with y:d < 1 also had its estimate changed by a relax- ation of some edge x with x:d  u:d, we must have y:d  x:d C W  u:d C W . Thus, at the time that we are relaxing edges from a vertex u, we must have, for all vertices  2 V  cid:0  S, that u:d  :d  u:d C W or :d D 1. Since shortest-path estimates are integer values  except for 1 , at any given moment we have at most W C 2 different ones: u:d; u:d C 1; u:d C 2; : : : ; u:d C W and 1. Therefore, we can maintain the min-priorty queue as a binary min-heap in which each node points to a doubly linked list of all vertices with a given key value. There are at most W C 2 nodes in the heap, and so EXTRACT-MIN runs in O.lg W   time. To perform DECREASE-KEY, we need to be able to Ô¨Ånd the heap node corresponding to a given key in O.lg W   time. We can do so in O.1  time as follows. First, keep a pointer inf to the node containing all the 1 keys. Second, maintain an array loc≈í0 : : W ¬ç, where loc≈íi ¬ç points to the unique heap entry whose key value is congruent to i .mod .W C 1  . As keys move around in the heap, we can update this array in O.1  time per movement. Alternatively, instead of using a binary min-heap, we could use a red-black tree. Now INSERT, DELETE, MINIMUM, and SEARCH‚Äîfrom which we can construct the priority-queue operations‚Äîeach run in O.lg W   time.  Solution to Exercise 24.4-4  Let ƒ±.u  be the shortest-path weight from s to u. Then we want to Ô¨Ånd ƒ±.t  . ƒ± must satisfy  ƒ±.s  D 0  ƒ±.   cid:0  ƒ±.u   w.u;   for all .u;   2 E where w.u;   is the weight of edge .u;  . Thus x D ƒ±.  is a solution to  xs D 0  x  cid:0  xu  w.u;   :   Lemma 24.10  ;   24-18  Solutions for Chapter 24: Single-Source Shortest Paths  To turn this into a set of inequalities of the required form, replace xs D 0 by xs  0 and  cid:0 xs  0  i.e., xs  0 . The constraints are now  xs  0 ;  cid:0 xs  0 ;  x  cid:0  xu  w.u;   ; which still has x D ƒ±.  as a solution. However, ƒ± isn‚Äôt the only solution to this set of inequalities.  For example, if all edge weights are nonnegative, all xi D 0 is a solution.  To force xt D ƒ±.t   as required by the shortest-path problem, add the requirement to maximize  the ob- jective function  xt. This is correct because  max.xt    ƒ±.t   because xt D ƒ±.t   is part of one solution to the set of inequali-  max.xt    ƒ±.t   can be demonstrated by a technique similar to the proof of  ties,  Theorem 24.9: Let p be a shortest path from s to t. Then by deÔ¨Ånition,  w.u;   :  ƒ±.t   D X.u; 2p But for each edge .u;   we have the inequality x  cid:0  xu  w.u;  , so ƒ±.t   D X.u; 2p But xs D 0, so xt  ƒ±.t  .  w.u;    X.u; 2p  .x  cid:0  xu  D xt  cid:0  xs :  Note: Maximizing xt subject to the above inequalities solves the single-pair shortest-path problem when t is reachable from s and there are no negative-weight cycles. But if there‚Äôs a negative-weight cycle, the inequalities have no feasible so- lution  as demonstrated in the proof of Theorem 24.9 ; and if t is not reachable from s, then xt is unbounded.  Solution to Exercise 24.4-7 This solution is also posted publicly  Observe that after the Ô¨Årst pass, all d values are at most 0, and that relaxing edges .0; i   will never again change a d value. Therefore, we can eliminate 0 by running the Bellman-Ford algorithm on the constraint graph without the 0 vertex but initializing all shortest path estimates to 0 instead of 1.  Solution to Exercise 24.4-10  To allow for single-variable constraints, we add the variable x0 and let it correspond to the source vertex 0 of the constraint graph. The idea is that, if there are no   Solutions for Chapter 24: Single-Source Shortest Paths  24-19  negative-weight cycles containing 0, we will Ô¨Ånd that ƒ±.0; 0  D 0. In this case, we set x0 D 0, and so we can treat any single-variable constraint using xi as if it were a 2-variable constraint with x0 as the other variable. SpeciÔ¨Åcally, we treat the constraint xi  bk as if it were xi  cid:0  x0  bk, and we add the edge .0; i   with weight bk to the constraint graph. We treat the constraint  cid:0 xi  bk as if it were x0  cid:0  xi  bk, and we add the edge .i ; 0  with weight bk to the constraint graph. Once we Ô¨Ånd shortest-path weights from 0, we set xi D ƒ±.0; i   for all i D 0; 1; : : : ; n; that is, we do as before but also include x0 as one of the vari- ables that we set to a shortest-path weight. Since 0 is the source vertex, either x0 D 0 or x0 < 0. If ƒ±.0; 0  D 0, so that x0 D 0, then setting xi D ƒ±.0; i   for all i D 0; 1; : : : ; n gives a feasible solution for the system. The only new constraints beyond those in the text are those involving x0. For constraints xi  bk, we use xi  cid:0  x0  bk. By the triangle inequality, ƒ±.0; i    ƒ±.0; 0  C w.0; i   D bk, and so xi  bk. For constraints  cid:0 xi  bk, we use x0  cid:0  xi  bk. By the triangle inequality, 0 D ƒ±.0; 0   ƒ±.0; i   C w.i ; 0 ; thus, 0  xi C bk or, equivalently,  cid:0 xi  bk. If ƒ±.0; 0  < 0, so that x0 < 0, then there is a negative-weight cycle containing 0. The portion of the proof of Theorem 24.9 that deals with negative-weight cycles carries through but with 0 on the negative-weight cycle, and we see that there is no feasible solution.  Solution to Exercise 24.5-4 This solution is also posted publicly  Whenever RELAX sets  for some vertex, it also reduces the vertex‚Äôs d value. Thus if s: gets set to a non-NIL value, s:d is reduced from its initial value of 0 to a negative number. But s:d is the weight of some path from s to s, which is a cycle including s. Thus, there is a negative-weight cycle.  Solution to Exercise 24.5-7  Suppose we have a shortest-paths tree G. Relax edges in G according to the order in which a BFS would visit them. Then we are guaranteed that the edges along each shortest path are relaxed in order. By the path-relaxation property, we would then have :d D ƒ±.s;   for all  2 V . Since G contains at most jV j  cid:0  1 edges, we need to relax only jV j  cid:0  1 edges to get :d D ƒ±.s;   for all  2 V .  Solution to Exercise 24.5-8  Suppose that there is a negative-weight cycle c D h0; 1; : : : ; ki, where 0 D k, that is reachable from the source vertex s; thus, w.c  < 0. Without loss of general-   24-20  Solutions for Chapter 24: Single-Source Shortest Paths  ity, c is simple. There must be an acyclic path from s to some vertex of c that uses no other vertices in c. Without loss of generality let this vertex of c be 0, and let this path from s to 0 be p D hu0; u1; : : : ; uli, where u0 D s and ul D 0 D k.  It may be the case that ul D s, in which case path p has no edges.  After the call to INITIALIZE-SINGLE-SOURCE sets :d D 1 for all  2 V  cid:0  fsg, perform the following sequence of relaxations. First, relax every edge in path p, in order. Then relax every edge in cycle c, in order, and repeatedly relax the cycle. That is, we relax the edges .u0; u1 , .u1; u2 , . . . , .ul cid:0 1; 0 , .0; 1 , .1; 2 , . . . , .k cid:0 1; 0 , .0; 1 , .1; 2 , . . . , .k cid:0 1; 0 , .0; 1 , .1; 2 , . . . , .k cid:0 1; 0 , . . . . We claim that every edge relaxation in this sequence reduces a shortest-path es- timate. Clearly, the Ô¨Årst time we relax an edge .ui cid:0 1; ui   or .j cid:0 1; j  , for i D 1; 2; : : : ; l and j D 1; 2; : : : ; k  cid:0  1  note that we have not yet relaxed the last edge of cycle c , we reduce ui :d or j :d from 1 to a Ô¨Ånite value. Now consider the relaxation of any edge .j cid:0 1; j   after this opening sequence of relaxations. We use induction on the number of edge relaxations to show that this relaxation reduces j :d. Basis: The next edge relaxed after the opening sequence is .k cid:0 1; k . Before relaxation, k:d D w.p , and after relaxation, k:d D w.p  C w.c  < w.p , since w.c  < 0. Inductive step: Consider the relaxation of edge .j cid:0 1; j  . Since c is a sim- the last time j :d was updated was by a relaxation of this same ple cycle, edge. By the inductive hypothesis, j cid:0 1:d has just been reduced. Thus, j cid:0 1:d C w.j cid:0 1; j   < j :d, and so the relaxation will reduce the value of j :d.  a. Assume for the purpose contradiction that Gf is not acyclic; thus Gf has a cycle. A cycle must have at least one edge .u;   in which u has higher index than . This edge is not in Ef  by the deÔ¨Ånition of Ef  , in contradition to the assumption that Gf has a cycle. Thus Gf is acyclic. The sequence h1; 2; : : : ; jV ji is a topological sort for Gf , because from the deÔ¨Ånition of Ef we know that all edges are directed from smaller indices to larger indices. The proof for Eb is similar.  b. For all vertices  2 V , we know that either ƒ±.s;   D 1 or ƒ±.s;   is Ô¨Ånite. If ƒ±.s;   D 1, then :d will be 1. Thus, we need to consider only the case where :d is Ô¨Ånite. There must be some shortest path from s to . Let p D h0; 1; : : : ; k cid:0 1; ki be that path, where 0 D s and k D . Let us now consider how many times there is a change in direction in p, that is, a situation in which .i cid:0 1; i   2 Ef and .i ; iC1  2 Eb or vice versa. There can be at most jV j cid:0 1 edges in p, so there can be at most jV j cid:0 2 changes in direction. Any portion of the path where there is no change in direction is computed with the correct d values in the Ô¨Årst or second half of a single pass once the vertex that begins the no-change-in-direction sequence has the correct d value, because the edges are relaxed in the order of the direction of the sequence. Each change in  Solution to Problem 24-1   Solution to Problem 24-2  Solutions for Chapter 24: Single-Source Shortest Paths  24-21  direction requires a half pass in the new direction of the path. The following table shows the maximum number of passes needed depending on the parity of jV j  cid:0  1 and the direction of the Ô¨Årst edge: jV j  cid:0  1 Ô¨Årst edge direction even even odd odd  passes .jV j  cid:0  1 =2 .jV j  cid:0  1 =2 C 1 jV j =2 jV j =2  forward backward forward backward  In any case, the maximum number of passes that we will need is djV j =2e.  c. This scheme does not affect the asymptotic running time of the algorithm be- cause even though we perform only djV j =2e passes instead of jV j  cid:0  1 passes, it is still O.V   passes. Each pass still takes ‚Äö.E  time, so the running time remains O.VE .  a. Consider boxes with dimensions x D .x1; : : : ; xd  , y D .y1; : : : ; yd  , and ¬¥ D .¬¥1; : : : ; ¬¥d  . Suppose there exists a permutation  such that x.i   < yi for i D 1; : : : ; d and there exists a permutation 0 such that y 0.i   < ¬¥i for i D 1; : : : ; d , so that x nests inside y and y nests inside ¬¥. Construct a permutation 00, where 00.i   D 0..i   . Then for i D 1; : : : ; d , we have x 00.i   D x 0..i    < y 0.i   < ¬¥i , and so x nests inside ¬¥.  b. Sort the dimensions of each box from longest to shortest. A box X with sorted dimensions .x1; x2; : : : ; xd   nests inside a box Y with sorted dimensions .y1; y2; : : : ; yd   if and only if xi < yi for i D 1; 2; : : : ; d . The sorting can be done in O.d lg d   time, and the test for nesting can be done in O.d   time, and so the algorithm runs in O.d lg d   time. This algorithm works because a d -dimensional box can be oriented so that every permutation of its dimensions is possible.  Experiment with a 3-dimensional box if you are unsure of this .  c. Construct a dag G D .V; E , where each vertex i corresponds to box Bi, and .i ; j   2 E if and only if box Bi nests inside box Bj . Graph G is indeed a dag, because nesting is transitive and antireÔ¨Çexive  i.e., no box nests inside itself . The time to construct the dag is O.d n2 C d n lg d  , from comparing each of the  cid:0 n 2 pairs of boxes after sorting the dimensions of each. Add a supersource vertex s and a supersink vertex t to G, and add edges .s; i   for all vertices i with in-degree 0 and .j ; t   for all vertices j with out- degree 0. Call the resulting dag G0. The time to do so is O.n . Find a longest path from s to t in G0.  Section 24.2 discusses how to Ô¨Ånd a longest path in a dag.  This path corresponds to a longest sequence of nesting boxes. The time to Ô¨Ånd a longest path is O.n2 , since G0 has nC 2 vertices and O.n2  edges. Overall, this algorithm runs in O.d n2 C d n lg d   time.   24-22  Solutions for Chapter 24: Single-Source Shortest Paths  Solution to Problem 24-3 This solution is also posted publicly  a. We can use the Bellman-Ford algorithm on a suitable weighted, directed graph G D .V; E , which we form as follows. There is one vertex in V for each currency, and for each pair of currencies ci and cj , there are directed edges .i ; j   and .j ; i  .  Thus, jV j D n and jEj D n.n  cid:0  1 .  We are looking for a cycle hi1; i2; i3; : : : ; ik; i1i such that R≈íi1; i2¬ç  R≈íi2; i3¬ç R≈íik cid:0 1; ik¬ç  R≈íik; i1¬ç > 1 : Taking logarithms of both sides of this inequality gives lg R≈íi1; i2¬ç C lg R≈íi2; i3¬ç C  C lg R≈íik cid:0 1; ik¬ç C lg R≈íik; i1¬ç > 0 : If we negate both sides, we get . cid:0  lg R≈íi1; i2¬ç  C . cid:0  lg R≈íi2; i3¬ç  C  C .lgR≈íik cid:0 1; ik¬ç  C . cid:0  lg R≈íik; i1¬ç  < 0 ; and so we want to determine whether G contains a negative-weight cycle with these edge weights. We can determine whether there exists a negative-weight cycle in G by adding an extra vertex 0 with 0-weight edges .0; i   for all i 2 V , running BELLMAN-FORD from 0, and using the boolean result of BELLMAN-FORD  which is TRUE if there are no negative-weight cycles and FALSE if there is a negative-weight cycle  to guide our answer. That is, we invert the boolean result of BELLMAN-FORD. This method works because adding the new vertex 0 with 0-weight edges from 0 to all other vertices cannot introduce any new cycles, yet it ensures that all negative-weight cycles are reachable from 0. It takes ‚Äö.n2  time to create G, which has ‚Äö.n2  edges. Then it takes O.n3  time to run BELLMAN-FORD. Thus, the total time is O.n3 . Another way to determine whether a negative-weight cycle exists is to create G and, without adding 0 and its incident edges, run either of the all-pairs shortest- paths algorithms. If the resulting shortest-path distance matrix has any negative values on the diagonal, then there is a negative-weight cycle.  b. Note: The solution to this part also serves as a solution to Exercise 24.1-6.  Assuming that we ran BELLMAN-FORD to solve part  a , we only need to Ô¨Ånd the vertices of a negative-weight cycle. We can do so as follows. Go through the edges once again. Once we Ô¨Ånd an edge .u;   for which u:d C w.u;   < :d, then we know that either vertex  is on a negative-weight cycle or is reachable from one. We can Ô¨Ånd a vertex on the negative-weight cycle by tracing back the  values from , keeping track of which vertices we‚Äôve visited until we reach a vertex x that we‚Äôve visited before. Then we can trace back  values from x until we get back to x, and all vertices in between, along with x, will constitute a negative-weight cycle. We can use the recursive method given by the PRINT- PATH procedure of Section 22.2, but stop it when it returns to vertex x.   Solutions for Chapter 24: Single-Source Shortest Paths  24-23  The running time is O.n3  to run BELLMAN-FORD, plus O.m  to check all the edges and O.n  to print the vertices of the cycle, for a total of O.n3  time.  Solution to Problem 24-4  a. Since all weights are nonnegative, use Dijkstra‚Äôs algorithm.  Implement the priority queue as an array Q≈í0 : : jEj C 1¬ç, where Q≈íi ¬ç is a list of vertices  for which :d D i. Initialize :d for  ¬§ s to jEj C 1 instead of to 1, so that all vertices have a place in Q.  Any initial :d > ƒ±.s;   works in the algorithm, since :d decreases until it reaches ƒ±.s;  .  The jV j EXTRACT-MINs can be done in O.E  total time, and decreasing a d value during relaxation can be done in O.1  time, for a total running time of O.E .   When :d decreases, just add  to the front of the list in Q≈í:d¬ç.  EXTRACT-MIN removes the head of the list in the Ô¨Årst nonempty slot of Q. To do EXTRACT-MIN without scanning all of Q, keep track of the small- est i for which Q≈íi ¬ç is not empty. The key point is that when :d decreases due to relaxation of edge .u;  , :d remains  u:d, so it never moves to an earlier slot of Q than the one that had u, the previous minimum. Thus EXTRACT-MIN can always scan upward in the array, taking a total of O.E  time for all EXTRACT-MINs.  b. For all .u;   2 E, we have w1.u;   2 f0; 1g, so ƒ±1.s;    jV j  cid:0  1  jEj.  Use part  a  to get the O.E  time bound.  c. To show that wi .u;   D 2wi cid:0 1.u;   or wi .u;   D 2wi cid:0 1.u;   C 1, observe that the i bits of wi .u;   consist of the i  cid:0  1 bits of wi cid:0 1.u;   followed by one more bit. If that low-order bit is 0, then wi .u;   D 2wi cid:0 1.u;  ; if it is 1, then wi .u;   D 2wi cid:0 1.u;   C 1. Notice the following two properties of shortest paths:  1. If all edge weights are multiplied by a factor of c, then all shortest-path  weights are multiplied by c.  2. If all edge weights are increased by at most c, then all shortest-path weights are increased by at most c.jV j  cid:0  1 , since all shortest paths have at most jV j  cid:0  1 edges.  The lowest possible value for wi .u;   is 2wi cid:0 1.u;  , so by the Ô¨Årst observa- tion, the lowest possible value for ƒ±i .s;   is 2ƒ±i cid:0 1.s;  . The highest possible value for wi .u;   is 2wi cid:0 1.u;   C 1. Therefore, us- ing the two observations together, the highest possible value for ƒ±i .s;   is 2ƒ±i cid:0 1.s;   C jV j  cid:0  1.  d. We have  ywi .u;   D wi .u;   C 2ƒ±i cid:0 1.s; u   cid:0  2ƒ±i cid:0 1.s;     2wi cid:0 1.u;   C 2ƒ±i cid:0 1.s; u   cid:0  2ƒ±i cid:0 1.s;    0 :   24-24  Solutions for Chapter 24: Single-Source Shortest Paths  The second line follows from part  c , and the third line follows from Lemma 24.10: ƒ±i cid:0 1.s;    ƒ±i cid:0 1.s; u  C wi cid:0 1.u;  .  e. Observe that if we compute ywi .p  for any path p W u ; , the terms ƒ±i cid:0 1.s; t    cancel for every intermediate vertex t on the path. Thus,  ywi .p  D wi .p  C 2ƒ±i cid:0 1.s; u   cid:0  2ƒ±i cid:0 1.s;   :  This relationship will be shown in detail in equation  25.10  within the proof of Lemma 25.1.  The ƒ±i cid:0 1 terms depend only on u, , and s, but not on the path p; therefore the same paths will be of minimum wi weight and of minimum ywi weight between u and . Letting u D s, we get yƒ±i .s;   D ƒ±i .s;   C 2ƒ±i cid:0 1.s; s   cid:0  2ƒ±i cid:0 1.s;    D ƒ±i .s;    cid:0  2ƒ±i cid:0 1.s;   :  Rewriting this result as ƒ±i .s;   D yƒ±i .s;   C 2ƒ±i cid:0 1.s;   and combining it with ƒ±i .s;    2ƒ±i cid:0 1.s;  CjV j cid:0 1  from part  c   gives us yƒ±i .s;    jV j cid:0 1  jEj.  f. To compute ƒ±i .s;   from ƒ±i cid:0 1.s;   for all  2 V in O.E  time:  1. Compute the weights ywi .u;   in O.E  time, as shown in part  d . 2. By part  e , yƒ±i .s;    jEj, so use part  a  to compute all yƒ±i .s;   in O.E  3. Compute all ƒ±i .s;   from yƒ±i .s;   and ƒ±i cid:0 1.s;   as shown in part  e , in  time.  O.V   time.  To compute all ƒ±.s;   in O.E lg W   time: 1. Compute ƒ±1.s;   for all  2 V . As shown in part  b , this takes O.E  time. 2. For each i D 2; 3; : : : ; k, compute all ƒ±i .s;   from ƒ±i cid:0 1.s;   in O.E  time as shown above. This procedure computes ƒ±.s;   D ƒ±k.u;   in time O.Ek  D O.E lg W  .  Observe that a bitonic sequence can increase, then decrease, then increase, or it can decrease, then increase, then decrease. That is, there can be at most two changes of direction in a bitonic sequence. Any sequence that increases, then decreases, then increases, then decreases has a bitonic sequence as a subsequence. Now, let us suppose that we had an even stronger condition than the bitonic prop- erty given in the problem: for each vertex  2 V , the weights of the edges along any shortest path from s to  are increasing. Then we could call INITIALIZE- SINGLE-SOURCE and then just relax all edges one time, going in increasing order of weight. Then the edges along every shortest path would be relaxed in order of their appearance on the path.  We rely on the uniqueness of edge weights to ensure that the ordering is correct.  The path-relaxation property  Lemma 24.15  would guarantee that we would have computed correct shortest paths from s to each vertex.  Solution to Problem 24-6   Solutions for Chapter 24: Single-Source Shortest Paths  24-25  If we weaken the condition so that the weights of the edges along any shortest path increase and then decrease, we could relax all edges one time, in increasing order of weight, and then one more time, in decreasing order of weight. That order, along with uniqueness of edge weights, would ensure that we had relaxed the edges of every shortest path in order, and again the path-relaxation property would guarantee that we would have computed correct shortest paths. To make sure that we handle all bitonic sequences, we do as suggested above. That is, we perform four passes, relaxing each edge once in each pass. The Ô¨Årst and third passes relax edges in increasing order of weight, and the second and fourth passes in decreasing order. Again, by the path-relaxation property and the uniqueness of edge weights, we have computed correct shortest paths. The total time is O.V C E lg V  , as follows. The time to sort jEj edges by weight is O.E lg E  D O.E lg V    since jEj D O.V 2  . INITIALIZE-SINGLE-SOURCE takes O.V   time. Each of the four passes takes O.E  time. Thus, the total time is O.E lg V C V C E  D O.V C E lg V  .   Lecture Notes for Chapter 25: All-Pairs Shortest Paths  Chapter 25 overview  Given a directed graph G D .V; E , weight function w W E ! R, jV j D n. Assume that we can number the vertices 1; 2; : : : ; n. Goal: create an n  n matrix D D .dij   of shortest-path distances, so that dij D ƒ±.i; j   for all vertices i and j . Could run BELLMAN-FORD once from each vertex:  O.V 2E ‚Äîwhich is O.V 4  if the graph is dense  E D ‚Äö.V 2  . If no negative-weight edges, could run Dijkstra‚Äôs algorithm once from each vertex:   O.VE lg V   with binary heap‚ÄîO.V 3 lg V   if dense,  O.V 2 lg V C VE  with Fibonacci heap‚ÄîO.V 3  if dense. We‚Äôll see how to do in O.V 3  in all cases, with no fancy data structure.  Shortest paths and matrix multiplication  Assume that G is given as adjacency matrix of weights: W D .wij  , with vertices numbered 1 to n.  wij D¬Ä 0  weight of .i; j   1  if i D j ; if i ¬§ j , .i; j   2 E ; if i ¬§ j , .i; j   ‚Ä¶ E :  Won‚Äôt worry about predecessors‚Äîsee book. Will use dynamic programming at Ô¨Årst.  Optimal substructure Recall: subpaths of shortest paths are shortest paths.   25-2  Lecture Notes for Chapter 25: All-Pairs Shortest Paths  Recursive solution Let l .m   m D 0  ij D weight of shortest path i ; j that contains  m edges.    there is a shortest path i ; j with  m edges if and only if i D j   l .0   m  1   l .m   min  1knÀöl .m cid:0 1  i k C wkj cid:9   i k C wkj cid:9   k ranges over all possible predecessors of j    since wjj D 0 for all j   .  ij D  0 if i D j ; 1 if i ¬§ j : ij D minl .m cid:0 1  1knÀöl .m cid:0 1   D min  ij   Observe that when m D 1, must have l .1   ij D wij .  Conceptually, when the path is restricted to at most 1 edge, the weight of the shortest path i ; j must be wij . And the math works out, too: l .1  ij D min D l .0  D wij :  is the only non-1 among l .0  i k    1knÀöl .0  i i C wij  i k C wkj cid:9    l .0  i i  All simple shortest paths contain  n  cid:0  1 edges   ƒ±.i; j   D l .n cid:0 1   ij D l .nC1   D l .n   D : : :  ij  ij  Compute a solution bottom-up Compute L.1 ; L.2 ; : : : ; L.n cid:0 1 . Start with L.1  D W , since l .1  Go from L.m cid:0 1  to L.m :  ij D wij .  EXTEND.L; W; n   let L0 D cid:0 l0ij be a new n  n matrix for i D 1 to n  for j D 1 to n l0ij D 1 for k D 1 to n  return L0  l0ij D min.l0ij ; li k C wkj    Compute each L.m :  SLOW-APSP.W; n  L.1  D W for m D 2 to n  cid:0  1  let L.m  be a new n  n matrix L.m  D EXTEND.L.m cid:0 1 ; W; n   return L.n cid:0 1    Lecture Notes for Chapter 25: All-Pairs Shortest Paths  25-3  Time  EXTEND: ‚Äö.n3 .  SLOW-APSP: ‚Äö.n4 .  Observation EXTEND is like matrix multiplication: L ! A W ! B L0 ! C min ! C C !  1 ! 0 let C be an n  n matrix for i D 1 to n cij D 0 for k D 1 to n  for j D 1 to n  cij D cij C ai k  bkj  return C  So, we can view EXTEND as just like matrix multiplication! Why do we care? Because our goal is to compute L.n cid:0 1  as fast as we can. Don‚Äôt need to compute all the intermediate L.1 ; L.2 ; L.3 ; : : : ; L.n cid:0 2 . Suppose we had a matrix A and we wanted to compute An cid:0 1  like calling EXTEND n  cid:0  1 times . Could compute A; A2; A4; A8; : : : If we knew Am D An cid:0 1 for all m  n  cid:0  1, could just Ô¨Ånish with Ar, where r is the smallest power of 2 that‚Äôs  n  cid:0  1.  r D 2dlg.n cid:0 1 e  FASTER-APSP.W; n  L.1  D W m D 1 while m < n  cid:0  1  let L.2m  be a new n  n matrix L.2m  D EXTEND.L.m ; L.m ; n  m D 2m return L.m   OK to overshoot, since products don‚Äôt change after L.n cid:0 1 .  Time ‚Äö.n3 lg n .   25-4  Lecture Notes for Chapter 25: All-Pairs Shortest Paths  Floyd-Warshall algorithm  A different dynamic-programming approach. For path p D h1; 2; : : : ; li, an intermediate vertex is any vertex of p other than 1 or l. Let d .k  ij D shortest-path weight of any path i ; j with all intermediate vertices in f1; 2; : : : ; kg. Consider a shortest path i  ; j with all intermediate vertices in f1; 2; : : : ; kg:  p  If k is not an intermediate vertex, then all intermediate vertices of p are in f1; 2; : : : ; k  cid:0  1g. If k is an intermediate vertex:      p1  i  p2  k  j  all intermediate vertices in {1, 2, ..., k‚Äì1}  Recursive formulation  d .k   ij D  wij  if k D 0 ; if k  1 :  ij  kj  ; d .k cid:0 1    Have d .0   i k C d .k cid:0 1   min cid:0 d .k cid:0 1  ij D wij because can‚Äôt have intermediate vertices    1 edge.  Want D.n  D cid:0 d .n   ij , since all vertices numbered  n.    Compute bottom-up  Compute in increasing order of k:  FLOYD-WARSHALL.W; n  D.0  D W for k D 1 to n  let D.k  D cid:0 d .k  for i D 1 to n for j D 1 to n d .k   ij  be a new n  n matrix ij D min cid:0 d .k cid:0 1   ; d .k cid:0 1   ij  i k C d .k cid:0 1   kj    return D.n   Can drop superscripts.  See Exercise 25.2-4 in text.   Time ‚Äö.n3 .   Lecture Notes for Chapter 25: All-Pairs Shortest Paths  25-5  Transitive closure  Given G D .V; E , directed. Compute G D .V; E .  E D f.i; j   W there is a path i ; j in Gg. Could assign weight of 1 to each edge, then run FLOYD-WARSHALL.    If dij < n, then there is a path i ; j .   Otherwise, dij D 1 and there is no path.  Simpler way Substitute other values and operators in FLOYD-WARSHALL.   Use unweighted adjacency matrix  min ! _  OR   C ! ^  AND         t .0   t .k   0 otherwise :  ij D  1 if there is path i ; j with all intermediate vertices in f1; 2; : : : ; kg ; ij D  0 if i ¬§ j and .i; j   ‚Ä¶ E ; 1 if i D j or .i; j   2 E : . ij D t .k cid:0 1  t .k   _ cid:0 t .k cid:0 1   ^ t .k cid:0 1   i k  kj  ij  TRANSITIVE-CLOSURE .G; n  n D jG:Vj let T .0  D cid:0 t .0  for i D 1 to n  ij  be a new n  n matrix for j D 1 to n if i == j or .i; j   2 G:E t .0  ij D 1 else t .0  ij D 0 for k D 1 to n ij  be a new n  n matrix let T .k  D cid:0 t .k  for i D 1 to n for j D 1 to n ^ t .k cid:0 1  ij D t .k cid:0 1  t .k   kj  ij  return T .n   i k  _ cid:0 t .k cid:0 1     Time ‚Äö.n3 , but simpler operations than FLOYD-WARSHALL.   25-6  Lecture Notes for Chapter 25: All-Pairs Shortest Paths  Johnson‚Äôs algorithm  Idea If the graph is sparse, it pays to run Dijkstra‚Äôs algorithm once from each vertex. If we use a Fibonacci heap for the priority queue, the running time is down to O.V 2 lg V C VE , which is better than FLOYD-WARSHALL‚Äôs ‚Äö.V 3  time if E D o.V 2 . But Dijkstra‚Äôs algorithm requires that all edge weights be nonnegative. Donald Johnson Ô¨Ågured out how to make an equivalent graph that does have all edge weights  0.  Reweighting  path u ;  using yw.  Compute a new weight function yw such that 1. For all u;  2 V , p is a shortest path u ;  using w if and only if p is a shortest 2. For all .u;   2 E; yw.u;    0. Property  1  says that it sufÔ¨Åces to Ô¨Ånd shortest paths with yw. Property  2  says we can do so by running Dijkstra‚Äôs algorithm from each vertex. How to come up with yw? Lemma shows it‚Äôs easy to get property  1 :  Lemma  Reweighting doesn‚Äôt change shortest paths  Given a directed, weighted graph G D .V; E ; w W E ! R. Let h be any function such that h W V ! R. For all .u;   2 E, deÔ¨Åne yw.u;   D w.u;   C h.u   cid:0  h.  : Let p D h0; 1; : : : ; ki be any path 0 ; k. Then p is a shortest path 0 ; k with w if and only if p is a shortest path 0 ; k with yw.  Formally, w.p  D ƒ±.0; k  if and only if yw D yƒ±.0; k , where yƒ± is the shortest-path weight with yw.  Also, G has a negative-weight cycle with w if and only if G has a negative-weight cycle with yw. Proof First, we‚Äôll show that yw.p  D w.p  C h.0   cid:0  h.k : yw.p  D  yw.i cid:0 1; i    k  k  XiD1 XiD1 XiD1  k  D  .w.i cid:0 1; i   C h.i cid:0 1   cid:0  h.i     w.i cid:0 1; i   C h.0   cid:0  h.k   D D w.p  C h.0   cid:0  h.k  :   sum telescopes    Lecture Notes for Chapter 25: All-Pairs Shortest Paths  25-7  p  Therefore, any path 0 ; k has yw.p  D w.p  C h.0   cid:0  h.k . Since h.0  and h.k  don‚Äôt depend on the path from 0 to k, if one path 0 ; k is shorter than another with w, it‚Äôs also shorter with yw. Now show there exists a negative-weight cycle with w if and only if there exists a negative-weight cycle with yw:  Let cycle c D h0; 1; : : : ; ki, where 0 D k.  Then  yw.c  D w.c  C h.0   cid:0  h.k   D w.c    since 0 D k  .  Therefore, c has a negative-weight cycle with w if and only if it has a negative-  lemma  weight cycle with yw. So, now to get property  2 , we just need to come up with a function h W V ! R such that when we compute yw.u;   D w.u;   C h.u   cid:0  h. , it‚Äôs  0. Do what we did for difference constraints:  G0 D .V 0; E0    V 0 D V [ fsg, where s is a new vertex.  E0 D E [ f.s;   W  2 V g.  w.s;   D 0 for all  2 V .   Since no edges enter s, G0 has the same set of cycles as G. In particular, G0 has  a negative-weight cycle if and only if G does.  DeÔ¨Åne h.  D ƒ±.s;   for all  2 V . Claim yw.u;   D w.u;   C h.u   cid:0  h.   0. Proof By the triangle inequality, ƒ±.s;    ƒ±.s; u  C w.u;   h.   h.u  C w.u;   : Therefore, w.u;   C h.u   cid:0  h.   0.   claim    25-8  Lecture Notes for Chapter 25: All-Pairs Shortest Paths  Johnson‚Äôs algorithm  form G0 run BELLMAN-FORD on G0 to compute ƒ±.s;   for all  2 G0:V if BELLMAN-FORD returns FALSE G has a negative-weight cycle  else compute yw.u;   D w.u;   C ƒ±.s; u   cid:0  ƒ±.s;   for all .u;   2 E  let D D .du  be a new n  n matrix for each vertex u 2 G:V  run Dijkstra‚Äôs algorithm from u using weight function yw for each vertex  2 G:V  to compute yƒ±.u;   for all  2 V    Compute entry du in matrix D. du D yƒ±.u;   C ƒ±.s;    cid:0  ƒ±.s; u  ‚Ä¶ because if p is a path u ; , then yw.p  D w.p  C h.u   cid:0  h.   ∆í‚Äö  ‚Äû  return D  Time  ‚Äö.V C E  to compute G0.  O.VE  to run BELLMAN-FORD.  ‚Äö.E  to compute yw.  O.V 2 lg V C VE  to run Dijkstra‚Äôs algorithm jV j times  using Fibonacci heap .  ‚Äö.V 2  to compute D matrix. Total: O.V 2 lg V C VE .   Solutions for Chapter 25: All-Pairs Shortest Paths  Solution to Exercise 25.1-3 This solution is also posted publicly  The matrix L.0  corresponds to the identity matrix  :::  :::  : : :  0 1 0  0 0 0 1  0 ::: :::  I D cid:0  1 0 0  0 0 0 0  1  Solution to Exercise 25.1-5 This solution is also posted publicly  of regular matrix multiplication. Substitute 0  the identity for C  for 1  the iden- tity for min , and 1  the identity for   for 0  the identity for C .  ij  The all-pairs shortest-paths algorithm in Section 25.1 computes L.n cid:0 1  D W n cid:0 1 D L.0   W n cid:0 1 ; where l .n cid:0 1  D ƒ±.i; j   and L.0  is the identity matrix. That is, the entry in the ith row and j th column of the matrix ‚Äúproduct‚Äù is the shortest-path distance from vertex i to vertex j , and row i of the product is the solution to the single-source shortest-paths problem for vertex i. Notice that in a matrix ‚Äúproduct‚Äù C D A  B, the ith row of C is the ith row of A ‚Äúmultiplied‚Äù by B. Since all we want is the ith row of C , we never need more than the ith row of A. Thus the solution to the single-source shortest-paths from vertex i is L.0  where L.0  entries are 1. Doing the above ‚Äúmultiplications‚Äù starting from the left is essentially the same as the BELLMAN-FORD algorithm. The vector corresponds to the d values in BELLMAN-FORD‚Äîthe shortest-path estimates from the source to each vertex.   W n cid:0 1, is the ith row of L.0 ‚Äîa vector whose ith entry is 0 and whose other  i  i   25-10  Solutions for Chapter 25: All-Pairs Shortest Paths  the values set up for d by INITIALIZE-SINGLE-SOURCE.   The vector is initially 0 for the source and 1 for all other vertices, the same as  Each ‚Äúmultiplication‚Äù of the current vector by W relaxes all edges just as BELLMAN-FORD does. That is, a distance estimate in the row, say the distance to , is updated to a smaller estimate, if any, formed by adding some w.u;   to the current estimate of the distance to u.   The relaxation multiplication is done n  cid:0  1 times.  Solution to Exercise 25.1-10  Run SLOW-ALL-PAIRS-SHORTEST-PATHS on the graph. Look at the diagonal el- ements of L.m . Return the Ô¨Årst value of m for which one  or more  of the diagonal elements  l .m    is negative. If m reaches n C 1, then stop and declare that there are no negative-weight cycles. Let the number of edges in a minimum-length negative-weight cycle be m, where m D 1 if the graph has no negative-weight cycles.  i i  i i  Correctness Let‚Äôs assume that for some value m  n and some value of i, we Ô¨Ånd that l .m  < 0. Then the graph has a cycle with m edges that goes from vertex i to itself, and this cycle has negative weight  stored in l .m   . This is the minimum- length negative-weight cycle because SLOW-ALL-PAIRS-SHORTEST-PATHS com- putes all paths of 1 edge, then all paths of 2 edges, and so on, and all cycles shorter than m edges were checked before and did not have negative weight. Now assume that for all m  n, there is no negative l .m  element. Then, there is no negative- weight cycle in the graph, because all cycles have length at most n.  i i  i i  Time O.n4 . More precisely, ‚Äö.n3  min.n; m  .  Faster solution  Run FASTER-ALL-PAIRS-SHORTEST-PATHS on the graph until the Ô¨Årst time that the matrix L.m  has one or more negative values on the diagonal, or until we have computed L.m  for some m > n. If we Ô¨Ånd any negative entries on the diagonal, we know that the minimum-length negative-weight cycle has more than m=2 edges and at most m edges. We just need to binary search for the value of m in the range m=2 < m  m. The key observation is that on our way to computing L.m , we computed L.1 , L.2 , L.4 , L.8 , . . . , L.m=2 , and these matrices sufÔ¨Åce to compute every matrix we‚Äôll need. Here‚Äôs pseudocode:   Solutions for Chapter 25: All-Pairs Shortest Paths  25-11  FIND-MIN-LENGTH-NEG-WEIGHT-CYCLE .W   n D W:rows L.1  D W m D 1 while m  n and no diagonal entries of L.m  are negative L.2m  D EXTEND-SHORTEST-PATHS.L.m ; L.m   m D 2m if m > n and no diagonal entries of L.m  are negative return ‚Äúno negative-weight cycles‚Äù  elseif m  2 return m else  low D m=2 high D m d D m=4 while d  1  high D s else low D s d D d=2  return high  s D low C d L.s  D EXTEND-SHORTEST-PATHS.L.low ; L.d    if L.s  has any negative entries on the diagonal  Correctness If, after the Ô¨Årst while loop, m > n and no diagonal entries of L.m  are negative, then there is no negative-weight cycle. Otherwise, if m  2, then either m D 1 or m D 2, and L.m  is the Ô¨Årst matrix with a negative entry on the diagonal. Thus, the correct value to return is m. If m > 2, then we maintain an interval bracketed by the values low and high, such that the correct value m is in the range low < m  high. We use the following loop invariant:  Loop invariant: At the start of each iteration of the ‚Äúwhile d  1‚Äù loop, 1. d D 2p for some integer p   cid:0 1, 2. d D .high  cid:0  low =2, 3. low < m  high.  Initialization: Initially, m is an integer power of 2 and m > 2. Since d D m=4, we have that d is an integer power of 2 and d > 1=2, so that d D 2p for some integer p  0. We also have .high  cid:0  low =2 D .m  cid:0  .m=2  =2 D m=4 D d . Finally, L.m  has a negative entry on the diagonal and L.m=2  does not. Since low D m=2 and high D m, we have that low < m  high. Maintenance: We use high, low, and d to denote variable values in a given it- eration, and high0, low0, and d 0 to denote the same variable values in the next iteration. Thus, we wish to show that d D 2p for some integer p   cid:0 1 im- plies d 0 D 2p0 for some integer p0   cid:0 1, that d D .high  cid:0  low =2 implies d 0 D .high0  cid:0  low0 =2, and that low < m  high implies low0 < m  high0.   25-12  Solutions for Chapter 25: All-Pairs Shortest Paths  To see that d 0 D 2p0, note that d 0 D d=2, and so d D 2p cid:0 1. The condition that d  1 implies that p  0, and so p0   cid:0 1. Within each iteration, s is set to low C d , and one of the following actions occurs:      If L.s  has any negative entries on the diagonal, then high0 is set to s and d 0 is set to d=2. Upon entering the next iteration, .high0  cid:0  low0 =2 D .s  cid:0  low0 =2 D ..lowC d   cid:0  low =2 D d=2 D d 0. Since L.s  has a negative diagonal entry, we know that m  s. Because high0 D s and low0 D low, we have that low0 < m  high0. If L.s  has no negative entries on the diagonal, then low0 is set to s, and d 0 is set to d=2. Upon entering the next iteration, .high0  cid:0  low0 =2 D .high0  cid:0  s =2 D .high cid:0 .lowCd   =2 D .high cid:0 low =2 cid:0 d=2 D d  cid:0 d=2 D d=2 D d 0. Since L.s  has no negative diagonal entries, we know that m > s. Because low0 D s and high0 D high, we have that low0 < m  high0.  Termination: At termination, d < 1. Since d D 2p for some integer p   cid:0 1, we must have p D  cid:0 1, so that d D 1=2. By the second part of the loop invariant, if we multiply both sides by 2, we get that high  cid:0  low D 2d D 1. By the third part of the loop invariant, we know that low < m  high. Since high  cid:0  low D 2d D 1 and m > low, the only possible value for m is high, which the procedure returns.  Time If there is no negative-weight cycle, the Ô¨Årst while loop iterates ‚Äö.lg n  times, and the total time is ‚Äö.n3 lg n . Now suppose that there is a negative-weight cycle. We claim that each time we call EXTEND-SHORTEST-PATHS.L.low ; L.d   , we have already computed L.low  and L.d  . Initially, since low D m=2, we had already computed L.low  in the Ô¨Årst while loop. In succeeding iterations of the second while loop, the only way that low changes is when it gets the value of s, and we have just computed L.s . As for L.d  , observe that d takes on the values m=4; m=8; m=16; : : : ; 1, and again, we computed all of these L matrices in the Ô¨Årst while loop. Thus, the claim is proven. Each of the two while loops iterates ‚Äö.lg m  times. Since we have already computed the parameters to each call of EXTEND-SHORTEST-PATHS, each iteration is dominated by the ‚Äö.n3 -time call to EXTEND-SHORTEST-PATHS. Thus, the total time is ‚Äö.n3 lg m . In general, therefore, the running time is ‚Äö.n3 lg min.n; m  .  Space The slower algorithm needs to keep only three matrices at any time, and so its space requirement is ‚Äö.n3 . This faster algorithm needs to maintain ‚Äö.lg min.n; m   matrices, and so the space requirement increases to ‚Äö.n3 lg min.n; m  .   Solutions for Chapter 25: All-Pairs Shortest Paths  25-13  Solution to Exercise 25.2-4 This solution is also posted publicly  Solution to Exercise 25.2-6  ij  ij  ij  kj  kj  ; d .k cid:0 1   . If,  i k C d .k cid:0 1   ; d .k  ; d .k cid:0 1  ; d .k   ij D min cid:0 d .k cid:0 1   With the superscripts, the computation is d .k  having dropped the superscripts, we were to compute and store di k or dkj before using these values to compute dij , we might be computing one of the following: d .k  d .k  d .k   ij D min cid:0 d .k cid:0 1  ij D min cid:0 d .k cid:0 1  ij D min cid:0 d .k cid:0 1  In any of these scenarios, we‚Äôre computing the weight of a shortest path from i to j i k , rather than d .k cid:0 1  with all intermediate vertices in f1; 2; : : : ; kg. If we use d .k  , in the computation, then we‚Äôre using a subpath from i to k with all intermediate vertices in f1; 2; : : : ; kg. But k cannot be an intermediate vertex on a shortest path from i to k, since otherwise there would be a cycle on this shortest path. Thus, i k D d .k cid:0 1  d .k  . Hence, we can drop the superscripts in the computation.   ; i k C d .k cid:0 1  kj  ; i k C d .k  kj  : i k C d .k   . A similar argument applies to show that d .k   kj D d .k cid:0 1   i k  i k  kj  ij  Here are two ways to detect negative-weight cycles:  1. Check the main-diagonal entries of the result matrix for a negative value. There  is a negative weight cycle if and only if d .n   i i < 0 for some vertex i:        is a path weight from i to itself; so if it is negative, there is a path from i   d .n  i i to itself  i.e., a cycle , with negative weight. If there is a negative-weight cycle, consider the one with the fewest vertices. If it has just one vertex, then some wi i < 0, so di i starts out negative, and since d values are never increased, it is also negative when the algorithm terminates. If it has at least two vertices, let k be the highest-numbered vertex in the cycle, and let i be some other vertex in the cycle. d .k cid:0 1  have correct shortest-path weights, because they are not based on negative- weight cycles.  Neither d .k cid:0 1  can include k as an intermedi- ate vertex, and i and k are on the negative-weight cycle with the fewest vertices.  Since i ; k ; i is a negative-weight cycle, the sum of those two weights is negative, so d .k  i i will be set to a negative value. Since d values are never increased, it is also negative when the algorithm termi- nates.  and d .k cid:0 1   nor d .k cid:0 1   i k  i k  ki  ki  In fact, it sufÔ¨Åces to check whether d .n cid:0 1  < 0 for some vertex i. Here‚Äôs why. A negative-weight cycle containing vertex i either contains vertex n or it does not. If it does not, then clearly d .n cid:0 1  < 0. If the negative-weight cycle contains  i i  i i   25-14  Solutions for Chapter 25: All-Pairs Shortest Paths  . This value must be negative, since the cycle, vertex n, then consider d .n cid:0 1  starting and ending at vertex n, does not include vertex n as an intermediate vertex.  nn  2. Alternatively, one could just run the normal FLOYD-WARSHALL algorithm one extra iteration to see if any of the d values change. If there are negative cycles, then some shortest-path cost will be cheaper. If there are no such cycles, then no d values will change because the algorithm gives the correct shortest paths.  Solution to Exercise 25.3-4 This solution is also posted publicly  It changes shortest paths. Consider the following graph. V D fs; x; y; ¬¥g, and there are 4 edges: w.s; x  D 2, w.x; y  D 2, w.s; y  D 5, and w.s; ¬¥  D  cid:0 10. So we‚Äôd add 10 to every weight to make yw. With w, the shortest path from s to y is s ! x ! y, with weight 4. With yw, the shortest path from s to y is s ! y, with weight 15.  The path s ! x ! y has weight 24.  The problem is that by just adding the same amount to every edge, you penalize paths with more edges, even if their weights are low.  Solution to Exercise 25.3-6  In this solution, we assume that 1  cid:0  1 is undeÔ¨Åned; in particular, it‚Äôs not 0. Let G D .V; E , where V D fs; ug, E D f.u; s g, and w.u; s  D 0. There is only one edge, and it enters s. When we run Bellman-Ford from s, we get h.s  D ƒ±.s; s  D 0 and h.u  D ƒ±.s; u  D 1. When we reweight, we get yw.u; s  D 0 C 1  cid:0  0 D 1. We compute yƒ±.u; s  D 1, and so we compute dus D 1 C 0  cid:0  1 ¬§ 0. Since ƒ±.u; s  D 0, we get an incorrect answer. If the graph G is strongly connected, then we get h.  D ƒ±.s;   < 1 for all vertices  2 V . Thus, the triangle inequality says that h.   h.u Cw.u;   for all edges .u;   2 E, and so yw.u;   D w.u;  Ch.u  cid:0 h.   0. Moreover, all edge weights yw.u;   used in Lemma 25.1 are Ô¨Ånite, and so the lemma holds. Therefore, the conditions we need in order to use Johnson‚Äôs algorithm hold: that reweighting does not change shortest paths, and that all edge weights yw.u;   are nonnegative. Again relying on G being strongly connected, we get that yƒ±.u;   < 1 for all edges .u;   2 E, which means that du D yƒ±.u;   C h.   cid:0  h.u  is Ô¨Ånite and correct.  Solution to Problem 25-1  a. Let T D .tij   be the jV j  jV j matrix representing the transitive closure, such  that tij is 1 if there is a path from i to j , and 0 otherwise.   Solutions for Chapter 25: All-Pairs Shortest Paths  25-15  Initialize T  when there are no edges in G  as follows:  tij D  1 if i D j ;  0 otherwise :  We update T as follows when an edge .u;   is added to G:  TRANSITIVE-CLOSURE-UPDATE .T; u;   let T be jV j  jV j for i D 1 to jV j  if ti u == 1 and tj == 1  for j D 1 to jV j tij D 1   With this procedure, the effect of adding edge .u;   is to create a path  via the new edge  from every vertex that could already reach u to every vertex that could already be reached from .   Note that the procedure sets tu D 1, because both tuu and t are initialized  This procedure takes ‚Äö.V 2  time because of the two nested loops.  to 1.  b. Consider inserting the edge .jV j; 1  into the straight-line graph 1 ! 2 !  ! jV j. Before this edge is inserted, only jV j .jV j C 1 =2 entries in T are 1  the entries on and above the main diagonal . After the edge is inserted, the graph is a cycle in which every vertex can reach every other vertex, so all jV j2 entries in T are 1. Hence jV j2 cid:0  .jV j .jV jC 1 =2  D ‚Äö.V 2  entries must be changed in T , so any algorithm to update the transitive closure must take .V 2  time on this graph.  c. The algorithm in part  a  would take ‚Äö.V 4  time to insert all possible ‚Äö.V 2  edges, so we need a more efÔ¨Åcient algorithm in order for any sequence of in- sertions to take only O.V 3  total time. To improve the algorithm, notice that the loop over j is pointless when ti  D 1. That is, if there is already a path i ; , then adding the edge .u;   cannot make any new vertices reachable from i. The loop to set tij to 1 for j such that there exists a path  ; j is just setting entries that are already 1. Eliminate this redundant processing as follows:  TRANSITIVE-CLOSURE-UPDATE .T; u;   let T be jV j  jV j for i D 1 to jV j  if ti u == 1 and ti  == 0 for j D 1 to jV j if tj == 1 tij D 1  We show that this procedure takes O.V 3  time to update the transitive closure for any sequence of n insertions:  There cannot be more than jV j2 edges in G, so n  jV j2.   25-16  Solutions for Chapter 25: All-Pairs Shortest Paths   Summed over n insertions, the time for the outer for loop header and the test  for ti u == 1 and ti  == 0 is O.nV   D O.V 3 .  The last three lines, which take ‚Äö.V   time, are executed only O.V 2  times for n insertions. To see why, notice that the last three lines are executed only when ti  equals 0, and in that case, the last line sets ti  D 1. Thus, the number of 0 entries in T is reduced by at least 1 each time the last three lines run. Since there are only jV j2 entries in T , these lines can run at most jV j2 times.   Hence, the total running time over n insertions is O.V 3 .   Lecture Notes for Chapter 26: Maximum Flow  Chapter 26 overview  Network Ô¨Çow  [ThethirdeditiontreatsÔ¨Çownetworks differently fromtheÔ¨Årsttwoeditions. The conceptofnetÔ¨Çowisgone,exceptthatwedodiscussnetÔ¨Çowacrossacut. Skew symmetryisalsogone,asisimplicitsummationnotation. Thethirdeditioncounts Ô¨Çowsonedgesdirectly. WeÔ¨Åndthatalthoughthemathematicsisnotquiteasslick asintheÔ¨Årsttwoeditions,theapproachinthethirdeditionmatchesintuitionmore closely,andthereforestudentstendtopickitupmorequickly.] Use a graph to model material that Ô¨Çows through conduits. Each edge represents one conduit, and has a capacity, which is an upper bound on the Ô¨Çow rate D units time. Can think of edges as pipes of different sizes. But Ô¨Çows don‚Äôt have to be of liquids. Book has an example where a Ô¨Çow is how many trucks per day can ship hockey pucks between cities. Want to compute max rate that we can ship material from a designated source to a designated sink.  Flow networks  G D .V; E  directed. Each edge .u;   has a capacity c.u;    0. If .u;   62 E, then c.u;   D 0. If .u;   2 E, then reverse edge .; u  62 E.  Can work around this restriction.  Source vertex s, sink vertex t, assume s ;  ; t for all  2 V , so that each vertex lies on a path from source to sink. Example: [Edgesarelabeledwithcapacities.]   26-2  Lecture Notes for Chapter 26: Maximum Flow  s  3  2  w  3  y  2  1  3  x  3  z  3  2  t  t  Flow A function f W V  V ! R satisfying  Capacity constraint: For all u;  2 V; 0  f .u;    c.u;  ,  Flow conservation: For all u 2 V  cid:0  fs; tg,X2V D X2V ‚Äû ∆í‚Äö ‚Ä¶ Equivalently,X2V f .; u  D 0.  f .u;    cid:0 X2V  ‚Äû ∆í‚Äö ‚Ä¶  Ô¨Çow into u  f .; u   f .u;    Ô¨Çow out of u  .  [Add Ô¨Çows to previous example. Edges here are labeled as Ô¨Çow capacity. Leave onboard.]  s  1 3  1 1  1 3  1   3  2 2  w  2 2  y  2 3  x  z  2 3  1   2   Note that all Ô¨Çows are  capacities.  Verify Ô¨Çow conservation by adding up Ô¨Çows at a couple of vertices.  Note that all Ô¨Çows D 0 is legitimate. Value of Ô¨Çow f D jf j D X2V D Ô¨Çow out of source  cid:0  Ô¨Çow into source :  f .s;    cid:0 X2V  f .; s   In the example above, value of Ô¨Çow f D jf j D 3.  Maximum-Ô¨Çow problem Given G, s, t, and c, Ô¨Ånd a Ô¨Çow whose value is maximum.  Antiparallel edges DeÔ¨Ånition of Ô¨Çow network does not allow both .u;   and .; u  to be edges. These edges would be antiparallel. What if we really need antiparallel edges?   Lecture Notes for Chapter 26: Maximum Flow  26-3   Choose one of them, say .u;  .  Create a new vertex 0.  Replace .u;   by two new edges .u; 0  and .0;  , with c.u; 0  D c.0;   D  Get an equivalent Ô¨Çow network with no antiparallel edges.  c.u;  .  Cuts  A cut .S; T   of Ô¨Çow network G D .V; E  is a partition of V into S and T D V  cid:0 S such that s 2 S and t 2 T .  Similar to cut used in minimum spanning trees, except that here the graph is  directed, and we require s 2 S and t 2 T . For Ô¨Çow f , the net Ô¨Çow across cut .S; T   is f .S; T   DXu2SX2T c.S; T   DXu2SX2T  f .u;    cid:0 Xu2SX2T  Capacity of cut .S; T   is  c.u;   :  f .; u  :  A minimum cut of G is a cut whose capacity is minimum over all cuts of G. Asymmetry between net Ô¨Çow across a cut and capacity of a cut: For capacity, count only capacities of edges going from S to T . Ignore edges going in the reverse direction. For net Ô¨Çow, count Ô¨Çow on all edges across the cut: Ô¨Çow on edges going from S to T minus Ô¨Çow on edges going from T to S. In previous example, consider the cut S D fs; w; yg ; T D fx; ¬¥; tg. f .S; T   D f .w; x  C f .y; ¬¥  ‚Ä¶   cid:0  f .x; y  ‚Äû∆í‚Äö‚Ä¶ from T to S  from S to T  ∆í‚Äö ‚Äû D 2 C 2  cid:0  1 D 3 : ‚Äû D 2 C 3 D 5 :  ∆í‚Äö  c.S; T   D c.w; x  C c.y; ¬¥  ‚Ä¶  from S to T  Now consider the cut S D fs; w; x; yg ; T D f¬¥; tg. f .S; T   D f .x; t   C f .y; ¬¥  ‚Ä¶   cid:0  f .¬¥; x  ‚Äû∆í‚Äö‚Ä¶ from T to S  from S to T  ‚Äû ∆í‚Äö D 2 C 2  cid:0  1 D 3 : ‚Äû D 3 C 3 D 6 :  ∆í‚Äö  c.S; T   D c.x; t   C c.y; ¬¥  ‚Ä¶  from S to T  Same Ô¨Çow as previous cut, higher capacity.   26-4  Lecture Notes for Chapter 26: Maximum Flow  Lemma For any cut .S; T  , f .S; T   D jf j.  Net Ô¨Çow across the cut equals value of the Ô¨Çow.  [Leaveonboard.]  f .; u  D 0 :  [This proof is much more involved than the proof in the Ô¨Årst two editions. You might want to omit it, or just give the intuition that no matter where you cut the pipesinanetwork,you‚ÄôllseethesameÔ¨Çowvolumecomingoutoftheopenings.] Proof Rewrite Ô¨Çow conservation: for any u 2 V  cid:0  fs; tg, X2V f .u;    cid:0 X2V Take deÔ¨Ånition of jf j and add in left-hand side of above equation, summed over all vertices in S  cid:0  fsg. Above equation applies to each vertex in S  cid:0  fsg  since t 62 S and obviously s 62 S  cid:0  fsg , so just adding in lots of 0s: f .u;    cid:0 X2V jf j DX2V f .u;    cid:0  Xu2S cid:0 fsgX2V jf j D X2V f .u;  !  cid:0 X2V f .; s  C Xu2S cid:0 fsg   X2V f .; s  C Xu2S cid:0 fsg f .; s  C Xu2S cid:0 fsgX2V  f .s;    cid:0 X2V f .s;    cid:0 X2V  Expand right-hand summation and regroup terms:  f .; u ! :  f .; u !  f .; u   D X2V f .s;   C Xu2S cid:0 fsg f .u;    cid:0 X2VXu2S D X2VXu2S  Partition V into S [ T and split each summation over V into summations over S and T : jf j D X2SXu2S D X2TXu2S  f .u;    cid:0 X2SXu2S  f .; u   cid:0 X2TXu2S  f .; u   f .; u   f .; u  :  f .u;   CX2TXu2S f .u;    cid:0 X2TXu2S C X2SXu2S  f .u;    cid:0 X2SXu2S  f .; u ! :  Summations within parentheses are the same, since f .x; y  appears once in each summation, for any x; y 2 V . These summations cancel: jf j D Xu2SX2T D f .S; T   :  f .u;    cid:0 Xu2SX2T   lemma   f .; u   Corollary The value of any Ô¨Çow  capacity of any cut. [Leaveonboard.]   The Ford-Fulkerson method  Residual network  Lecture Notes for Chapter 26: Maximum Flow  26-5  f .u;    cid:0 Xu2SX2T  Proof Let .S; T   be any cut, f be any Ô¨Çow. jf j D f .S; T   D Xu2SX2T  Xu2SX2T  Xu2SX2T D c.S; T   :  f .u;    c.u;     lemma   f .; u   deÔ¨Ånition of f .S; T      f .; u   0   capacity constraint   Therefore, maximum Ô¨Çow  capacity of minimum cut. Will see a little later that this is in fact an equality.   deÔ¨Ånition of c.S; T      corollary   Given a Ô¨Çow f in network G D .V; E . Consider a pair of vertices u;  2 V . How much additional Ô¨Çow can we push directly from u to ? That‚Äôs the residual capacity,  cf .u;   D¬Ä c.u;    cid:0  f .u;    f .; u   0  if .u;   2 E ; if .; u  2 E ; otherwise  i.e., .u;  ; .; u  62 E  :  The residual network is Gf D .V; Ef  , where Ef D f.u;   2 V  V W cf .u;   > 0g : Each edge of the residual network can admit a positive Ô¨Çow. For our example:  Gf  s  2  1  2  w  y  2  1  1 2  x  z  1  2  1  1  1  2  2  1  t  Every edge .u;   2 Ef corresponds to an edge .u;   2 E or .; u  2 E  or both . Therefore, jEf j  2jEj. Residual network is similar to a Ô¨Çow network, except that it may contain antiparal- lel edges  .u;   and .; u  . Can deÔ¨Åne a Ô¨Çow in a residual network that satisÔ¨Åes the deÔ¨Ånition of a Ô¨Çow, but with respect to capacities cf in Gf . Given Ô¨Çows f in G and f 0 in Gf , deÔ¨Åne .f " f 0 , the augmentation of f by f 0, as a function V  V ! R:   26-6  Lecture Notes for Chapter 26: Maximum Flow  0  .f " f 0 .u;   D  f .u;   C f 0.u;    cid:0  f 0.; u  for all u;  2 V . Intuition: Increase the Ô¨Çow on .u;   by f 0.u;   but decrease it by f 0.; u  be- cause pushing Ô¨Çow on the reverse edge in the residual network decreases the Ô¨Çow in the original network. Also known as cancellation.  if .u;   2 E ; otherwise  Lemma Given a Ô¨Çow network G, a Ô¨Çow f in G, and the residual network Gf , let f 0 be a Ô¨Çow in Gf . Then f " f 0 is a Ô¨Çow in G with value jf " f 0j D jf j C jf 0j. [See book for proof. Ithas a lotof summations in it. Probably not worth writing ontheboard.]  Augmenting path  A simple path s ; t in Gf .   Admits more Ô¨Çow along each edge.  Like a sequence of pipes through which we can squirt more Ô¨Çow from s to t.  How much more Ô¨Çow can we push from s to t along augmenting path p? cf .p  D minfcf .u;   W .u;   is on pg : For our example, consider the augmenting path p D hs; w; y; ¬¥; x; ti. Minimum residual capacity is 1. After we push 1 additional unit along p: [Continue from G left on board from before. Edge.y; w  hasf .y; w  D 0,whichweomit,showingonlyc.y; w  D 3.]  G  s  Gf  s  2   3  2 2  1  2  2  1 1  2 3  w  3  y  w  3  y  2 2  3 3  2  1  3  x  z  x  z  1  2  3 3  1   2  3  1  1  t  t  Observe that Gf now has no augmenting path. Why? No edges cross the cut .fs; wg ;fx; y; ¬¥; tg  in the forward direction in Gf . So no path can get from s to t. Claim that the Ô¨Çow shown in G is a maximum Ô¨Çow.   Lecture Notes for Chapter 26: Maximum Flow  26-7  0  Lemma Given Ô¨Çow network G, Ô¨Çow f in G, residual network Gf . Let p be an augmenting path in Gf . DeÔ¨Åne fp W V  V ! R: fp.u;   D  cf .p  if .u;   is on p ; otherwise : Then fp is a Ô¨Çow in Gf with value jfpj D cf .p  > 0. Corollary Given Ô¨Çow network G, Ô¨Çow f in G, and an augmenting path p in Gf , deÔ¨Åne fp as in lemma. Then f " fp is a Ô¨Çow in G with value jf " fpj D jf j C jfpj > jf j. Theorem  Max-Ô¨Çow min-cut theorem  The following are equivalent:  1. f is a maximum Ô¨Çow. 2. Gf has no augmenting path. 3.  jf j D c.S; T   for some cut .S; T  .      Proof  1     2 : Show the contrapositive: if Gf has an augmenting path, then f is not a maximum Ô¨Çow. If Gf has augmenting path p, then by the above corollary, f " fp is a Ô¨Çow in G with value jf j C jfpj > jf j, so that f was not a maximum Ô¨Çow.  2     3 : Suppose Gf has no augmenting path. DeÔ¨Åne S D f 2 V W there exists a path s ;  in Gf g ; T D V  cid:0  S : Must have t 2 T ; otherwise there is an augmenting path. Therefore, .S; T   is a cut. Consider u 2 S and  2 T :  If .u;   2 E, must have f .u;   D c.u;  ; otherwise, .u;   2 Ef    2 S. If .; u  2 E, must have f .; u  D 0; otherwise, cf .u;   D f .; u  > 0   .u;   2 Ef    2 S. If .u;  ; .; u  62 E, must have f .u;   D f .; u  D 0. f .S; T   D Xu2SX2T D Xu2SX2T D c.S; T   :  f .u;    cid:0 X2TXu2S c.u;    cid:0 X2TXu2S By lemma, jf j D f .S; T   D c.S; T  .  3     1 : By corollary, jf j  c.S; T  . Therefore, jf j D c.S; T     f is a max Ô¨Çow.   theorem   f .; u   Then,  0     26-8  Lecture Notes for Chapter 26: Maximum Flow  Ford-Fulkerson algorithm  Keep augmenting Ô¨Çow along an augmenting path until there is no augmenting path. Represent the Ô¨Çow attribute using the usual dot-notation, but on an edge: .u;  :f .  FORD-FULKERSON.G; s; t   for all .u;   2 G:E while there is an augmenting path p in Gf  .u;  :f D 0 augment f by cf .p   Analysis If capacities are all integer, then each augmenting path raises jf j by  1. If max Ô¨Çow is f , then need  jf j iterations   time is O.E jf j . [Handwaving‚Äîseebookforbetterexplanation.] Note that this running time is not polynomial in input size. It depends on jf j, which is not a function of jV j and jEj. If capacities are rational, can scale them to integers. If irrational, FORD-FULKERSON might never terminate!  Edmonds-Karp algorithm  Do FORD-FULKERSON, but compute augmenting paths by BFS of Gf . Augment- ing paths are shortest paths s ; t in Gf , with all edge weights D 1. Edmonds-Karp runs in O.VE 2  time. To prove, need to look at distances to vertices in Gf . Let ƒ±f .u;   D shortest path distance u to  in Gf , with unit edge weights. Lemma For all  2 V  cid:0  fs; tg, ƒ±f .s;   increases monotonically with each Ô¨Çow augmenta- tion. Proof Suppose there exists  2 V  cid:0 fs; tg such that some Ô¨Çow augmentation causes ƒ±f .s;   to decrease. Will derive a contradiction. Let f be the Ô¨Çow before the Ô¨Årst augmentation that causes a shortest-path distance to decrease, f 0 be the Ô¨Çow afterward. Let  be a vertex with minimum ƒ±f 0 .s;   whose distance was decreased by the augmentation, so ƒ±f 0.s;   < ƒ±f .s;  . Let a shortest path s to  in Gf 0 be s ; u ! , so .u;   2 Ef 0 and ƒ±f 0.s;   D ƒ±f 0.s; u  C 1.  Or ƒ±f 0.s; u  D ƒ±f 0.s;    cid:0  1.  Since ƒ±f 0.s; u  < ƒ±f 0 .s;   and how we chose , we have ƒ±f 0.s; u   ƒ±f .s; u . Claim .u;   62 Ef .   Lecture Notes for Chapter 26: Maximum Flow  26-9  Proof If .u;   2 Ef , then ƒ±f .s;    ƒ±f .s; u  C 1  ƒ±f 0 .s; u  C 1 D ƒ±f 0 .s;   ;  contradicting ƒ±f 0.s;   < ƒ±f .s;  .   triangle inequality    claim   How can .u;   62 Ef and .u;   2 Ef 0? The augmentation must increase Ô¨Çow  to u. Since Edmonds-Karp augments along shortest paths, the shortest path s to u in Gf has .; u  as its last edge. Therefore, ƒ±f .s;   D ƒ±f .s; u   cid:0  1  ƒ±f 0 .s; u   cid:0  1 D ƒ±f 0 .s;    cid:0  2 ;  contradicting ƒ±f 0.s;   < ƒ±f .s;  . Therefore,  cannot exist.   lemma   Theorem Edmonds-Karp performs O.VE  augmentations. Proof Suppose p is an augmenting path and cf .u;   D cf .p . Then call .u;   a critical edge in Gf , and it disappears from the residual network after augmenting along p.  1 edge on any augmenting path is critical. Will show that each of the jEj edges can become critical  jV j =2 times. Consider u;  2 V such that either .u;   2 E or .; u  2 E or both. Since augmenting paths are shortest paths, when .u;   becomes critical Ô¨Årst time, ƒ±f .s;   D ƒ±f .s; u  C 1. Augment Ô¨Çow, so that .u;   disppears from the residual network. This edge cannot reappear in the residual network until Ô¨Çow from u to  decreases, which happens only if .; u  is on an augmenting path in Gf 0: ƒ±f 0.s; u  D ƒ±f 0.s;   C 1.  f 0 is Ô¨Çow when this occurs.  By lemma, ƒ±f .s;    ƒ±f 0.s;     ƒ±f 0 .s; u  D ƒ±f 0.s;   C 1  ƒ±f .s;   C 1 D ƒ±f .s; u  C 2 :  Therefore, from the time .u;   becomes critical to the next time, distance of u from s increases by  2. Initially, distance to u is  0, and augmenting path can‚Äôt have s, u, and t as intermediate vertices. Therefore, until u becomes unreachable from source, its distance is  jV j  cid:0  2   after .u;   becomes critical the Ô¨Årst time, it can become critical  .jV j  cid:0  2 =2 D jV j =2  cid:0  1 times more   .u;   can become critical  jV j =2 times.   26-10  Lecture Notes for Chapter 26: Maximum Flow  Since O.E  pairs of vertices can have an edge between them in residual network, total  of critical edges during execution of Edmonds-Karp is O.VE . Since each  theorem  augmenting path has  1 critical edge, have O.VE  augmentations. Use BFS to Ô¨Ånd each augmenting path in O.E  time   O.VE 2  time. Can get better bounds. Push-relabel algorithms in Sections 26.4‚Äì26.5 give O.V 3 . Can do even better.  Maximum bipartite matching  Example of a problem that can be solved by turning it into a Ô¨Çow problem. G D .V; E   undirected  is bipartite if we can partition V D L [ R such that all edges in E go between L and R.  L  R  L  R  matching  maximum matching  A matching is a subset of edges M  E such that for all  2 V ,  1 edge of M is incident on .  Vertex  is matched if an edge of M is incident on it; otherwise unmatched . Maximum matching: a matching of maximum cardinality. matching if jMj  jM 0j for all matchings M 0.    M is a maximum  Problem Given a bipartite graph  with the partition , Ô¨Ånd a maximum matching.  Application Matching planes to routes.  L D set of planes.  R D set of routes.    .u;   2 E if plane u can Ô¨Çy route .   Lecture Notes for Chapter 26: Maximum Flow  26-11   Want maximum  of routes to be served by planes. Given G, deÔ¨Åne Ô¨Çow network G0 D .V 0; E0 .  V 0 D V [ fs; tg.  E0 D f.s; u  W u 2 Lg [ f.u;   W .u;   2 Eg [ f.; t   W  2 Rg.    c.u;   D 1 for all .u;   2 E0.  s  t  Each vertex in V has  1 incident edge   jEj  jV j =2. Therefore, jEj  jE0j D jEj C jV j  3jEj. Therefore, jE0j D ‚Äö.E . Find a max Ô¨Çow in G0. Book shows that it will have integer values for all .u;  . Use edges that carry Ô¨Çow of 1 in matching. Book proves that this method produces a maximum matching.   Solutions for Chapter 26: Maximum Flow  Solution to Exercise 26.1-1  We will prove that for every Ô¨Çow in G D .V; E , we can construct a Ô¨Çow in G0 D .V 0; E0  that has the same value as that of the Ô¨Çow in G. The required result follows since a maximum Ô¨Çow in G is also a Ô¨Çow. Let f be a Ô¨Çow in G. By construction, V 0 D V [ fxg and E0 D .E  cid:0  f.u;  g  [ f.u; x ; .x;  g. Construct f 0 in G0 as follows: f 0.y; ¬¥  D  f .y; ¬¥   if .y; ¬¥  ¬§ .u; x  and .y; ¬¥  ¬§ .x;   ; if .y; ¬¥  D .u; x  or .y; ¬¥  D .x;   :  f .u;    Informally, f 0 is the same as f , except that the Ô¨Çow f .u;   now passes through an intermediate vertex x. The vertex x has incoming Ô¨Çow  if any  only from u, and has outgoing Ô¨Çow  if any  only to vertex . We Ô¨Årst prove that f 0 satisÔ¨Åes the required properties of a Ô¨Çow. It is obvious that the capacity constraint is satisÔ¨Åed for every edge in E0 and that every vertex in V 0  cid:0  fu; ; xg obeys Ô¨Çow conservation. To show that edges .u; x  and .x;   obey the capacity constraint, we have f .u; x  D f .u;    c.u;   D c.u; x  ; f .x;   D f .u;    c.u;   D c.x;   : We now prove Ô¨Çow conservation for u. Assuming that u 62 fs; tg, we have Xy2V 0  f 0.u; y  C f 0.u; x   f .u; y  C f .u;    f .u; y   f 0.u; y  D Xy2V 0 cid:0 fxg D Xy2V  cid:0 fg D Xy2V D Xy2V D Xy2V 0  f 0.y; u  :  For vertex , a symmetric argument proves Ô¨Çow conservation.  f .y; u    because f obeys Ô¨Çow conservation    Solutions for Chapter 26: Maximum Flow  26-13  For vertex x, we have  Xy2V 0  f 0.y; x  D f 0.u; x  D f 0.x;   D Xy2V 0  f 0.x; y  :  f 0.y; u   Thus, f 0 is a valid Ô¨Çow in G0. We now prove that the values of the Ô¨Çow in both cases are equal. If the source s is not in fu; g, the proof is trivial, since our construction assigns the same Ô¨Çows to incoming and outgoing edges of s. If s D u, then jf 0j D Xy2V 0 D Xy2V 0 cid:0 fxg D Xy2V  cid:0 fg D Xy2V D jf j :  f 0.u; y   cid:0  Xy2V 0 f 0.u; y   cid:0  Xy2V 0 f .u; y   cid:0 Xy2V f .u; y   cid:0 Xy2V  The case when s D  is symmetric. We conclude that f 0 is a valid Ô¨Çow in G0 with jf 0j D jf j.  f 0.y; u  C f 0.u; x   f .y; u  C f .u;    f .y; u   We show that, given any Ô¨Çow f 0 in the Ô¨Çow network G D .V; E , we can construct a Ô¨Çow f as stated in the exercise. The result will follow when f 0 is a maximum Ô¨Çow. The idea is that even if there is a path from s to the connected component of u, no Ô¨Çow can enter the component, since the Ô¨Çow has no path to reach t. Thus, all the Ô¨Çow inside the component must be cyclic, which can be made zero without affecting the net value of the Ô¨Çow. Two cases are possible: where u is not connected to t, and where u is not connected to s. We only analyze the former case. The analysis for the latter case is similar. Let Y be the set of all vertices that have no path to t. Our roadmap will be to Ô¨Årst prove that no Ô¨Çow can leave Y . We use this result and Ô¨Çow conservation to prove that no Ô¨Çow can enter Y . We shall then constuct the Ô¨Çow f , which has the required properties, and prove that jf j D jf 0j. The Ô¨Årst step is to prove that there can be no Ô¨Çow from a vertex y 2 Y to a vertex  2 V  cid:0  Y . That is, f 0.y;   D 0. This is so, because there are no edges .y;   in E. If there were an edge .y;   2 E, then there would be a path from y to t, which contradicts how we deÔ¨Åned the set Y . We will now prove that f 0.; y  D 0, too. We will do so by applying Ô¨Çow conser- vation to each vertex in Y and taking the sum over Y . By Ô¨Çow conservation, we have  Solution to Exercise 26.1-3   26-14  Solutions for Chapter 26: Maximum Flow  f 0.; y  :  Xy2YX2V Partitioning V into Y and V  cid:0  Y gives Xy2Y X2V  cid:0 Y f 0.y;    f 0.y;   DXy2YX2V f 0.y;   CXy2YX2Y D Xy2Y X2V  cid:0 Y f 0.y;   DXy2YX2Y  Xy2YX2Y  But we also have  f 0.; y  ;  f 0.; y  CXy2YX2Y  f 0.; y  :      since the left-hand side is the same as the right-hand side, except for a change of variable names  and y. We also have  f 0.; y  D 0 :  f 0.y;   D 0 ;  Xy2Y X2V  cid:0 Y since f 0.y;   D 0 for each y 2 Y and  2 V  cid:0  Y . Thus, equation    simpliÔ¨Åes to Xy2Y X2V  cid:0 Y Because the Ô¨Çow function is nonnegative, f .; y  D 0 for each  2 V and y 2 Y . We conclude that there can be no Ô¨Çow between any vertex in Y and any vertex in V  cid:0  Y . The same technique can show that if there is a path from u to t but not from s to u, and we deÔ¨Åne Z as the set of vertices that do not have have a path from s to u, then there can be no Ô¨Çow between any vertex in Z and any vertex in V  cid:0  Z. Let X D Y [ Z. We thus have f 0.; x  D f 0.x;   D 0 if x 2 X and  62 X. We are now ready to construct Ô¨Çow f : f .u;   D  f 0.u;   if u;  62 X ; otherwise :  0  We note that f satisÔ¨Åes the requirements of the exercise. We now prove that f also satisÔ¨Åes the requirements of a Ô¨Çow function. The capacity constraint is satisÔ¨Åed, since whenever f .u;   D f 0.u;  , we have f .u;   D f 0.u;    c.u;   and whenever f .u;   D 0, we have f .u;   D 0  c.u;  . For Ô¨Çow conservation, let x be some vertex other than s or t. If x 2 X, then from the construction of f , we have f .x;   DX2V X2V  f .; x  D 0 :   Solution to Exercise 26.1-4  Solutions for Chapter 26: Maximum Flow  26-15  f .; x  :  f 0.; x   f 0.x;    Otherwise, if x 62 X, note that f .x;   D f 0.x;   and f .; x  D f 0.; x  for all vertices  2 V . Thus, X2V f .x;   D X2V D X2V D X2V   because f 0 obeys Ô¨Çow conservation   Finally, we prove that the value of the Ô¨Çow remains the same. Since s 62 X, we have f .s;   D f 0.s;   and f .; x  D f 0.; x  for all vertices  2 V , and so jf j D X2V D X2V D jf 0j :  f .s;    cid:0 X2V f 0.s;    cid:0 X2V  f 0.; s   f .; s   To see that the Ô¨Çows form a convex set, we show that if f1 and f2 are Ô¨Çows, then so is Àõf1 C .1  cid:0  Àõ f2 for all Àõ such that 0  Àõ  1. For the capacity constraint, Ô¨Årst observe that Àõ  1 implies that 1  cid:0  Àõ  0. Thus, for any u;  2 V , we have Àõf1.u;   C .1  cid:0  Àõ f2.u;    0  f1.u;   C 0  .1  cid:0  Àõ f2.u;    D 0 :  Since f1.u;    c.u;   and f2.u;    c.u;  , we also have Àõf1.u;   C .1  cid:0  Àõ f2.u;    Àõc.u;   C .1  cid:0  Àõ c.u;    D .Àõ C .1  cid:0  Àõ  c.u;   D c.u;   :  For Ô¨Çow conservation, observe that since f1 and f2 obey Ô¨Çow conservation, we  have P2V f1.; u  D P2V f1.u;   and P2V f1.; u  D P2V f1.u;   for any u 2 V  cid:0  fs; tg. We need to show that .Àõf1.; u  C .1  cid:0  Àõ f2.; u   DX2V X2V for any u 2 V  cid:0 fs; tg. We multiply both sides of the equality for f1 by Àõ, multiply both sides of the equality for f2 by 1  cid:0  Àõ, and add the left-hand and right-hand sides of the resulting equalities to get ÀõX2V  f1.u;   C .1  cid:0  Àõ X2V  f1.; u  C .1  cid:0  Àõ X2V  f2.; u  D ÀõX2V  .Àõf1.u;   C .1  cid:0  Àõ f2.u;     f2.u;   :   26-16  Solutions for Chapter 26: Maximum Flow  Observing that  ÀõX2V  f1.; u  C .1  cid:0  Àõ X2V  and, likewise, that  ÀõX2V  f1.u;   C .1  cid:0  Àõ X2V  f2.; u  D X2V D X2V f2.u;   DX2V  .1  cid:0  Àõ f2.; u   Àõf1.; u  CX2V .Àõf1.; u  C .1  cid:0  Àõ f2.; u    .Àõf1.u;   C .1  cid:0  Àõ f2.u;     completes the proof that Ô¨Çow conservation holds, and thus that Ô¨Çows form a convex set.  Solution to Exercise 26.1-6  Solution to Exercise 26.1-7  Create a vertex for each corner, and if there is a street between corners u and , create directed edges .u;   and .; u . Set the capacity of each edge to 1. Let the source be corner on which the professor‚Äôs house sits, and let the sink be the corner on which the school is located. We wish to Ô¨Ånd a Ô¨Çow of value 2 that also has the property that f .u;   is an integer for all vertices u and . Such a Ô¨Çow represents two edge-disjoint paths from the house to the school.  We will construct G0 by splitting each vertex  of G into two vertices 1; 2, joined by an edge of capacity l. . All incoming edges of  are now incoming edges to 1. All outgoing edges from  are now outgoing edges from 2. More formally, construct G0 D .V 0; E0  with capacity function c0 as follows. For every  2 V , create two vertices 1; 2 in V 0. Add an edge .1; 2  in E0 with c0.1; 2  D l. . For every edge .u;   2 E, create an edge .u2; 1  in E0 with capacity c0.u2; 1  D c.u;  . Make s1 and t2 as the new source and target vertices in G0. Clearly, jV 0j D 2jV j and jE0j D jEj C jV j. Let f be a Ô¨Çow in G that respects vertex capacities. Create a Ô¨Çow function f 0 in G0 as follows. For each edge .u;   2 G, let f 0.u2; 1  D f .u;  . For each vertex u 2 V  cid:0  ftg, let f 0.u1; u2  DP2V f .u;  . Let f 0.t1; t2  DP2V f .; t  . We readily see that there is a one-to-one correspondence between Ô¨Çows that respect vertex capacities in G and Ô¨Çows in G0. For the capacity constraint, every edge in G0 of the form .u2; 1  has a corresponding edge in G with a corresponding capacity and Ô¨Çow and thus satisÔ¨Åes the capacity constraint. For edges in E0 of the form .u1; u2 , the capacities reÔ¨Çect the vertex capacities in G. Therefore, for  u 2 V  cid:0  fs; tg, we have f 0.u1; u2  D P2V f .u;    l.u  D c0.u1; u2 . We also have f 0.t1; t2  D P2V f .; t    l.t   D c0.t1; t2 . Note that this constraint  also enforces the vertex capacities in G.   Solutions for Chapter 26: Maximum Flow  26-17  f 0.; u1   Now, we prove Ô¨Çow conservation. By construction, every vertex of the form u1 in G0 has exactly one outgoing edge .u1; u2 , and every incoming edge to u1 cor- responds to an incoming edge of u 2 G. Thus, for all vertices u 2 V  cid:0  fs; tg, we have incoming Ô¨Çow to u1 D X2V 0 D X2V D X2V D f 0.u1; u2  D outgoing Ô¨Çow from u1 :   because f obeys Ô¨Çow conservation   f .u;    f .; u   Vertices of the form u2 have exactly one incoming edge .u1; u2 , and every outgo- ing edge of u2 corresponds to an outgoing edge of u 2 G. Thus, for u2 ¬§ t2, incoming Ô¨Çow D f 0.u1; u2    because there are no other outgoing edges from s1   For t1, we have  f 0.; t1   incoming Ô¨Çow D X2V 0 D X2V D f 0.t1; t2  D outgoing Ô¨Çow :  f .; t    f .u;    D X2V D X2V 0 D outgoing Ô¨Çow :  f 0.u2;    f 0.s1;    Finally, we prove that jf 0j D jf j: jf 0j D X2V 0 D f 0.s1; s2  D X2V D jf j :  f .s;    Solution to Exercise 26.2-1  Lemma 1. If  62 V1, then f .s;   D 0. 2. If  62 V2, then f .; s  D 0. 3. If  62 V1 [ V2, then f 0.s;   D 0. 4. If  62 V1 [ V2, then f 0.; s  D 0.   26-18  Solutions for Chapter 26: Maximum Flow  Proof 1. Let  62 V1 be some vertex. From the deÔ¨Ånition of V1, there is no edge from s to . Thus, f .s;   D 0. 2. Let  62 V2 be some vertex. From the deÔ¨Ånition of V2, there is no edge from  to s. Thus, f .; s  D 0. 3. Let  62 V1[ V2 be some vertex. From the deÔ¨Ånition of V1 and V2, neither .s;   nor .; s  exists. Therefore, the third condition of the deÔ¨Ånition of residual capacity  equation  26.2   applies, and cf .s;   D 0. Thus, f 0.s;   D 0. and thus f 0.; s  D 0.  4. Let  62 V1 [ V2 be some vertex. By equation  26.2 , we have that cf .; s  D 0  lemma   We conclude that the summations in equation  26.6  equal the summations in equa- tion  26.7 .  Solution to Exercise 26.2-8  Let Gf be the residual network just before an iteration of the while loop of FORD- FULKERSON, and let Es be the set of residual edges of Gf into s. We‚Äôll show that the augmenting path p chosen by FORD-FULKERSON does not include an edge in Es. Thus, even if we redeÔ¨Åne Gf to disallow edges in Es, the path p still remains an augmenting path in the redeÔ¨Åned network. Since p remains unchanged, an iteration of the while loop of FORD-FULKERSON updates the Ô¨Çow in the same way as before the redeÔ¨Ånition. Furthermore, by disallowing some edges, we do not introduce any new augmenting paths. Thus, FORD-FULKERSON still correctly computes a maximum Ô¨Çow. Now, we prove that FORD-FULKERSON never chooses an augmenting path p that includes an edge .; s  2 Es. Why? The path p always starts from s, and if p included an edge .; s , the vertex s would be repeated twice in the path. Thus, p would no longer be a simple path. Since FORD-FULKERSON chooses only simple paths, p cannot include .; s .  Solution to Exercise 26.2-9  The augmented Ô¨Çow f " f 0 satisÔ¨Åes the Ô¨Çow conservation property but not the capacity constraint property. First, we prove that f " f 0 satisÔ¨Åes the Ô¨Çow conservation property. We note that if edge .u;   2 E, then .; u  62 E and f 0.; u  D 0. Thus, we can rewrite the deÔ¨Ånition of Ô¨Çow augmentation  equation  26.4  , when applied to two Ô¨Çows, as .f " f 0 .u;   D  f .u;   C f 0.u;   The deÔ¨Ånition implies that the new Ô¨Çow on each edge is simply the sum of the two Ô¨Çows on that edge. We now prove that in f " f 0, the net incoming Ô¨Çow for each  if .u;   2 E ; otherwise :  0   Solutions for Chapter 26: Maximum Flow  26-19  f 0.; u   f 0.u;    vertex equals the net outgoing Ô¨Çow. Let u 62 fs; tg be any vertex of G. We have X2V .f " f 0 .; u  D X2V D X2V D X2V D X2V D X2V  .f .; u  C f 0.; u   f .; u  CX2V f .u;   CX2V .f .u;   C f 0.u;    .f " f 0 .u;   :   because f , f 0 obey Ô¨Çow conservation   We conclude that f " f 0 satisÔ¨Åes Ô¨Çow conservation. We now show that f " f 0 need not satisfy the capacity constraint by giving a sim- ple counterexample. Let the Ô¨Çow network G have just a source and a target vertex, with a single edge .s; t   having c.s; t   D 1. DeÔ¨Åne the Ô¨Çows f and f 0 to have f .s; t   D f 0.s; t   D 1. Then, we have .f " f 0 .s; t   D 2 > c.s; t  . We conclude that f " f 0 need not satisfy the capacity constraint.  For any two vertices u and  in G, we can deÔ¨Åne a Ô¨Çow network Gu consisting of the directed version of G with s D u, t D , and all edge capacities set to 1.  The Ô¨Çow network Gu has V vertices and 2jEj edges, so that it has O.V   vertices and O.E  edges, as required. We want all capacities to be 1 so that the number of edges of G crossing a cut equals the capacity of the cut in Gu.  Let fu denote a maximum Ô¨Çow in Gu. 2V  cid:0 fugfjfujg. We‚Äôll We claim that for any u 2 V , the edge connectivity k equals min show below that this claim holds. Assuming that it holds, we can Ô¨Ånd k as follows:  EDGE-CONNECTIVITY .G   k D 1select any vertex u 2 G:V for each vertex  2 G:V  cid:0  fug  set up the Ô¨Çow network Gu as described above Ô¨Ånd the maximum Ô¨Çow fu on Gu k D min.k;jfuj   return k  The claim follows from the max-Ô¨Çow min-cut theorem and how we chose capac- ities so that the capacity of a cut is the number of edges crossing it. We prove  Solution to Exercise 26.2-11 This solution is also posted publicly   26-20  Solutions for Chapter 26: Maximum Flow  Solution to Exercise 26.2-12  2V  cid:0 fugfjfujg, for any u 2 V by showing separately that k is at least this  that k D min minimum and that k is at most this minimum.  Proof that k  min  2V  cid:0 fugfjfujg:  2V  cid:0 fugfjfujg. Suppose we remove only m  cid:0  1 edges from G. For Let m D min any vertex , by the max-Ô¨Çow min-cut theorem, u and  are still connected.  The max Ô¨Çow from u to  is at least m, hence any cut separating u from  has capacity at least m, which means at least m edges cross any such cut. Thus at least one edge is left crossing the cut when we remove m cid:0 1 edges.  Thus every vertex is connected to u, which implies that the graph is still connected. So at 2V  cid:0 fugfjfujg. least m edges must be removed to disconnect the graph‚Äîi.e., k  min   Proof that k  min  2V  cid:0 fugfjfujg:  Consider a vertex  with the minimum jfuj. By the max-Ô¨Çow min-cut the- orem, there is a cut of capacity jfuj separating u and . Since all edge ca- pacities are 1, exactly jfuj edges cross this cut. If these edges are removed, there is no path from u to , and so our graph becomes disconnected. Hence k  min  2V  cid:0 fugfjfujg.   Thus, the claim that k D min  2V  cid:0 fugfjfujg, for any u 2 V is true.  The idea of the proof is that if f .; s  D 1, then there must exist a cycle containing the edge .; s  and for which each edge carries one unit of Ô¨Çow. If we reduce the Ô¨Çow on each edge in the cycle by one unit, we can reduce f .; s  to 0 without affecting the value of the Ô¨Çow. Given the Ô¨Çow network G and the Ô¨Çow f , we say that vertex y is Ô¨Çow-connected to vertex ¬¥ if there exists a path p from y to ¬¥ such that each edge of p has a positive Ô¨Çow on it. We also deÔ¨Åne y to be Ô¨Çow-connected to itself. In particular, s is Ô¨Çow-connected to s. We start by proving the following lemma:  Lemma Let G D .V; E  be a Ô¨Çow network and f be a Ô¨Çow in G. If s is not Ô¨Çow-connected to , then f .; s  D 0. Proof The idea is that since s is not Ô¨Çow-connected to , there cannot be any Ô¨Çow from s to . By using Ô¨Çow conservation, we will prove that there cannot be any Ô¨Çow from  to s either, and thus that f .; s  D 0. Let Y be the set of all vertices y such that s is Ô¨Çow-connected to y. By applying Ô¨Çow conservation to vertices in V  cid:0  Y and taking the sum, we obtain X¬¥2V  cid:0 YXx2V  f .x; ¬¥  D X¬¥2V  cid:0 YXx2V  f .¬¥; x  :   Solutions for Chapter 26: Maximum Flow  26-21  Partitioning V into Y and V  cid:0  Y gives X¬¥2V  cid:0 Y Xx2V  cid:0 Y  f .x; ¬¥   f .x; ¬¥  C X¬¥2V  cid:0 YXx2Y D X¬¥2V  cid:0 Y Xx2V  cid:0 Y f .x; ¬¥  D X¬¥2V  cid:0 Y Xx2V  cid:0 Y  But we have  X¬¥2V  cid:0 Y Xx2V  cid:0 Y  f .¬¥; x  C X¬¥2V  cid:0 YXx2Y  f .¬¥; x  ;  f .¬¥; x  :   ¬é   since the left-hand side is the same as the right-hand side, except for a change of variable names x and ¬¥. We also have  f .¬¥; x  D 0 :  f .x; ¬¥  D 0 ;  X¬¥2V  cid:0 YXx2Y since the Ô¨Çow from any vertex in Y to any vertex in V  cid:0  Y must be 0. Thus, equation  ¬é  simpliÔ¨Åes to X¬¥2V  cid:0 YXx2Y The above equation implies that f .¬¥; x  D 0 for each ¬¥ 2 V  cid:0  Y and x 2 Y . In particular, since  2 V  cid:0  Y and s 2 Y , we have that f .; s  D 0. Now, we show how to construct the required Ô¨Çow f 0. By the contrapositive of the lemma, f .; s  > 0 implies that s is Ô¨Çow-connected to  through some path p. Let path p0 be the path s ;  ! s. Path p0 is a cycle that has positive Ô¨Çow on each edge. Because we assume that all edge capacities are integers, the Ô¨Çow on each edge of p0 is at least 1. If we subtract 1 from each edge of the cycle to obtain a Ô¨Çow f 0, then f 0 still satisÔ¨Åes the properties of a Ô¨Çow network and has the same value as jf j. Because edge .; s  is in the cycle, we have that f 0.; s  D f .; s   cid:0  1 D 0.  p  Let .S; T   and .X; Y   be two cuts in G  and G0 . Let c0 be the capacity function of G0. One way to deÔ¨Åne c0 is to add a small amount ƒ± to the capacity of each edge in G. That is, if u and  are two vertices, we set c0.u;   D c.u;   C ƒ± : Thus, if c.S; T   D c.X; Y   and .S; T   has fewer edges than .X; Y  , then we would have c0.S; T   < c0.X; Y  . We have to be careful and choose a small ƒ±, lest we change the relative ordering of two unequal capacities. That is, if c.S; T   < c.X; Y  , then no matter many more edges .S; T   has than .X; Y  , we still need to have c0.S; T   < c0.X; Y  . With this deÔ¨Ånition of c0, a minimum cut in G0 will be a minimum cut in G that has the minimum number of edges. How should we choose the value of ƒ±? Let m be the minimum difference between capacities of two unequal-capacity cuts in G. Choose ƒ± D m=.2jEj . For any cut .S; T  , since the cut can have at most jEj edges, we can bound c0.S; T   by  Solution to Exercise 26.2-13   26-22  Solutions for Chapter 26: Maximum Flow  c.S; T    c0.S; T    c.S; T   C jEj  ƒ± : Let c.S; T   < c.X; Y  . We need to prove that c0.S; T   < c0.X; Y  . We have c0.S; T    c.S; T   C jEj  ƒ± D c.S; T   C m=2 < c.X; Y     since c.X; Y    cid:0  c.S; T    m    c0.X; Y   :  Because all capacities are integral, we can choose m D 1, obtaining ƒ± D 1=2jEj. To avoid dealing with fractional values, we can scale all capacities by 2jEj to obtain c0.u;   D 2jEj  c.u;   C 1 :  Solution to Exercise 26.3-3 This solution is also posted publicly  By deÔ¨Ånition, an augmenting path is a simple path s ; t in the residual net- work G0f . Since G has no edges between vertices in L and no edges between vertices in R, neither does the Ô¨Çow network G0 and hence neither does G0f . Also, the only edges involving s or t connect s to L and R to t. Note that although edges in G0 can go only from L to R, edges in G0f can also go from R to L. Thus any augmenting path must go s ! L ! R !  ! L ! R ! t ; crossing back and forth between L and R at most as many times as it can do so without using a vertex twice. It contains s, t, and equal numbers of dis- tinct vertices from L and R‚Äîat most 2 C 2  min.jLj ;jRj  vertices in all. The length of an augmenting path  i.e., its number of edges  is thus bounded above by 2  min.jLj ;jRj  C 1.  Solution to Exercise 26.4-1  We apply the deÔ¨Ånition of excess Ô¨Çow  equation  26.14   to the initial preÔ¨Çow f created by INITIALIZE-PREFLOW  equation  26.15   to obtain  e.s  D X2V  f .; s   cid:0 X2V  f .s;    c.s;    c.s;   :  D 0  cid:0 X2V D  cid:0 X2V  cid:0 jf j D X2V  Now,  f .; s   cid:0 X2V  f .s;     Solutions for Chapter 26: Maximum Flow  26-23  c.s;     since f .; s   0 and f .s;    c.s;      0  cid:0 X2V D e.s  :  Solution to Exercise 26.4-3  Each time we call RELABEL.u , we examine all edges .u;   2 Ef . Since the number of relabel operations is at most 2jV j  cid:0  1 per vertex, edge .u;   will be examined during relabel operations at most 4jV j  cid:0  2 D O.V   times  at most 2jV j  cid:0  1 times during calls to RELABEL.u  and at most 2jV j  cid:0  1 times during calls to RELABEL.  . Summing up over all the possible residual edges, of which there are at most 2jEj D O.E , we see that the total time spent relabeling vertices is O.VE .  Solution to Exercise 26.4-4  We can Ô¨Ånd a minimum cut, given a maximum Ô¨Çow found in G D .V; E  by a push-relabel algorithm, in O.V   time. First, Ô¨Ånd a height yh such that 0 < yh < jV j and there is no vertex whose height equals yh at termination of the algorithm. We need consider only jV j  cid:0  2 vertices, since s:h D jV j and t:h D 0. Because yh can be one of at most jV j  cid:0  1 possible values, we know that for at least one number in 1; 2; : : : ;jV j  cid:0  1, there will be no vertex of that height. Hence, yh is well deÔ¨Åned, and it is easy to Ô¨Ånd in O.V   time by using a simple boolean array indexed by heights 1; 2; : : : ;jV j  cid:0  1. Let S D Àöu 2 V W u:h > yh cid:9  and T D Àö 2 V W :h < yh cid:9 . Because we know that s:h D jV j > yh, we have s 2 S, and because t:h D 0 < yh, we have t 2 T , as required for a cut. We need to show that f .u;   D c.u;  , i.e., that .u;   62 Ef , for all u 2 S and  2 T . Once we do that, we have that f .S; T   D c.S; T  , and by Corollary 26.5, .S; T   is a minimum cut. Suppose for the purpose of contradiction that there exist vertices u 2 S and  2 T such that .u;   2 Ef . Because h is always maintained as a height function  Lemma 26.16 , we have that u:h  :h C 1. But we also have :h < yh < u:h, and because all values are integer, :h  u:h  cid:0  2. Thus, we have u:h  :h C 1  u:h cid:0  2C 1 D u:h cid:0  1, which gives the contradiction that u:height  u:height cid:0  1. Thus, .S; T   is a minimum cut.  Solution to Exercise 26.4-7  If we set s:h D jV j  cid:0  2, we have to change our deÔ¨Ånition of a height function to allow s:h D jV j  cid:0  2, rather than s:h D jV j. The only change we need to make to   26-24  Solutions for Chapter 26: Maximum Flow  Solution to Problem 26-2  the proof of correctness is to update the proof of Lemma 26.17. The original proof derives the contradiction that s:h  k < jV j, which is at odds with s:h D jV j. When s:h D jV j  cid:0  2, there is no contradiction. As in the original proof, let us suppose that we have a simple augmenting path h0; 1; : : : ; ki, where 0 D s and k D t, so that k < jV j. How could .s; 1  be a residual edge? It had been saturated in INITIALIZE-PREFLOW, which means that we had to have pushed some Ô¨Çow from 1 to s. In order for that to have happened, we must have had 1:h D s:h C 1. If we set s:h D jV j  cid:0  2, then 1:h was jV j  cid:0  1 at the time. Since then, 1:h did not decrease, and so we have 1:h  jV j  cid:0  1. Working backwards over our augmenting path, we have k cid:0 i :h  t:h C i for i D 0; 1; : : : ; k. As before, because the augmenting path is simple, k < jV j. Letting i D k  cid:0  1, we have 1:h  t:h C k  cid:0  1 < 0 C jV j  cid:0  1. We now have the contradiction that 1:h  jV j  cid:0  1 and 1:h < jV j  cid:0  1, which shows that Lemma 26.17 still holds. Nothing in the analysis changes asymptotically.  a. The idea is to use a maximum-Ô¨Çow algorithm to Ô¨Ånd a maximum bipartite matching that selects the edges to use in a minimum path cover. We must show how to formulate the max-Ô¨Çow problem and how to construct the path cover from the resulting matching, and we must prove that the algorithm indeed Ô¨Ånds a minimum path cover. DeÔ¨Åne G0 as suggested, with directed edges. Make G0 into a Ô¨Çow network with source x0 and sink y0 by deÔ¨Åning all edge capacities to be 1. G0 is the Ô¨Çow network corresponding to a bipartite graph G00 in which L D fx1; : : : ; xng, R D fy1; : : : ; yng, and the edges are the  undirected version of the  subset of E0 that doesn‚Äôt involve x0 or y0. The relationship of G to the bipartite graph G00 is that every vertex i in G is represented by two vertices, xi and yi, in G00. Edge .i; j   in G corresponds to edge .xi ; yj   in G00. That is, an edge .xi ; yj   in G00 means that an edge in G leaves i and enters j . Vertex xi tells us about edges leaving i, and yi tells us about edges entering i. The edges in a bipartite matching in G00 can be used in a path cover of G, for the following reasons:      In a bipartite matching, no vertex is used more than once. In a bipartite matching in G00, since no xi is used more than once, at most one edge in the matching leaves any vertex i in G. Similarly, since no yj is used more than once, at most one edge in the matching enters any vertex j in G. In a path cover, since no vertex appears in more than one path, at most one path edge enters each vertex and at most one path edge leaves each vertex.  We can construct a path cover P from any bipartite matching M  not just a maximum matching  by moving from some xi to its matching yj  if any , then from xj to its matching yk, and so on, as follows:   Solutions for Chapter 26: Maximum Flow  26-25  1. Start a new path containing a vertex i that has not yet been placed in a path. 2. If xi is unmatched, the path can‚Äôt go any farther; just add it to P . 3. If xi is matched to some yj , add j to the current path. If j has already been placed in a path  i.e., though we‚Äôve just entered j by processing yj , we‚Äôve already built a path that leaves j by processing xj  , combine this path with that one and go back to step 1. Otherwise go to step 2 to process xj .  This algorithm constructs a path cover, for the following reasons:   Every vertex is put into some path, because we keep picking an unused vertex  from which to start a path until there are no unused vertices.   No vertex is put into two paths, because every xi is matched to at most one yj , and vice versa. That is, at most one candidate edge leaves each vertex, and at most one candidate edge enters each vertex. When building a path, we start or enter a vertex and then leave it, building a single path. If we ever enter a vertex that was left earlier, it must have been the start of another path, since there are no cycles, and we combine those paths so that the vertex is entered and left on a single path.  Every edge in M is used in some path because we visit every xi, and we incor- porate the single edge, if any, from each visited xi. Thus, there is a one-to-one correspondence between edges in the matching and edges in the constructed path cover. We now show that the path cover P constructed above has the fewest possible paths when the matching is maximum. Let f be the Ô¨Çow corresonding to the bipartite matching M .    vertices in p    every vertex is on exactly 1 path   jV j D Xp2P D Xp2P 1 CXp2P D Xp2P D jPj C jMj D jPj C jf j   1 +  edges in p     edges in p    by 1-to-1 correspondence   by Lemma 26.9  .  Thus, for the Ô¨Åxed set V in our graph G, jPj  the number of paths  is minimized when the Ô¨Çow f is maximized. The overall algorithm is as follows:   Use FORD-FULKERSON to Ô¨Ånd a maximum Ô¨Çow in G0 and hence a maxi-  mum bipartite matching M in G00.   Construct the path cover as described above.  Time O.VE  total:  O.V C E  to set up G0,  O.VE  to Ô¨Ånd the maximum bipartite matching,   26-26  Solutions for Chapter 26: Maximum Flow   O.E  to trace the paths, because each edge 2 M is traversed only once and  there are O.E  edges in M .  b. The algorithm does not work if there are cycles.  Consider a graph G with 4 vertices, consisting of a directed triangle and an edge pointing to the triangle:  E D f.1; 2 ; .2; 3 ; .3; 1 ; .4; 1 g G can be covered with a single path: 4 ! 1 ! 2 ! 3, but our algorithm might Ô¨Ånd only a 2-path cover. In the bipartite graph G0, the edges .xi ; yj   are  .x1; y2 ; .x2; y3 ; .x3; y1 ; .x4; y1  :  There are 4 edges from an xi to a yj , but 2 of them lead to y1, so a maximum bipartite matching can have only 3 edges  and the maximum Ô¨Çow in G0 has value 3 . In fact, there are 2 possible maximum matchings. It is always pos- sible to match .x1; y2  and .x2; y3 , and then either .x3; y1  or .x4; y1  can be chosen, but not both. The maximum Ô¨Çow found by one of our max-Ô¨Çow algorithms could Ô¨Ånd the Ô¨Çow corresponding to either of these matchings, since both are maximal. If it Ô¨Ånds the matching with edge .x3; x1 , then the matching would not con- tain .x4; x1 ; given that matching, our path algorithm is forced to produce 2 paths, one of which contains just the vertex 4.  a. Assume for the sake of contradiction that Ak 62 T for some Ak 2 Ri . Since Ak 62 T , we must have Ak 2 S. On the other hand, we have Ji 2 T . Thus, the edge .Ak; Ji   crosses the cut .S; T  . But c.Ak; Ji   D 1 by construction, which contradicts the assumption that .S; T   is a Ô¨Ånite-capacity cut.  b. Let us deÔ¨Åne a project-plan as a set of jobs to accept and experts to hire. Let P be a project-plan. We assume that P has two attributes. The attribute P:J denotes the set of accepted jobs, and P:A denotes the set of hired experts. A valid project-plan is one in which we have hired all experts that are required by the accepted jobs. SpeciÔ¨Åcally, let P be a valid project plan. If Ji 2 P:J, then Ak 2 P:A for each Ak 2 Ri. Note that Professor Gore might decide to hire more experts than those that are actually required. We deÔ¨Åne the revenue of a project-plan as the total proÔ¨Åt from the accepted jobs minus the total cost of the hired experts. The problem asks us to Ô¨Ånd a valid project plan with maximum revenue. We start by proving the following lemma, which establishes the relationship between the capacity of a cut in Ô¨Çow network G and the revenue of a valid project-plan.  Solution to Problem 26-3   Solutions for Chapter 26: Maximum Flow  26-27  Lemma  Min-cut max-revenue  There exists a Ô¨Ånite-capacity cut .S; T   of G with capacity c.S; T   if and only  if there exists a valid project-plan with net revenue cid:0 PJi2J pi  cid:0  c.S; T  . Proof Let .S; T   be a Ô¨Ånite-capacity cut of G with capacity c.S; T  . We prove one direction of the lemma by constructing the required project-plan. Construct the project-plan P by including Ji in P:J if and only if Ji 2 T and including Ak in P:A if and only if Ak 2 T . From part  a , P is a valid project-plan, since, for every Ji 2 P:J, we have Ak 2 P:A for each Ak 2 Ri . there cannot be any edges of the Since the capacity of the cut is Ô¨Ånite, form .Ak; Ji   crossing the cut, where Ak 2 S and Ji 2 T . All edges going from a vertex in S to a vertex in T must be either of the form .s; Ak  or of the form .Ji ; t  . Let EA be the set of edges of the form .s; Ak   that cross the cut, and let EJ be the set of edges of the form .Ji ; t   that cross the cut, so that  c.Ji; t   :  c.S; T   D X.s;Ak 2EA c.s; Ak   C X.Ji ;t  2EJ Consider edges of the form .s; Ak . We have .s; Ak   2 EA if and only if Ak 2 T  if and only if Ak 2 P:A :  By construction, c.s; Ak  D ck. Taking summations over EA and over P:A, we obtain X.s;Ak 2EA Similarly, consider edges of the form .Ji ; t  . We have .Ji ; t   2 EJ  c.s; Ak  D XAk2P: A  ck :  if and only if Ji 2 S if and only if Ji 62 T if and only if Ji 62 P:J :  By construction, c.Ji; t   D pi. Taking summations over EJ and over P:J, we obtain X.Ji ;t  2EJ  c.Ji ; t   D XJi62P: J  pi :  Let  be the net revenue of P . Then, we have   26-28  Solutions for Chapter 26: Maximum Flow  ck   D XJi2P: J D  XJi2J D XJi2J D XJi2J D  XJi2J  pi  cid:0  XAk2P: A pi  cid:0  XJi62P: J pi  cid:0   XJi62P: J pi  cid:0   X.Ji ;t  2EJ pi!  cid:0  c.S; T   :  ck  pi!  cid:0  XAk2P: A ck! pi C XAk2P: A c.Ji ; t   C X.s;Ak  2EA  c.s; Ak !  Now, we prove the other direction of the lemma by constructing the required cut from a valid project-plan. Construct the cut .S; T   as follows. For every Ji 2 P:J, let Ji 2 T . For every Ak 2 P:A, let Ak 2 T . First, we prove that the cut .S; T   is a Ô¨Ånite-capacity cut. Since edges of the form .Ak; Ji   are the only inÔ¨Ånite-capacity edges, it sufÔ¨Åces to prove that there are no edges .Ak; Ji   such that Ak 2 S and Ji 2 T . For the purpose of contradiction, assume there is an edge .Ak; Ji   such that Ak 2 S and Ji 2 T . By our constuction, we must have Ji 2 P:J and Ak 62 P:A. But since the edge .Ak; Ji   exists, we have Ak 2 Ri. Since P is a valid project-plan, we derive the contradiction that Ak must have been in P:A. From here on, the analysis is the same as the previous direction. In particular, the net revenue  equals the last equation from the previous analysis holds:  We conclude that the problem of Ô¨Ånding a maximum-revenue project-plan re- duces to the problem of Ô¨Ånding a minimum cut in G. Let .S; T   be a minimum cut. From the lemma, the maximum net revenue is given by   cid:0 PJi2J pi  cid:0  c.S; T  .   Xji2J  pi!  cid:0  c.S; T   :  c. Construct the Ô¨Çow network G as shown in the problem statement. Obtain a minimum cut .S; T   by running any of the maximum-Ô¨Çow algorithms  say, Edmonds-Karp . Construct the project plan P as follows: add Ji to P:J if and only if Ji 2 T . Add Ak to P:A if and only if Ak 2 T . First, we note that the number of vertices in G is jV j D m C n C 2, and the number of edges in G is jEj D r C m C n. Constructing G and recovering the project-plan from the minimum cut are clearly linear-time operations. The running time of our algorithm is thus asymptotically the same as the running time of the algorithm used to Ô¨Ånd the minimum cut. If we use Edmonds-Karp to Ô¨Ånd the minimum cut, the running time is O.VE 2 .   Solutions for Chapter 26: Maximum Flow  26-29  Solution to Problem 26-4 This solution is also posted publicly  a. Just execute one iteration of the Ford-Fulkerson algorithm. The edge .u;   in E with increased capacity ensures that the edge .u;   is in the residual network. So look for an augmenting path and update the Ô¨Çow if a path is found.  Time O.V C E  D O.E  if we Ô¨Ånd the augmenting path with either depth-Ô¨Årst or breadth-Ô¨Årst search. To see that only one iteration is needed, consider separately the cases in which .u;   is or is not an edge that crosses a minimum cut. If .u;   does not cross a minimum cut, then increasing its capacity does not change the capacity of any minimum cut, and hence the value of the maximum Ô¨Çow does not change. If .u;   does cross a minimum cut, then increasing its capacity by 1 increases the capacity of that minimum cut by 1, and hence possibly the value of the maxi- mum Ô¨Çow by 1. In this case, there is either no augmenting path  in which case there was some other minimum cut that .u;   does not cross , or the augment- ing path increases Ô¨Çow by 1. No matter what, one iteration of Ford-Fulkerson sufÔ¨Åces.  b. Let f be the maximum Ô¨Çow before reducing c.u;  .  If f .u;   D 0, we don‚Äôt need to do anything. If f .u;   > 0, we will need to update the maximum Ô¨Çow. Assume from now on that f .u;   > 0, which in turn implies that f .u;    1. DeÔ¨Åne f 0.x; y  D f .x; y  for all x; y 2 V , except that f 0.u;   D f .u;   cid:0 1. Although f 0 obeys all capacity contraints, even after c.u;   has been reduced, it is not a legal Ô¨Çow, as it violates Ô¨Çow conservation at u  unless u D s  and   unless  D t . f 0 has one more unit of Ô¨Çow entering u than leaving u, and it has one more unit of Ô¨Çow leaving  than entering . The idea is to try to reroute this unit of Ô¨Çow so that it goes out of u and into  via some other path. If that is not possible, we must reduce the Ô¨Çow from s to u and from  to t by one unit. Look for an augmenting path from u to   note: not from s to t .      If there is such a path, augment the Ô¨Çow along that path. If there is no such path, reduce the Ô¨Çow from s to u by augmenting the Ô¨Çow from u to s. That is, Ô¨Ånd an augmenting path u ; s and augment the Ô¨Çow along that path.  There deÔ¨Ånitely is such a path, because there is Ô¨Çow from s to u.  Similarly, reduce the Ô¨Çow from  to t by Ô¨Ånding an augmenting path t ;  and augmenting the Ô¨Çow along that path.  Time O.V C E  D O.E  if we Ô¨Ånd the paths with either DFS or BFS.   26-30  Solutions for Chapter 26: Maximum Flow  Solution to Problem 26-5  a. The capacity of a cut is deÔ¨Åned to be the sum of the capacities of the edges crossing it. Since the number of such edges is at most jEj, and the capacity of each edge is at most C , the capacity of any cut of G is at most C jEj.  b. The capacity of an augmenting path is the minimum capacity of any edge on the path, so we are looking for an augmenting path whose edges all have capacity at least K. Do a breadth-Ô¨Årst search or depth-Ô¨Årst-search as usual to Ô¨Ånd the path, considering only edges with residual capacity at least K.  Treat lower-capacity edges as though they don‚Äôt exist.  This search takes O.V C E  D O.E  time.  Note that jV j D O.E  in a Ô¨Çow network.   c. MAX-FLOW-BY-SCALING uses the Ford-Fulkerson method. It repeatedly aug- ments the Ô¨Çow along an augmenting path until there are no augmenting paths with capacity at least 1. Since all the capacities are integers, and the capacity of an augmenting path is positive, when there are no augmenting paths with ca- pacity at least 1, there must be no augmenting paths whatsoever in the residual network. Thus, by the max-Ô¨Çow min-cut theorem, MAX-FLOW-BY-SCALING returns a maximum Ô¨Çow.  d.  The Ô¨Årst time line 4 is executed, the capacity of any edge in Gf equals its capacity in G, and by part  a  the capacity of a minimum cut of G is at most C jEj. Initially K D 2blg Cc, and so 2K D 2  2blg Cc D 2blg CcC1 > 2lg C D C . Thus, the capacity of a minimum cut of Gf is initially less than 2K jEj.  The other times line 4 is executed, K has just been halved, and so the ca- pacity of a cut of Gf is at most 2K jEj at line 4 if and only if that capacity was at most K jEj when the while loop of lines 5‚Äì6 last terminated. Thus, we want to show that when line 7 is reached, the capacity of a minimum cut of Gf is at most K jEj. Let Gf be the residual network when line 7 is reached. When we reach line 7, Gf contains no augmenting path with capacity at least K. Therefore, a maximum Ô¨Çow f 0 in Gf has value jf 0j < K jEj. Then, by the max-Ô¨Çow min-cut theorem, a minimum cut in Gf has capacity less than K jEj.  e. By part  d , when line 4 is reached, the capacity of a minimum cut of Gf is at most 2K jEj, and thus the maximum Ô¨Çow in Gf is at most 2K jEj. The following lemma shows that the value of a maximum Ô¨Çow in G equals the value of the current Ô¨Çow f in G plus the value of a maximum Ô¨Çow in Gf .  Lemma Let f be a Ô¨Çow in Ô¨Çow network G, and f 0 be a maximum Ô¨Çow in the residual network Gf . Then f " f 0 is a maximum Ô¨Çow in G. Proof By the max-Ô¨Çow min-cut theorem, jf 0j D cf .S; T   for some cut .S; T   of Gf , which is also a cut of G. By Lemma 26.4, jf j D f .S; T  . By Lemma 26.1, f " f 0 is a Ô¨Çow in G with value jf " f 0j D jf j C jf 0j. We   Solutions for Chapter 26: Maximum Flow  26-31  will show that jf j C jf 0j D c.S; T   which, by the max-Ô¨Çow min-cut theorem, will prove that f " f 0 is a maximum Ô¨Çow in G. We have jf j C jf 0j D f .S; T   C cf .S; T    D  Xu2SX2T f .u;    cid:0 Xu2SX2T D   Xu2S;2T f .; u ! f .u;    cid:0 Xu2S;2T C0 B@ Xu2S;2T; c.u;    cid:0 Xu2S;2T;  .u; 2E  .u; 2E  f .; u ! CXu2SX2T  cf .u;    f .u;   CXu2S;2T;  .;u 2E  f .; u 1 CA  :  Noting that .u;   62 E implies f .u;   D 0, we have that Xu2S;2T  f .u;   D Xu2S;2T;  f .u;   :  .u; 2E  Thus, the summations of f .u;   cancel each other out, as do the summations of f .; u . Therefore,  Similarly,  Xu2S;2T  f .; u  D Xu2S;2T;  .;u 2E  f .; u  :  jf j C jf 0j D Xu2S;2T; D Xu2SX2T D c.S; T   :  .u; 2E  c.u;    c.u;     lemma   By this lemma, we see that the value of a maximum Ô¨Çow in G is at most 2K jEj more than the value of the current Ô¨Çow f in G. Every time the inner while loop Ô¨Ånds an augmenting path of capacity at least K, the Ô¨Çow in G increases by at least K. Since the Ô¨Çow cannot increase by more than 2K jEj, the loop executes at most .2K jEj =K D 2jEj times.  f. The time complexity is dominated by the while loop of lines 4‚Äì7.  The lines outside the loop take O.E  time.  The outer while loop executes O.lg C   times, since K is initially O.C   and is halved on each iteration, until K < 1. By part  e , the inner while loop executes O.E  times for each value of K, and by part  b , each iteration takes O.E  time. Thus, the total time is O.E 2 lg C  .   Solutions for Chapter 27: Multithreaded Algorithms  Solution to Exercise 27.1-1  There will be no change in the asymptotic work, span, or parallelism of P-FIB even if we were to spawn the recursive call to P-FIB.n  cid:0  2 . The serialization of P-FIB under consideration would yield the same recurrence as that for FIB; we can, therefore, calculate the work as T1.n  D ‚Äö.n . Similarly, because the spawned calls to P-FIB.n  cid:0  1  and P-FIB.n  cid:0  2  can run in parallel, we can calculate the span in exactly the same way as in the text, T1.n  D ‚Äö.n , resulting in ‚Äö.n=n  parallelism.  Solution to Exercise 27.1-5  By the work law for P D 4, we have 80 D T4  T1=4, or T1  320. By the span law for P D 64, we have T1  T64 D 10. Now we will use inequality  27.5  from Exercise 27.1-3 to derive a contradiction. For P D 10, we have 42 D T10  320  cid:0  T1   10 D 32 C or, equivalently,  9  10  C T1 T1  T1   10 9  10  > 10 ;  which contradicts T1  10. Therefore, the running times reported by the professor are suspicious.   27-2  Solutions for Chapter 27: Multithreaded Algorithms  Solution to Exercise 27.1-6  FAST-MAT-VEC.A; x  n D A:rows let y be a new vector of length n parallel for i D 1 to n parallel for i D 1 to n return y  yi D 0 yi D MAT-SUB-LOOP.A; x; i; 1; n   MAT-SUB-LOOP.A; x; i; j; j 0   if j == j 0  return aij xj  else mid D b.j C j 0 =2c  lhalf D spawn MAT-SUB-LOOP.A; x; i; j; mid  uhalf D MAT-SUB-LOOP.A; x; i; mid C 1; j 0  sync return lhalf C uhalf  We calculate the work T1.n  of FAST-MAT-VEC by computing the running time of its serialization, i.e., by replacing the parallel for loop by an ordinary for loop. Therefore, we have T1.n  D n T 01.n , where T 01.n  denotes the work of MAT-SUB- LOOP to compute a given output entry yi . The work of MAT-SUB-LOOP is given by the recurrence T 01.n  D 2T 01.n=2  C ‚Äö.1  : By applying case 1 of the master theorem, we have T 01.n  D ‚Äö.n . Therefore, T1.n  D ‚Äö.n2 . To calculate the span, we use T1.n  D ‚Äö.lg n  C max 1in Note that each iteration of the second parallel for loop calls procedure MAT- SUB-LOOP with the same parameters, except for the index i. Because MAT-SUB- LOOP recursively halves the space between its last two parameters  1 and n , does constant-time work in the base case, and spawns one of the recursive calls in paral- lel with the other, it has span ‚Äö.lg n . The procedure FAST-MAT-VEC, therefore, has a span of ‚Äö.lg n  and ‚Äö.n2= lg n  parallelism.  iter1.i   :  Solution to Exercise 27.1-7  We analyze the work of P-TRANSPOSE, as usual, by computing the running time of its serialization, where we replace both the parallel for loops with simple for   Solutions for Chapter 27: Multithreaded Algorithms  27-3  loops. We can compute the work of P-TRANSPOSE using the summation  T1.n  D ‚Äö  n XjD2 D ‚Äö  n cid:0 1 XjD1 D ‚Äö.n2  :  .j  cid:0  1 ! j!  The span of P-TRANSPOSE is determined by the span of the doubly nested parallel for loops. Although the number of iterations of the inner loop depends on the value of the variable j of the outer loop, each iteration of the inner loop does constant work. Let iter1.j   denote the span of the j th iteration of the outer loop and iter0 .i   denote the span of the ith iteration of the inner loop. We characterize the 1 span T1.n  of P-TRANSPOSE as T1.n  D ‚Äö.lg n  C max 2jn The maximum occurs when j D n, and in this case, iter1.n  D ‚Äö.lg n  C max 1in cid:0 1 As we noted, each iteration of the inner loop does constant work, and therefore iter0 .i   D ‚Äö.1  for all i. Thus, we have 1 T1.n  D ‚Äö.lg n  C ‚Äö.lg n  C ‚Äö.1   iter1.j   :  iter0 1  .i   :  D ‚Äö.lg n  :  Since the work P-TRANSPOSE is ‚Äö.n2  and its span is ‚Äö.lg n , the parallelism of P-TRANSPOSE is ‚Äö.n2= lg n .  Solution to Exercise 27.1-8  Solution to Exercise 27.1-9  If we were to replace the inner parallel for loop of P-TRANSPOSE with an ordinary for loop, the work would still remain ‚Äö.n2 . The span, however, would increase to ‚Äö.n  because the last iteration of the parallel for loop, which dominates the span of the computation, would lead to .n  cid:0  1  iterations of the inner, serial for loop. The parallelism, therefore, would reduce to ‚Äö.n2 =‚Äö.n  D ‚Äö.n .  Based on the values of work and span given for the two versions of the chess program, we solve for P using  2048 P C 1 D The solution gives P between 146 and 147.  1024 P C 8 :   27-4  Solutions for Chapter 27: Multithreaded Algorithms  Solution to Exercise 27.2-3  P-FAST-MATRIX-MULTIPLY .A; B  n D A:rows let C be a new n  n matrix parallel for i D 1 to n parallel for j D 1 to n  cij D MATRIX-MULT-SUBLOOP .A; B; i; j; 1; n   return C  MATRIX-MULT-SUBLOOP .A; B; i; j; k; k0   if k == k0  return ai kbkj  else mid D b.k C k0 =2c  lhalf D spawn MATRIX-MULT-SUBLOOP .A; B; i; j; k; mid  uhalf D MATRIX-MULT-SUBLOOP.A; B; i; j; mid C 1; k0  sync return lhalf C uhalf  We calculate the work T1.n  of P-FAST-MATRIX-MULTIPLY by computing the running time of its serialization, i.e., by replacing the parallel for loops by ordinary for loops. Therefore, we have T1.n  D n2 T 01.n , where T 01.n  denotes the work of MATRIX-MULT-SUBLOOP to compute a given output entry cij . The work of MATRIX-MULT-SUBLOOP is given by the recurrence T 01.n  D 2T 01.n=2  C ‚Äö.1  : By applying case 1 of the master theorem, we have T 01.n  D ‚Äö.n . Therefore, T1.n  D ‚Äö.n3 . To calculate the span, we use T1.n  D ‚Äö.lg n  C max 1in Note that each iteration of the outer parallel for loop does the same amount of work: it calls the inner parallel for loop. Similarly, each iteration of the inner parallel for loop calls procedure MATRIX-MULT-SUBLOOP with the same pa- rameters, except for the indices i and j . Because MATRIX-MULT-SUBLOOP re- cursively halves the space between its last two parameters  1 and n , does constant- time work in the base case, and spawns one of the recursive calls in parallel with the other, it has span ‚Äö.lg n . Since each iteration of the inner parallel for loop, which has n iterations, has span ‚Äö.lg n , the inner parallel for loop has span ‚Äö.lg n . By similar logic, the outer parallel for loop, and hence procedure P-FAST-MATRIX- MULTIPLY, has span ‚Äö.lg n  and ‚Äö.n3= lg n  parallelism.  iter1.i   :  Solution to Exercise 27.2-4  We can efÔ¨Åciently multiply a p  q matrix by a q  r matrix in parallel by using the solution to Exercise 27.2-3 as a base. We will replace the upper limits of the   Solutions for Chapter 27: Multithreaded Algorithms  27-5  nested parallel for loops with p and r respectively and we will pass q as the last argument to the call of MATRIX-MULT-SUBLOOP. We present the pseudocode for a multithreaded algorithm for multiplying a pq matrix by a qr matrix in proce- dure P-GEN-MATRIX-MULTIPLY below. Because the pseudocode for procedure MATRIX-MULT-SUBLOOP  which P-GEN-MATRIX-MULTIPLY calls  remains the same as was presented in the solution to Exercise 27.2-3, we do not repeat it here.  P-GEN-MATRIX-MULTIPLY .A; B  p D A:rows q D A:columns r D B:columns let C be a new p  r matrix parallel for i D 1 to p parallel for j D 1 to r  cij D MATRIX-MULT-SUBLOOP.A; B; i; j; 1; q   return C  To calculate the work for P-GEN-MATRIX-MULTIPLY, we replace the parallel for loops with ordinary for loops. As before, we can calculate the work of MATRIX- MULT-SUBLOOP to be ‚Äö.q   because the input size to the procedure is q here . Therefore, the work of P-GEN-MATRIX-MULTIPLY is T1 D ‚Äö.pqr . We can analyze the span of P-GEN-MATRIX-MULTIPLY as we did in the solution to Exercise 27.2-3, but we must take into account the different number of loop iterations. Each of the p iterations of the outer parallel for loop executes the inner parallel for loop, and each of the r iterations of the inner parallel for loop calls MATRIX-MULT-SUBLOOP, whose span is given by ‚Äö.lg q . We know that, in general, the span of a parallel for loop with n iterations, where the ith iteration has span iter1.i   is given by T1 D ‚Äö.lg n  C max 1in Based on the above observations, we can calculate the span of P-GEN-MATRIX- MULTIPLY as T1 D ‚Äö.lg p  C ‚Äö.lg r  C ‚Äö.lg q   iter1.i   :  D ‚Äö.lg.pqr   :  The parallelism of the procedure is, therefore, given by ‚Äö.pqr= lg.pqr  . To check whether this analysis is consistent with Exercise 27.2-3, we note that if p D q D r D n, then the parallelism of P-GEN-MATRIX-MULTIPLY would be ‚Äö.n3= lg n3  D ‚Äö.n3= lg n .   27-6  Solutions for Chapter 27: Multithreaded Algorithms  Solution to Exercise 27.2-5  P-MATRIX-TRANSPOSE-RECURSIVE .A; r; c; s     Transpose the s  s submatrix starting at arc. if s == 1  return  else s0 D bs=2c  spawn P-MATRIX-TRANSPOSE-RECURSIVE .A; r; c; s0  spawn P-MATRIX-TRANSPOSE-RECURSIVE .A; r C s0; c C s0; s  cid:0  s0  P-MATRIX-TRANSPOSE-SWAP .A; r; c C s0; r C s0; c; s0; s  cid:0  s0  sync  P-MATRIX-TRANSPOSE-SWAP .A; r1; c1; r2; c2; s1; s2     Transpose the s1  s2 submatrix starting at ar1c1 with the s2  s1 submatrix    starting at ar2c2. if s1 < s2  elseif s1 == 1  P-MATRIX-TRANSPOSE-SWAP .A; r2; c2; r1; c1; s2; s1     since s1  s2, must have that s2 equals 1  exchange ar1;c1 with ar2;c2  else s0 D bs1=2c  spawn P-MATRIX-TRANSPOSE-SWAP.A; r2; c2; r1; c1; s2; s0  P-MATRIX-TRANSPOSE-SWAP .A; r2; c2 C s0; r1 C s0; c1; s2; s1  cid:0  s0  sync  In order to transpose an n  n matrix A, we call P-MATRIX-TRANSPOSE- RECURSIVE A; 1; 1; n . Let us Ô¨Årst calculate the work and span of P-MATRIX-TRANSPOSE-SWAP so that we can plug in these values into the work and span calculations of P-MATRIX- TRANSPOSE-RECURSIVE. The work T 01.N   of P-MATRIX-TRANSPOSE-SWAP on an N -element matrix is the running time of its serialization. We have the recur- rence T 01.N   D 2T 01.N=2  C ‚Äö.1   .N   is similarly described by the recurrence  D ‚Äö.N   :  The span T 0 1 T 0 1  .N   D T 0 1  .N=2  C ‚Äö.1   D ‚Äö.lg N   :  In order to calculate the work of P-MATRIX-TRANSPOSE-RECURSIVE, we calcu- late the running time of its serialization. Let T1.N   be the work of the algorithm on an N -element matrix, where N D n2, and assume for simplicity that n is an exact power of 2. Because the procedure makes two recursive calls with square submatrices of sizes n=2  n=2 D N=4 and because it does ‚Äö.n2  D ‚Äö.N   work to swap all the elements of the other two submatrices of size n=2  n=2, its work is given by the recurrence T1.N   D 2T1.N=4  C ‚Äö.N    D ‚Äö.N   :   Solutions for Chapter 27: Multithreaded Algorithms  27-7  The two parallel recursive calls in P-MATRIX-TRANSPOSE-RECURSIVE execute on matrices of size n=2  n=2. The span of the procedure is given by maximum of the span of one of these two recursive calls and the ‚Äö.lg N   span of P-MATRIX- TRANSPOSE-SWAP, plus ‚Äö.1 . Since the recurrence  T1.N   D T1.N=4  C ‚Äö.1  has the solution T1.N   D ‚Äö.lg N   by case 2 of Theorem 4.1, the span of the recursive call is asymptotically the same as the span of P-MATRIX-TRANSPOSE- SWAP, and hence the span of P-MATRIX-TRANSPOSE-RECURSIVE is ‚Äö.lg N  . Thus, P-MATRIX-TRANSPOSE-RECURSIVE has parallelism ‚Äö.N= lg N   D ‚Äö.n2= lg n .  Solution to Exercise 27.2-6  P-FLOYD-WARSHALL.W   n D W:rows parallel for i D 1 to n  parallel for j D 1 to n  dij D wij  for k D 1 to n  parallel for i D 1 to n  parallel for j D 1 to n  dij D min.dij ; di k C dkj    return D  By Exercise 25.2-4, we can compute all the dij values in parallel. The work of P-FLOYD-WARSHALL is the same as the running time of its serializa- tion, which we computed as ‚Äö.n3  in Section 25.2. The span of the doubly nested parallel for loops, which do constant work inside, is ‚Äö.lg n . Note, however, that the second set of doubly nested parallel for loops is executed within each of the n iterations of the outermost serial for loop. Therefore, P-FLOYD-WARSHALL has span ‚Äö.n lg n  and ‚Äö.n2= lg n  parallelism.  Solution to Problem 27-1  a. Similar to MAT-VEC-MAIN-LOOP, the required procedure, which we name NESTED-SUM-ARRAYS, will take parameters i and j to specify the range of the array that is being computed in parallel. In order to perform the pairwise addition of two n-element arrays A and B and store the result into array C , we call NESTED-SUM-ARRAYS A, B, C , 1, A:length .   27-8  Solutions for Chapter 27: Multithreaded Algorithms  NESTED-SUM-ARRAYS .A; B; C; i; j    if i == j  else k D b.i C j  =2c spawn NESTED-SUM-ARRAYS .A; B; C; i; k   C ≈íi ¬ç D A≈íi ¬ç C B≈íi ¬ç NESTED-SUM-ARRAYS.A; B; C; k C 1; j   sync  The work of NESTED-SUM-ARRAYS is given by the recurrence T1.n  D 2T1.n=2  C ‚Äö.1   D ‚Äö.n  ;  D ‚Äö.lg n  ;  by case 1 of the master theorem. The span of the procedure is given by the recurrence T1.n  D T1.n=2  C ‚Äö.1   by case 2 of the master theorem. Therefore, the above algorithm has ‚Äö.n= lg n  parallelism.  b. Because ADD-SUBARRAY is serial, we can calculate both its work and span to be ‚Äö.j  cid:0  i C 1 , which based on the arguments from the call in SUM-ARRAYS0 is ‚Äö.grain-size , for all but the last call  which is O.grain-size  . If grain-size D 1, the procedure SUM-ARRAYS0 calculates r to be n, and each of the n iterations of the serial for loop spawns ADD-SUBARRAY with the same value, k C 1, for the last two arguments. For example, when k D 0, the last two arguments to ADD-SUBARRAY are 1, when k D 1, the last two arguments are 2, and so on. That is, in each call to ADD-SUBARRAY, its for loop iterates once and calculates a single value in the array C . When grain-size D 1, the for loop in SUM-ARRAYS0 iterates n times and each iteration takes ‚Äö.1  time, resulting in ‚Äö.n  work. Although the for loop in SUM-ARRAYS0 looks serial, note that each iteration spawns the call to ADD-SUBARRAY and the procedure waits for all its spawned children at the end of the for loop. That is, all loop iterations of SUM-ARRAYS0 execute in parallel. Therefore, one might be tempted to say that the span of SUM-ARRAYS0 is equal to the span of a single call to ADD-SUBARRAY plus the constant work done by the Ô¨Årst three lines in SUM-ARRAYS0, giving ‚Äö.1  span and ‚Äö.n  parallelism. This calculation of span and parallelism would be wrong, however, because there are r spawns of ADD-SUBARRAY in SUM-ARRAYS0, where r is not a constant. Hence, we must add a ‚Äö.r  term to the span of SUM-ARRAYS0 in order to account for the overhead of spawning r calls to ADD-SUBARRAY. Based on the above discussion, is ‚Äö.r  C ‚Äö.grain-size  C ‚Äö.1 . When grain-size D 1, we get r D n; therefore, SUM-ARRAYS0 has ‚Äö.n  span and ‚Äö.1  parallelism.  the span of SUM-ARRAYS0  c. For a general grain-size, each iteration of the for loop in SUM-ARRAYS0 except for the last results in grain-size iterations of the for loop in ADD-SUBARRAY. In the last iteration of SUM-ARRAYS0, the for loop in ADD-SUBARRAY iter- ates n mod grain-size times. Therefore, we can claim that the span of ADD- SUBARRAY is ‚Äö.max.grain-size; n mod grain-size   D ‚Äö.grain-size .   Solutions for Chapter 27: Multithreaded Algorithms  27-9  SUM-ARRAYS0 achieves maximum parallelism when its span, given by ‚Äö.r C ‚Äö.grain-size  C ‚Äö.1 , is minimum. Since r D dn=grain-sizee, the minimum occurs when r  grain-size, i.e., when grain-size  pn.  Solution to Problem 27-2  a. We initialize the output matrix C using doubly nested parallel for loops and  then call P-MATRIX-MULTIPLY-RECURSIVE0, deÔ¨Åned below.  P-MATRIX-MULTIPLY-LESS-MEM.C; A; B  n D A:rows parallel for i D 1 to n  parallel for j D 1 to n  cij D 0  P-MATRIX-MULTIPLY-RECURSIVE0.C; A; B   P-MATRIX-MULTIPLY-RECURSIVE0.C; A; B  n D A:rows if n == 1  c11 D c11 C a11b11  else partition A, B, and C into n=2  n=2 submatrices  A11; A12; A21; A22; B11; B12; B21; B22; and C11; C12; C21; C22  spawn P-MATRIX-MULTIPLY-RECURSIVE0.C11; A11; B11  spawn P-MATRIX-MULTIPLY-RECURSIVE0.C12; A11; B12  spawn P-MATRIX-MULTIPLY-RECURSIVE0.C21; A21; B11  P-MATRIX-MULTIPLY-RECURSIVE0.C22; A21; B12  sync spawn P-MATRIX-MULTIPLY-RECURSIVE0.C11; A12; B21  spawn P-MATRIX-MULTIPLY-RECURSIVE0.C12; A12; B22  spawn P-MATRIX-MULTIPLY-RECURSIVE0.C21; A22; B21  P-MATRIX-MULTIPLY-RECURSIVE0.C22; A22; B22  sync  b. The procedure P-MATRIX-MULTIPLY-LESS-MEM performs ‚Äö.n2  work in the doubly nested parallel for loops, and then it calls the procedure P-MATRIX-MULTIPLY-RECURSIVE0. The recurrence for the work M 01.n  of P-MATRIX-MULTIPLY-RECURSIVE0 is 8M 01.n=2  C ‚Äö.1 , which gives us M 01.n  D ‚Äö.n3 . Therefore, T1.n  D ‚Äö.n3 . The span of the doubly nested parallel for loops that initialize the out- In P-MATRIX-MULTIPLY-RECURSIVE0, there are put array C is ‚Äö.lg n . two groups of spawned recursive calls; .n  of the span M 0 1 P-MATRIX-MULTIPLY-RECURSIVE0 is given by the recurrence M 0 .n  D 1 .n  D ‚Äö.n . Because the span ‚Äö.n  2M 0 1 of P-MATRIX-MULTIPLY-RECURSIVE0 dominates, we have T1.n  D ‚Äö.n .  .n=2  C ‚Äö.1 , which gives us M 0 1  therefore,   27-10  Solutions for Chapter 27: Multithreaded Algorithms  c. The parallelism of P-MATRIX-MULTIPLY-LESS-MEM is ‚Äö.n3=n  D ‚Äö.n2 . Ignoring the constants in the ‚Äö-notation, the parallelism for multiplying 1000 1000 matrices is 10002 D 106, which is only a factor of 10 less than that of P-MATRIX-MULTIPLY-RECURSIVE. Although the parallelism of the new procedure is much less than that of P-MATRIX-MULTIPLY-RECURSIVE, the algorithm still scales well for a large number of processors.  Solution to Problem 27-4  a. Here is a multithreaded Àù-reduction algorithm:  P-REDUCE.x; i; j    if i == j  return x≈íi ¬ç  else mid D b.i C j  =2c  lh D spawn P-REDUCE.x; i; mid  rh D P-REDUCE.x; mid C 1; j   sync return lh Àù rh  If we denote the length j  cid:0 i C1 of the subarray x≈íi : : j ¬ç by n, then the work for the above algorithm is given by the recurrence T1.n  D 2T1.n=2  C ‚Äö.1  D ‚Äö.n . Because one of the recursive calls to P-REDUCE is spawned and the procedure does constant work following the recursive calls and in the base case, the span is given by the recurrence T1.n  D T1.n=2  C ‚Äö.1  D ‚Äö.lg n .  b. The work and span of P-SCAN-1-AUX dominate the work and span of P- SCAN-1. We can calculate the work of P-SCAN-1-AUX by replacing the par- allel for loop with an ordinary for loop and noting that in each iteration, the running time of P-REDUCE will be equal to ‚Äö.l . Since P-SCAN-1 calls P- SCAN-1-AUX with 1 and n as the last two arguments, the running time of P-SCAN-1, and hence its work, is ‚Äö.1 C 2 C  C n  D ‚Äö.n2 . As we noted earlier, the parallel for loop in P-SCAN-1-AUX undergoes n it- erations; therefore, the span of P-SCAN-1-AUX is given by ‚Äö.lg n  for the recursive splitting of the loop iterations plus the span of the iteration that has maximum span. Among the loop iterations, the call to P-REDUCE in the last iteration  when l D n  has the maximum span, equal to ‚Äö.lg n . Thus, P- SCAN-1 has ‚Äö.lg n  span and ‚Äö.n2= lg n  parallelism.  c.  In P-SCAN-2-AUX, before the parallel for loop in lines 7 and 8 executes, the following invariant is satisÔ¨Åed: y≈íl¬ç D x≈íi ¬ç Àù x≈íi C 1¬ç Àù  Àù x≈íl¬ç for l D i; i C 1; : : : ; k and y≈íl¬ç D x≈ík C 1¬ç Àù x≈ík C 2¬ç Àù  Àù x≈íl¬ç for l D kC 1; kC 2; : : : ; j . The parallel for loop need not update y≈íi ¬ç; : : : ; y≈ík¬ç, since they have the correct values after the call to P-SCAN-2-AUX.x; y; i; k . For l D k C 1; k C 2; : : : ; j , the parallel for loop sets y≈íl¬ç D y≈ík¬ç Àù y≈íl¬ç  D x≈íi ¬ç Àù  Àù x≈ík¬ç Àù x≈ík C 1¬ç Àù  Àù x≈íl¬ç   Solutions for Chapter 27: Multithreaded Algorithms  27-11  D x≈íi ¬ç Àù  Àù x≈íl¬ç ;  as desired. We can run this loop in parallel because the lth iteration depends only on the values of y≈ík¬ç, which is the same in all iterations, and y≈íl¬ç. There- fore, when the call to P-SCAN-2-AUX from P-SCAN-2 returns, array y repre- sents the Àù-preÔ¨Åx computation of array x. Because the work and span of P-SCAN-2-AUX dominate the work and span of P-SCAN-2, we will concentrate on calculating these values for P-SCAN-2- AUX working on an array of size n. The work PS 2A1.n  of P-SCAN-2-AUX is given by the recurrence PS 2A1.n  D 2PS 2A1.n=2  C ‚Äö.n , which equals ‚Äö.n lg n  by case 2 of the master theorem. The span PS 2A1.n  of P-SCAN-2- AUX is given by the recurrence PS 2A1.n  D PS 2A1.n=2 C ‚Äö.lg n , which equals ‚Äö.lg2 n  per Exercise 4.6-2. That is, the work, span, and parallelism of P-SCAN-2 are ‚Äö.n lg n , ‚Äö.lg2 n , and ‚Äö.n= lg n , respectively.  d. The missing expression in line 8 of P-SCAN-UP is t ≈ík¬ç Àù right. The missing expressions in lines 5 and 6 of P-SCAN-DOWN are  and  Àù t ≈ík¬ç, respectively. As suggested in the hint, we will prove that the value  passed to P-SCAN-DOWN.; x; t; y; i; j   satisÔ¨Åes  D x≈í1¬ç Àù x≈í2¬ç Àù  Àù x≈íi  cid:0  1¬ç, so that the value  Àù x≈íi ¬ç stored into y≈íi ¬ç in the base case of P-SCAN-DOWN is correct. In order to compute the arguments that are passed to P-SCAN-DOWN, we must Ô¨Årst understand what t ≈ík¬ç holds as a result of the call to P-SCAN-UP. A call to P-SCAN-UP.x; t; i; j   returns x≈íi ¬ç Àù  Àù x≈íj ¬ç; because t ≈ík¬ç stores the return value of P-SCAN-UP.x; t; i; k , we can say that t ≈ík¬ç D x≈íi ¬ç Àù  Àù x≈ík¬ç. The value  D x≈í1¬ç when P-SCAN-DOWN.x≈í1¬ç; x; t; y; 2; n  is called from P-SCAN-3 clearly satisiÔ¨Åes  D x≈í1¬ç Àù  Àù x≈íi  cid:0  1¬ç. Let us suppose that  D x≈í1¬ç Àù x≈í2¬ç Àù  Àù x≈íi  cid:0  1¬ç in a call of P-SCAN-DOWN.; x; t; y; i; j  . Therefore,  meets the required condition in the Ô¨Årst recursive call, with i and k as the last two arguments, in P-SCAN-DOWN. If we can prove that the value  Àù t ≈ík¬ç passed to the second recursive call in P-SCAN-DOWN equals x≈í1¬ç Àù x≈í2¬ç Àù  Àù x≈ík¬ç, we would have proved the required condition on  for all calls to P-SCAN-DOWN. Earlier, we proved that t ≈ík¬ç D x≈íi ¬ç Àù  Àù x≈ík¬ç; therefore,  Àù t ≈ík¬ç D x≈í1¬ç Àù x≈í2¬ç Àù  Àù x≈íi  cid:0  1¬ç Àù x≈íi ¬ç Àù  x≈ík¬ç  D x≈í1¬ç Àù x≈í2¬ç Àù  Àù x≈ík¬ç :  Thus, the value  passed to P-SCAN-DOWN.; x; t; y; i; j   satisÔ¨Åes  D x≈í1¬çÀù x≈í2¬ç Àù  Àù x≈íi  cid:0  1¬ç.  e. Let PS U1.n  and PS U1.n  denote the work and span of P-SCAN-UP and let PSD1.n  and PSD1.n  denote the work and span of P-SCAN-DOWN. Then the expressions T1.n  D PS U1.n  C PSD1.n  C ‚Äö.1  and T1.n  D PS U1.n  C PSD1.n  C ‚Äö.1  characterize the work and span of P-SCAN-3. The work PS U1.n  of P-SCAN-UP is given by the recurrence PS U1.n  D 2PS U1.n=2  C ‚Äö.1  ; and its span is deÔ¨Åned by the recurrence   27-12  Solutions for Chapter 27: Multithreaded Algorithms  PS U1.n  D PS U1.n=2  C ‚Äö.1  : Using the master theorem to solve these recurrences, we get PS U1.n  D ‚Äö.n  and PS U1.n  D ‚Äö.lg n . Similarly, the recurrences    PSD1.n  D 2PSD1.n=2  C ‚Äö.1  ;  ¬é  PSD1.n  D PSD1.n=2  C ‚Äö.1  deÔ¨Åne the work and span of P-SCAN-DOWN, and they evaluate to PSD1.n  D ‚Äö.n  and PSD1.n  D ‚Äö.lg n . Applying the results for the work and span of P-SCAN-UP and P-SCAN-DOWN obtained above in the expressions for the work and span of P-SCAN-3, we get T1.n  D ‚Äö.n  and T1.n  D ‚Äö.lg n . Hence, P-SCAN-3 has ‚Äö.n= lg n  parallelism. P-SCAN-3 performs less work than P-SCAN-1, but with the same span, and it has the same parallelism as P-SCAN-2 with less work and a lower span.  Solution to Problem 27-5  a. In this part of the problem, we will assume that n is an exact power of 2, so that in a recursive step, when we divide the n  n matrix A into four n=2  n=2 matrices, we will be guaranteed that n=2 is an integer, for all n  2. We make this assumption simply to avoid introducing bn=2c and dn=2e terms in the pseudocode and the analysis that follow. In the pseudocode below, we assume that we have a procedure BASE-CASE available to us, which calculates the base case of the stencil.  SIMPLE-STENCIL .A; i; j; n   if n == 1  else    Calculate submatrix A11.  A≈íi; j ¬ç D BASE-CASE.A; i; j   SIMPLE-STENCIL .A; i; j; n=2     Calculate submatrices A12 and A21 in parallel. spawn SIMPLE-STENCIL.A; i; j C n=2; n=2  SIMPLE-STENCIL .A; i C n=2; j; n=2  sync    Calculate submatrix A22. SIMPLE-STENCIL .A; i C n=2; j C n=2; n=2   To perform a simple stencil calculation on an n  n matrix A, we call SIMPLE-STENCIL .A; 1; 1; n . The recurrence for the work is T1.n  D 4T1.n=2  C ‚Äö.1  D ‚Äö.n2 . Of the four recursive calls in the algorithm above, only two run in parallel. Therefore, the recurrence for the span is T1.n  D 3T1.n=2  C ‚Äö.1  D ‚Äö.nlg 3 , and the parallelism is ‚Äö.n2 cid:0 lg 3   ‚Äö.n0:415 . b. Similar to SIMPLE-STENCIL of the previous part, we present P-STENCIL-3, which divides A into nine submatrices, each of size n=3 n=3, and solves them   Solutions for Chapter 27: Multithreaded Algorithms  27-13  recursively. To perform a stencil calculation on an n  n matrix A, we call P-STENCIL-3 .A; 1; 1; n .  P-STENCIL-3.A; i; j; n   if n == 1  else    Group 1: compute submatrix A11.  A≈íi; j ¬ç D BASE-CASE.A; i; j   P-STENCIL-3 .A; i; j; n=3     Group 2: compute submatrices A12 and A21. spawn P-STENCIL-3.A; i; j C n=3; n=3  P-STENCIL-3 .A; i C n=3; j; n=3  sync    Group 3: compute submatrices A13, A22, and A31. spawn P-STENCIL-3.A; i; j C 2n=3; n=3  spawn P-STENCIL-3.A; i C n=3; j C n=3; n=3  P-STENCIL-3 .A; i C 2n=3; j; n=3  sync    Group 4: compute submatrices A23 and A32. spawn P-STENCIL-3.A; i C n=3; j C 2n=3; n=3  P-STENCIL-3 .A; i C 2n=3; j C n=3; n=3  sync    Group 5: compute submatrix A33. P-STENCIL-3 .A; i C 2n=3; j C 2n=3; n=3   From the pseudocode, we can informally say that we can solve the nine sub- problems in Ô¨Åve groups, as shown in the following matrix:  2 3 4   cid:0  1 2 3 3 4 5 :  Each entry in the above matrix speciÔ¨Åes the group of the corresponding n=3  n=3 submatrix of A; we can compute in parallel the entries of all submatrices that fall in the same group. In general, for i D 2; 3; 4; 5, we can calculate group i after completing the computation of group i  cid:0  1. The recurrence for the work is T1.n  D 9T1.n=3  C ‚Äö.1  D ‚Äö.n2 . The recurrence for the span is T1.n  D 5T1.n=3 C ‚Äö.1  D ‚Äö.nlog3 5 . Therefore, the parallelism is ‚Äö.n2 cid:0 log3 5   ‚Äö.n0:535 .  c. Similar to the previous part, we can solve the b2 subproblems in 2b  cid:0  1 groups:  2 3 :::   1  b  2 3 4 :::  3 4 5 ::: b  b C 1 b C 2  b  cid:0  1 b b C 1  b  cid:0  2 b  cid:0  1 b :::     : : :  2b  cid:0  5 2b  cid:0  4 2b  cid:0  3 b C 1  2b  cid:0  4 2b  cid:0  3 2b  cid:0  2  b C 1 b C 2  2b  cid:0  3 2b  cid:0  2 2b  cid:0  1Àò :  :::  :::  b  cid:0  2 b  cid:0  1 b  cid:0  1 b  b   27-14  Solutions for Chapter 27: Multithreaded Algorithms  The recurrence for the work is T1.n  D b2T1.n=b  C ‚Äö.1  D ‚Äö.n2 . The recurrence for the span is T1.n  D .2b cid:0  1 T1.n=b C ‚Äö.1  D ‚Äö.nlogb .2b cid:0 1  . The parallelism is ‚Äö.n2 cid:0 logb .2b cid:0 1  . As the hint suggests, in order to show that the parallelism must be o.n  for any choice of b  2, we need to show that 2  cid:0  logb.2b  cid:0  1 , which is the exponent of n in the parallelism, is strictly less than 1 for any choice of b  2. Since b  2, we know that 2b cid:0 1 > b, which implies that logb.2b cid:0 1  > logb b D 1. Hence, 2  cid:0  logb.2b  cid:0  1  < 2  cid:0  1 D 1.  d. The idea behind achieving ‚Äö.n= lg n  parallelism is similar to that presented in the previous part, except without recursive division. We will compute A≈í1; 1¬ç serially, which will enable us to compute entries A≈í1; 2¬ç and A≈í2; 1¬ç in parallel, after which we can compute entries A≈í1; 3¬ç, A≈í2; 2¬ç and A≈í3; 1¬ç in parallel, and so on. Here is the pseudocode:  P-STENCIL.A  n D A:rows    Calculate all entries on the antidiagonal and above it. for i D 1 to n  parallel for j D 1 to i  A≈íi  cid:0  j C 1; j ¬ç D BASE-CASE.A; i  cid:0  j C 1; j       Calculate all entries below the antidiagonal. for i D 2 to n  parallel for j D i to n  A≈ín C i  cid:0  j; j ¬ç D BASE-CASE.A; n C i  cid:0  j; j    For each value of index i of the Ô¨Årst serial for loop, the inner loop iterates i times, doing constant work in each iteration. Because index i ranges from 1 to n in the Ô¨Årst for loop, we require ‚Äö.1 C 2 C  C n  D ‚Äö.n2  work to calculate all entries on the antidiagonal and above it. For each value of index i of the second serial for loop, the inner loop iterates n  cid:0  i C 1 times, doing constant work in each iteration. Because index i ranges from 2 to n in the second for loop, we require ‚Äö..n  cid:0  1  C .n  cid:0  2  C  C 1  D ‚Äö.n2  work to calculate all entries on the antidiagonal and above it. Therefore, the work of P-STENCIL is T1.n  D ‚Äö.n2 . Note that both for loops in P-STENCIL, which execute parallel for loops within, are serial. Therefore, in order to calculate the span of P-STENCIL, we must add the spans of all the parallel for loops. Given that any parallel for loop in P-STENCIL does constant work in each iteration, the span of a parallel for loop with n0 iterations is ‚Äö.lg n0 . Hence, T1.n  D ‚Äö..lg 1 C lg 2 C  C lg n  C .lg.n  cid:0  1  C  C 1    D ‚Äö.lg.n≈†  C lg.n  cid:0  1 ≈†  D ‚Äö.n lg n  ;  giving us ‚Äö.n= lg n  parallelism.   Index  This index covers exercises and problems from the textbook that are solved in this manual. The Ô¨Årst page in the manual that has the solution is listed here.  Exercise 2.2-2, 2-17 Exercise 2.2-4, 2-17 Exercise 2.3-3, 2-17 Exercise 2.3-4, 2-18 Exercise 2.3-5, 2-18 Exercise 2.3-6, 2-19 Exercise 2.3-7, 2-19 Exercise 3.1-1, 3-7 Exercise 3.1-2, 3-7 Exercise 3.1-3, 3-8 Exercise 3.1-4, 3-8 Exercise 3.1-8, 3-8 Exercise 3.2-4, 3-9 Exercise 3.2-5, 3-9 Exercise 3.2-6, 3-10 Exercise 3.2-7, 3-10 Exercise 4.1-1, 4-17 Exercise 4.1-2, 4-17 Exercise 4.1-4, 4-17 Exercise 4.1-5, 4-18 Exercise 4.2-2, 4-19 Exercise 4.2-4, 4-19 Exercise 4.3-1, 4-20 Exercise 4.3-7, 4-20 Exercise 4.4-6, 4-21 Exercise 4.4-9, 4-21 Exercise 4.5-2, 4-22 Exercise 5.1-3, 5-9 Exercise 5.2-1, 5-10 Exercise 5.2-2, 5-10 Exercise 5.2-4, 5-11 Exercise 5.2-5, 5-12 Exercise 5.3-1, 5-13 Exercise 5.3-2, 5-13  Exercise 5.3-3, 5-13 Exercise 5.3-4, 5-14 Exercise 5.3-7, 5-14 Exercise 5.4-6, 5-16 Exercise 6.1-1, 6-10 Exercise 6.1-2, 6-10 Exercise 6.1-3, 6-10 Exercise 6.2-6, 6-11 Exercise 6.3-3, 6-11 Exercise 6.4-1, 6-14 Exercise 6.5-2, 6-15 Exercise 6.5-6, 6-15 Exercise 7.2-3, 7-9 Exercise 7.2-5, 7-9 Exercise 7.3-1, 7-10 Exercise 7.4-2, 7-10 Exercise 8.1-3, 8-10 Exercise 8.1-4, 8-10 Exercise 8.2-2, 8-11 Exercise 8.2-3, 8-11 Exercise 8.2-4, 8-11 Exercise 8.3-2, 8-12 Exercise 8.3-3, 8-12 Exercise 8.3-4, 8-13 Exercise 8.4-2, 8-13 Exercise 9.1-1, 9-10 Exercise 9.3-1, 9-10 Exercise 9.3-3, 9-11 Exercise 9.3-5, 9-12 Exercise 9.3-8, 9-13 Exercise 9.3-9, 9-14 Exercise 11.1-4, 11-16 Exercise 11.2-1, 11-17 Exercise 11.2-4, 11-17   I-2  Index  Exercise 11.2-6, 11-18 Exercise 11.3-3, 11-19 Exercise 11.3-5, 11-20 Exercise 12.1-2, 12-15 Exercise 12.2-5, 12-15 Exercise 12.2-7, 12-16 Exercise 12.3-3, 12-17 Exercise 12.4-1, 12-12 Exercise 12.4-2, 12-17 Exercise 12.4-3, 12-9 Exercise 12.4-4, 12-18 Exercise 13.1-3, 13-13 Exercise 13.1-4, 13-13 Exercise 13.1-5, 13-13 Exercise 13.2-4, 13-14 Exercise 13.3-3, 13-14 Exercise 13.3-4, 13-15 Exercise 13.4-6, 13-16 Exercise 13.4-7, 13-16 Exercise 14.1-5, 14-9 Exercise 14.1-6, 14-9 Exercise 14.1-7, 14-9 Exercise 14.2-2, 14-10 Exercise 14.3-3, 14-13 Exercise 14.3-6, 14-14 Exercise 14.3-7, 14-15 Exercise 15.1-1, 15-21 Exercise 15.1-2, 15-21 Exercise 15.1-3, 15-22 Exercise 15.1-4, 15-22 Exercise 15.1-5, 15-23 Exercise 15.2-4, 15-23 Exercise 15.2-5, 15-24 Exercise 15.3-1, 15-25 Exercise 15.3-5, 15-26 Exercise 15.3-6, 15-27 Exercise 15.4-4, 15-28 Exercise 16.1-1, 16-9 Exercise 16.1-2, 16-10 Exercise 16.1-3, 16-11 Exercise 16.1-4, 16-11 Exercise 16.1-5, 16-13 Exercise 16.2-2, 16-14 Exercise 16.2-4, 16-16 Exercise 16.2-6, 16-16 Exercise 16.2-7, 16-17 Exercise 16.3-1, 16-17 Exercise 16.4-2, 16-17  Exercise 16.4-3, 16-18 Exercise 17.1-3, 17-14 Exercise 17.2-1, 17-15 Exercise 17.2-2, 17-15 Exercise 17.2-3, 17-16 Exercise 17.3-3, 17-17 Exercise 21.2-3, 21-6 Exercise 21.2-5, 21-7 Exercise 21.2-6, 21-7 Exercise 21.3-3, 21-7 Exercise 21.3-4, 21-8 Exercise 21.3-5, 21-8 Exercise 21.4-4, 21-9 Exercise 21.4-5, 21-9 Exercise 21.4-6, 21-9 Exercise 22.1-6, 22-13 Exercise 22.1-7, 22-15 Exercise 22.2-3, 22-15 Exercise 22.2-5, 22-15 Exercise 22.2-6, 22-15 Exercise 22.2-7, 22-16 Exercise 22.3-4, 22-16 Exercise 22.3-5, 22-16 Exercise 22.3-8, 22-17 Exercise 22.3-9, 22-17 Exercise 22.3-11, 22-17 Exercise 22.3-12, 22-18 Exercise 22.4-3, 22-19 Exercise 22.4-5, 22-20 Exercise 22.5-5, 22-21 Exercise 22.5-6, 22-22 Exercise 22.5-7, 22-23 Exercise 23.1-1, 23-8 Exercise 23.1-4, 23-8 Exercise 23.1-6, 23-8 Exercise 23.1-10, 23-9 Exercise 23.2-4, 23-9 Exercise 23.2-5, 23-10 Exercise 23.2-7, 23-10 Exercise 24.1-3, 24-13 Exercise 24.2-3, 24-13 Exercise 24.3-3, 24-14 Exercise 24.3-4, 24-14 Exercise 24.3-5, 24-15 Exercise 24.3-6, 24-15 Exercise 24.3-8, 24-16 Exercise 24.3-9, 24-17 Exercise 24.4-4, 24-17   Index  Exercise 24.4-7, 24-18 Exercise 24.4-10, 24-18 Exercise 24.5-4, 24-19 Exercise 24.5-7, 24-19 Exercise 24.5-8, 24-19 Exercise 25.1-3, 25-9 Exercise 25.1-5, 25-9 Exercise 25.1-10, 25-10 Exercise 25.2-4, 25-13 Exercise 25.2-6, 25-13 Exercise 25.3-4, 25-14 Exercise 25.3-6, 25-14 Exercise 26.1-1, 26-12 Exercise 26.1-3, 26-13 Exercise 26.1-4, 26-15 Exercise 26.1-6, 26-16 Exercise 26.1-7, 26-16 Exercise 26.2-1, 26-17 Exercise 26.2-8, 26-18 Exercise 26.2-9, 26-18 Exercise 26.2-11, 26-19 Exercise 26.2-12, 26-20 Exercise 26.2-13, 26-21 Exercise 26.3-3, 26-22 Exercise 26.4-1, 26-22 Exercise 26.4-3, 26-23 Exercise 26.4-4, 26-23 Exercise 26.4-7, 26-23 Exercise 27.1-1, 27-1 Exercise 27.1-5, 27-1 Exercise 27.1-6, 27-2 Exercise 27.1-7, 27-2 Exercise 27.1-8, 27-3 Exercise 27.1-9, 27-3 Exercise 27.2-3, 27-4 Exercise 27.2-4, 27-4 Exercise 27.2-5, 27-6 Exercise 27.2-6, 27-7  Problem 2-1, 2-20 Problem 2-2, 2-21 Problem 2-4, 2-22 Problem 3-3, 3-10 Problem 4-1, 4-22 Problem 4-3, 4-24 Problem 5-1, 5-17 Problem 6-1, 6-15 Problem 6-2, 6-16  I-3  Problem 7-2, 7-11 Problem 7-4, 7-12 Problem 8-1, 8-13 Problem 8-3, 8-16 Problem 8-4, 8-17 Problem 8-7, 8-20 Problem 9-1, 9-15 Problem 9-2, 9-16 Problem 9-3, 9-19 Problem 9-4, 9-21 Problem 11-1, 11-21 Problem 11-2, 11-22 Problem 11-3, 11-24 Problem 12-2, 12-19 Problem 12-3, 12-20 Problem 13-1, 13-16 Problem 14-1, 14-15 Problem 14-2, 14-17 Problem 15-1, 15-29 Problem 15-2, 15-31 Problem 15-3, 15-34 Problem 15-4, 15-36 Problem 15-5, 15-39 Problem 15-8, 15-42 Problem 15-9, 15-45 Problem 15-11, 15-47 Problem 15-12, 15-50 Problem 16-1, 16-20 Problem 16-5, 16-23 Problem 17-2, 17-19 Problem 17-4, 17-20 Problem 21-1, 21-10 Problem 21-2, 21-11 Problem 22-1, 22-24 Problem 22-3, 22-24 Problem 22-4, 22-27 Problem 23-1, 23-12 Problem 24-1, 24-20 Problem 24-2, 24-21 Problem 24-3, 24-22 Problem 24-4, 24-23 Problem 24-6, 24-24 Problem 25-1, 25-14 Problem 26-2, 26-24 Problem 26-3, 26-26 Problem 26-4, 26-29 Problem 26-5, 26-30 Problem 27-1, 27-7   I-4  Index  Problem 27-2, 27-9 Problem 27-4, 27-10 Problem 27-5, 27-12
