Coding Theory    Coding Theory  Algorithms, Architectures, and  Applications  Andr´e Neubauer  M¨unster University of Applied Sciences, Germany  J¨urgen Freudenberger  HTWG Konstanz, University of Applied Sciences, Germany  Volker K¨uhn  University of Rostock, Germany   Copyright cid:211  2007JohnWiley&SonsLtd,TheAtrium,SouthernGate,Chichester,  West Sussex PO19 8SQ, England Telephone  +44  1243 779777  Email  for orders and customer service enquiries : cs-books@wiley.co.uk Visit our Home Page on www.wileyeurope.com or www.wiley.com  All Rights Reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording, scanning or otherwise, except under the terms of the Copyright, Designs and Patents Act 1988 or under the terms of a licence issued by the Copyright Licensing Agency Ltd, 90 Tottenham Court Road, London W1T 4LP, UK, without the permission in writing of the Publisher. Requests to the Publisher should be addressed to the Permissions Department, John Wiley & Sons Ltd, The Atrium, Southern Gate, Chichester, West Sussex PO19 8SQ, England, or emailed to permreq@wiley.co.uk, or faxed to  +44  1243 770620. Designations used by companies to distinguish their products are often claimed as trademarks. All brand names and product names used in this book are trade names, service marks, trademarks or registered trademarks of their respective owners. The Publisher is not associated with any product or vendor mentioned in this book. All trademarks referred to in the text of this publication are the property of their respective owners.  This publication is designed to provide accurate and authoritative information in regard to the subject matter covered. It is sold on the understanding that the Publisher is not engaged in rendering professional services. If professional advice or other expert assistance is required, the services of a competent professional should be sought. Other Wiley Editorial Ofﬁces  John Wiley & Sons Inc., 111 River Street, Hoboken, NJ 07030, USA  Jossey-Bass, 989 Market Street, San Francisco, CA 94103-1741, USA  Wiley-VCH Verlag GmbH, Boschstr. 12, D-69469 Weinheim, Germany  John Wiley & Sons Australia Ltd, 42 McDougall Street, Milton, Queensland 4064, Australia  John Wiley & Sons  Asia  Pte Ltd, 2 Clementi Loop 02-01, Jin Xing Distripark, Singapore 129809  John Wiley & Sons Canada Ltd, 6045 Freemont Blvd, Mississauga, Ontario, L5R 4J3, Canada  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books.  Anniversary Logo Design: Richard J. Paciﬁco  Library of Congress Cataloging-in-Publication Data  Coding theory : algorithms, architectures and applications   Andre  Neubauer, J¨urgen Freudenberger, Volker K¨uhn.  ISBN 978-0-470-02861-2  cloth  1. Coding theory. I Freudenberger, Jrgen. II. K¨uhn, Volker. III.  Neubauer, Andre.  p. cm.  Title  QA268.N48 2007  cid:1  003  .54–dc22  British Library Cataloguing in Publication Data  A catalogue record for this book is available from the British Library  ISBN 978-0-470-02861-2  HB   Typeset in 10 12pt Times by Laserwords Private Limited, Chennai, India Printed and bound in Great Britain by Antony Rowe Ltd, Chippenham, Wiltshire This book is printed on acid-free paper responsibly manufactured from sustainable forestry in which at least two trees are planted for each one used for paper production.   Contents  Preface  1 Introduction  ix  1 1 3 3 4 5 6 8  1.1 Communication Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . Information Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.2 1.2.1 Entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.2.2 Channel Capacity . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.2.3 Binary Symmetric Channel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.2.4 AWGN Channel 1.3 A Simple Channel Code . . . . . . . . . . . . . . . . . . . . . . . . . . . .  2 Algebraic Coding Theory  13 2.1 Fundamentals of Block Codes . . . . . . . . . . . . . . . . . . . . . . . . . 14 2.1.1 Code Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 2.1.2 Maximum Likelihood Decoding . . . . . . . . . . . . . . . . . . . . 19 2.1.3 Binary Symmetric Channel . . . . . . . . . . . . . . . . . . . . . . 23 2.1.4 Error Detection and Error Correction . . . . . . . . . . . . . . . . . 25 2.2 Linear Block Codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 2.2.1 Deﬁnition of Linear Block Codes . . . . . . . . . . . . . . . . . . . 27 2.2.2 Generator Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 Parity-Check Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . 30 2.2.3 2.2.4 Syndrome and Cosets . . . . . . . . . . . . . . . . . . . . . . . . . 31 2.2.5 Dual Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 2.2.6 Bounds for Linear Block Codes . . . . . . . . . . . . . . . . . . . . 37 2.2.7 Code Constructions . . . . . . . . . . . . . . . . . . . . . . . . . . 41 2.2.8 Examples of Linear Block Codes . . . . . . . . . . . . . . . . . . . 46 2.3 Cyclic Codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 . . . . . . . . . . . . . . . . . . . . . . 62 2.3.1 Deﬁnition of Cyclic Codes . . . . . . . . . . . . . . . . . . . . . . . . . 63 2.3.2 Generator Polynomial 2.3.3 Parity-Check Polynomial . . . . . . . . . . . . . . . . . . . . . . . . 67 2.3.4 Dual Codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 2.3.5 Linear Feedback Shift Registers . . . . . . . . . . . . . . . . . . . . 71 2.3.6 BCH Codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 2.3.7 Reed–Solomon Codes . . . . . . . . . . . . . . . . . . . . . . . . . 81   vi  CONTENTS  2.3.8 Algebraic Decoding Algorithm . . . . . . . . . . . . . . . . . . . . 84 2.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93  3 Convolutional Codes  97 3.1 Encoding of Convolutional Codes . . . . . . . . . . . . . . . . . . . . . . . 98 3.1.1 Convolutional Encoder . . . . . . . . . . . . . . . . . . . . . . . . . 98 3.1.2 Generator Matrix in the Time Domain . . . . . . . . . . . . . . . . 101 3.1.3 State Diagram of a Convolutional Encoder . . . . . . . . . . . . . . 103 3.1.4 Code Termination . . . . . . . . . . . . . . . . . . . . . . . . . . . 104 3.1.5 Puncturing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106 3.1.6 Generator Matrix in the D-Domain . . . . . . . . . . . . . . . . . . 108 3.1.7 Encoder Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . 110 3.2 Trellis Diagram and the Viterbi Algorithm . . . . . . . . . . . . . . . . . . 112 3.2.1 Minimum Distance Decoding . . . . . . . . . . . . . . . . . . . . . 113 3.2.2 Trellises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115 3.2.3 Viterbi Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . 116 3.3 Distance Properties and Error Bounds . . . . . . . . . . . . . . . . . . . . . 121 3.3.1 Free Distance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121 3.3.2 Active Distances . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122 3.3.3 Weight Enumerators for Terminated Codes . . . . . . . . . . . . . . 126 Path Enumerators . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129 3.3.4 3.3.5 Pairwise Error Probability . . . . . . . . . . . . . . . . . . . . . . . 131 3.3.6 Viterbi Bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134 3.4 Soft-input Decoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136 3.4.1 Euclidean Metric . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136 Support of Punctured Codes . . . . . . . . . . . . . . . . . . . . . . 137 3.4.2 3.4.3 Implementation Issues . . . . . . . . . . . . . . . . . . . . . . . . . 138 3.5 Soft-output Decoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140 3.5.1 Derivation of APP Decoding . . . . . . . . . . . . . . . . . . . . . 141 3.5.2 APP Decoding in the Log Domain . . . . . . . . . . . . . . . . . . 145 3.6 Convolutional Coding in Mobile Communications . . . . . . . . . . . . . . 147 3.6.1 Coding of Speech Data . . . . . . . . . . . . . . . . . . . . . . . . 147 3.6.2 Hybrid ARQ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150 3.6.3 EGPRS Modulation and Coding . . . . . . . . . . . . . . . . . . . . 152 3.6.4 Retransmission Mechanism . . . . . . . . . . . . . . . . . . . . . . 155 3.6.5 Link Adaptation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156 3.6.6 Incremental Redundancy . . . . . . . . . . . . . . . . . . . . . . . . 157 3.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160  4 Turbo Codes  4.1 LDPC Codes  163 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165 4.1.1 Codes Based on Sparse Graphs . . . . . . . . . . . . . . . . . . . . 165 4.1.2 Decoding for the Binary Erasure Channel . . . . . . . . . . . . . . 168 4.1.3 Log-Likelihood Algebra . . . . . . . . . . . . . . . . . . . . . . . . 169 4.1.4 Belief Propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . 174 4.2 A First Encounter with Code Concatenation . . . . . . . . . . . . . . . . . 177 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177  Product Codes  4.2.1   CONTENTS  vii  4.2.2  4.4 EXIT Charts  4.3 Concatenated Convolutional Codes  4.4.1 Calculating an EXIT Chart 4.4.2  Iterative Decoding of Product Codes . . . . . . . . . . . . . . . . . 180 . . . . . . . . . . . . . . . . . . . . . . 182 4.3.1 Parallel Concatenation . . . . . . . . . . . . . . . . . . . . . . . . . 182 4.3.2 The UMTS Turbo Code . . . . . . . . . . . . . . . . . . . . . . . . 183 4.3.3 Serial Concatenation . . . . . . . . . . . . . . . . . . . . . . . . . . 184 4.3.4 Partial Concatenation . . . . . . . . . . . . . . . . . . . . . . . . . . 185 4.3.5 Turbo Decoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188 . . . . . . . . . . . . . . . . . . . . . . 189 Interpretation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191 4.5 Weight Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196 4.5.1 Partial Weights . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196 4.5.2 Expected Weight Distribution . . . . . . . . . . . . . . . . . . . . . 197 4.6 Woven Convolutional Codes . . . . . . . . . . . . . . . . . . . . . . . . . . 198 4.6.1 Encoding Schemes . . . . . . . . . . . . . . . . . . . . . . . . . . . 200 4.6.2 Distance Properties of Woven Codes . . . . . . . . . . . . . . . . . 202 4.6.3 Woven Turbo Codes . . . . . . . . . . . . . . . . . . . . . . . . . . 205 4.6.4 Interleaver Design . . . . . . . . . . . . . . . . . . . . . . . . . . . 208 4.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212  5 Space–Time Codes  5.1  5.2 Spatial Channels  215 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215 5.1.1 Digital Modulation Schemes . . . . . . . . . . . . . . . . . . . . . . 216 5.1.2 Diversity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229 5.2.1 Basic Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229 5.2.2 Spatial Channel Models . . . . . . . . . . . . . . . . . . . . . . . . 234 5.2.3 Channel Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . 239 5.3 Performance Measures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241 5.3.1 Channel Capacity . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241 5.3.2 Outage Probability and Outage Capacity . . . . . . . . . . . . . . . 250 5.3.3 Ergodic Error Probability . . . . . . . . . . . . . . . . . . . . . . . 252 5.4 Orthogonal Space–Time Block Codes . . . . . . . . . . . . . . . . . . . . . 257 5.4.1 Alamouti’s Scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . 257 5.4.2 Extension to More than Two Transmit Antennas . . . . . . . . . . . 260 5.4.3 Simulation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . 263 5.5 Spatial Multiplexing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265 5.5.1 General Concept 5.5.2 Iterative APP Preprocessing and Per-layer Decoding . . . . . . . . . 267 5.5.3 Linear Multilayer Detection . . . . . . . . . . . . . . . . . . . . . . 272 5.5.4 Original BLAST Detection . . . . . . . . . . . . . . . . . . . . . . 275 5.5.5 QL Decomposition and Interference Cancellation . . . . . . . . . . 278 5.5.6 Performance of Multi-Layer Detection Schemes . . . . . . . . . . . 287 5.5.7 Uniﬁed Description by Linear Dispersion Codes . . . . . . . . . . . 291 5.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 294   viii  CONTENTS  A Algebraic Structures  295 A.1 Groups, Rings and Finite Fields . . . . . . . . . . . . . . . . . . . . . . . . 295 A.1.1 Groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295 A.1.2 Rings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296 A.1.3 Finite Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298 A.2 Vector Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299 A.3 Polynomials and Extension Fields . . . . . . . . . . . . . . . . . . . . . . . 300 A.4 Discrete Fourier Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . 305  B Linear Algebra  C Acronyms  Bibliography  Index  311  319  325  335   Preface  Modern information and communication systems are based on the reliable and efﬁcient transmission of information. Channels encountered in practical applications are usually disturbed regardless of whether they correspond to information transmission over noisy and time-variant mobile radio channels or to information transmission on optical discs that might be damaged by scratches. Owing to these disturbances, appropriate channel coding schemes have to be employed such that errors within the transmitted information can be detected or even corrected. To this end, channel coding theory provides suitable coding schemes for error detection and error correction. Besides good code characteristics with respect to the number of errors that can be detected or corrected, the complexity of the architectures used for implementing the encoding and decoding algorithms is important for practical applications.  The present book provides a concise overview of channel coding theory and practice as well as the accompanying algorithms, architectures and applications. The selection of the topics presented in this book is oriented towards those subjects that are relevant for information and communication systems in use today or in the near future. The focus is on those aspects of coding theory that are important for the understanding of these systems. This book places emphasis on the algorithms for encoding and decoding and their architectures, as well as the applications of the corresponding coding schemes in a uniﬁed framework.  The idea for this book originated from a two-day seminar on coding theory in the industrial context. We have tried to keep this seminar style in the book by highlighting the most important facts within the ﬁgures and by restricting the scope to the most important topics with respect to the applications of coding theory, especially within communication systems. This also means that many important and interesting topics could not be covered in order to be as concise as possible.  The target audience for the book are students of communication and information engi- neering as well as computer science at universities and also applied mathematicians who are interested in a presentation that subsumes theory and practice of coding theory without sac- riﬁcing exactness or relevance with regard to real-world practical applications. Therefore, this book is well suited for engineers in industry who want to know about the theoretical basics of coding theory and their application in currently relevant communication systems. The book is organised as follows. In Chapter 1 a brief overview of the principle architecture of a communication system is given and the information theory fundamentals underlying coding theory are summarised. The most important concepts of information the- ory, such as entropy and channel capacity as well as simple channel models, are described.   x  PREFACE  Chapter 2 presents the classical, i.e. algebraic, coding theory. The fundamentals of the encoding and decoding of block codes are explained, and the maximum likelihood decoding rule is derived as the optimum decoding strategy for minimising the word error probability after decoding a received word. Linear block codes and their deﬁnition based on generator and parity-check matrices are discussed. General performance measures and bounds relating important code characteristics such as the minimum Hamming distance and the code rate are presented, illustrating the compromises necessary between error detection and error correction capabilities and transmission efﬁciency. It is explained how new codes can be constructed from already known codes. Repetition codes, parity-check-codes, Hamming codes, simplex codes and Reed–Muller codes are presented as examples. Since the task of decoding linear block codes is difﬁcult in general, the algebraic properties of cyclic codes are exploited for efﬁcient decoding algorithms. These cyclic codes, together with their generator and parity-check polynomials, are discussed, as well as efﬁcient encoding and decoding architectures based on linear feedback shift registers. Important cyclic codes such as BCH codes and Reed–Solomon codes are presented, and an efﬁcient algebraic decoding algorithm for the decoding of these cyclic codes is derived.  Chapter 3 deals with the fundamentals of convolutional coding. Convolutional codes can be found in many applications, for instance in dial-up modems, satellite communications and digital cellular systems. The major reason for this popularity is the existence of efﬁcient decoding algorithms that can utilise soft input values from the demodulator. This so-called soft-input decoding leads to signiﬁcant performance gains. Two famous examples for a soft-input decoding algorithm are the Viterbi algorithm and the Bahl, Cocke, Jelinek, Raviv  BCJR  algorithm which also provides a reliability output. Both algorithms are based on the trellis representation of the convolutional code. This highly repetitive structure makes trellis-based decoding very suitable for hardware implementations.  We start our discussion with the encoding of convolutional codes and some of their basic properties. It follows a presentation of the Viterbi algorithm and an analysis of the error correction performance with this maximum likelihood decoding procedure. The concept of soft-output decoding and the BCJR algorithm are considered in Section 3.5. Soft- output decoding is a prerequisite for the iterative decoding of concatenated convolutional codes as introduced in Chapter 4. Finally, we consider an application of convolutional codes for mobile communication channels as deﬁned in the Global System for Mobile communications  GSM  standard. In particular, the considered hybrid ARQ protocols are excellent examples of the adaptive coding systems that are required for strongly time-variant mobile channels.  As mentioned above, Chapter 4 is dedicated to the construction of long powerful codes based on the concatenation of simple convolutional component codes. These concatenated convolutional codes, for example the famous turbo codes, are capable of achieving low bit error rates at signal-to-noise ratios close to the theoretical Shannon limit. The term turbo reﬂects a property of the employed iterative decoding algorithm, where the decoder output of one iteration is used as the decoder input of the next iteration. This concept of iterative decoding was ﬁrst introduced for the class of low-density parity-check codes. Therefore, we ﬁrst introduce low-density parity-check codes in Section 4.1 and discuss the relation between these codes and concatenated code constructions. Then, we introduce some popular encoding schemes for concatenated convolutional codes and present three methods to analyse the performance of the corresponding codes. The EXIT chart method   PREFACE  xi  in Section 4.4 makes it possible to predict the behaviour of the iterative decoder by looking at the input output relations of the individual constituent soft-output decoders. Next, we present a common approach in coding theory. We estimate the code performance with maximum likelihood decoding for an ensemble of concatenated codes. This method explains why many concatenated code constructions lead to a low minimum Hamming distance and therefore to a relatively poor performance for high signal-to-noise ratios. In Section 4.6 we consider code designs that lead to a higher minimum Hamming distance owing to a special encoder construction, called the woven encoder, or the application of designed interleavers. The ﬁfth chapter addresses space–time coding concepts, a still rather new topic in the area of radio communications. Although these techniques do not represent error-correcting codes in the classical sense, they can also be used to improve the reliability of a data link. Since space–time coding became popular only a decade ago, only a few concepts have found their way into current standards hitherto. However, many other approaches are currently being discussed. As already mentioned before, we restrict this book to the most important and promising concepts.  While classical encoders and decoders are separated from the physical channel by modulators, equalisers, etc., and experience rather simple hyperchannels, this is not true for space–time coding schemes. They directly work on the physical channel. Therefore, Chapter 5 starts with a short survey of linear modulation schemes and explains the prin- ciple of diversity. Next, spatial channel models are described and different performance measures for their quantitative evaluation are discussed. Sections 5.4 and 5.5 introduce two space–time coding concepts with the highest practical relevance, namely orthogonal space–time block codes increasing the diversity degree and spatial multiplexing techniques boosting the achievable data rate. For the latter approach, sophisticated signal processing algorithms are required at the receiver in order to separate superimposed data streams again. In the appendices a brief summary of algebraic structures such as ﬁnite ﬁelds and poly- nomial rings is given, which are needed for the treatment especially of classical algebraic codes, and the basics of linear algebra are brieﬂy reviewed.  Finally, we would like to thank the Wiley team, especially Sarah Hinton as the respon- sible editor, for their support during the completion of the manuscript. We also thank Dr. rer. nat. Jens Schlembach who was involved from the beginning of this book project and who gave encouragement when needed. Last but not least, we would like to give spe- cial thanks to our families – Fabian, Heike, Jana and Erik, Claudia, Hannah and Jakob and Christiane – for their emotional support during the writing of the manuscript.  Andr´e Neubauer  M¨unster University of Applied Sciences  J ¨urgen Freudenberger  HTWG Konstanz University of Applied Sciences  Volker K ¨uhn  University of Rostock    1  Introduction  The reliable transmission of information over noisy channels is one of the basic require- ments of digital information and communication systems. Here, transmission is understood both as transmission in space, e.g. over mobile radio channels, and as transmission in time by storing information in appropriate storage media. Because of this requirement, modern communication systems rely heavily on powerful channel coding methodologies. For practi- cal applications these coding schemes do not only need to have good coding characteristics with respect to the capability of detecting or correcting errors introduced on the channel. They also have to be efﬁciently implementable, e.g. in digital hardware within integrated circuits. Practical applications of channel codes include space and satellite communica- tions, data transmission, digital audio and video broadcasting and mobile communications, as well as storage systems such as computer memories or the compact disc  Costello et al., 1998 .  In this introductory chapter we will give a brief introduction into the ﬁeld of channel coding. To this end, we will describe the information theory fundamentals of channel coding. Simple channel models will be presented that will be used throughout the text. Furthermore, we will present the binary triple repetition code as an illustrative example of a simple channel code.  1.1 Communication Systems  In Figure 1.1 the basic structure of a digital communication system is shown which repre- sents the architecture of the communication systems in use today. Within the transmitter of such a communication system the following tasks are carried out:  ● source encoding,  ● channel encoding,  ● modulation.  Coding Theory – Algorithms, Architectures, and Applications Andr´e Neubauer, J¨urgen Freudenberger, Volker K¨uhn  cid:148  2007 John Wiley & Sons, Ltd   2  INTRODUCTION  Principal structure of digital communication systems  FEC source source encoder encoder encoder  FEC  channel encoder encoder  FEC modu- encoder lator  u  ˆu  b  r  FEC source encoder decoder  FEC  channel encoder decoder  FEC demo- encoder dulator  FEC  channel encoder    The sequence of information symbols u is encoded into the sequence of code symbols b which are transmitted across the channel after modulation.    The sequence of received symbols r is decoded into the sequence of information symbols ˆu which are estimates of the originally transmitted information symbols.  Figure 1.1: Basic structure of digital communication systems  In the receiver the corresponding inverse operations are implemented:  ● demodulation,  ● channel decoding,  ● source decoding.  According to Figure 1.1 the modulator generates the signal that is used to transmit the sequence of symbols b across the channel  Benedetto and Biglieri, 1999; Neubauer, 2007; Proakis, 2001 . Due to the noisy nature of the channel, the transmitted signal is disturbed. The noisy received signal is demodulated by the demodulator in the receiver, leading to the sequence of received symbols r. Since the received symbol sequence r usually differs from the transmitted symbol sequence b, a channel code is used such that the receiver is able to detect or even correct errors  Bossert, 1999; Lin and Costello, 2004; Neubauer, 2006b . To this end, the channel encoder introduces redundancy into the information sequence u. This redundancy can be exploited by the channel decoder for error detection or error correction by estimating the transmitted symbol sequence ˆu.  In his fundamental work, Shannon showed that it is theoretically possible to realise an information transmission system with as small an error probability as required  Shannon, 1948 . The prerequisite for this is that the information rate of the information source be smaller than the so-called channel capacity. In order to reduce the information rate, source coding schemes are used which are implemented by the source encoder in the transmitter and the source decoder in the receiver  McEliece, 2002; Neubauer, 2006a .   INTRODUCTION  3  Further information about source coding can be found elsewhere  Gibson et al., 1998; Sayood, 2000, 2003 .  In order better to understand the theoretical basics of information transmission as well as channel coding, we now give a brief overview of information theory as introduced by Shannon in his seminal paper  Shannon, 1948 . In this context we will also introduce the simple channel models that will be used throughout the text.  1.2 Information Theory  An important result of information theory is the ﬁnding that error-free transmission across a noisy channel is theoretically possible – as long as the information rate does not exceed the so-called channel capacity. In order to quantify this result, we need to measure information. Within Shannon’s information theory this is done by considering the statistics of symbols emitted by information sources.  1.2.1 Entropy  Let us consider the discrete memoryless information source shown in Figure 1.2. At a given time instant, this discrete information source emits the random discrete symbol X = xi which assumes one out of M possible symbol values x1, x2, . . . , xM. The rate at which these symbol values appear are given by the probabilities PX  x1 , PX  x2 , . . . , PX  xM   with  PX  xi   = Pr{X = xi}.  Discrete information source  Information  source  X    The discrete information source emits the random discrete symbol X .   The symbol values x1, x2, . . . , xM appear with probabilities PX  x1 , PX  x2 ,  . . . , PX  xM  .    Entropy  I  X   = − M cid:1   i=1  PX  xi   · log2 PX  xi      1.1   Figure 1.2: Discrete information source emitting discrete symbols X   4 INTRODUCTION The average information associated with the random discrete symbol X is given by the so-called entropy measured in the unit ‘bit’  I  X   = − M cid:1   i=1  PX  xi   · log2  PX  xi    .  For a binary information source that emits the binary symbols X = 0 and X = 1 with probabilities Pr{X = 0} = p0 and Pr{X = 1} = 1 − Pr{X = 0} = 1 − p0, the entropy is given by the so-called Shannon function or binary entropy function I  X   = −p0 log2 p0  −  1 − p0  log2 1 − p0 .  1.2.2 Channel Capacity  With the help of the entropy concept we can model a channel according to Berger’s channel diagram shown in Figure 1.3  Neubauer, 2006a . Here, X refers to the input symbol and R denotes the output symbol or received symbol. We now assume that M input symbol values x1, x2, . . . , xM and N output symbol values r1, r2, . . . , rN are possible. With the help of the conditional probabilities  and  and  the conditional entropies are given by  PXR xirj   = Pr{X = xiR = rj} PRX  rjxi   = Pr{R = rjX = xi}  cid:2   PX ,R xi , rj   · log2   cid:3   PXR xirj    PX ,R xi , rj   · log2 PRX  rjxi   .  I  XR  = − M cid:1  I  RX   = − M cid:1   i=1  N cid:1  N cid:1   j=1  i=1  j=1  With these conditional probabilities the mutual information  I  X;R  = I  X   − I  XR  = I  R  − I  RX    can be derived which measures the amount of information that is transmitted across the channel from the input to the output for a given information source. The so-called channel capacity C is obtained by maximising the mutual information I  X;R  with respect to the statistical properties of the input X , i.e. by appropriately choosing the probabilities {PX  xi  }1≤i≤M. This leads to  If the input entropy I  X   is smaller than the channel capacity C  C =  max  {PX  xi  }1≤i≤M  I  X;R .  I  X    ! < C,  then information can be transmitted across the noisy channel with arbitrarily small error probability. Thus, the channel capacity C in fact quantiﬁes the information transmission capacity of the channel.   INTRODUCTION  5  Berger’s channel diagram  I  X    I  R   I  XR   I  X;R   I  RX      Mutual information  I  X;R  = I  X   − I  XR  = I  R  − I  RX      Channel capacity  C =  max  {PX  xi  }1≤i≤M  I  X;R    1.2    1.3   Figure 1.3: Berger’s channel diagram  1.2.3 Binary Symmetric Channel  As an important example of a memoryless channel we turn to the binary symmetric channel or BSC. Figure 1.4 shows the channel diagram of the binary symmetric channel with bit error probability ε. This channel transmits the binary symbol X = 0 or X = 1 correctly with probability 1 − ε, whereas the incorrect binary symbol R = 1 or R = 0 is emitted with probability ε. By maximising the mutual information I  X;R , the channel capacity of a binary  symmetric channel is obtained according to  C = 1 + ε log2 ε  +  1 − ε  log2 1 − ε .  This channel capacity is equal to 1 if ε = 0 or ε = 1; for ε = 1 2 the channel capacity is 0. In contrast to the binary symmetric channel, which has discrete input and output symbols taken from binary alphabets, the so-called AWGN channel is deﬁned on the basis of continuous real-valued random variables.1  1In Chapter 5 we will also consider complex-valued random variables.   6  INTRODUCTION  Binary symmetric channel  X = 0  X = 1  1 − ε  1 − ε  ε  ε  R = 0  R = 1    Bit error probability ε    Channel capacity  C = 1 + ε log2 ε  +  1 − ε  log2 1 − ε    1.4   Figure 1.4: Binary symmetric channel with bit error probability ε  1.2.4 AWGN Channel  Up to now we have exclusively considered discrete-valued symbols. The concept of entropy can be transferred to continuous real-valued random variables by introducing the so-called differential entropy. It turns out that a channel with real-valued input and output symbols can again be characterised with the help of the mutual information I  X;R  and its maximum, the channel capacity C. In Figure 1.5 the so-called AWGN channel is illustrated which is described by the additive white Gaussian noise term Z.  S = E  and the noise power  With the help of the signal power   cid:5   cid:4 X 2  cid:4 Z2  cid:5  N = E  cid:6  the channel capacity of the AWGN channel is given by 1 + S   cid:7   log2  C = 1 2  .  N  The channel capacity exclusively depends on the signal-to-noise ratio S N.  In order to compare the channel capacities of the binary symmetric channel and the AWGN channel, we assume a digital transmission scheme using binary phase shift keying  BPSK  and optimal reception with the help of a matched ﬁlter  Benedetto and Biglieri, 1999; Neubauer, 2007; Proakis, 2001 . The signal-to-noise ratio of the real-valued output   INTRODUCTION  AWGN channel  X  R  +  Z    Signal-to-noise ratio S N    Channel capacity   cid:7    cid:6  1 + S  N  C = 1 2  log2  Figure 1.5: AWGN channel with signal-to-noise ratio S N  7   1.5   R of the matched ﬁlter is then given by  S  N  = Eb N0 2  ε = 1 2  erfc  Eb N0  .  with bit energy Eb and noise power spectral density N0. If the output R of the matched ﬁlter is compared with the threshold 0, we obtain the binary symmetric channel with bit error probability   cid:8  cid:9    cid:10   Here, erfc ·  denotes the complementary error function. In Figure 1.6 the channel capaci- ties of the binary symmetric channel and the AWGN channel are compared as a function of Eb N0. The signal-to-noise ratio S N or the ratio Eb N0 must be higher for the binary symmetric channel compared with the AWGN channel in order to achieve the same channel capacity. This gain also translates to the coding gain achievable by soft-decision decoding as opposed to hard-decision decoding of channel codes, as we will see later  e.g. in Section 2.2.8 .  Although information theory tells us that it is theoretically possible to ﬁnd a channel code that for a given channel leads to as small an error probability as required, the design of good channel codes is generally difﬁcult. Therefore, in the next chapters several classes of channel codes will be described. Here, we start with a simple example.   8  INTRODUCTION  Channel capacity of BSC vs AWGN channel  1.5  1  0.5  C  AWGN BSC  0  cid:358 5   cid:358 4   cid:358 3   cid:358 2  2  3  4  5  Eb N0   Signal-to-noise ratio of AWGN channel    Bit error probability of binary symmetric channel   cid:11    cid:12   1   cid:358 1 0 10 log10  S  N  = Eb N0 2  cid:8  cid:9    cid:10   ε = 1 2  erfc  Eb N0   1.6    1.7   Figure 1.6: Channel capacity of the binary symmetric channel vs the channel capacity of  the AWGN channel  1.3 A Simple Channel Code  As an introductory example of a simple channel code we consider the transmission of the binary information sequence over a binary symmetric channel with bit error probability ε = 0.25  Neubauer, 2006b . On average, every fourth binary symbol will be received incorrectly. In this example we assume that the binary sequence  0 0 1 0 1 1 1 0  0 0 0 0 0 1 1 0  is received at the output of the binary symmetric channel  see Figure 1.7 .   INTRODUCTION  Channel transmission  9  00101110  BSC  00000110    Binary symmetric channel with bit error probability ε = 0.25   Transmission w o channel code  Figure 1.7: Channel transmission without channel code  Encoder  00101110  Encoder  000000111000111111111000    Binary information symbols 0 and 1    Binary code words 000 and 111   Binary triple repetition code {000, 111}  Figure 1.8: Encoder of a triple repetition code  In order to implement a simple error correction scheme we make use of the so-called binary triple repetition code. This simple channel code is used for the encoding of binary data. If the binary symbol 0 is to be transmitted, the encoder emits the code word 000. Alternatively, the code word 111 is issued by the encoder when the binary symbol 1 is to be transmitted. The encoder of a triple repetition code is illustrated in Figure 1.8.  For the binary information sequence given above we obtain the binary code sequence  000 000 111 000 111 111 111 000  at the output of the encoder. If we again assume that on average every fourth binary symbol is incorrectly transmitted by the binary symmetric channel, we may obtain the received sequence  010 000 011 010 111 010 111 010.  This is illustrated in Figure 1.9.   10  INTRODUCTION  Channel transmission  000000111000111111111000  BSC  010000011010111010111010    Binary symmetric channel with bit error probability ε = 0.25   Transmission with binary triple repetition code  Figure 1.9: Channel transmission of a binary triple repetition code  Decoder  010000011010111010111010  Decoder  00101010    Decoding of triple repetition code by majority decision  000  cid:3 → 000 001  cid:3 → 000 010  cid:3 → 000 011  cid:3 → 111  ...  110  cid:3 → 111 111  cid:3 → 111  Figure 1.10: Decoder of a triple repetition code  The decoder in Figure 1.10 tries to estimate the original information sequence with the help of a majority decision. If the number of 0s within a received 3-bit word is larger than the number of 1s, the decoder emits the binary symbol 0; otherwise a 1 is decoded. With this decoding algorithm we obtain the decoded information sequence  0 0 1 0 1 0 1 0.   INTRODUCTION  11  As can be seen from this example, the binary triple repetition code is able to correct a single error within a code word. More errors cannot be corrected. With the help of this simple channel code we are able to reduce the number of errors. Compared with the unprotected transmission without a channel code, the number of errors has been reduced from two to one. However, this is achieved by a signiﬁcant reduction in the transmission bandwidth because, for a given symbol rate on the channel, it takes 3 times longer to transmit an information symbol with the help of the triple repetition code. It is one of the main topics of the following chapters to present more efﬁcient coding schemes.    2  Algebraic Coding Theory  In this chapter we will introduce the basic concepts of algebraic coding theory. To this end, the fundamental properties of block codes are ﬁrst discussed. We will deﬁne important code parameters and describe how these codes can be used for the purpose of error detection and error correction. The optimal maximum likelihood decoding strategy will be derived and applied to the binary symmetric channel.  With these fundamentals at hand we will then introduce linear block codes. These channel codes can be generated with the help of so-called generator matrices owing to their special algebraic properties. Based on the closely related parity-check matrix and the syndrome, the decoding of linear block codes can be carried out. We will also introduce dual codes and several techniques for the construction of new block codes based on known ones, as well as bounds for the respective code parameters and the accompanying code characteristics. As examples of linear block codes we will treat the repetition code, parity- check code, Hamming code, simplex code and Reed–Muller code.  Although code generation can be carried out efﬁciently for linear block codes, the decoding problem for general linear block codes is difﬁcult to solve. By introducing further algebraic structures, cyclic codes can be derived as a subclass of linear block codes for which efﬁcient algebraic decoding algorithms exist. Similar to general linear block codes, which are deﬁned using the generator matrix or the parity-check matrix, cyclic codes are deﬁned with the help of the so-called generator polynomial or parity-check polynomial. Based on linear feedback shift registers, the respective encoding and decoding architectures for cyclic codes can be efﬁciently implemented. As important examples of cyclic codes we will discuss BCH codes and Reed–Solomon codes. Furthermore, an algebraic decoding algorithm is presented that can be used for the decoding of BCH and Reed–Solomon codes.  In this chapter the classical algebraic coding theory is presented. In particular, we will follow work  Berlekamp, 1984; Bossert, 1999; Hamming, 1986; Jungnickel, 1995; Lin and Costello, 2004; Ling and Xing, 2004; MacWilliams and Sloane, 1998; McEliece, 2002; Neubauer, 2006b; van Lint, 1999  that contains further details about algebraic coding theory.  Coding Theory – Algorithms, Architectures, and Applications Andr´e Neubauer, J¨urgen Freudenberger, Volker K¨uhn  cid:148  2007 John Wiley & Sons, Ltd   14 2.1 Fundamentals of Block Codes  ALGEBRAIC CODING THEORY  In Section 1.3, the binary triple repetition code was given as an introductory example of a simple channel code. This speciﬁc channel code consists of two code words 000 and 111 of length n = 3, which represent k = 1 binary information symbol 0 or 1 respectively. Each symbol of a binary information sequence is encoded separately. The respective code word of length 3 is then transmitted across the channel. The potentially erroneously received word is ﬁnally decoded into a valid code word 000 or 111 – or equivalently into the respective information symbol 0 or 1. As we have seen, this simple code is merely able to correct one error by transmitting three code symbols instead of just one information symbol across the channel.  In order to generalise this simple channel coding scheme and to come up with more efﬁ- cient and powerful channel codes, we now consider an information sequence u0 u1 u2 u3 u4 u5 u6 u7 . . . of discrete information symbols ui. This information sequence is grouped into blocks of length k according to  cid:13   cid:16  u0 u1 ··· uk−1   cid:13   cid:16  u2k u2k+1 ··· u3k−1   cid:13   cid:16  uk uk+1 ··· u2k−1  ··· .   cid:14  cid:15    cid:14  cid:15    cid:14  cid:15   block  block  block  In so-called q-nary  n, k  block codes the information words  of length k with ui ∈ {0, 1, . . . , q − 1} are encoded separately from each other into the corresponding code words  u0 u1 ··· uk−1, uk uk+1 ··· u2k−1, u2k u2k+1 ··· u3k−1, ...  b0 b1 ··· bn−1, bn bn+1 ··· b2n−1, b2n b2n+1 ··· b3n−1, ...  of length n with bi ∈ {0, 1, . . . , q − 1}  see Figure 2.1 .1 These code words are trans- mitted across the channel and the received words are appropriately decoded, as shown in Figure 2.2. In the following, we will write the information word u0 u1 ··· uk−1 and the code word b0 b1 ··· bn−1 as vectors u =  u0, u1, . . . , uk−1  and b =  b0, b1, . . . , bn−1  respectively. Accordingly, the received word is denoted by r =  r0, r1, . . . , rn−1 , whereas  1For q = 2 we obtain the important class of binary channel codes.   ALGEBRAIC CODING THEORY  15  Encoding of an  n, k  block code   u0, u1, . . . , uk−1   Encoder   b0, b1, . . . , bn−1     The sequence of information symbols is grouped into words  or blocks  of  equal length k which are independently encoded.    Each information word  u0, u1, . . . , uk−1  of length k is uniquely mapped  onto a code word  b0, b1, . . . , bn−1  of length n.  Figure 2.1: Encoding of an  n, k  block code  Decoding of an  n, k  block code   r0, r1, . . . , rn−1   Decoder   ˆb0, ˆb1, . . . , ˆbn−1     The received word  r0, r1, . . . , rn−1  of length n at the output of the channel is decoded into the code word  ˆb0, ˆb1, . . . , ˆbn−1  of length n or the information word ˆu =  ˆu0, ˆu1, . . . , ˆuk−1  of length k respectively.  Figure 2.2: Decoding of an  n, k  block code  the decoded code word and decoded information word are given by ˆb =  ˆb0, ˆb1, . . . , ˆbn−1  and ˆu =  ˆu0, ˆu1, . . . , ˆuk−1  respectively. In general, we obtain the transmission scheme for an  n, k  block code as shown in Figure 2.3.  Without further algebraic properties of the  n, k  block code, the encoding can be carried out by a table look-up procedure. The information word u to be encoded is used to address a table that for each information word u contains the corresponding code word b at the respective address. If each information symbol can assume one out of q possible values,   16  ALGEBRAIC CODING THEORY  Encoding, transmission and decoding of an  n, k  block code    Information word u =  u0, u1, . . . , uk−1  of length k   Code word b =  b0, b1, . . . , bn−1  of length n   Received word r =  r0, r1, . . . , rn−1  of length n   Decoded code word ˆb =  ˆb0, ˆb1, . . . , ˆbn−1  of length n   Decoded information word ˆu =  ˆu0, ˆu1, . . . , ˆuk−1  of length k  u  Encoder  Channel  Decoder  b  r  ˆb    The information word u is encoded into the code word b.  word r.    The code word b is transmitted across the channel which emits the received   Based on the received word r, the code word ˆb  or equivalently the  information word ˆu  is decoded.  Figure 2.3: Encoding, transmission and decoding of an  n, k  block code  the number of code words is given by q k. Since each entry carries n q-nary symbols, the total size of the table is n q k. The size of the table grows exponentially with increasing information word length k. For codes with a large number of code words – corresponding to a large information word length k – a coding scheme based on a table look-up procedure is inefﬁcient owing to the large memory size needed. For that reason, further algebraic properties are introduced in order to allow for a more efﬁcient encoder architecture of an  n, k  block code. This is the idea lying behind the so-called linear block codes, which we will encounter in Section 2.2.  2.1.1 Code Parameters  Channel codes are characterised by so-called code parameters. The most important code parameters of a general  n, k  block code that are introduced in the following are the code rate and the minimum Hamming distance  Bossert, 1999; Lin and Costello, 2004; Ling and Xing, 2004 . With the help of these code parameters, the efﬁciency of the encoding process and the error detection and error correction capabilities can be evaluated for a given  n, k  block code.   ALGEBRAIC CODING THEORY  Code Rate  17  Under the assumption that each information symbol ui of the  n, k  block code can assume q values, the number of possible information words and code words is given by2  Since the code word length n is larger than the information word length k, the rate at which information is transmitted across the channel is reduced by the so-called code rate  M = q k.  R = logq  M   n  = k n  .  For the simple binary triple repetition code with k = 1 and n = 3, the code rate is R = = 1  ≈ 0,3333.  k n  3  Weight and Hamming Distance Each code word b =  b0, b1, . . . , bn−1  can be assigned the weight wt b  which is deﬁned as the number of non-zero components bi  cid:7 = 0  Bossert, 1999 , i.e.3  wt b  = {i : bi  cid:7 = 0, 0 ≤ i < n} .  cid:5  cid:17  cid:17  i , 0 ≤ i < n  cid:1     = cid:17  cid:17  cid:4   i : bi  cid:7 = b  Accordingly, the distance between two code words b =  b0, b1, . . . , bn−1  and b cid:1  =  b  cid:1  0,  cid:1  1, . . . , b b   cid:1  n−1  is given by the so-called Hamming distance  Bossert, 1999    cid:1  dist b, b The Hamming distance dist b, b cid:1    provides the number of different components of b and b cid:1  are to each other. For a code B consisting of M code words b1, b2, . . . , bM, the minimum Hamming distance is given by  and thus measures how close the code words b and b cid:1   .  d = min∀b cid:7 =b cid:1  dist b, b   cid:1    .  } with M = q k q-nary code words We will denote the  n, k  block code B = {b1, b2, . . . , bM of length n and minimum Hamming distance d by B n, k, d . The minimum weight of the wt b . The code parameters of B n, k, d  are summarised block code B is deﬁned as min∀b cid:7 =0 in Figure 2.4.  Weight Distribution } The so-called weight distribution W  x  of an  n, k  block code B = {b1, b2, . . . , bM describes how many code words exist with a speciﬁc weight. Denoting the number of code words with weight i by  wi = {b ∈ B : wt b  = i}  2In view of the linear block codes introduced in the following, we assume here that all possible information words u =  u0, u1, . . . , uk−1  are encoded. 3B denotes the cardinality of the set B.   18  ALGEBRAIC CODING THEORY  Code parameters of  n, k  block codes B n, k, d     Code rate    Minimum weight    Minimum Hamming distance  R = k  n  wt b   min∀b cid:7 =0  d = min∀b cid:7 =b cid:1  dist b, b   cid:1       2.1    2.2    2.3   W  x  = n cid:1   i=0  wi xi .  wt b  = min  wi .  i>0  min∀b cid:7 =0  Figure 2.4: Code parameters of  n, k  block codes B n, k, d   with 0 ≤ i ≤ n and 0 ≤ wi ≤ M, the weight distribution is deﬁned by the polynomial  Bossert, 1999   The minimum weight of the block code B can then be obtained from the weight distribution according to  A q-nary block code B n, k, d  with code word length n can be illustrated as a subset of the so-called code space Fn q. Such a code space is a graphical illustration of all possible q-nary words or vectors.4 The total number of vectors of length n with weight w and  Code Space  q-nary components is given by cid:6   cid:7   w with the binomial coefﬁcients  w The total number of vectors within Fn  n   q − 1 w =  cid:6   cid:7   n!  w!  n − w !   q − 1 w  n!  =  cid:6   n  .  w!  n − w !  cid:7   q is then obtained from  q − 1 w = q n.  n   = n cid:1   w=0  w  Fn  q  4In Section 2.2 we will identify the code space with the ﬁnite vector space Fn  q . For a brief overview of  algebraic structures such as ﬁnite ﬁelds and vector spaces the reader is referred to Appendix A.   ALGEBRAIC CODING THEORY  19  Four-dimensional binary code space F4 2   cid:2   cid:2   cid:2   cid:2   cid:2    cid:3   cid:3   cid:3   cid:3   cid:3   4 0  4 1  4 2  4 3  4 4  0000  cid:1   cid:3   cid:2    cid:3    cid:2    cid:3    cid:2    cid:2    cid:2    cid:2    cid:3    cid:3    cid:1 0010   cid:3  cid:3    cid:1 0001  cid:3    cid:1 1000  cid:3    cid:1  cid:1  cid:1  cid:1  cid:1  cid:1  cid:1  cid:1  cid:1   cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4    cid:3  cid:3  1001   cid:2  cid:2   cid:1 0100   cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4   cid:1  cid:1  cid:1  cid:1  cid:1  cid:1  cid:1  cid:1  cid:1   cid:6  cid:6  cid:6  cid:6  cid:6  cid:6  cid:6  cid:6  cid:6  cid:6  cid:6  cid:6  cid:6   cid:6  cid:6  cid:6  cid:6  cid:6  cid:6  cid:6  cid:6  cid:6  cid:6  cid:6  cid:6  cid:6   cid:5  cid:5  cid:5  cid:5  cid:5  cid:5  cid:5  cid:5  cid:5  cid:5  cid:5  cid:5  cid:5   cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4   cid:5  cid:5  cid:5  cid:5  cid:5  cid:5  cid:5  cid:5  cid:5  cid:5  cid:5  cid:5  cid:5   cid:6  cid:6  cid:6  cid:6  cid:6  cid:6  cid:6  cid:6  cid:6  cid:6  cid:6  cid:6  cid:6   cid:6  cid:6  cid:6  cid:6  cid:6  cid:6  cid:6  cid:6  cid:6  cid:6  cid:6  cid:6  cid:6   cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4   cid:1  cid:1  cid:1  cid:1  cid:1  cid:1  cid:1  cid:1  cid:1   cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4    cid:1  1011   cid:3  cid:3  1110   cid:2  cid:2   cid:1   cid:3    cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4  cid:4   cid:1  cid:1  cid:1  cid:1  cid:1  cid:1  cid:1  cid:1  cid:1    cid:1 0110  cid:3    cid:3  cid:3   cid:2    cid:2  cid:2   cid:1  0111   cid:1   cid:3    cid:3  cid:3    cid:1 0101  1010  1101   cid:3    cid:2    cid:3    cid:2    cid:3    cid:3    cid:2    cid:3    cid:3    cid:3    cid:3    cid:2    cid:2    cid:1    cid:1    cid:1    cid:2  cid:2   cid:3  cid:3   cid:1  1111  1100   cid:1 0011  wt = 2  wt = 0  wt = 1  wt = 3  wt = 4    The four-dimensional binary code space F4  vectors of length 4.  2 consists of 24 = 16 binary  Figure 2.5: Four-dimensional binary code space F4  2. Reproduced by permission of  J. Schlembach Fachverlag  The four-dimensional binary code space F4  Neubauer, 2006b .  2 with q = 2 and n = 4 is illustrated in Figure 2.5  2.1.2 Maximum Likelihood Decoding  Channel codes are used in order to decrease the probability of incorrectly received code words or symbols. In this section we will derive a widely used decoding strategy. To this end, we will consider a decoding strategy to be optimal if the corresponding word error probability  is minimal  Bossert, 1999 . The word error probability has to be distinguished from the symbol error probability  perr = Pr{ˆu  cid:7 = u} = Pr{ˆb  cid:7 = b}  k−1 cid:1   k  i=0  psym = 1  Pr{ˆui  cid:7 = ui}   20  ALGEBRAIC CODING THEORY  Optimal decoder  r  Decoder  ˆb r     The received word r is decoded into the code word ˆb r  such that the word  perr = Pr{ˆu  cid:7 = u} = Pr{ˆb  cid:7 = b}   2.4   error probability  is minimal.  Figure 2.6: Optimal decoder with minimal word error probability  which denotes the probability of an incorrectly decoded information symbol ui. In general, the symbol error probability is harder to derive analytically than the word error probability. However, it can be bounded by the following inequality  Bossert, 1999   perr ≤ psym ≤ perr.  1 k  In the following, a q-nary channel code B ∈ Fn q with M code words b1, b2, . . . , bM in q is considered. Let bj be the transmitted code word. Owing to the noisy the code space Fn channel, the received word r may differ from the transmitted code word bj . The task of the decoder in Figure 2.6 is to decode the transmitted code word based on the sole knowledge of r with minimal word error probability perr. This decoding step can be written according to the decoding rule r  cid:3 → ˆb = ˆb r . For hard-decision decoding the received word r is an element of the discrete code space Fn q. To each code word bj we assign a corresponding subspace Dj of the code space Fn q, the so-called decision region. These non-overlapping decision regions create the whole code q and Di ∩ Dj = ∅ for i  cid:7 = j as illustrated in Figure 2.7. If space Fn the received word r lies within the decision region Di, the decoder decides in favour of the code word bi. That is, the decoding of the code word bi according to the decision rule ˆb r  = bi is equivalent to the event r ∈ Di. By properly choosing the decision regions Di,  Dj = Fn  q, i.e.   cid:18   j=1  the decoder can be designed. For an optimal decoder the decision regions are chosen such that the word error probability perr is minimal. The probability of the event that the code word b = bj is transmitted and the code word ˆb r  = bi is decoded is given by  M  Pr{   ˆb r  = bi   ∧  b = bj  } = Pr{ r ∈ Di   ∧  b = bj  }.  We obtain the word error probability perr by averaging over all possible events for which the transmitted code word b = bj is decoded into a different code word ˆb r  = bi with   ALGEBRAIC CODING THEORY  21  Decision regions  Fn q  D1  Dj  Di  . . .  DM   cid:18     Decision regions Dj with  Dj = Fn  q and Di ∩ Dj = ∅ for i  cid:7 = j  M  j=1  Figure 2.7: Non-overlapping decision regions Dj in the code space Fn q  i  cid:7 = j. This leads to  Neubauer, 2006b   cid:1   cid:1   cid:1   perr = Pr{ˆb r   cid:7 = b} = M cid:1  Pr{  = M cid:1  = M cid:1   i=1  i=1  j cid:7 =i  j cid:7 =i  i=1  j cid:7 =i  r∈D  i  ˆb r  = bi   ∧  b = bj  }  Pr{ r ∈ Di   ∧  b = bj  }  cid:1   Pr{r ∧  b = bj  }.  With the help of Bayes’ rule Pr{r ∧  b = bj  } = Pr{b = bjr} Pr{r} and by changing the order of summation, we obtain  perr = M cid:1  = M cid:1   i=1   cid:1   cid:1   r∈D  i   cid:1   cid:1   j cid:7 =i  i=1  r∈D  i  j cid:7 =i  Pr{r ∧  b = bj  }  Pr{b = bjr} Pr{r}   22  ALGEBRAIC CODING THEORY   cid:1   = M cid:1   i=1  r∈D  i   cid:1   j cid:7 =i  Pr{r}  Pr{b = bjr}.  The inner sum can be simpliﬁed by observing the normalisation condition  Pr{b = bjr} = Pr{b = bir} +  Pr{b = bjr} = 1.   cid:1   j cid:7 =i  M cid:1   j=1  This leads to the word error probability  perr = M cid:1    cid:1   i=1  r∈D  i  Pr{r}  1 − Pr{b = bir}  .  In order to minimise the word error probability perr, we deﬁne the decision regions Di by assigning each possible received word r to one particular decision region. If r is assigned to the particular decision region Di for which the inner term Pr{r}  1 − Pr{b = bir}  is smallest, the word error probability perr will be minimal. Therefore, the decision regions are obtained from the following assignment  r ∈ Dj ⇔ Pr{r}  cid:2   1 − Pr{b = bjr} cid:3  = min  Pr{r}  1 − Pr{b = bir}  .  1≤i≤M  Since the probability Pr{r} does not change with index i, this is equivalent to  r ∈ Dj ⇔ Pr{b = bjr} = max 1≤i≤M  Pr{b = bir}.  Finally, we obtain the optimal decoding rule according to  ˆb r  = bj ⇔ Pr{b = bjr} = max 1≤i≤M  Pr{b = bir}.  The optimal decoder with minimal word error probability perr emits the code word ˆb = ˆb r  for which the a-posteriori probability Pr{b = bir} = Pr{bir} is maximal. This decoding strategy  ˆb r  = argmax b∈B  Pr{br}  is called MED  minimum error probability decoding  or MAP  maximum a-posteriori  decoding  Bossert, 1999 . For this MAP decoding strategy the a-posteriori probabilities Pr{br} have to be deter- mined for all code words b ∈ B and received words r. With the help of Bayes’ rule  and by omitting the term Pr{r} which does not depend on the speciﬁc code word b, the decoding rule  Pr{br} = Pr{rb} Pr{b}  Pr{r}  ˆb r  = argmax b∈B  Pr{rb} Pr{b}   ALGEBRAIC CODING THEORY  23  Optimal decoding strategies  r  Decoder  ˆb r    2.5    2.6   M    For Minimum Error Probability Decoding  MED  or Maximum A-Posteriori   MAP  decoding the decoder rule is    For Maximum Likelihood Decoding  MLD  the decoder rule is  ˆb r  = argmax b∈B  Pr{br}  ˆb r  = argmax b∈B  Pr{rb}    MLD is identical to MED if all code words are equally likely, i.e. Pr{b} = 1  .  Figure 2.8: Optimal decoding strategies  follows. For MAP decoding, the conditional probabilities Pr{rb} as well as the a-priori probabilities Pr{b} have to be known. If all M code words b appear with equal probabil- ity Pr{b} = 1 M, we obtain the so-called MLD  maximum likelihood decoding  strategy  Bossert, 1999   ˆb r  = argmax b∈B  Pr{rb}.  These decoding strategies are summarised in Figure 2.8. In the following, we will assume that all code words are equally likely, so that maximum likelihood decoding can be used as the optimal decoding rule. In order to apply the maximum likelihood decoding rule, the conditional probabilities Pr{rb} must be available. We illustrate how this decoding rule can be further simpliﬁed by considering the binary symmetric channel.  2.1.3 Binary Symmetric Channel  In Section 1.2.3 we deﬁned the binary symmetric channel as a memoryless channel with the conditional probabilities   cid:19   Pr{ribi} =  1 − ε,  ε,  ri = bi ri  cid:7 = bi  with channel bit error probability ε. Since the binary symmetric channel is assumed to be memoryless, the conditional probability Pr{rb} can be calculated for code word b =   24  b0, b1, . . . , bn−1  and received word r =  r0, r1, . . . , rn−1  according to  ALGEBRAIC CODING THEORY  Pr{rb} = n−1 cid:20   i=0  Pr{ribi}.  If the words r and b differ in dist r, b  symbols, this yields  Pr{rb} =  1 − ε n−dist r,b  εdist r,b  =  1 − ε n   cid:6   ε  1 − ε   cid:7 dist r,b   .  Taking into account 0 ≤ ε < 1  ˆb r  = argmax b∈B  Pr{rb} = argmax b∈B   1 − ε n  ε  1 − ε  2 and therefore  ε  1−ε < 1, the MLD rule is given by   cid:6    cid:7 dist r,b  = argmin  b∈B  dist r, b ,  i.e. for the binary symmetric channel the optimal maximum likelihood decoder  Bossert, 1999   ˆb r  = argmin b∈B  dist r, b   emits that particular code word which differs in the smallest number of components from the received word r, i.e. which has the smallest Hamming distance to the received word r  see Figure 2.9 . This decoding rule is called minimum distance decoding. This minimum distance decoding rule is also optimal for a q-nary symmetric channel  Neubauer, 2006b . We now turn to the error probabilities for the binary symmetric channel during transmission before decoding. The probability of w errors at w given positions within the n-dimensional binary received word r is given by εw  1 − ε n−w. Since there are different possibilities   cid:2    cid:3   n w  Minimum distance decoding for the binary symmetric channel  r  Decoder  ˆb r     The optimal maximum likelihood decoding rule for the binary symmetric  channel is given by the minimum distance decoding rule  ˆb r  = argmin b∈B  dist r, b    2.7   Figure 2.9: Minimum distance decoding for the binary symmetric channel   ALGEBRAIC CODING THEORY  25  of choosing w out of n positions, the probability of w errors at arbitrary positions within an n-dimensional binary received word follows the binomial distribution   cid:6    cid:7   n  w  Pr{w errors} =  εw  1 − ε n−w  2 , the probability Pr{w errors} decreases with The probability of error-free transmission is Pr{0 errors} =  1 − ε n, whereas the prob-  with mean n ε. Because of the condition ε < 1 increasing number of errors w, i.e. few errors are more likely than many errors. ability of a disturbed transmission with r  cid:7 = b is given by  Pr{r  cid:7 = b} = n cid:1   w=1   cid:6    cid:7   n  w  εw  1 − ε n−w = 1 −  1 − ε n.  2.1.4 Error Detection and Error Correction  Based on the minimum distance decoding rule and the code space concept, we can assess the error detection and error correction capabilities of a given channel code. To this end, let b and b cid:1  be two code words of an  n, k  block code B n, k, d . The distance of these code   = d. We are able to words shall be equal to the minimum Hamming distance, i.e. dist b, b cid:1  detect errors as long as the erroneously received word r is not equal to a code word different from the transmitted code word. This error detection capability is guaranteed as long as the number of errors is smaller than the minimum Hamming distance d, because another code word  e.g. b cid:1    can be reached from a given code word  e.g. b  merely by changing at least d components. For an  n, k  block code B n, k, d  with minimum Hamming distance d, the number of detectable errors is therefore given by  Bossert, 1999; Lin and Costello, 2004; Ling and Xing, 2004; van Lint, 1999   edet = d − 1.  For the analysis of the error correction capabilities of the  n, k  block code B n, k, d  we deﬁne for each code word b the corresponding correction ball of radius  cid:2  as the subset of all words that are closer to the code word b than to any other code word b cid:1  of the block code B n, k, d   see Figure 2.10 . As we have seen in the last section, for minimum distance decoding, all received words within a particular correction ball are decoded into the respective code word b. According to the radius  cid:2  of the correction balls, besides the code word b, all words that differ in 1, 2, . . . ,  cid:2  components from b are elements of the corresponding correction ball. We can uniquely decode all elements of a correction ball into the corresponding code word b as long as the correction balls do not intersect. This 2 holds. Therefore, the number of correctable errors of a block condition is true if  cid:2  < d code B n, k, d  with minimum Hamming distance d is given by  Bossert, 1999; Lin and Costello, 2004; Ling and Xing, 2004; van Lint, 1999 5   cid:21    cid:22   .  d − 1 2  ecor =  5The term  cid:13 z cid:14  denotes the largest integer number that is not larger than z.   26  ALGEBRAIC CODING THEORY  Error detection and error correction  b  b cid:1     If the minimum Hamming distance between two arbitrary code words is d  the code is able to detect up to    If the minimum Hamming distance between two arbitrary code words is d  the code is able to correct up to   2.8    2.9   correction  balls  d  edet = d − 1   cid:21    cid:22   ecor =  d − 1 2  errors.  errors.  Figure 2.10: Error detection and error correction  transmitted code word is binomially distributed according to Pr{w errors} = cid:2   For the binary symmetric channel the number of errors w within the n-dimensional εw  1 − ε n−w. Since an edet-error detecting code is able to detect w ≤ edet = d − 1 errors, the remaining detection error probability is bounded by   cid:3   n w  εw  1 − ε n−w = 1 − edet cid:1   εw  1 − ε n−w.  If an ecor-error correcting code is used with ecor =  cid:13  d − 1  2 cid:14 , the word error probability for a binary symmetric channel can be similarly bounded by  εw  1 − ε n−w = 1 − ecor cid:1   εw  1 − ε n−w.  n cid:1   n cid:1    cid:6    cid:7   n  w   cid:6    cid:7   n  w  pdet ≤  w=edet+1  perr ≤  w=ecor+1   cid:6    cid:7   n  w   cid:6    cid:7   n  w  w=0  w=0   ALGEBRAIC CODING THEORY 2.2 Linear Block Codes  27  In the foregoing discussion of the fundamentals of general block codes we exclusively focused on the distance properties between code words and received words in the code q. If we consider the code words to be vectors in the ﬁnite vector space Fn space Fn q, taking into account the respective algebraic properties of vector spaces, we gain efﬁciency especially with regard to the encoding scheme.6  2.2.1 Deﬁnition of Linear Block Codes The  n, k  block code B n, k, d  with minimum Hamming distance d over the ﬁnite ﬁeld Fq is called linear, if B n, k, d  is a subspace of the vector space Fn q of dimension k  Lin and Costello, 2004; Ling and Xing, 2004 . The number of code words is then given by  according to the code rate  M = q k  R = k  .  n  Because of the linearity property, an arbitrary superposition of code words again leads to a valid code word of the linear block code B n, k, d , i.e.  α2b1 + α2b2 + ··· + αlbl ∈ B n, k, d   with α1, α2, . . . , αl ∈ Fq and b1, b2, . . . , bl ∈ B n, k, d . Owing to the linearity, the n- dimensional zero row vector 0 =  0, 0, . . . , 0  consisting of n zeros is always a valid code word. It can be shown that the minimum Hamming distance of a linear block code B n, k, d  is equal to the minimum weight of all non-zero code words, i.e.  d = min∀b cid:7 =b cid:1  dist b, b   cid:1     = min∀b cid:7 =0  wt b .  These properties are summarised in Figure 2.11. As a simple example of a linear block code, the binary parity-check code is described in Figure 2.12  Bossert, 1999 .  For each linear block code an equivalent code can be found by rearranging the code word symbols.7 This equivalent code is characterised by the same code parameters as the original code, i.e. the equivalent code has the same dimension k and the same minimum Hamming distance d.  2.2.2 Generator Matrix The linearity property of a linear block code B n, k, d  can be exploited for efﬁciently encoding a given information word u =  u0, u1, . . . , uk−1 . To this end, a basis {g0, g1, . . . , gk−1} of the subspace spanned by the linear block code is chosen, consisting of k linearly independent n-dimensional vectors  gi =  gi,0, gi,1,··· , gi,n−1   6Finite ﬁelds and vector spaces are brieﬂy reviewed in Sections A.1 and A.2 in Appendix A. 7In general, an equivalent code is obtained by suitable operations on the rows and columns of the generator  matrix G which is deﬁned in Section 2.2.2.   28  ALGEBRAIC CODING THEORY  Linear block codes    A linear q-nary  n, k  block code B n, k, d  is deﬁned as a subspace of the    The number of code words is given by M = q k.   The minimum Hamming distance d is given by the minimum weight  vector space Fn q.  wt b  = d.  min∀b cid:7 =0  Figure 2.11: Linear block codes B n, k, d   Binary parity-check code    This code takes a k-dimensional    The binary parity-check code is a linear block code over the ﬁnite ﬁeld F2 . information word u =  u0, u1, . . . , uk−1  for  and generates the code word b =  b0, b1, . . . , bk−1, bk  with bi = ui 0 ≤ i ≤ k − 1 and  bk = u0 + u1 + ··· + uk−1 = k−1 cid:1   ui  i=0   2.10     The bit bk is called the parity bit; it is chosen such that the resulting code  word is of even parity.  Figure 2.12: Binary parity-check code  with 0 ≤ i ≤ k − 1. The corresponding code word b =  b0, b1, . . . , bn−1  is then given by   with the q-nary information symbols ui ∈ Fq. If we deﬁne the k × n matrix   b = u0 g0 + u1 g1 + ··· + uk−1 gk−1   =    g0,0    g0  G =  g0,1 g1,1 ...  g1,0 ...  ··· g0,n−1 ··· g1,n−1 . . . ··· gk−1,n−1  ...  gk−1,0  gk−1,1  g1 ... gk−1   ALGEBRAIC CODING THEORY  29  Generator matrix    The generator matrix G of a linear block code is constructed by a suitable  set of k linearly independent basis vectors gi according to ··· g0,n−1 ··· g1,n−1 . . . ··· gk−1,n−1  g1 ... gk−1  g0,1 g1,1 ...  g1,0 ...  G =  gk−1,1  gk−1,0  ...    g0,0    g0    =       The k-dimensional information word u is encoded into the n-dimensional  code word b by the encoding rule   2.11    2.12   b = u G  Figure 2.13: Generator matrix G of a linear block code B n, k, d   the information word u =  u0, u1, . . . , uk−1  is encoded according to the matrix–vector multiplication  b = u G.  Since all M = q k code words b ∈ B n, k, d  can be generated by this rule, the matrix G is called the generator matrix of the linear block code B n, k, d   see Figure 2.13 . Owing to this property, the linear block code B n, k, d  is completely deﬁned with the help of the generator matrix G  Bossert, 1999; Lin and Costello, 2004; Ling and Xing, 2004 .  In Figure 2.14 the so-called binary  7, 4  Hamming code is deﬁned by the given gen- erator matrix G. Such a binary Hamming code has been deﬁned by Hamming  Hamming, 1950 . These codes or variants of them are used, e.g. in memories such as dynamic random access memories  DRAMs , in order to correct deteriorated data in memory cells. is deﬁned by the k × n generator matrix  For each linear block code B n, k, d  an equivalent linear block code can be found that   cid:3  Owing to the k × k identity matrix Ik and the encoding rule  Ik  G = cid:2  b = u G = cid:2    cid:17  cid:17  Ak,n−k  cid:3   cid:17  cid:17  u Ak,n−k  u  .  the ﬁrst k code symbols bi are identical to the k information symbols ui. Such an encoding scheme is called systematic. The remaining m = n − k symbols within the vector u Ak,n−k correspond to m parity-check symbols which are attached to the information vector u for the purpose of error detection or error correction.   30  ALGEBRAIC CODING THEORY  Generator matrix of a binary Hamming code    The generator matrix G of a binary  7,4  Hamming code is given by    1 0 0 0 0 1 1  0 1 0 0 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 1 1     G =    The code parameters of this binary Hamming code are n = 7, k = 4 and  d = 3, i.e. this code is a binary B 7, 4, 3  code.    The information word u =  0, 0, 1, 1  is encoded into the code word    1 0 0 0 0 1 1  0 1 0 0 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 1 1    =  0, 0, 1, 1, 0, 0, 1   b =  0, 0, 1, 1   Figure 2.14: Generator matrix of a binary  7, 4  Hamming code  2.2.3 Parity-Check Matrix With the help of the generator matrix G =  Ik the so-called parity-check matrix – can be deﬁned  Bossert, 1999; Lin and Costello, 2004; Ling and Xing, 2004  with the  n − k  ×  n − k  identity matrix In−k. The  n − k  × k matrix Bn−k,k is given by   cid:17  cid:17  Ak,n−k , the following  n − k  × n matrix –  cid:17  cid:17  In−k  H = cid:2   Bn−k,k   cid:3   Bn−k,k = −AT  k,n−k.  For the matrices G and H the following property can be derived  with the  n − k  × k zero matrix 0n−k,k. The generator matrix G and the parity-check matrix H are orthogonal, i.e. all row vectors of G are orthogonal to all row vectors of H. Using the n-dimensional basis vectors g0, g1, . . . , gk−1 and the transpose of the generator  matrix GT = cid:2   H GT = Bn−k,k + AT  cid:3   = 0n−k,k  k,n−k  gT 0 , gT H GT = H  1 , . . . , gT gT 0 , gT  k−1  1 , . . . , gT  k−1  , we obtain   cid:3  = cid:2    cid:2    cid:3  =  0, 0, . . . , 0   H gT  0 , H gT  1 , . . . , H gT  k−1  with the  n − k -dimensional all-zero column vector 0 =  0, 0, . . . , 0 T. This is equivalent = 0 for 0 ≤ i ≤ k − 1. Since each code vector b ∈ B n, k, d  can be written as to H gT i  b = u G = u0 g0 + u1 g1 + ··· + uk−1 gk−1   ALGEBRAIC CODING THEORY  31  Parity-check matrix  matrix G = cid:2    cid:3    cid:17  cid:17 Ak,n−k  Ik    The parity-check matrix H of a linear block code B n, k, d  with generator  is deﬁned by  H = cid:2 −AT  k,n−k   cid:3    cid:17  cid:17  In−k  H GT = 0n−k,k    Generator matrix G and parity-check matrix H are orthogonal    The system of parity-check equations is given by  H rT = 0 ⇔ r ∈ B n, k, d    2.13    2.14    2.15   Figure 2.15: Parity-check matrix H of a linear block code B n, k, d   with the information vector u =  u0, u1, . . . , uk−1 , it follows that + ··· + uk−1 H gT k−1  H bT = u0 H gT  + u1 H gT  1  0  = 0.  Each code vector b ∈ B n, k, d  of a linear  n, k  block code B n, k, d  fulﬁls the condition  H bT = 0.  Equivalently, if H rT  cid:7 = 0, the vector r does not belong to the linear block code B n, k, d . We arrive at the following parity-check condition  H rT = 0 ⇔ r ∈ B n, k, d   which amounts to a total of n − k parity-check equations. Therefore, the matrix H is called the parity-check matrix of the linear  n, k  block code B n, k, d   see Figure 2.15 .  There exists an interesting relationship between the minimum Hamming distance d and the parity-check matrix H which is stated in Figure 2.16  Lin and Costello, 2004 . In Figure 2.17 the parity-check matrix of the binary Hamming code with the generator matrix given in Figure 2.14 is shown. The corresponding parity-check equations of this binary Hamming code are illustrated in Figure 2.18.  2.2.4 Syndrome and Cosets As we have seen in the last section, a vector r corresponds to a valid code word of a given linear block code B n, k, d  with parity-check matrix H if and only if the parity-check equation H rT = 0 is true. Otherwise, r is not a valid code word of B n, k, d . Based on   32  ALGEBRAIC CODING THEORY  Parity-check matrix and minimum Hamming distance    Let H be the parity-check matrix of a linear block code B n, k, d  with  minimum Hamming distance d.    The minimum Hamming distance d is equal to the smallest number of  linearly dependent columns of the parity-check matrix H.  Figure 2.16: Parity-check matrix H and minimum Hamming distance d of a linear block  code B n, k, d   Parity-check matrix of a binary Hamming code    The parity-check matrix H of a binary  7,4  Hamming code is given by    0 1 1 1 1 0 0  1 0 1 1 0 1 0 1 1 0 1 0 0 1     H =  It consists of all non-zero binary column vectors of length 3.    Two arbitrary columns are linearly independent. However, the ﬁrst three columns are linearly dependent. The minimum Hamming distance of a binary Hamming code is d = 3.    The code word b =  0, 0, 1, 1, 0, 0, 1  fulﬁls the parity-check equation    0 1 1 1 1 0 0  1 0 1 1 0 1 0 1 1 0 1 0 0 1     H bT =     =      0 0 1 1 0 0 1       0  0 0  Figure 2.17: Parity-check matrix H of a binary Hamming code   ALGEBRAIC CODING THEORY  33  Graphical representation of the parity-check equations  +  +  +  b0  b1  b2  b3  b4  b5  b6    The parity-check equations of a binary  7,4  Hamming code with code word  b =  b0, b1, b2, b3, b4, b5, b6  are given by  b1 + b2 + b3 + b4 = 0 b0 + b2 + b3 + b5 = 0 b0 + b1 + b3 + b6 = 0  Figure 2.18: Graphical representation of the parity-check equations of a binary  7, 4   Hamming code B 7, 4, 3   the algebraic channel model shown in Figure 2.19, we interpret r as the received vector which is obtained from  r = b + e  with the transmitted code vector b and the error vector e. The jth component of the error vector e is ej = 0 if no error has occurred at this particular position; otherwise the jth component is ej  cid:7 = 0.  In view of the parity-check equation, we deﬁne the so-called syndrome  Lin and  Costello, 2004; Ling and Xing, 2004   which is used to check whether the received vector r belongs to the channel code B n, k, d . Inserting the received vector r into this deﬁnition, we obtain  sT = H rT = H  b + e   + H eT = H eT  .  Here, we have taken into account that for each code vector b ∈ B n, k, d  the condition H bT = 0 holds. Finally, we recognize that the syndrome does exclusively depend on the error vector e, i.e.  sT = H rT   cid:13  cid:14  cid:15  cid:16  T = H bT =0  sT = H eT.   +  e  r = b + e  sT = H rT  34  ALGEBRAIC CODING THEORY  Algebraic channel model  b  r    The transmitted n-dimensional code vector b is disturbed by the  n-dimensional error vector e.    The received vector r is given by    The syndrome  exclusively depends on the error vector e according to sT = H eT.  Figure 2.19: Algebraic channel model   2.16    2.17   Thus, for the purpose of error detection the syndrome can be evaluated. If s is zero, the received vector r is equal to a valid code vector, i.e. r ∈ B n, k, d . In this case no error can be detected and it is assumed that the received vector corresponds to the transmitted code vector. If e is zero, the received vector r = b delivers the transmitted code vector b. However, all non-zero error vectors e that fulﬁl the condition  H eT = 0  sT = H eT  also lead to a valid code word.8 These errors cannot be detected. In general, the  n − k -dimensional syndrome sT = H eT of a linear  n, k  block code B n, k, d  corresponds to n − k scalar equations for the determination of the n-dimensional error vector e. The matrix equation  does not uniquely deﬁne the error vector e. All vectors e with H eT = sT form a set, the so-called coset of the k-dimensional subspace B n, k, d  in the ﬁnite vector space Fn q. This coset has q k elements. For an error-correcting q-nary block code B n, k, d  and a given  8This property directly follows from the linearity of the block code B n, k, d .   ALGEBRAIC CODING THEORY  35  syndrome s, the decoder has to choose one out of these q k possible error vectors. As we have seen for the binary symmetric channel, it is often the case that few errors are more likely than many errors. An optimal decoder implementing the minimum distance decoding rule therefore chooses the error vector out of the coset that has the smallest number of non- zero components. This error vector is called the coset leader  Lin and Costello, 2004; Ling and Xing, 2004 . The decoding of the linear block code B n, k, d  can be carried out by a table look- up procedure as illustrated in Figure 2.20. The  n − k -dimensional syndrome s is used to address a table that for each particular syndrome contains the corresponding n-dimensional coset leader e at the respective address. Since there are q n−k syndromes and each entry carries n q-nary symbols, the total size of the table is n q n−k. The size of the table grows exponentially with n − k. For codes with large n − k, a decoding scheme based on a table look-up procedure is inefﬁcient.9 By introducing further algebraic properties, more efﬁcient  Syndrome decoding  r  Syndrome Calculation  s  Table Lookup  ˆe  +-  +  ˆb    The syndrome is calculated with the help of the received vector r and the  parity-check matrix H according to    The syndrome s is used to address a table that for each syndrome stores  the respective coset leader as the decoded error vector ˆe.    By subtracting the decoded error vector ˆe from the received vector r, the  decoded code word ˆb is obtained  sT = H rT  ˆb = r − ˆe   2.18    2.19   Figure 2.20: Syndrome decoding of a linear block code B n, k, d  with a table look-up  procedure. Reproduced by permission of J. Schlembach Fachverlag  9It can be shown that in general the decoding of a linear block code is NP-complete.   36  ALGEBRAIC CODING THEORY  decoder architectures can be deﬁned. This will be apparent in the context of cyclic codes in Section 2.3.  2.2.5 Dual Code So far we have described two equivalent ways to deﬁne a linear block code B n, k, d  based on the k × n generator matrix G and the  n − k  × n parity-check matrix H. By  cid:1  simply exchanging these matrices, we can deﬁne a new linear block code B⊥ with generator matrix   cid:1   n  , d  , k      cid:1   ⊥ = H G  , k ⊥ b  bT = 0.  and parity-check matrix as shown in Figure 2.21. Because of the orthogonality condition H GT = 0, each n n-dimensional code word b⊥ B n, k, d , i.e.   cid:1  =   is orthogonal to every code word b of  ⊥ = G H  of B⊥   cid:1   n  , d   cid:1    cid:1    cid:1    cid:1   , k  , d   cid:1   n  The resulting code B⊥   is called the dual code or orthogonal code,  Bossert, 1999; Lin and Costello, 2004; Ling and Xing, 2004; van Lint, 1999 . Whereas the original code B n, k, d  with M = q k code words has dimension k, the dimension of the dual code is  cid:1  = n − k. Therefore, it includes M ⊥ = q n = Fn . k By again exchanging the generator matrix and the parity-check matrix, we end up with the original code. Formally, this means that the dual of the dual code is the original code  ⊥ = q n−k code words, leading to M M  q   cid:2    cid:3 ⊥ = B n, k, d .  B⊥   cid:1   n   cid:1    cid:1   , k  , d     A linear block code B n, k, d  that fulﬁls B⊥   cid:1   n   cid:1   , k  , d   cid:1     = B n, k, d  is called self-dual.  Dual code and MacWilliams identity    The dual code B⊥   cid:1   n   cid:1    cid:1   , k  , d    of a linear q-nary block code B n, k, d  with  parity-check matrix H is deﬁned by the generator matrix    The weight distribution W  from the MacWilliams identity  ⊥   x  of the q-nary dual code B⊥   cid:1   n   cid:1    cid:1   , k  , d    follows  ⊥ = H G   2.20    2.21    cid:7    cid:6   1 − x  1 +  q − 1  x  ⊥   x  = q  −k  1 +  q − 1  x n W  W  Figure 2.21: Dual code B⊥   cid:1   n   cid:1    cid:1   , k  , d    and MacWilliams identity   ALGEBRAIC CODING THEORY Because of the close relationship between B n, k, d  and its dual B⊥  , we can derive the properties of the dual code B⊥   from the properties of the original code B n, k, d . In particular, the MacWilliams identity holds for the corresponding weight  x  and W  x   Bossert, 1999; Lin and Costello, 2004; Ling and Xing, distributions W 2004; MacWilliams and Sloane, 1998 . The MacWilliams identity states that   cid:1   n   cid:1   n  37  , d  , d  , k  , k  ⊥   cid:1    cid:1    cid:1    cid:1   ⊥  W   x  = q  −k  1 +  q − 1  x n W For linear binary block codes with q = 2 we obtain −k  1 + x n W   x  = 2  W  ⊥   cid:7   1 +  q − 1  x  .   cid:6    cid:6   1 − x  cid:7   .  1 − x 1 + x  The MacWilliams identity can, for example, be used to determine the minimum Hamming of the dual code B⊥   on the basis of the known weight distribution distance d of the original code B n, k, d .   cid:1   n  , d  , k   cid:1    cid:1    cid:1   2.2.6 Bounds for Linear Block Codes The code rate R = k n and the minimum Hamming distance d are important parameters of a linear block code B n, k, d . It is therefore useful to know whether a linear block code theoretically exists for a given combination of R and d. In particular, for a given minimum Hamming distance d we will consider a linear block code to be better than another linear block code with the same minimum Hamming distance d if it has a higher code rate R. There exist several theoretical bounds which we will brieﬂy discuss in the following  Berlekamp, 1984; Bossert, 1999; Ling and Xing, 2004; van Lint, 1999   see also Figure 2.22 . Block codes that fulﬁl a theoretical bound with equality are called optimal codes.  Singleton Bound  The simplest bound is the so-called Singleton bound. For a linear block code B n, k, d  it is given by  k ≤ n − d + 1.  A linear block code that fulﬁls the Singleton bound with equality according to k = n − d + 1 is called MDS  maximum distance separable . Important representatives of MDS codes are Reed–Solomon codes which are used, for example, in the channel coding scheme within the audio compact disc  see also Section 2.3.7 .  Sphere Packing Bound The so-called sphere packing bound or Hamming bound can be derived for a linear ecor =  cid:13  d − 1  2 cid:14 -error correcting q-nary block code B n, k, d  by considering the correction balls within the code space Fn  q. Each correction ball encompasses a total of   cid:6   ecor cid:1    cid:7   n  i  i=0   q − 1 i   38  ALGEBRAIC CODING THEORY  Bounds for linear block codes B n, k, d     Singleton bound    Hamming bound  with ecor =  cid:13  d − 1  2 cid:14     Plotkin bound  for d > θ n with θ = 1 − 1 d−2 cid:1    Gilbert–Varshamov bound  q    Griesmer bound  k ≤ n − d + 1  cid:7   cid:6    q − 1 i ≤ q n−k  ecor cid:1   i=0  n  i  q k ≤  d  d − θ n   cid:6   cid:7  n − 1  i=0  i   q − 1 i < q n−k  cid:29    cid:30   n ≥ k−1 cid:1   i=0  d q i   2.22    2.23    2.24    2.25    2.26   Figure 2.22: Bounds for linear q-nary block codes B n, k, d   vectors in the code space Fn q. Since there is exactly one correction ball for each code word, we have M = q k correction balls. The total number of vectors within any correction ball of radius ecor is then given by  Because this number must be smaller than or equal to the maximum number Fn vectors in the ﬁnite vector space Fn  q, the sphere packing bound  q   = q n of   cid:6   ecor cid:1    cid:7   q k  i=0  n  i   q − 1 i .   cid:6   ecor cid:1    cid:7   q k  i=0  n  i   q − 1 i ≤ q n   ALGEBRAIC CODING THEORY follows. For binary block codes with q = 2 we obtain ≤ 2n−k.   cid:6    cid:7   n  i  ecor cid:1   cid:6   cid:7   i=0  n  i  ecor cid:1   i=0   q − 1 i = q n−k.  A so-called perfect code fulﬁls the sphere packing bound with equality, i.e.  39  Perfect codes exploit the complete code space; every vector in the code space Fn q is part of one and only one correction ball and can therefore uniquely be assigned to a particular code word. For perfect binary codes with q = 2 which are capable of correcting ecor = 1 error we obtain  1 + n = 2n−k.  Figure 2.23 shows the parameters of perfect single-error correcting binary block codes up to length 1023.  Plotkin Bound  Under the assumption that d > θ n with  θ = q − 1  q  = 1 − 1  q  Perfect binary block codes    The following table shows the parameters of perfect single-error correcting  binary block codes B n, k, d  with minimum Hamming distance d = 3.  n  k  3 7 15 31 63 127 255 511 1023  1 4 11 26 57 120 247 502 1013  m  2 3 4 5 6 7 8 9 10  Figure 2.23: Code word length n, information word length k and number of parity bits  m = n − k of a perfect single-error correcting code   ALGEBRAIC CODING THEORY  40  the Plotkin bound states that  q k ≤  d  d − θ n . = 1  2 we obtain  For binary block codes with q = 2 and θ = 2−1 2k ≤ 2 d 2 d − n  2  for 2 d > n. The code words of a code that fulﬁls the Plotkin bound with equality all have the same distance d. Such codes are called equidistant.  Gilbert–Varshamov Bound  By making use of the fact that the minimal number of linearly dependent columns in the parity-check matrix is equal to the minimum Hamming distance, the Gilbert–Varshamov bound can be derived for a linear block code B n, k, d . If   cid:6   cid:7  n − 1  i  d−2 cid:1   i=0   q − 1 i < q n−k  is fulﬁlled, it is possible to construct a linear q-nary block code B n, k, d  with code rate R = k n and minimum Hamming distance d.  Griesmer Bound  The Griesmer bound yields a lower bound for the code word length n of a linear q-nary block code B n, k, d  according to10  n ≥ k−1 cid:1   i=0   cid:29    cid:30   d q i   cid:29    cid:30   d  q  = d +  + ··· +   cid:29    cid:30   d  q k−1  .  Asymptotic Bounds  For codes with very large code word lengths n, asymptotic bounds are useful which are obtained for n → ∞. These bounds relate the code rate  and the relative minimum Hamming distance δ = d  R = k  n  .  n  In Figure 2.24 some asymptotic bounds are given for linear binary block codes with q = 2  Bossert, 1999 .  10The term  cid:18 z cid:19  denotes the smallest integer number that is not smaller than z.   ALGEBRAIC CODING THEORY  41  Asymptotic bounds for linear binary block codes  1  0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  n   k = R  Singleton  Gilbert cid:358  Varshamov  Plotkin  Hamming  0 0  0.1  0.2  0.3  0.6  0.7  0.8  0.9  1  0.4  0.5  δ = d n    Asymptotic Singleton bound    Asymptotic Hamming bound    Asymptotic Plotkin bound  R  cid:1  1 + δ 2  log2   cid:19   R  cid:1    cid:6   R  cid:1  1 − δ  cid:6   cid:7  1 − δ 2  +  δ 2   cid:7    cid:7    cid:6  1 − δ 2  log2  1 − 2 δ, 0 ≤ δ ≤ 1 2 < δ ≤ 1  0,  2  1    Gilbert–Varshamov bound  with 0 ≤ δ ≤ 1 2    R  cid:2  1 + δ log2  δ  +  1 − δ  log2 1 − δ    2.27    2.28    2.29    2.30   Figure 2.24: Asymptotic bounds for linear binary block codes B n, k, d   2.2.7 Code Constructions  In this section we turn to the question of how new block codes can be generated on the basis of known block codes. We will consider the code constructions shown in Figure 2.25  Berlekamp, 1984; Bossert, 1999; Lin and Costello, 2004; Ling and Xing, 2004 . Some of these code constructions will reappear in later chapters  e.g. code puncturing for convolu- tional codes in Chapter 3 and code interleaving for Turbo codes in Chapter 4 .   42  ALGEBRAIC CODING THEORY  Code constructions for linear block codes    Code shortening    Code puncturing    Code extension    Code interleaving   cid:1  = n − 1, k   cid:1  = k − 1, d   cid:1  = d  n   cid:1  = n − 1, k   cid:1  = k, d   cid:1  ≤ d  n   cid:1  = n + 1, k   cid:1  = k, d   cid:1  ≥ d  n   cid:1  = n t, k   cid:1  = k t, d   cid:1  = d  n    Plotkin’s  uv  code construction  cid:1  = 2 max{n1, n2}, k  n   cid:1  = k1 + k2, d   cid:1  = min{2 d1, d2}   2.31    2.32    2.33    2.34    2.35   Figure 2.25: Code constructions for linear block codes and their respective code  parameters  Code Shortening  Based on a linear block code B n, k, d  with code word length n, information word length k and minimum Hamming distance d, we can create a new code by considering all code words b =  b0, b1, . . . , bn−1  with bj = 0. The so-called shortened code is then given by   cid:31   B cid:1    cid:1   n   cid:1   , k  , d   cid:1     =   b0, . . . , bj−1, bj+1, . . . , bn−1  :   b0, . . . , bj−1, bj , bj+1, . . . , bn−1  ∈ B n, k, d  ∧ bj = 0  .     This shortened code B cid:1    cid:1   n   cid:1    cid:1   , k  , d    is characterised by the following code parameters11   cid:1  = n − 1,  cid:1  = k − 1,  cid:1  = d.  n  k  d  If B n, k, d  is an MDS code, then the code B cid:1    is also maximum distance separable because the Singleton bound is still fulﬁlled with equality: d = n − k + 1 =  n − 1  −  k − 1  + 1 = n 11We exclude the trivial case where all code words b ∈ B n, k, d  have a zero at the j th position.   cid:1  + 1 = d   cid:1  − k   cid:1   n  , d  , k  .   cid:1    cid:1    cid:1    ALGEBRAIC CODING THEORY  Code Puncturing  43  Starting from the systematic linear block code B n, k, d  with code word length n, infor- mation word length k and minimum Hamming distance d, the so-called punctured code is obtained by eliminating the jth code position bj of a code word b =  b0, b1, . . . , bn−1  ∈ B n, k, d  irrespective of its value. Here, we assume that bj is a parity-check symbol. The resulting code consists of the code words b cid:1  =  b0, . . . , bj−1, bj+1, . . . , bn−1 . The punctured code    cid:1    b0, . . . , bj−1, bj+1, . . . , bn−1  :   b0, . . . , bj−1, bj , bj+1, . . . , bn−1  ∈ B n, k, d  ∧ bj is parity-check symbol    =   cid:1   n   cid:31   B cid:1   , d  , k   cid:1   is characterised by the following code parameters  cid:1  = n − 1,  cid:1  = k,  cid:1  ≤ d.  n  d  k  Code Extension  The so-called extension of a linear block code B n, k, d  with code word length n, infor- mation word length k and minimum Hamming distance d is generated by attaching an additional parity-check symbol bn to each code word b =  b0, b1, . . . , bn−1  ∈ B n, k, d . The additional parity-check symbol is calculated according to  For a linear binary block code B n, k, d  this yields  bn = − b0 + b1 + ··· + bn−1  = − n−1 cid:1  bn = b0 + b1 + ··· + bn−1 = n−1 cid:1   i=0  bi .  bi .  i=0  We obtain the extended code  B cid:1    cid:1   n   cid:1   , k  , d   cid:1     =   b0, b1,··· , bn−1, bn  :   cid:31    b0, b1, . . . , bn−1  ∈ B n, k, d  ∧ b0 + b1 + ··· + bn−1 + bn = 0     with code parameters   cid:1  = n + 1,  cid:1  = k,  cid:1  ≥ d.  n  k  d   44 ALGEBRAIC CODING THEORY In the case of a linear binary block code B n, k, d  with q = 2 and an odd minimum Hamming distance d, the extended code’s minimum Hamming distance is increased by 1, i.e. d   cid:1  = d + 1. The additional parity-check symbol bn corresponds to an additional parity-check equa-  tion that augments the existing parity-check equations  of the original linear block code B n, k, d  with the  n − k  × n parity-check matrix H. The  n − k + 1  × n parity-check matrix H cid:1    is then given by  of the extended block code B cid:1    cid:1   n  , d  , k   cid:1    cid:1   H rT = 0 ⇔ r ∈ B n, k, d       cid:1  = H  H  1 1 ··· 1    .  0 0 ... 0 1  Code Interleaving  Up to now we have considered the correction of independent errors within a given received word in accordance with a memoryless channel. In real channels, however, error bursts might occur, e.g. in fading channels in mobile communication systems. With the help of so-called code interleaving, an existing error-correcting channel code can be adapted such that error bursts up to a given length can also be corrected.  Starting with a linear block code B n, k, d  with code word length n, information word length k and minimum Hamming distance d, the code interleaving to interleaving is obtained as follows. We arrange t code words b1, b2, . . . , bt with bj = depth t  bj,0, bj,1, . . . , bj,n−1  as rows in the t × n matrix    b1,0  b2,0 ... bt,0    .  ··· b1,n−1 ··· b2,n−1 . . . ···  bt,n−1  ...  b1,1 b2,1 ... bt,1  The resulting code word   cid:1  = b1,0 b2,0 ··· bt,0 b1,1 b2,1 ··· bt,1 ··· b1,n−1 b2,n−1 ··· bt,n−1 b  of length n t of the interleaved code B cid:1    is generated by reading the symbols off the matrix column-wise. This encoding scheme is illustrated in Figure 2.26. The interleaved code is characterised by the following code parameters   cid:1   n  , d  , k   cid:1    cid:1    cid:1  = n t,  cid:1  = k t,  cid:1  = d.  n  k  d   ALGEBRAIC CODING THEORY  45  Code interleaving of a linear block code  b1  b2  ...  bt−1  bt  b1,0  b1,1  b1,n−2  b1,n−1  b2,0  b2,1  b2,n−2  b2,n−1  ...  ...  ...  ...  bt−1,0  bt−1,1  bt−1,n−2 bt−1,n−1  bt,0  bt,1  bt,n−2  bt,n−1  . . .  . . .  . . .  . . .  . . .    Resulting code word of length n t   cid:1  = b1,0 b2,0 ··· bt,0 b1,1 b2,1 ··· bt,1 ··· b1,n−1 b2,n−1 ··· bt,n−1 b   2.36   Figure 2.26: Code interleaving of a linear block code B n, k, d   R  Neither the code rate   cid:1   cid:1  = k  cid:1  = k t n  cid:1  = d are changed. The latter means that the error nor the minimum Hamming distance d detection and error correction capabilities of the linear block code B n, k, d  with respect to independent errors are not improved by the interleaving step. However, error bursts consisting of  cid:6 burst neighbouring errors within the received word can now be corrected. If the linear block code B n, k, d  is capable of correcting up to ecor errors, the interleaved code is capable of correcting error bursts up to length  = k n  = R  n t   cid:6 burst = ecor t  with interleaving depth t. This is achieved by spreading the error burst in the interleaved code word b cid:1  onto t different code words bj such that in each code word a maximum of ecor errors occurs. In this way, the code words bj can be corrected by the original code B n, k, d  without error if the length of the error burst does not exceed ecor t.   46  ALGEBRAIC CODING THEORY  For practical applications it has to be observed that this improved correction of error  bursts comes along with an increased latency for the encoding and decoding steps. Plotkin’s  uv  Code Construction In contrast to the aforementioned code constructions, the  uv  code construction originally proposed by Plotkin takes two linear block codes B n1, k1, d1  and B n2, k2, d2  with code word lengths n1 and n2, information word lengths k1 and k2 and minimum Hamming distances d1 and d2. For simpliﬁcation, we ﬁll the code words of the block code with smaller code word length by an appropriate number of zeros. This step is called zero padding. Let u and v be two arbitrary code words thus obtained from the linear block codes B n1, k1, d1  and B n2, k2, d2  respectively.12 Then we have   cid:19   cid:2    cid:17  cid:17  0, . . . , 0  cid:3   b1  b1,  , n1 < n2  n1 ≥ n2  b2,   cid:2   b2   cid:17  cid:17  0, . . . , 0  cid:3   n1 < n2  , n1 ≥ n2  u =  v =   cid:19   and  with the arbitrarily chosen code words b1 ∈ B n1, k1, d1  and b2 ∈ B n2, k2, d2 . We now identify the code words u and v with the original codes B n1, k1, d1  and B n2, k2, d2 , i.e. we write u ∈ B n1, k1, d1  and v ∈ B n2, k2, d2 . The code resulting from Plotkin’s  uv  code construction is then given by  B cid:1    cid:1   n   cid:1   , k  , d   cid:1     = { uu + v  : u ∈ B n1, k1, d1  ∧ v ∈ B n2, k2, d2 } .  This code construction creates all vectors of the form  uu + v  by concatenating all possible vectors u and u + v with u ∈ B n1, k1, d1  and v ∈ B n2, k2, d2 . The resulting code is characterised by the following code parameters   cid:1  = 2 max{n1, n2} ,  cid:1  = k1 + k2,  cid:1  = min{2 d1, d2} .  n  k  d  2.2.8 Examples of Linear Block Codes  In this section we will present some important linear block codes. In particular, we will consider the already introduced repetition codes, parity-check codes and Hamming codes. Furthermore, simplex codes and Reed–Muller codes and their relationships are discussed  Berlekamp, 1984; Bossert, 1999; Lin and Costello, 2004; Ling and Xing, 2004 .  12In the common notation of Plotkin’s  uv  code construction the vector u does not correspond to the infor-  mation vector.   ALGEBRAIC CODING THEORY  47  Repetition code    The repetition code B n, 1, n  repeats the information symbol u0 in the code  vector b =  b0, b1, . . . , bn−1 , i.e.  b0 = b1 = ··· = bn−1 = u0    Minimum Hamming distance    Code rate  d = n  R = 1  n  Figure 2.27: Repetition code   2.37    2.38    2.39   Repetition Codes  We have already introduced the binary triple repetition code in Section 1.3 as a simple introductory example of linear block codes. In this particular code the binary information symbol 0 or 1 is transmitted with the binary code word 000 or 111 respectively. In general, a repetition code over the alphabet Fq assigns to the q-nary information symbol u0 the n-dimensional code word b =  u0, u0, . . . , u0 . Trivially, this block code is linear. The minimum Hamming distance is  Therefore, the repetition code in Figure 2.27 is a linear block code B n, 1, n  that is able to detect edet = d − 1 = n − 1 errors or to correct ecor =  cid:13  d − 1  2 cid:14  =  cid:13  n − 1  2 cid:14  errors. The code rate is  d = n.  R = 1  .  n  For the purpose of error correction, the minimum distance decoding can be implemented by a majority decoding scheme. The decoder emits the information symbol ˆu0 which appears most often in the received word.  The weight distribution of a repetition code is simply given by  W  x  = 1 +  q − 1  xn.  Although this repetition code is characterised by a low code rate R, it is used in some communication standards owing to its simplicity. For example, in the short-range wireless communication system BluetoothTM a triple repetition code is used as part of the coding scheme of the packet header of a transmitted baseband packet  Bluetooth, 2004 .   48  ALGEBRAIC CODING THEORY  Parity-check code    The parity-check code B n, n − 1, 2  attaches a parity-check symbol so that the resulting code vector b =  b0, b1, . . . , bn−1  fulﬁls the condition b0 + b1 + ··· + bn−1 = 0, i.e.  bn−1 = − u0 + u1 + ··· + un−2     Minimum Hamming distance    Code rate  d = 2 R = n − 1  n  = 1 − 1  n  Figure 2.28: Parity-check code   2.40    2.41    2.42   Parity-Check Codes Starting from the information word u =  u0, u1, . . . , uk−1  with ui ∈ Fq, a parity-check code attaches a single parity-check symbol to the information word u as illustrated in Figure 2.28. The n =  k + 1 -dimensional code word  b =  b0, b1, . . . , bn−2, bn−1  =  b0, b1, . . . , bk−1, bk   is given by bi = ui for 0 ≤ i ≤ k − 1, and the parity-check symbol bk which is chosen such that  k cid:1   i=0  bi = b0 + b1 + ··· + bk = 0  is fulﬁlled. Over the ﬁnite ﬁeld Fq we have  bk = − u0 + u1 + ··· + uk−1  = − k−1 cid:1   ui .  i=0  In the case of a binary parity-check code over the alphabet F2, the parity-check bit bk is chosen such that the resulting code word b is of even parity, i.e. the number of binary symbols 1 is even. The parity-check code is a linear block code B k + 1, k, 2  with a minimum Hamming distance d = 2. This code is capable of detecting edet = d − 1 = 1 error. The correction of errors is not possible. The code rate is  R = k k + 1  = n − 1  n  = 1 − 1  .  n   ALGEBRAIC CODING THEORY 49 The error detection is carried out by checking whether the received vector r =  r0, r1, . . . , rn−1  fulﬁls the parity-check condition  n−1 cid:1   i=0  ri = r0 + r1 + ··· + rn−1 = 0.  If this condition is not met, then at least one error has occurred.  The weight distribution of a binary parity-check code is equal to   cid:6    cid:7    cid:6    cid:7   for an even code word length n and W  x  = 1 +  4 + ··· + xn  x  W  x  = 1 +  cid:6   n 2   cid:7   x  2 +  cid:6   n 4   cid:7   2 +  x  n 2  n 4  4 + ··· + n xn−1  x  for an odd code word length n. Binary parity-check codes are often applied in simple serial interfaces such as, for example, UART  Universal Asynchronous Receiver Transmitter .  Hamming Codes  Hamming codes can be deﬁned as binary or q-nary block codes  Ling and Xing, 2004 . In this section we will exclusively focus on binary Hamming codes, i.e. q = 2. Origi- nally, these codes were developed by Hamming for error correction of faulty memory entries  Hamming, 1950 . Binary Hamming codes are most easily deﬁned by their corres- ponding parity-check matrix. Assume that we want to attach m parity-check symbols to an information word u of length k. The parity-check matrix is obtained by writing down all m-dimensional non-zero column vectors. Since there are n = 2m − 1 binary column vectors of length m, the parity-check matrix      1 0 1 0 1 0 1 0 ··· 0 1 0 1 0 1 0 1 1 0 0 1 1 0 ··· 0 0 1 1 0 0 0 0 0 1 1 1 1 0 ··· 0 0 0 0 1 1 0 0 0 0 0 0 0 1 ··· 1 1 1 1 1 1 ... ... 0 0 0 0 0 0 0 0 ··· 1 1 1 1 1 1 0 0 0 0 0 0 0 0 ··· 1 1 1 1 1 1  . . .  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  0 1 1 1 1 1 1 1 ... ... 1 1 1 1      is of dimension m ×  2m − 1 . By suitably rearranging the columns, we obtain the  n − k  × n or m × n parity-check matrix  of the equivalent binary Hamming code. The corresponding k × n generator matrix is then given by  H = cid:2  G = cid:2   Bn−k,k   cid:17  cid:17  In−k  cid:3   cid:17  cid:17  − BT  cid:3   n−k,k  Ik  .   50  ALGEBRAIC CODING THEORY  Because of the number of columns within the parity-check matrix H, the code word length of the binary Hamming code is given by n = 2m − 1 = 2n−k − 1 with the number of binary information symbols k = n − m = 2m − m − 1. Since the columns of the parity- check matrix are pairwise linearly independent and there exist three columns which sum up to the all-zero vector, the minimum Hamming distance of the binary Hamming code is d = 3.13 The binary Hamming code can therefore be characterised as a linear block code B 2m − 1, 2m − m − 1, 3  that is able to detect edet = 2 errors or to correct ecor = 1 error.  Because of n = 2n−k − 1 and ecor = 1 we have   cid:6   ecor cid:1    cid:7   n  i  i=0  = 1 + n = 2n−k,  i.e. the binary Hamming code is perfect because it fulﬁls the sphere packing bound.14 Since the binary Hamming code B 2m − 1, 2m − m − 1, 3  only depends on the number of parity symbols m, we also call this code H m  with the code parameters  n = 2m − 1, k = 2m − m − 1, d = 3.  sT = H rT = H eT  For a binary Hamming code the decoding of an erroneously received vector r = b + e with error vector e of weight wt e  = 1 can be carried out by ﬁrst calculating the syndrome  which is then compared with all columns of the parity-check matrix H. If the non-zero syndrome sT agrees with the jth column vector, the error vector e must have its single non-zero component at the jth position. The received vector  r =  r0, . . . , rj−1, rj , rj+1, . . . , rn−1   can therefore be decoded into the code word  ˆb =  r0, . . . , rj−1, rj + 1, rj+1, . . . , rn−1   by calculating ˆbj = rj + 1.  The weight distribution W  x  of the binary Hamming code H m  is given by  W  x  = 1 n + 1  i wi =   cid:2   1 + x n + n  1 − x  n+1  2  1 + x  n−1  2  cid:6   cid:7  i − 1  − wi−1 −  n − i + 2  wi−2  n   cid:3   .  The coefﬁcients wi can be recursively calculated according to  with w0 = 1 and w1 = 0. Figure 2.29 summarises the properties of the binary Hamming code H m . 13In the given matrix the ﬁrst three columns sum up to the zero column vector of length m. 14There are only two other non-trivial perfect linear codes, the binary Golay code B 23, 12, 7  with n = 23, k = 12 and d = 7 and the ternary Golay code B 11, 6, 5  with n = 11, k = 6 and d = 5. The extended binary Golay code has been used, for example, for the Voyager 1 and 2 spacecrafts in deep-space communications.   ALGEBRAIC CODING THEORY  Binary Hamming code  51   2.43    2.44    2.45     The binary Hamming code H m  is a perfect single-error correcting code with parity-check-matrix H consisting of all 2m − 1 non-zero binary column vectors of length m.    Code word length  n = 2m − 1    Minimum Hamming distance    Code rate  d = 3 R = 2m − m − 1 2m − 1  = 1 − m  2m − 1  Figure 2.29: Binary Hamming code H m  = B 2m − 1, 2m − m − 1, 3   The extended binary Hamming code H cid:1    m  is obtained from the binary Hamming code H m  by attaching to each code word an additional parity-check symbol such that the resulting code word is of even parity. The corresponding non-systematic parity-check matrix is equal to      1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 ... ... 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1  ...  ...  ...  0 1 0 ··· 0 1 0 1 0 1 0 1 1 1 0 ··· 0 0 1 1 0 0 1 1 1 1 0 ··· 0 0 0 0 1 1 1 1 0 0 1 ··· 1 1 1 1 1 1 1 1 ... ... 0 0 0 ··· 1 1 1 1 1 1 1 1 0 0 0 ··· 1 1 1 1 1 1 1 1 1 1 1 ··· 1 1 1 1 1 1 1 1  . . .  ...  ...  ...  ...  ...  ...  ...  ...  ...      .  0 0 0 0 ... 0 0 1  cid:1  = 4.  The minimum Hamming distance of the extended Hamming code is d  Hamming codes are used, for example, in semiconductor memories such as DRAMs for error correction of single bit errors or in the short-range wireless communication system BluetoothTM as part of the coding scheme of the packet header of a baseband packet  Bluetooth, 2004 .  Simplex Codes The dual code of the binary Hamming code H m  is the so-called simplex code S m , i.e.  m  = S m . This binary code over the ﬁnite ﬁeld F2 consists of the zero code word 0 H⊥ and 2m − 1 code words of code word length n and weight 2m−1, i.e. wt b  = 2m−1 for all   52  Simplex code  ALGEBRAIC CODING THEORY    The simplex code S m  is the dual of the binary Hamming code H m , i.e.  S m  = H⊥   m .    Code word length    Minimum Hamming distance    Code rate  n = 2m − 1  d = 2m−1  R = m  2m − 1   2.46    2.47    2.48   Figure 2.30: Simplex code S m  = B 2m − 1, m, 2m−1   code words b ∈ S m  with b  cid:7 = 0. The total number of code words is M = 2m and the code word length is n = 2m − 1. The minimum weight of the simplex code is min∀b cid:7 =0 wt b  = 2m−1, which is equal to the minimum Hamming distance d = 2m−1 owing to the linearity of the simplex code. From these considerations, the weight distribution W  x  of the simplex code  W  x  = 1 +  2m − 1  x  2m−1 = 1 + n x n+1  2  easily follows.  meters  According to Figure 2.30 the binary simplex code is characterised by the code para-  n = 2m − 1, k = m, d = 2m−1.  m  is the dual of the binary Hamming code H m ,  Since the simplex code S m  = H⊥ the m × n parity-check matrix      1 0 1 0 1 0 1 0 ··· 0 1 0 1 0 1 0 1 1 0 0 1 1 0 ··· 0 0 1 1 0 0 0 0 0 1 1 1 1 0 ··· 0 0 0 0 1 1 0 0 0 0 0 0 0 1 ··· 1 1 1 1 1 1 ... ... ... . . . ··· 1 1 1 1 1 1 0 0 0 0 0 0 0 0 ··· 1 1 1 1 1 1 0 0 0 0 0 0 0 0  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  0 1 1 1 1 1 1 1 ... ... 1 1 1 1       ALGEBRAIC CODING THEORY  53  Hadamard matrix Hm    Recursive deﬁnition of 2m × 2m Hadamard matrix Hm−1 Hm−1 Hm−1 −Hm−1  Hm =   cid:6    cid:7     Initialisation  H0 =  1    2.49    2.50   Figure 2.31: Hadamard matrix Hm. Reproduced by permission of J. Schlembach  of the Hamming code H m  yields the generator matrix of the simplex code S m . If required, the generator matrix can be brought into the systematic form  There exists an interesting relationship between the binary simplex code S m  and the so-called Hadamard matrix Hm. This 2m × 2m matrix is recursively deﬁned according to  with the initialisation H0 =  1 . In Figure 2.31 the Hadamard matrix Hm is illustrated for 2 ≤ m ≤ 5; a black rectangle corresponds to the entry 1 whereas a white rectangle corresponds to the entry −1.  As an example we consider m = 2 for which we obtain the 4 × 4 matrix  Fachverlag   cid:17  cid:17 Am,n−m   cid:3   .  Im  G = cid:2   cid:6   Hm =  Hm−1 Hm−1 Hm−1 −Hm−1  H2 =    1   0 0  1 1 1 1 −1 1 −1 1 −1 −1 1 1 −1 −1 1   .  0 0 0 1 1 1 1 0  0 1 0 0 0 1   cid:7     .  Replacing all 1s with 0s and all −1s with 1s yields the matrix   54  ALGEBRAIC CODING THEORY  If the ﬁrst column is deleted, the resulting matrix    0 0 0  1 0 1 0 1 1 1 1 0     includes as its rows all code vectors of the simplex code S 2 .  In general, the simplex code S m  can be obtained from the 2m × 2m Hadamard matrix  Hm by ﬁrst applying the mapping  1  cid:3 → 0 and − 1  cid:3 → 1  and then deleting the ﬁrst column. The rows of the resulting matrix deliver all 2m code words of the binary simplex code S m .  As a further example we consider the 23 × 23 = 8 × 8 Hadamard matrix      H3 =  1 1 1 1 1 1 1 1 1 −1 1 −1 1 −1 1 −1 1 −1 −1 1 −1 −1 1 1 1 −1 −1 1 −1 −1 1 1 1 −1 −1 −1 −1 1 1 1 1 −1 1 −1 1 −1 −1 1 1 −1 −1 −1 −1 1 1 1 1 −1 1 −1 −1 1 −1 1   With the mapping 1  cid:3 → 0 and −1  cid:3 → 1, the matrix     .  follows. If we delete the ﬁrst column, the rows of the resulting matrix       0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 1 0 1 1 0 0 1 1 0 0 0 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 0 1 1 0 1 0 0 1  0 1 0 1 0 1 0 1  0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 0 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1       yield the binary simplex code S 3  with code word length n = 23 − 1 = 7 and minimum Hamming distance d = 23−1 = 4.   ALGEBRAIC CODING THEORY  Reed–Muller Codes  55  The Hadamard matrix can further be used for the construction of other codes. The so-called Reed–Muller codes form a class of error-correcting codes that are capable of correcting more than one error. Although these codes do not have the best code parameters, they are characterised by a simple and efﬁciently implementable decoding strategy both for hard-decision as well as soft-decision decoding. There exist ﬁrst-order and higher-order Reed–Muller codes, as we will explain in the following. First-Order Reed–Muller Codes R 1, m  The binary ﬁrst-order Reed–Muller code R 1, m  can be deﬁned with the help of the following m × 2m matrix which contains all possible 2m binary column vectors of length m  ...  ...  0 0 0 0 0 0 0 0 0 ··· 1 1 1 1 1 1 1 1 ··· 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 ... ... ... . . . 0 0 0 0 0 0 0 0 1 ··· 1 1 1 1 1 1 1 1 ··· 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 ··· 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 ··· 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  By attaching the 2m-dimensional all-one vector  1 1 1 1 1  1 1 1 ··· 1 1 1 1 1 1 1 1 1  as the ﬁrst row, we obtain the  m + 1  × 2m generator matrix  ...  ...  1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ... ... 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1  ...  ...  ...  ...  1 ··· 1 0 ··· 1 0 ··· 1 ... ... 1 ··· 1 0 ··· 0 0 ··· 0 0 ··· 0  . . .  ...  ...  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ... ... 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 1  ...  ...  ...  The binary ﬁrst-order Reed–Muller code R 1, m  in Figure 2.32 is characterised by the following code parameters       cid:2       G =     .   cid:3       .  n = 2m, k = 1 + m, d = 2m−1.  cid:3  2m+1 − 2  W  x  = 1 + cid:2   x2m−1 + x2m  ,  The corresponding weight distribution W  x  is given by   56  ALGEBRAIC CODING THEORY  First-order Reed–Muller code R 1, m     The ﬁrst-order Reed–Muller code R 1, m  is a binary code that is able to  correct more than one error.    Code word length    Minimum Hamming distance    Code rate  n = 2m  d = 2m−1  R = 1 + cid:2   m 1   cid:3   2m   2.51    2.52    2.53   Figure 2.32: First-order Reed–Muller code R 1, m   i.e. the binary ﬁrst-order Reed–Muller code R 1, m  consists of code vectors with weights 0, 2m−1 and 2m. The hard-decision decoding of a Reed–Muller code can be carried out by a majority logic decoding scheme. There is a close relationship between the binary ﬁrst-order Reed–Muller code R 1, m  and the simplex code S m  with code word length n = 2m − 1 and k = m information symbols as well as the Hamming code H m . The m × 2m matrix used in the construction of the generator matrix of the ﬁrst-order Reed–Muller code R 1, m  corresponds to the parity-check matrix of the binary Hamming code H m  with the all-zero column attached. This parity-check matrix in turn yields the generator matrix of the simplex code S m . Therefore, this m × 2m matrix generates all code words of the simplex code S m  with a zero attached in the ﬁrst position. The attachment of the all-one row vector to the m × 2m matrix is equivalent to adding all inverted code words to the code. In summary, the simplex code S m  yields the Reed–Muller code R 1, m  by attaching a zero to all code words in the ﬁrst position and adding all inverted vectors to the code  Bossert, 1999 . If we compare this code construction scheme with the relationship between simplex codes S m  and Hadamard matrices Hm, we recognise that the rows of the 2m+1 × 2m matrix   cid:6    cid:7   subjected to the mapping 1  cid:3 → 0 and −1  cid:3 → 1 yield the code vectors of the ﬁrst-order Reed–Muller code R 1, m .  This observation in combination with the orthogonality property  Hm−Hm  Hm Hm = 2m I2m   ALGEBRAIC CODING THEORY 57 of the Hadamard matrix Hm with the 2m × 2m identity matrix I2m can be used to implement a soft-decision decoding algorithm. To this end, we assume that the received vector  corresponds to the real-valued output signal of an AWGN channel  see also Figure 1.5  with n-dimensional normally distributed noise vector e.15 The bipolar signal vector x = xj with components ±1 is given by the jth row of the matrix  r = x + e   cid:6    cid:7   .  Hm−Hm  This bipolar signal vector x is obtained from the binary code vector b by the mapping 1  cid:3 → −1 and 0  cid:3 → 1. With the help of the 2m+1-dimensional row vector  v =  0, . . . , 0, 1, 0, . . . , 0 ,  the components of which are 0 except for the jth component which is equal to 1, we can express the bipolar signal vector according to   cid:6    cid:7   x = v  .  v  "  !  r Hm = = v  Hm−Hm Within the soft-decision decoder architecture the real-valued received vector r is trans-  cid:7  formed with the Hadamard matrix Hm, leading to  cid:7  Hm−Hm Hm−Hm Hm Hm −Hm Hm  cid:6  2m I2m −2m I2m = 2m v I2m−I2m = ±2m v + e Hm.  + e Hm Hm + e Hm  cid:7  + e Hm  cid:7  + e Hm  cid:7  + e Hm   cid:6   cid:6   cid:6   cid:6   = v  = v  Because of r Hm = ±2m v + e Hm, the soft-decision decoder searches for the largest mod- ulus of all components in the transformed received vector r Hm. This component in con- junction with the respective sign delivers the decoded signal vector ˆx or code vector ˆb. The transform r Hm can be efﬁciently implemented with the help of the fast Hadamard transform  FHT . In Figure 2.33 this soft-decision decoding is compared with the optimal hard-decision minimum distance decoding of a ﬁrst-order Reed–Muller code R 1, 4 . 15Here, the arithmetics are carried out in the vector space Rn.   58  ALGEBRAIC CODING THEORY  Hard-decision and soft-decision decoding  r r e p  100  10 cid:358 1  10 cid:358 2  10 cid:358 3  10 cid:358 4  10 cid:358 5  10 cid:358 6  cid:358 5   cid:358 4   cid:358 3   cid:358 2   cid:358 1  0  2  3  4  5   cid:11    cid:12   1 Eb N0  10 log10    Comparison between hard-decision decoding and soft-decision decoding of a binary ﬁrst-order Reed–Muller code R 1, 4  with code parameters n = 16, k = 5 and d = 8 with respect to the word error probability perr  Figure 2.33: Hard-decision and soft-decision decoding of a binary ﬁrst-order  Reed–Muller code R 1, 4   The FHT is used, for example, in UMTS  Holma and Toskala, 2004  receivers for the decoding of TFCI  transport format combination indicator  symbols which are encoded with the help of a subset of a second-order Reed–Muller code R 2, 5  with code word length n = 25 = 32  see, for example,  3GPP, 1999  . Reed–Muller codes have also been used in deep-space explorations, e.g. the ﬁrst-order Reed–Muller code R 1, 5  in the Mariner spacecraft  Costello et al., 1998 .  Higher-order Reed–Muller Codes R r, m  The construction principle of binary ﬁrst- order Reed–Muller codes R 1, m  can be extended to higher-order Reed–Muller codes R r, m . For this purpose we consider the 5 × 16 generator matrix     G =  1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 0 1  1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 1     of a ﬁrst-order Reed–Muller code R 1, 4 . The rows of this matrix correspond to the Boolean functions f0, f1, f2, f3 and f4 shown in Figure 2.34. If we also consider the   ALGEBRAIC CODING THEORY  59  Boolean functions for the deﬁnition of a ﬁrst-order Reed–Muller code R 1, 4   x1 0 x2 0 x3 0 x4 0 f0 1 f1 0 f2 0 f3 0 f4 0  0 0 0 1  0 0 1 0  0 0 1 1  0 1 0 0  0 1 0 1  0 1 1 0  0 1 1 1  1 0 0 0  1 0 0 1  1 0 0 0 1  1 1 0 0 1   Boolean functions with x1, x2, x3, x4 ∈ {0, 1}  1 0 0 1 0  1 0 1 0 1  1 0 1 1 1  1 0 1 1 0  1 0 0 1 1  1 0 1 0 0  1 1 0 0 0  1 0 1 0  1 1 0 1 0  1 0 1 1  1 1 0 1 1  1 1 0 0  1 1 1 0 0  1 1 0 1  1 1 1 0 1  1 1 1 0  1 1 1 1 0  1 1 1 1  1 1 1 1 1  f0 x1, x2, x3, x4  = 1 f1 x1, x2, x3, x4  = x1 f2 x1, x2, x3, x4  = x2 f3 x1, x2, x3, x4  = x3 f4 x1, x2, x3, x4  = x4  Figure 2.34: Boolean functions for the deﬁnition of a ﬁrst-order Reed–Muller code  R 1, 4   additional Boolean functions f1,2, f1,3, f1,4, f2,3, f2,4 and f3,4 in Figure 2.35, we obtain the 11 × 16 generator matrix      G =  1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 of the second-order Reed–Muller code R 2, 4  from the table in Figure 2.35.  1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0       60  ALGEBRAIC CODING THEORY  Boolean functions for Reed–Muller code R 2, 4   the deﬁnition of a second-order  x1 x2 x3 x4  0 0 0 0  1 f0 0 f1 0 f2 0 f3 0 f4 f1,2 0 f1,3 0 f1,4 0 f2,3 0 f2,4 0 f3,4 0  0 0 0 1  1 0 0 0 1 0 0 0 0 0 0  0 0 1 0  1 0 0 1 0 0 0 0 0 0 0  0 0 1 1  1 0 0 1 1 0 0 0 0 0 1  0 1 0 0  1 0 1 0 0 0 0 0 0 0 0  0 1 0 1  1 0 1 0 1 0 0 0 0 1 0  0 1 1 0  1 0 1 1 0 0 0 0 1 0 0  0 1 1 1  1 0 1 1 1 0 0 0 1 1 1  1 0 0 0  1 1 0 0 0 0 0 0 0 0 0  1 0 0 1  1 1 0 0 1 0 0 1 0 0 0  1 0 1 0  1 1 0 1 0 0 1 0 0 0 0  1 0 1 1  1 1 0 1 1 0 1 1 0 0 1  1 1 0 0  1 1 1 0 0 1 0 0 0 0 0  1 1 0 1  1 1 1 0 1 1 0 1 0 1 0  1 1 1 0  1 1 1 1 0 1 1 0 1 0 0  1 1 1 1  1 1 1 1 1 1 1 1 1 1 1    Boolean functions with x1, x2, x3, x4 ∈ {0, 1}  f1,2 x1, x2, x3, x4  = x1 x2 f1,3 x1, x2, x3, x4  = x1 x3 f1,4 x1, x2, x3, x4  = x1 x4 f2,3 x1, x2, x3, x4  = x2 x3 f2,4 x1, x2, x3, x4  = x2 x4 f3,4 x1, x2, x3, x4  = x3 x4  Figure 2.35: Boolean functions for the deﬁnition of a second-order Reed–Muller code  R 2, 4   This code construction methodology can be extended to the deﬁnition of higher-order  Reed–Muller codes R r, m  based on the Boolean functions  with  i2 i1 1 x x 2  ··· xim  m  i1 + i2 + ··· + im ≤ r   ALGEBRAIC CODING THEORY  Reed–Muller code R r, m     The Reed–Muller code R r, m  is a binary code that is able to correct more  than one error.    Code word length    Minimum Hamming distance  n = 2m    Code rate  d = 2m−r  R = 1 + cid:2   m 1   cid:3  + cid:2    cid:3  + ··· + cid:2    cid:3   m r  m 2 2m  Figure 2.36: Reed–Muller code R r, m   61   2.54    2.55    2.56   and ij ∈ {0, 1}. These Boolean functions are used to deﬁne the rows of the generator matrix G. The resulting Reed–Muller code R r, m  of order r in Figure 2.36 is characterised by the following code parameters   cid:7   +  m 1   cid:6    cid:7   m 2   cid:6   n = 2m, k = 1 + d = 2m−r .   cid:6    cid:7   m  r  ,  + . . . +  There also exists a recursive deﬁnition of Reed–Muller codes based on Plotkin’s  uv  code construction  Bossert, 1999   R r + 1, m + 1  = { uu + v  : u ∈ R r + 1, m  ∧ v ∈ R r, m } .  The code vectors b =  uu + v  of the Reed–Muller code R r + 1, m + 1  are obtained from all possible combinations of the code vectors u ∈ R r + 1, m  of the Reed–Muller code R r + 1, m  and v ∈ R r, m  of the Reed–Muller code R r, m . Furthermore, the dual code of the Reed–Muller code R r, m  yields the Reed–Muller code  R⊥   r, m  = R m − r − 1, m .  Finally, Figure 2.37 summarises the relationship between ﬁrst-order Reed–Muller codes R 1, m , binary Hamming codes H m  and simplex codes S m   Bossert, 1999 .   62  ALGEBRAIC CODING THEORY  First-order Reed–Muller codes R 1, m , binary Hamming codes H m  and simplex codes S m     Reed–Muller code R 1, m  and extended Hamming code H cid:1    m    i  R 1, m  = H cid:1 ⊥  ii  H cid:1    m  = R⊥   m    1, m  = R m − 2, m     Hamming code H m  and simplex code S m    i  S m  = H⊥  ii  H m  = S⊥   m    m    i  S m   cid:3 → R 1, m     Simplex code S m  and Reed–Muller code R 1, m   1. All code words from S m  are extended by the ﬁrst position 0. 2. R 1, m  consists of all corresponding code words including the  inverted code words.   ii  R 1, m   cid:3 → S m   1. All code words with the ﬁrst component 0 are removed from  R 1, m .  2. The ﬁrst component is deleted.  Figure 2.37: Relationship between ﬁrst-order Reed–Muller codes R 1, m , binary  Hamming codes H m  and simplex codes S m   2.3 Cyclic Codes  Linear block codes make it possible efﬁciently to implement the encoding of information words with the help of the generator matrix G. In general, however, the problem of decoding an arbitrary linear block code is difﬁcult. For that reason we now turn to cyclic codes as special linear block codes. These codes introduce further algebraic properties in order to be able to deﬁne more efﬁcient algebraic decoding algorithms  Berlekamp, 1984; Lin and Costello, 2004; Ling and Xing, 2004 .  2.3.1 Deﬁnition of Cyclic Codes  A cyclic code is characterised as a linear block code B n, k, d  with the additional property that for each code word  b =  b0, b1, . . . , bn−2, bn−1    ALGEBRAIC CODING THEORY  all cyclically shifted words  63   bn−1 , b0 , . . . , bn−3 , bn−2 ,  bn−2 , bn−1 , . . . , bn−4 , bn−3 ,  ...   b2 , b3 , . . . , b0 , b1 ,  b1 , b2 , . . . , bn−1 , b0   are also valid code words of B n, k, d   Lin and Costello, 2004; Ling and Xing, 2004 . This property can be formulated concisely if a code word b ∈ Fn q is represented as a polynomial  b z  = b0 + b1 z + ··· + bn−2 zn−2 + bn−1 zn−1  over the ﬁnite ﬁeld Fq.16 A cyclic shift   b0, b1, . . . , bn−2, bn−1   cid:3 →  bn−1, b0, b1, . . . , bn−2   of the code polynomial b z  ∈ Fq[z] can then be expressed as  b0 + b1 z + ··· + bn−2 zn−2 + bn−1 zn−1  cid:3 → bn−1 + b0 z + b1 z2 + ··· + bn−2 zn−1.  Because of  bn−1 + b0 z + b1 z  2 + ··· + bn−2 zn−1 = z b z  − bn−1  and by observing that a code polynomial b z  is of maximal degree n − 1, we represent the cyclically shifted code polynomial modulo zn − 1, i.e.  bn−1 + b0 z + b1 z  2 + ··· + bn−2 zn−1 ≡ z b z  mod zn − 1.   cid:2    cid:3  zn − 1  Cyclic codes B n, k, d  therefore fulﬁl the following algebraic property  b z  ∈ B n, k, d  ⇔ z b z  mod zn − 1 ∈ B n, k, d .  For that reason – if not otherwise stated – we consider polynomials in the factorial ring Fq[z]  zn − 1 . Figure 2.38 summarises the deﬁnition of cyclic codes.  Similarly to general linear block codes, which can be deﬁned by the generator matrix G or the corresponding parity-check matrix H, cyclic codes can be characterised by the generator polynomial g z  and the parity-check polynomial h z , as we will show in the following  Berlekamp, 1984; Bossert, 1999; Lin and Costello, 2004; Ling and Xing, 2004 .  2.3.2 Generator Polynomial  A linear block code B n, k, d  is deﬁned by the k × n generator matrix     g0,0    g0    =  G =  g0,1 g1,1 ...  g1,0 ...  ··· g0,n−1 ··· g1,n−1 . . . ··· gk−1,n−1  ...  gk−1,0  gk−1,1  g1 ... gk−1  16Polynomials over ﬁnite ﬁelds are explained in Section A.3 in Appendix A.   64  ALGEBRAIC CODING THEORY  Deﬁnition of cyclic codes    Each code word b =  b0, b1, . . . , bn−2, bn−1  of a cyclic code B n, k, d  is  represented by the polynomial  b z  = b0 + b1 z + ··· + bn−2 zn−2 + bn−1 zn−1   2.57     All cyclic shifts of a code word b are also valid code words in the cyclic  code B n, k, d , i.e.  b z  ∈ B n, k, d  ⇔ z b z  mod zn − 1 ∈ B n, k, d    2.58   Figure 2.38: Deﬁnition of cyclic codes  with k linearly independent basis vectors g0, g1, . . . , gk−1 which themselves are valid code vectors of the linear block code B n, k, d . Owing to the algebraic properties of a cyclic code there exists a unique polynomial  g z  = g0 + g1 z + ··· + gn−k−1 zn−k−1 + gn−k zn−k  of minimal degree deg g z   = n − k with gn−k = 1 such that the corresponding generator matrix can be written as  g0 0 ... 0 0  g1 g0 ... 0 0  ··· gn−k ··· gn−k−1 . . . ··· ···  ... 0 0  0 gn−k ... 0 0  ··· 0 ··· 0 ... . . . ··· g0 ··· 0  ··· 0 ··· 0 ... . . . ··· gn−k ··· gn−k−1  0 0 ... g1 g0  0 0 ... 0 gn−k    .     G =  This polynomial g z  is called the generator polynomial of the cyclic code B n, k, d   Berlekamp, 1984; Bossert, 1999; Lin and Costello, 2004; Ling and Xing, 2004 . The rows of the generator matrix G are obtained from the generator polynomial g z  and all cyclic shifts z g z , z2 g z , . . . , zk−1 g z  which correspond to valid code words of the cyclic code. Formally, we can write the generator matrix as     G =    .  g z  z g z   ...  zk−2 g z  zk−1 g z    ALGEBRAIC CODING THEORY  In view of the encoding rule for linear block codes b = u G, we can write  65     g z  z g z   ...  zk−2 g z  zk−1 g z     = u0 g z  + u1 z g z  + ··· + uk−1 zk−1  g z .   u0, u1, . . . , uk−1   For the information word u =  u0, u1,··· , uk−1  we deﬁne the corresponding information polynomial  u z  = u0 + u1 z + u2 z  2 + ··· + uk−1 zk−1  .  This information polynomial u z  can thus be encoded according to the polynomial multi- plication  b z  = u z  g z .  Because of b z  = u z  g z , the generator polynomial g z  divides every code polyno- mial b z . If g z  does not divide a given polynomial, this polynomial is not a valid code polynomial, i.e.  g z  b z  ⇔ b z  ∈ B n, k, d   b z  ≡ 0 mod g z  ⇔ b z  ∈ B n, k, d .  or equivalently The simple multiplicative encoding rule b z  = u z  g z , however, does not lead to a systematic encoding scheme where all information symbols are found at speciﬁed positions. By making use of the relation b z  ≡ 0 modulo g z , we can derive a systematic in the k upper  encoding scheme. To this end, we place the k information symbols ui positions in the code word   cid:16  b =  b0, b1, . . . , bn−k−1, u0, u1, . . . , uk−1   cid:14  cid:15    cid:13    .  =u  The remaining code symbols b0, b1, . . . , bn−k−1 correspond to the n − k parity-check symbols which have to be determined. By applying the condition b z  ≡ 0 modulo g z  to the code polynomial  b z  = b0 + b1 z + ··· + bn−k−1 zn−k−1 + u0 zn−k + u1 zn−k+1 + ··· + uk−1 zn−1  = b0 + b1 z + ··· + bn−k−1 zn−k−1 + zn−k u z   we obtain  b0 + b1 z + ··· + bn−k−1 zn−k−1 ≡ −zn−k u z  mod g z .  The parity-check symbols b0, b1, . . . , bn−k−1 are determined from the remainder of the division of the shifted information polynomial zn−k u z  by the generator polynomial g z . Figure 2.39 summarises the non-systematic and systematic encoding schemes for cyclic codes.  It can be shown that the binary Hamming code in Figure 2.29 is equivalent to a cyclic code. The cyclic binary  7, 4  Hamming code, for example, is deﬁned by the generator polynomial  g z  = 1 + z + z  3 ∈ F2[z].   66  ALGEBRAIC CODING THEORY  Generator polynomial    The cyclic code B n, k, d  is deﬁned by the unique generator polynomial  g z  = g0 + g1 z + ··· + gn−k−1 zn−k−1 + gn−k zn−k   2.59   of minimal degree deg g z   = n − k with gn−k = 1.    Non-systematic encoding  b z  = u z  g z     Systematic encoding  b0 + b1 z + ··· + bn−k−1 zn−k−1 ≡ −zn−k u z  mod g z    2.60    2.61   Figure 2.39: Encoding of a cyclic code with the help of the generator polynomial g z   The non-systematic and systematic encoding schemes for this cyclic binary Hamming code are illustrated in Figure 2.40.  Cyclic Redundancy Check  With the help of the generator polynomial g z  of a cyclic code B n, k, d , the so-called cyclic redundancy check  CRC  can be deﬁned for the detection of errors  Lin and Costello, 2004 . Besides the detection of edet = d − 1 errors by a cyclic code B n, k, d  with min- imum Hamming distance d, cyclic error bursts can also be detected. With a generator polynomial g z  of degree deg g z   = n − k, all cyclic error bursts of length   cid:6 burst ≤ n − k  can be detected  Jungnickel, 1995 . This can be seen by considering the error model r z  = b z  + e z  with the received polynomial r z , the code polynomial b z  and the error polynomial e z   see also Figure 2.46 . Errors can be detected as long as the parity-check equation of the cyclic code B n, k, d  is fulﬁlled. Since g z b z , all errors for which the error polynomial e z  is not divisible by the generator polynomial g z  can be detected. As long as the degree deg e z   is smaller than deg g z   = n − k, the error polynomial e z  cannot be divided by the generator polynomial. This is also true if cyclically shifted variants zi e z  of such an error polynomial are considered. Since for an error burst of length  cid:6 burst the degree of the error polynomial is equal to  cid:6 burst − 1, the error detection is possible if  g z  r z  ⇔ r z  ∈ B n, k, d   deg e z   =  cid:6 burst − 1 < n − k = deg g z  .   ALGEBRAIC CODING THEORY  67  Cyclic binary Hamming code    The generator polynomial of a cyclic binary  7,4  Hamming code is given  by  g z  = 1 + z + z3 ∈ F2[z]    Non-systematic encoding of the information polynomial u z  = 1 + z3 yields  b z  = u z  g z  = cid:2    cid:3   cid:2   1 + z  3  1 + z + z  3   cid:3  = 1 + z + z  4 + z  6    Systematic encoding of the information polynomial u z  = 1 + z3 yields  z3  z3 + 1  ≡ z2 + z mod 1 + z + z3  leading to the code polynomial  b z  = z + z  2 + z  3 + z  6  Figure 2.40: Cyclic binary  7, 4  Hamming code  Figure 2.41 gives some commonly used CRC generator polynomials  Lin and Costello, 2004 .  2.3.3 Parity-Check Polynomial  It can be shown that the generator polynomial g z  of a cyclic code divides the polynomial zn − 1 in the polynomial ring Fq[z], i.e.  In the factorial ring Fq[z]  zn − 1  this amounts to g z  h z  = 0 or equivalently  g z  h z  = zn − 1.  g z  h z  ≡ 0 mod zn − 1.  h z  = zn − 1  g z   The polynomial  is the so-called parity-check polynomial  Berlekamp, 1984; Bossert, 1999; Lin and Costello, 2004; Ling and Xing, 2004 . Since every code polynomial b z  is a multiple of the generator polynomial g z , the parity-check equation can also be written as  see Figure 2.42   b z  h z  ≡ 0 mod zn − 1 ⇔ b z  ∈ B n, k, d . This parity-check equation is in correspondence with the matrix equation  H bT = 0 ⇔ b ∈ B n, k, d    68  ALGEBRAIC CODING THEORY  Cyclic redundancy check    The generator polynomial g z  can be used for the detection of errors by  making use of the cyclic redundancy check  CRC   r z  ≡ 0 mod g z  ⇔ r z  ∈ B n, k, d    2.62     The received polynomial  r z  = r0 + r1 z + ··· + rn−2 zn−2 + rn−1 zn−1   2.63  of the received word r =  r0, r1, . . . , rn−2, rn−1  is divided by the generator polynomial g z . If the remainder is zero, the received word is a valid code word, otherwise transmission errors have been detected. In the so-called ARQ  automatic repeat request  scheme, the receiver can prompt the transmitter to retransmit the code word.    The following table shows some generator polynomials used for CRC.  CRC-12 CRC-16 CRC-CCITT  g z  = 1 + z + z2 + z3 + z4 + z12 g z  = 1 + z2 + z15 + z16 g z  = 1 + z5 + z12 + z16  Figure 2.41: Cyclic redundancy check  Parity-check polynomial    The parity-check polynomial h z  of a cyclic code B n, k, d  with generator  polynomial g z  is given by  h z  = zn − 1  g z    2.64     Parity-check equation  b z  h z  ≡ 0 mod zn − 1 ⇔ b z  ∈ B n, k, d    2.65   Figure 2.42: Parity-check polynomial h z    ALGEBRAIC CODING THEORY  69  Parity-check polynomial of the cyclic binary Hamming code    The generator polynomial of the cyclic binary  7,4  Hamming code is given  by  to  g z  = 1 + z + z  3 ∈ F2[z]  h z  = 1 + z + z  2 + z  4    The parity-check polynomial of this cyclic binary Hamming code is equal  Figure 2.43: Parity-check polynomial of the cyclic binary  7, 4  Hamming code  Because of deg g z   = n − k and deg zn − 1  = n = deg g z   + deg h z  ,  for general linear block codes. Figure 2.43 gives the parity-check polynomial of the cyclic  7, 4  binary Hamming code. the degree of the parity-check polynomial is given by deg h z   = k. Taking into account the normalisation gn−k = 1, we see that hk = 1, i.e.  h z  = h0 + h1 z + ··· + hk−1 zk−1 + zk.  Based on the parity-check polynomial h z , yet another systematic encoding algorithm can be derived. To this end, we make use of g z  h z  = zn − 1 and b z  = u z  g z . This yields   cid:2  zn − 1   cid:3  = −u z  + zn u z .  b z  h z  = u z  g z  h z  = u z   The degree of the information polynomial u z  is bounded by deg u z   ≤ k − 1, whereas the minimal exponent of the polynomial zn u z  is n. Therefore, the polynomial b z  h z  does not contain the exponentials zk, zk+1, . . . , zn−1. This yields the n − k parity-check equations  b0 hk + b1 hk−1 + ··· + bk h0 = 0, b1 hk + b2 hk−1 + ··· + bk+1 h0 = 0,  ...  bn−k−2 hk + bn−k−1 hk−1 + ··· + bn−2 h0 = 0, bn−k−1 hk + bn−k hk−1 + ··· + bn−1 h0 = 0  which can be written as the discrete convolution  k cid:1   j=0  hj bi−j = 0   70 ALGEBRAIC CODING THEORY for k ≤ i ≤ n − 1. This corresponds to the matrix equation H bT = 0 for general linear block codes with the  n − k  × n parity-check matrix 0 0 ...  H =  ··· 0 ··· 0 ... . . . ··· hk hk−1 ···  0  hk  ··· 0 0 ··· 0 0 ... ... . . . ··· h0 0 ··· h1 h0  ··· h0 0 ··· h1 h0 ... . . . ··· 0 ··· 0  ... 0 0  hk hk−1 0 hk ... ... 0 0 0 0    .     For the systematic encoding scheme the k code symbols bn−k, bn−k+1, . . . , bn−1 are set equal to the respective information symbols u0, u1, . . . , uk−1. This yields the following system of equations taking into account the normalisation hk = 1 b0 + b1 hk−1 + ··· + bk h0 = 0, b1 + b2 hk−1 + ··· + bk+1 h0 = 0,  which is recursively solved for the parity-check symbols bn−k−1, bn−k−2, . . . , b1, b0. This leads to the systematic encoding scheme  ...  bn−k−2 + bn−k−1 hk−1 + ··· + bn−2 h0 = 0, bn−k−1 + bn−k hk−1 + ··· + bn−1 h0 = 0  bn−k−1 = −  bn−k hk−1 + ··· + bn−1 h0  , bn−k−2 = −  bn−k−1 hk−1 + ··· + bn−2 h0  ,  ...  b1 = −  b2 hk−1 + ··· + bk+1 h0  , b0 = −  b1 hk−1 + ··· + bk h0  .  2.3.4 Dual Codes Similarly to general linear block codes, the dual code B⊥   of the cyclic code B n, k, d  with generator polynomial g z  and parity-check polynomial h z  can be deﬁned by changing the role of these polynomials  Jungnickel, 1995 . Here, we have to take into account that the generator polynomial must be normalised such that the highest exponent has coefﬁcient 1. To this end, we make use of the already derived  n − k  × n parity-check matrix   cid:1   n  , d  , k   cid:1    cid:1      H =  hk hk−1 0 hk ... ... 0 0 0 0  ··· h0 0 ··· h1 h0 ... . . . ··· 0 ··· 0  ... 0 0  0 0 ...  ··· 0 ··· 0 ... . . . ··· hk hk−1 ···  0  hk  ··· 0 0 ··· 0 0 ... ... . . . ··· h0 0 ··· h1 h0      ALGEBRAIC CODING THEORY  of a cyclic code. By comparing this matrix with the generator matrix  g0 0 ... 0 0  g1 g0 ... 0 0  ··· gn−k ··· gn−k−1 . . . ··· ···  ... 0 0  0 gn−k ... 0 0  ··· 0 ··· 0 ... . . . ··· g0 ··· 0  ··· 0 ··· 0 ... . . . ··· gn−k ··· gn−k−1  0 0 ... g1 g0  0 0 ... 0 gn−k     G =  71     of a cyclic code, we observe that, besides different dimensions and a different order of the matrix elements, the structure of these matrices is similar. With this observation, the generator polynomial g   can be obtained from the , d reversed and normalised parity-check polynomial according to  , k  ⊥   cid:1    cid:1    z  of the dual cyclic code B⊥  z  = hk + hk−1 z + ··· + h1 zk−1 + h0 zk   cid:1   n  ⊥  g  h0  −1 + ··· + hk−1 z  −k+1 + hk z  −k  h0  = zk h0 + h1 z −1  = zk h z h0  .  Figure 2.44 summarises the deﬁnition of the dual cyclic code B⊥ code B n, k, d .   cid:1   n   cid:1    cid:1   , k  , d    of the cyclic  2.3.5 Linear Feedback Shift Registers  As we have seen, the systematic encoding of a cyclic code can be carried out by dividing the shifted information polynomial zn−k u z  by the generator polynomial g z . The respective  Dual cyclic code    Let h z  be the parity-check polynomial of the cyclic code B n, k, d .   The dual code B⊥    is the cyclic code deﬁned by the generator   cid:1   n  , d  , k   cid:1    cid:1   polynomial  ⊥   z  = zk h z  g  −1  h0    For cyclic binary codes h0 = 1, and therefore −1    z  = zk h z  ⊥  g   2.66    2.67   Figure 2.44: Dual cyclic code B⊥   cid:1   n   cid:1    cid:1   , k  , d    of the cyclic code B n, k, d    72  ALGEBRAIC CODING THEORY  remainder yields the sought parity-check symbols. As is well known, polynomial division can be carried out with the help of linear feedback shift registers. Based on these linear feedback shift registers, efﬁcient encoding and decoding architectures for cyclic codes can be derived  Berlekamp, 1984; Lin and Costello, 2004 .  Encoding Architecture  For the information polynomial  u z  = u0 + u1 z + ··· + uk−2 zk−2 + uk−1 zk−1  and the generator polynomial  g z  = g0 + g1 z + ··· + gn−k−1 zn−k−1 + zn−k  the encoder architecture of a cyclic code over the ﬁnite ﬁeld Fq can be derived by mak- ing use of a linear feedback shift register with n − k registers as shown in Figure 2.45  Neubauer, 2006b . After the q-nary information symbols uk−1, uk−2, ··· , u1, u0 have been shifted into  the linear feedback shift register, the registers contain the components of the remainder  s z  = s0 + s1 z + ··· + sn−k−2 zn−k−2 + sn−k−1 zn−k−1  ≡ zn−k u z  mod g z .  Besides an additional sign, this term yields the remainder that is needed for the systematic encoding of the information polynomial u z . Therefore, after k clock cycles, the linear feedback shift register contains the negative parity-check symbols within the registers that can subsequently be emitted by the shift register. In summary, this encoding architecture yields the code polynomial  b z  = zn−k u z  − s z .  Decoding Architecture  The linear feedback shift register can also be used for the decoding of a cyclic code. Starting from the polynomial channel model illustrated in Figure 2.46  r z  = b z  + e z   with the error polynomial  e z  = e0 + e1 z + e2 z  2 + ··· + en−1 zn−1  the syndrome polynomial is deﬁned according to  s z  ≡ r z  mod g z .  The degree of the syndrome polynomial s z  is bounded by deg s z   ≤ n − k − 1.   ALGEBRAIC CODING THEORY  73  Encoder architecture for a cyclic code  . . .  +  g0  +  g1  +  gn−k−1  +  −1  +  . . .  +  +  u0 u1 ··· uk−2 uk−1    The sequence of information symbols uk−1, uk−2, ··· , u1, u0 is shifted into the linear feedback shift register within the ﬁrst k clock cycles  switch positions according to the solid lines .    At the end of the kth clock cycle the registers contain the negative parity-  check symbols.    The parity-check symbols are emitted during the next n − k clock cycles   switch positions according to the dashed lines .  Figure 2.45: Encoding of a cyclic code with a linear feedback shift register. Reproduced  by permission of J. Schlembach Fachverlag  If the received polynomial r z  is error free and therefore corresponds to the transmitted code polynomial b z , the syndrome polynomial s z  is zero. If, however, the received polynomial r z  is disturbed, the syndrome polynomial s z  ≡ r z  ≡ b z  + e z  ≡ e z   mod g z   exclusively depends on the error polynomial e z . Because of s z  ≡ r z  modulo g z , the syndrome polynomial is obtained as the polynomial remainder of the division of the received polynomial r z  by the generator polynomial g z . This division operation can again be car- ried out by a linear feedback shift register as illustrated in Figure 2.47  Neubauer, 2006b . Similarly to the syndrome decoding of linear block codes, we obtain the decoder archi- tecture shown in Figure 2.48 which is now based on a linear feedback shift register and a table look-up procedure  Neubauer, 2006b . Further information about encoder and decoder architectures based on linear feedback shift registers can be found elsewhere  Berlekamp, 1984; Lin and Costello, 2004 .   74  ALGEBRAIC CODING THEORY  Polynomial channel model  b z   r z   +  e z     The transmitted code polynomial b z  is disturbed by the error polynomial  e z .    The received polynomial r z  is given by    The syndrome polynomial  r z  = b z  + e z   s z  ≡ r z  mod g z    2.68    2.69   exclusively depends on the error polynomial e z .  Figure 2.46: Polynomial channel model  2.3.6 BCH Codes  We now turn to the so-called BCH codes as an important class of cyclic codes  Berlekamp, 1984; Bossert, 1999; Lin and Costello, 2004; Ling and Xing, 2004 . These codes make it possible to derive an efﬁcient algebraic decoding algorithm. In order to be able to deﬁne BCH codes, we ﬁrst have to introduce zeros of a cyclic code.17  Zeros of Cyclic Codes  A cyclic code B n, k, d  over the ﬁnite ﬁeld Fq is deﬁned by a unique generator polynomial g z  which divides each code polynomial b z  ∈ B n, k, d  according to g z  b z  or equivalently  The generator polynomial itself is a divisor of the polynomial zn − 1, i.e.  b z  ≡ 0 mod g z .  g z  h z  = zn − 1.  17For further details about the arithmetics in ﬁnite ﬁelds the reader is referred to Section A.3 in Appendix A.   ALGEBRAIC CODING THEORY  75  Decoder architecture for a cyclic code  . . .  +  g0  +  g1  +  gn−k−1  +  −1  +  . . .  +  +  rn−1 rn−2 ... r0    The sequence of received symbols rn−1, rn−2, . . . , r1, r0 is shifted into the  linear feedback shift register.    At the end of the nth clock cycle the registers contain the syndrome  polynomial coefﬁcients s0, s1, . . . , sn−k−2, sn−k−1 from left to right.  Figure 2.47: Decoding of a cyclic code with a linear feedback shift register. Reproduced  by permission of J. Schlembach Fachverlag  In the following, we assume that the polynomial zn − 1 has only single zeros which is equivalent to the condition  McEliece, 1987   gcd q, n  = 1.  This means that the cardinality q of the ﬁnite ﬁeld Fq and the code word length n have to be relatively prime. In this case, there exists a primitive nth root of unity α ∈ F ql in a ql with αn = 1 and αν  cid:7 = 1 for ν < n. For the extension ﬁeld F suitable extension ﬁeld F it follows that n q l − 1. With the help of such a primitive nth root of unity α, the n zeros ql of the polynomial zn − 1 are given by 1, α, α2, . . . , αn−1.  The polynomial zn − 1 can thus be factored into zn − 1 =  z − 1   z − α   z − α    ···  z − αn−1  2   .   76  r z   Syndrome decoding of a cyclic code  ALGEBRAIC CODING THEORY  Syndrome Calculation  s z   Table Lookup  ˆe z   +-  +  ˆb z     The syndrome polynomial s z  is calculated with the help of the received  polynomial r z  and the generator polynomial g z  according to  s z  ≡ r z  mod g z     The syndrome polynomial s z  is used to address a table that for each  syndrome stores the respective decoded error polynomial ˆe z .    By subtracting the decoded error polynomial ˆe z  from the received poly-  nomial r z , the decoded code polynomial is obtained by   2.70    2.71   ˆb z  = r z  − ˆe z   Figure 2.48: Syndrome decoding of a cyclic code with a linear feedback shift register and a table look-up procedure. Reproduced by permission of J. Schlembach Fachverlag  Since g z  divides the polynomial zn − 1, the generator polynomial of degree deg g z   = n − k can be deﬁned by the corresponding set of zeros αi1, αi2, . . . , αin−k . This yields  g z  =  z − αi1    z − αi2   ···  z − αin−k  .  However, not all possible choices for the zeros are allowed because the generator poly- nomial g z  of a cyclic code B n, k, d  over the ﬁnite ﬁeld Fq must be an element of the polynomial ring Fq[z], i.e. the polynomial coefﬁcients must be elements of the ﬁnite ﬁeld Fq. This is guaranteed if for each root αi its corresponding conjugate roots αi q, αi q2, . . . are also zeros of the generator polynomial g z . The product over all respective linear factors yields the minimal polynomial  mi  z  =  z − αi    z − αi q    z − αi q2    ···   ALGEBRAIC CODING THEORY  77  Cyclotomic cosets and minimal polynomials    Factorisation of z7 − 1 = z7 + 1 over the ﬁnite ﬁeld F2 into minimal polyno-  mials    Cyclotomic cosets Ci = cid:4   z7 + 1 =  z + 1   z3 + z + 1   z3 + z2 + 1    cid:5  i 2j mod 7 : 0 ≤ j ≤ 2  C0 = {0} C1 = {1, 2, 4} C3 = {3, 6, 5}  Figure 2.49: Cyclotomic cosets and minimal polynomials over the ﬁnite ﬁeld F2  with coefﬁcients in Fq. The set of exponents i, i q, i q 2, . . . of the primitive nth root of unity α ∈ F ql corresponds to the so-called cyclotomic coset  Berlekamp, 1984; Bossert, 1999; Ling and Xing, 2004; McEliece, 1987   Ci = cid:4    cid:5  i q j mod q l − 1 : 0 ≤ j ≤ l − 1  which can be used in the deﬁnition of the minimal polynomial   cid:20   κ∈Ci  mi  z  =   z − ακ  .  Figure 2.49 illustrates the cyclotomic cosets and minimal polynomials over the ﬁnite ﬁeld F2. The generator polynomial g z  can thus be written as the product of the corresponding minimal polynomials. Since each minimal polynomial occurs only once, the generator polynomial is given by the least common multiple   cid:2   g z  = lcm   cid:3   mi1  z , mi2  z , . . . , min−k  z   .  The characteristics of the generator polynomial g z  and the respective cyclic code B n, k, d  are determined by the minimal polynomials and the cyclotomic cosets respect- ively.  A cyclic code B n, k, d  with generator polynomial g z  can now be deﬁned by the set of minimal polynomials or the corresponding roots α1, α2, . . . , αn−k. Therefore, we will denote the cyclic code by its zeros according to  B n, k, d  = C α1, α2, . . . , αn−k .   78 ALGEBRAIC CODING THEORY Because of g α1  = g α2  = ··· = g αn−k  = 0 and g z  b z , the zeros of the generator polynomial g z  are also zeros  b α1  = b α2  = ··· = b αn−k  = 0  of each code polynomial b z  ∈ C α1, α2, . . . , αn−k .  BCH Bound Based on the zeros α1, α2, . . . , αn−k of the generator polynomial g z , a lower bound for the minimum Hamming distance d of a cyclic code C α1, α2, . . . , αn−k  has been derived by Bose, Ray-Chaudhuri and Hocquenghem. This so-called BCH bound, which is given in Figure 2.50, states that the minimum Hamming distance d is at least equal to δ if there are δ − 1 successive zeros αβ, αβ+1, αβ+2, . . . , αβ+δ−2  Berlekamp, 1984; Jungnickel, 1995; Lin and Costello, 2004 . Because of g z  b z  for every code polynomial b z , the condition in the BCH bound also amounts to b αβ   = b αβ+1  = b αβ+2  = ··· = b αβ+δ−2  = 0. With the help of the code polynomial b z  = b0 + b1z + b2z2 + ··· + bn−1zn−1, this yields  + b2 αβ 2  + ··· + bn−1 αβ  n−1   b0 + b1 αβ = 0, b0 + b1 αβ+1 + b2 α β+1  2 + ··· + bn−1 α β+1   n−1  = 0, b0 + b1 αβ+2 + b2 α β+2  2 + ··· + bn−1 α β+2   n−1  = 0, ... b0 + b1 αβ+δ−2 + b2 α β+δ−2  2 + ··· + bn−1 α β+δ−2   n−1  = 0  BCH bound    Let C α1, α2, . . . , αn−k  be a cyclic code of code word length n over the ql be an nth root  ﬁnite ﬁeld Fq with generator polynomial g z , and let α ∈ F of unity in the extension ﬁeld F  ql with αn = 1.    If the cyclic code incorporates δ − 1 zeros  αβ , αβ+1, αβ+2, . . . , αβ+δ−2  according to  g αβ   = g αβ+1    = g αβ+2    = ··· = g αβ+δ−2    = 0  the minimum Hamming distance d of the cyclic code is bounded below by  d ≥ δ   2.72   Figure 2.50: BCH bound   79  ALGEBRAIC CODING THEORY     which corresponds to the system of equations ··· αβ  n−1  ··· α β+1   n−1  ··· α β+2   n−1  . . .  1 αβ 1 αβ+1 1 αβ+2 ... 1 αβ+δ−2 α β+δ−2  2 ··· α β+δ−2   n−1   αβ 2 α β+1  2 α β+2  2 ...  ...  ...          =       .  0 0 0 ... 0  b0 b1 b2 ... bn−1  By comparing this matrix equation with the parity-check equation H bT = 0 of general linear block codes, we observe that the  δ − 1  × n matrix in the above matrix equation corresponds to a part of the parity-check matrix H. If this matrix has at least δ − 1 lin- early independent columns, then the parity-check matrix H also has at least δ − 1 linearly independent columns. Therefore, the smallest number of linearly dependent columns of H, and thus the minimum Hamming distance, is not smaller than δ, i.e. d ≥ δ. If we consider the determinant of the  δ − 1  ×  δ − 1  matrix consisting of the ﬁrst δ − 1 columns, we obtain  Jungnickel, 1995    cid:17  cid:17  cid:17  cid:17  cid:17  cid:17  cid:17  cid:17  cid:17  cid:17  cid:17    cid:17  cid:17  cid:17  cid:17  cid:17  cid:17  cid:17  cid:17  cid:17  cid:17  cid:17   =  1 αβ 1 αβ+1 1 αβ+2 ... 1 αβ+δ−2 α β+δ−2  2 ··· α β+δ−2   δ−2   ··· αβ  δ−2  ··· α β+1   δ−2  ··· α β+2   δ−2  . . .  αβ 2 α β+1  2 α β+2  2 ...  ...  ...  ··· 1 ··· αδ−2 ··· α2  δ−2  . . .  1 1 1 α1 1 α2 ... ... 1 αδ−2 α δ−2  2 ··· α δ−2   δ−2   1 α2 α4 ...  ...   cid:17  cid:17  cid:17  cid:17  cid:17  cid:17  cid:17  cid:17  cid:17  cid:17  cid:17   cid:17  cid:17  cid:17  cid:17  cid:17  cid:17  cid:17  cid:17  cid:17  cid:17  cid:17  αβ  δ−1   δ−2  2.  The resulting determinant on the right-hand side corresponds to a so-called Vander- monde matrix, the determinant of which is different from 0. Taking into account that αβ  δ−1   δ−2  2  cid:7 = 0, the  δ − 1  ×  δ − 1  matrix consisting of the ﬁrst δ − 1 columns is regular with δ − 1 linearly independent columns. This directly leads to the BCH bound d ≥ δ.  According to the BCH bound, the minimum Hamming distance of a cyclic code is determined by the properties of a subset of the zeros of the respective generator polynomial. In order to deﬁne a cyclic code by prescribing a suitable set of zeros, we will therefore merely note this speciﬁc subset. A cyclic binary Hamming code, for example, is determined by a single zero α; the remaining conjugate roots α2, α4, . . . follow from the condition that the coefﬁcients of the generator polynomial are elements of the ﬁnite ﬁeld F2. The respective cyclic code will therefore be denoted by C α .  Deﬁnition of BCH Codes  In view of the BCH bound in Figure 2.50, a cyclic code with a guaranteed minimum Hamming distance d can be deﬁned by prescribing δ − 1 successive powers  αβ , αβ+1  , αβ+2  , . . . , αβ+δ−2   80  BCH codes  ALGEBRAIC CODING THEORY    Let α ∈ F   The cyclic code C αβ , αβ+1, αβ+2, . . . , αβ+δ−2  over the ﬁnite ﬁeld Fq is  ql be an nth root of unity in the extension ﬁeld F  ql with αn = 1.  called the BCH code to the design distance δ.    The minimum Hamming distance is bounded below by  d ≥ δ   2.73     A narrow-sense BCH code is obtained for β = 1.   If n = q l − 1, the BCH code is called primitive.  Figure 2.51: Deﬁnition of BCH codes  of an appropriate nth root of unity α as zeros of the generator polynomial g z . Because of  the parameter δ is called the design distance of the cyclic code. The resulting cyclic code over the ﬁnite ﬁeld Fq is the so-called BCH code C αβ , αβ+1, αβ+2, . . . , αβ+δ−2  to the design distance δ  see Figure 2.51   Berlekamp, 1984; Lin and Costello, 2004; Ling and Xing, 2004 . If we choose β = 1, we obtain the narrow-sense BCH code to the design distance δ. For the code word length  d ≥ δ  n = q l − 1  the primitive nth root of unity α is a primitive element in the extension ﬁeld F ql due to αn = αql−1 = 1. The corresponding BCH code is called a primitive BCH code. BCH codes are often used in practical applications because they are easily designed for a wanted minimum Hamming distance d  Benedetto and Biglieri, 1999; Proakis, 2001 . Furthermore, efﬁcient algebraic decoding schemes exist, as we will see in Section 2.3.8. As an example, we consider the cyclic binary Hamming code over the ﬁnite ﬁeld F2 with n = 2m − 1 and k = 2m − m − 1. Let α be a primitive nth root of unity in the extension ﬁeld F2m. With the conjugate roots α, α2, α22, . . . , α2m−1, the cyclic code  C α  = cid:4   b z  ∈ F2[z]  zn − 1  : b α  = 0   cid:5   is deﬁned by the generator polynomial  g z  =  z − α   z − α  2     z − α  22    ···  z − α  2m−1   .   ALGEBRAIC CODING THEORY 81 Owing to the roots α and α2 there exist δ − 1 = 2 successive roots. According to the BCH bound, the minimum Hamming distance is bounded below by d ≥ δ = 3. In fact, as we already know, Hamming codes have a minimum Hamming distance d = 3. In general, for the deﬁnition of a cyclic BCH code we prescribe δ − 1 successive zeros αβ, αβ+1, αβ+2, . . . , αβ+δ−2. By adding the corresponding conjugate roots, we obtain the generator polynomial g z  which can be written as   cid:2    cid:3   g z  = lcm  mβ  z , mβ+1 z , . . . , mβ+δ−2 z   .  The generator polynomial g z  is equal to the least common multiple of the respective polynomials mi  z  which denote the minimal polynomials for αi with β ≤ i ≤ β + δ − 2.  2.3.7 Reed–Solomon Codes  As an important special case of primitive BCH codes we now consider BCH codes over the ﬁnite ﬁeld Fq with code word length  n = q − 1.  These codes are called Reed–Solomon codes  Berlekamp, 1984; Bossert, 1999; Lin and Costello, 2004; Ling and Xing, 2004 ; they are used in a wide range of applications ranging from communication systems to the encoding of audio data in a compact disc  Costello et al., 1998 . Because of αn = αq−1 = 1, the nth root of unity α is an element of the ﬁnite ﬁeld Fq. Since the corresponding minimal polynomial of αi over the ﬁnite ﬁeld Fq is simply given by  mi  z  = z − αi  the generator polynomial g z  of such a primitive BCH code to the design distance δ is  g z  =  z − αβ    z − αβ+1 The degree of the generator polynomial is equal to    ···  z − αβ+δ−2   .  deg g z   = n − k = δ − 1.  Because of the BCH bound, the minimum Hamming distance is bounded below by d ≥ δ = n − k + 1 whereas the Singleton bound delivers the upper bound d ≤ n − k + 1. Therefore, the minimum Hamming distance of a Reed–Solomon code is given by  d = n − k + 1 = q − k.  Since the Singleton bound is fulﬁlled with equality, a Reed–Solomon code is an MDS  maximum distance separable  code. In general, a Reed–Solomon code over the ﬁnite ﬁeld Fq is characterised by the following code parameters  n = q − 1, k = q − δ, d = δ.  In Figure 2.52 the characteristics of a Reed–Solomon code are summarised. For practically relevant code word lengths n, the cardinality q of the ﬁnite ﬁeld Fq is large. In practical applications q = 2l is usually chosen. The respective elements of the ﬁnite ﬁeld F2l are then represented as l-dimensional binary vectors over F2.   82  ALGEBRAIC CODING THEORY  Reed–Solomon codes    Let α be a primitive element of the ﬁnite ﬁeld Fq with n = q − 1.   The Reed–Solomon code is deﬁned as the primitive BCH code to the  design distance δ over the ﬁnite ﬁeld Fq with generator polynomial  g z  =  z − αβ    z − αβ+1  ···  z − αβ+δ−2     Code parameters  n = q − 1 k = q − δ d = δ   2.74    2.75    2.76    2.77     Because a Reed–Solomon code fulﬁls the Singleton bound with equality,  it is an MDS code.  Figure 2.52: Reed–Solomon codes over the ﬁnite ﬁeld Fq  Spectral Encoding  We now turn to an interesting relationship between Reed–Solomon codes and the discrete Fourier transform  DFT  over the ﬁnite ﬁeld Fq  Bossert, 1999; Lin and Costello, 2004; Neubauer, 2006b   see also Section A.4 in Appendix A . This relationship leads to an efﬁ- cient encoding algorithm based on FFT  fast Fourier transform  algorithms. The respective encoding algorithm is called spectral encoding. To this end, let α ∈ Fq be a primitive nth root of unity in the ﬁnite ﬁeld Fq with n = q − 1. Starting from the code polynomial  the discrete Fourier transform of length n over the ﬁnite ﬁeld Fq is deﬁned by  b z  = b0 + b1 z + ··· + bn−2 zn−2 + bn−1 zn−1 n−1 cid:1   Bj = b αj   = n−1 cid:1   −i   = n −1 with bi ∈ Fq and Bj ∈ Fq. The spectral polynomial is given by  bi = n −1  bi αij   −◦  i=0  B α  −ij  Bj α  j=0  B z  = B0 + B1 z + ··· + Bn−2 zn−2 + Bn−1 zn−1  .  Since every code polynomial b z  is divided by the generator polynomial  g z  =  z − αβ    z − αβ+1  ···  z − αβ+δ−2    ALGEBRAIC CODING THEORY 83 which is characterised by the zeros αβ, αβ+1, . . . , αβ+δ−2, every code polynomial b z  also has zeros at αβ, αβ+1, . . . , αβ+δ−2, i.e.   = b αβ+2    = ··· = b αβ+δ−2  b αβ   = b αβ+1    = 0.  In view of the discrete Fourier transform and the spectral polynomial B z   −◦ b z , this can be written as  Bj = b αj   = 0  for β ≤ j ≤ β + δ − 2. These spectral coefﬁcients are called parity frequencies. If we choose β = q − δ, we obtain the Reed–Solomon code of length n = q − 1, dimension k = q − δ and minimum Hamming distance d = δ. The information polynomial  u z  = u0 + u1 z + u2 z  2 + ··· + uq−δ−1 zq−δ−1  with k = q − δ information symbols uj is now used to deﬁne the spectral polynomial B z  according to  B z  = u z ,  i.e. Bj = uj for 0 ≤ j ≤ k − 1 and Bj = 0 for k ≤ j ≤ n − 1. This setting yields Bj = b αj   = 0 for q − δ ≤ j ≤ q − 2. The corresponding code polynomial b z  is obtained from the inverse discrete Fourier transform according to  bi = −B α  −ij .  Bj α  −i  = − q−2 cid:1   j=0  bi = − q−δ−1 cid:1   j=0  −ij .  uj α  Here, we have used the fact that n = q − 1 ≡ −1 modulo p, where p denotes the char- acteristic of the ﬁnite ﬁeld Fq, i.e. q = pl with the prime number p. Finally, the spectral encoding rule reads  Because there are fast algorithms available for the calculation of the discrete Fourier trans- form, this encoding algorithm can be carried out efﬁciently. It has to be noted, however, that the resulting encoding scheme is not systematic. The spectral encoding algorithm of a Reed–Solomon code is summarised in Figure 2.53  Neubauer, 2006b . A respective decoding algorithm is given elsewhere  Lin and Costello, 2004 .  Reed–Solomon codes are used in a wide range of applications, e.g. in communication systems, deep-space applications, digital video broadcasting  DVB  or consumer systems  Costello et al., 1998 . In the DVB system, for example, a shortened Reed–Solomon code with n = 204, k = 188 and t = 8 is used which is derived from a Reed–Solomon code over 28 = F256  ETSI, 2006 . A further important example is the encoding of the the ﬁnite ﬁeld F audio data in the compact disc with the help of the cross-interleaved Reed–Solomon code  CIRC  which is brieﬂy summarised in Figure 2.54  Costello et al., 1998; Hoeve et al., 1982 .   84  ALGEBRAIC CODING THEORY  Spectral encoding of a Reed–Solomon code  u0  u1  . . .  uk−2 uk−1  . . .  B0  B1  . . .  Bk−2 Bk−1  0  0  . . .  0    The k information symbols are chosen as the ﬁrst k spectral coefﬁcients,  δ − 1  Bj = uj  Bj = 0   2.78    2.79     The remaining n − k spectral coefﬁcients are set to 0 according to  i.e.  for 0 ≤ j ≤ k − 1.  for k ≤ j ≤ n − 1.    The code symbols are calculated with the help of the inverse discrete  Fourier transform according to  bi = −B α  −i   = − q−2 cid:1   Bj α  j=0  −ij = − q−δ−1 cid:1   j=0  −ij  uj α   2.80   for 0 ≤ i ≤ n − 1.  Figure 2.53: Spectral encoding of a Reed–Solomon code over the ﬁnite ﬁeld Fq.  Reproduced by permission of J. Schlembach Fachverlag  2.3.8 Algebraic Decoding Algorithm  Having deﬁned BCH codes and Reed–Solomon codes, we now discuss an algebraic decoding algorithm that can be used for decoding a received polynomial r z   Berlekamp, 1984; Bossert, 1999; Jungnickel, 1995; Lin and Costello, 2004; Neubauer, 2006b . To this end, without loss of generality we restrict the derivation to narrow-sense BCH codes   ALGEBRAIC CODING THEORY  85  Reed–Solomon codes and the compact disc    In the compact disc the encoding of the audio data is done with the help  of two interleaved Reed–Solomon codes.    The Reed–Solomon code with minimum distance d = δ = 5 over the ﬁnite ﬁeld F256 = F28 with length n = q − 1 = 255 and k = q − δ = 251 is short- ened such that two linear codes B 28, 24, 5  and B 32, 28, 5  over the ﬁnite ﬁeld F  28 arise.    The resulting interleaved coding scheme is called CIRC  cross-interleaved  Reed–Solomon code .    For each stereo channel, the audio signal is sampled with 16-bit resolution and a sampling frequency of 44.1 kHz, leading to a total of 2 × 16 × 44 100 = 1 411 200 bits per second. Each 16-bit stereo sample represents two 8-bit symbols in the ﬁeld F  28.    The inner shortened Reed–Solomon code B 28, 24, 5  encodes 24 infor-  mation symbols according to six stereo sample pairs.    The outer shortened Reed–Solomon code B 32, 28, 5  encodes the resul-  ting 28 symbols, leading to 32 code symbols.    In total, the CIRC leads to 4 231 800 channel bits on a compact disc which are further modulated and represented as so-called pits on the compact disc carrier.  Figure 2.54: Reed–Solomon codes and the compact disc  C α, α2, . . . , αδ−1  with α ∈ F  ql of a given designed distance  δ = 2t + 1.  It is important to note that the algebraic decoding algorithm we are going to derive is only capable of correcting up to t errors even if the true minimum Hamming distance d is larger than the designed distance δ. For the derivation of the algebraic decoding algorithm we make use of the fact that the generator polynomial g z  has as zeros δ − 1 = 2t successive powers of a primitive nth root of unity α, i.e.  g α  = g α  2    = ··· = g α  2t   = 0.  Since each code polynomial  b z  = b0 + b1 z + ··· + bn−2 zn−2 + bn−1 zn−1   r z  = b z  + e z   e z  = n−1 cid:1   ei zi  i=0  e z  = w cid:1   j=1  eij zij .  Xj = αij  Yj = eij  e z  = w cid:1   j=1  Yj zij .  86  ALGEBRAIC CODING THEORY  is a multiple of the generator polynomial g z , this property also translates to the code polynomial b z  as well, i.e.  b α  = b α  2    = ··· = b α  2t   = 0.  This condition will be used in the following for the derivation of an algebraic decoding algorithm for BCH and Reed–Solomon codes.  The polynomial channel model illustrated in Figure 2.46 according to  with error polynomial  will be presupposed. If we assume that w ≤ t errors have occurred at the error positions ij , the error polynomial can be written as  With the help of these error positions ij we deﬁne the so-called error locators according to  as well as the error values  for 1 ≤ j ≤ w  Berlekamp, 1984 . As shown in Figure 2.55, the error polynomial e z  can then be written as  The algebraic decoding algorithm has to determine the number of errors w as well as the error positions ij or error locators Xj and the corresponding error values Yj . This will be done in a two-step approach by ﬁrst calculating the error locators Xj ; based on the calculated error locators Xj , the error values Yj are determined next.  By analogy with our treatment of the decoding schemes for linear block codes and cyclic codes, we will introduce the syndromes on which the derivation of the algebraic decoding algorithm rests  see Figure 2.56 . Here, the syndromes are deﬁned according to  Sj = r αj   = n−1 cid:1   ri αij  i=0  for 1 ≤ j ≤ 2t. For a valid code polynomial b z  ∈ C α, α2, . . . , α2t   of the BCH code C α, α2, . . . , α2t  , the respective syndromes b αj  , which are obtained by evaluating the polynomial b z  at the given powers of the primitive nth root of unity α, are identically   ALGEBRAIC CODING THEORY  87  Error polynomial, error locators and error values   2.81    2.82    2.83    2.84    2.85    2.86     Error polynomial  e z  = ei1 zi1 + ei2 zi2 + ··· + eiw ziw  with error positions ij for 1 ≤ j ≤ w    Error locators    Error values    Error polynomial  Xj = αij  1 ≤ j ≤ w  for  Yj = eij  1 ≤ j ≤ w  for  e z  = Y1 zi1 + Y2 zi2 + ··· + Yw ziw  Figure 2.55: Algebraic characterisation of transmission errors based on the error  polynomial e z , error locators Xj and error values Yj  Syndromes    Received polynomial  r z  = b z  + e z     Syndromes of received polynomial r z   Sj = r αj   = n−1 cid:1   i=0  ri αij  for  1 ≤ j ≤ 2t    Since b αj   = 0 for the transmitted code polynomial b z  ∈ C α, α2, . . . , α2t   and 1 ≤ j ≤ 2t, the syndromes exclusively depend on the error polynomial e z , i.e.  Sj = r αj   = b αj   + e αj   = e αj    1 ≤ j ≤ 2t  for   2.87   Figure 2.56: Syndromes in the algebraic decoding algorithm   88  ALGEBRAIC CODING THEORY  Syndromes and error locator polynomial    Syndromes  for 1 ≤ j ≤ 2t    Error locator polynomial  Sj = r αj   = w cid:1   j Yi X i  i=1  λ z  = w cid:20    1 − Xj z  = w cid:1   j=1  λi zi  i=0  of degree deg λ z   = w   2.88    2.89   Figure 2.57: Syndromes Sj and error locator polynomial λ z   zero, i.e. b αj   = 0 for 1 ≤ j ≤ 2t. This is true because the powers αj correspond to the zeros used to deﬁne the BCH code C α, α2, . . . , α2t  . Therefore, the syndromes  Sj = r αj   = b αj   + e αj   = e αj    for 1 ≤ j ≤ 2t merely depend on the error polynomial e z . The syndrome Sj is an element of the extension ﬁeld F ql which also includes the primitive nth root of unity α. The consequence of this is that we need to carry out the arithmetics of the algebraic decoding algorithm in the extension ﬁeld F ql . For Reed–Solomon codes, however, the calculations can be done in the ﬁnite ﬁeld Fq because α ∈ Fq and therefore Sj ∈ Fq. Since the syndromes Sj = e αj   are completely determined by the error polynomial e z , they can be expressed using the error locators Xj and error values Yj  see Figure 2.57 . With and by making use of the error locators Xj = αij , we obtain  Sj = e αj   = Y1 αi1j + Y2 αi2j + ··· + Yw αiw j  Sj = Y1 X  j 1  + Y2 X  j 2  + ··· + Yw Xj  w  Yi X  j i .  = w cid:1   i=1  In order to determine the error polynomial e z  and to decode the transmitted code poly- nomial b z , we have to calculate the error locators Xj and error values Yj on the basis of the syndromes Sj . The latter is the only information available to the algebraic decoding algorithm. To this end, we deﬁne the so-called error locator polynomial  Berlekamp, 1984   λ z  = w cid:20   j=1   1 − Xj z    By exchanging the order of summation, we obtain  ALGEBRAIC CODING THEORY  89  as a polynomial of degree w by prescribing the w zeros as the inverses of the error locators Xj . Expanding the respective product, we arrive at the polynomial representation  with coefﬁcients λi.18 If these coefﬁcients λi or the error locator polynomial λ z  are known, the error locators Xj can be determined by searching for the zeros of the error locator polynomial λ z .  According to the deﬁning equation, the error locator polynomial λ z  has its zeros at  the inverse error locators X  for 1 ≤ k ≤ w. This expression is multiplied by YkX the summation index k, leading to  k  j+w  and accumulated with respect to  λ z  = w cid:1   λi zi  i=0  −1 k , i.e.  k   = w cid:1   −1  i=0  λ X  −i  = 0  λi X k  w cid:1   k=1  w cid:1   i=0  w cid:1   i=0  j+w  Yk X k  −i  = 0.  λi X k  λi  k=1  w cid:1   cid:13  w cid:1   i=0  j+w−i  = 0.  Yk X k   cid:14  cid:15   = Sj+w−i   cid:16   λi Sj+w−i = 0.  In this expression the syndromes Sj+w−i are identiﬁed, leading to the so-called key equation  The key equation in Figure 2.58 relates the known syndromes Sj to the unknown coefﬁ- cients λi of the error locator polynomial λ z  with λ0 = 1 by a linear recurrence equation. The solution to this key equation can be formulated as ﬁnding the shortest linear recursive ﬁlter over the ﬁnite ﬁeld F ql with ﬁlter coefﬁcients λi. For this purpose, the so-called Berlekamp–Massey algorithm can be used  Berlekamp, 1984; Bossert, 1999; Lin and Costello, 2004; Massey, 1969 . As soon as the error locator polynomial λ z  is known, we can determine its zeros as the inverses of the error locators Xj , leading to the error positions ij .  As an alternative to the Berlekamp–Massey algorithm, Euclid’s algorithm for calcu- lating the greatest common divisor of two polynomials can be used for determining the error locator polynomial λ z   Bossert, 1999; Dornstetter, 1987; Jungnickel, 1995; Lin and Costello, 2004; Neubauer, 2006b . In order to formulate the respective algorithm, we deﬁne the syndrome polynomial S z  as well as the so-called error evaluator polynomial ω z  as shown in Figure 2.59  Berlekamp, 1984 .  18The coefﬁcients λi relate to the so-called elementary symmetric polynomials.   90  ALGEBRAIC CODING THEORY  Key equation and Berlekamp–Massey algorithm    Key equation  for j = 1, 2, . . .  Sj+w + λ1 Sj+w−1 + λ2 Sj+w−2 + ··· + Sj = 0   2.90     The key equation can be solved with the help of the Berlekamp–Massey algorithm which ﬁnds the shortest linear recursive ﬁlter with ﬁlter coefﬁ- cients λi emitting the syndromes Sj .  Figure 2.58: Key equation and Berlekamp–Massey algorithm  Syndrome and error evaluator polynomial    Syndrome polynomial  S z  = 2t cid:1   j=1    Error evaluator polynomial  Sj zj−1 = S1 + S2 z + ··· + S2t z  2t−1   2.91   ω z  ≡ S z  λ z  mod z2t   2.92   Figure 2.59: Syndrome polynomial S z  and error evaluator polynomial ω z   With the help of the syndromes Sj , the error evaluator polynomial ω z  can be expressed  as follows  Jungnickel, 1995; Neubauer, 2006b   ω z  ≡ S z  λ z  mod z w cid:20   2t  ≡ 2t cid:1  ≡ 2t cid:1   j=1  Sj zj−1 w cid:1    1 − Xk z  mod z w cid:20   2t  k=1 i zj−1  j   1 − Xk z  mod z  2t .  Yi X  j=1  i=1  k=1   ALGEBRAIC CODING THEORY  By exchanging the order of summation, we obtain  ω z  ≡ w cid:1   Yi Xi  i=1  2t cid:1   j=1  w cid:20   k=1   Xi z j−1   1 − Xk z  mod z  2t .  With the help of the formula for the ﬁnite geometric series  91  2t cid:1   j=1   Xi z j−1 = 1 −  Xi z 2t 1 − Xi z  which is also valid over ﬁnite ﬁelds, we have 1 −  Xi z 2t 1 − Xi z  cid:2  1 −  Xi z   ω z  ≡ w cid:1  ≡ w cid:1   Yi Xi  Yi Xi  i=1  w cid:20   cid:3   k=1  2t   1 − Xk z  mod z2t w cid:20    1 − Xk z  mod z  2t .  k=1,k cid:7 =i  On account of the factor  1 −  Xi z 2t   and the modulo operation, the second term  Xi z 2t can be neglected, i.e.  i=1  ω z  ≡ w cid:1  = w cid:1   i=1  w cid:20  w cid:20    1 − Xk z  mod z  2t  Yi Xi  k=1,k cid:7 =i  Yi Xi  i=1  k=1,k cid:7 =i   1 − Xk z .  As can be seen from this expression, the degree of the error evaluator polynomial ω z  is equal to deg ω z   = w − 1, and therefore less than 2t, so that the modulo operation can be removed from the above equation. It can be shown that the error evaluator polynomial ω z  and the error locator polynomial λ z  are relatively prime. According to Figure 2.60, the error locator polynomial λ z  of degree deg λ z   = w ≤ t and the error evaluator polynomial ω z  of degree deg ω z   = w − 1 ≤ t − 1 can be determined with the help of Euclid’s algorithm. To this end, the equation ω z  ≡ S z  λ z  modulo z2t , which is also called the key equation of the algebraic decoding problem, is expressed by  ω z  = S z  λ z  + T  z  z2t  with a suitable polynomial T  z . Apart from a common factor, this equation corresponds to the relationship between the polynomials S z  and z2t and their greatest common divi- sor gcd z2t , S z  . By iterating Euclid’s algorithm  see also Figure A.3  until the degree deg ri  z   of the remainder  ri  z  = fi  z  S z  + gi  z  z  2t   92  ALGEBRAIC CODING THEORY  Euclid’s algorithm for solving the key equation    Key equation  with    On account of  ω z  ≡ S z  λ z  mod z  2t   2.93   deg λ z   = w ≤ t  and deg ω z   = w − 1 ≤ t − 1  ω z  = S z  λ z  + T  z  z2t  the error evaluator polynomial ω z  and the error locator polynomial λ z  can be determined with the help of Euclid’s algorithm for calculating the greatest common divisor gcd S z , z2t   of the polynomials z2t and S z .  Figure 2.60: Euclid’s algorithm for solving the key equation by calculating the error  evaluator polynomial ω z  and the error locator polynomial λ z   in iteration i is smaller than t, the error locator polynomial λ z  and the error evaluator polynomial ω z  are obtained, apart from a constant factor c, from19  λ z  = c ω z  = c  gcd ri  z , gi  z    gi  z   ri  z   gcd ri  z , gi  z    ,  .  In order to derive the error values Yj , we evaluate the error evaluator polynomial ω z  at the inverses of the error locators Xj . This yields  We also calculate the formal derivative  1 − Xk z    z  =  λ   cid:1   ω X  j   = Yj Xj −1  cid:10  cid:1  w cid:20    cid:8   k=1  −1 j  .   1 − Xk X w cid:20   k=1,k cid:7 =j  = − w cid:1   Xi  i=1  k=1,k cid:7 =i   1 − Xk z   of the error locator polynomial λ z  with respect to z. Evaluating this formal derivative at the inverses of the error locators Xj , we arrive at   cid:1   λ   X  j   = −Xj −1   1 − Xk X  −1 j  .  19As we will see in the following, this constant factor c does not matter in the calculation of the error values  Yj when using Forney’s formula derived below.  w cid:20   w cid:20   k=1,k cid:7 =j   ALGEBRAIC CODING THEORY 93 Besides the factor −Yj , this expression is identical to the expression derived above for the error evaluator polynomial. By relating these expressions, Forney’s formula  Yj = − ω X   cid:1   λ   X  −1 j   −1 j    results for calculating the error values Yj .  In summary, we obtain the algebraic decoding algorithm for narrow-sense BCH codes in Figure 2.61  Neubauer, 2006b . This algebraic decoding algorithm can be illustrated by the block diagram in Figure 2.62 showing the individual steps of the algorithm  Lee, 2003 . In this block diagram the main parameters and polynomials determined during the course of the algorithm are shown. Here, Chien search refers to the sequential search for the zeros of the error locator polynomial  Berlekamp, 1984 . The arithmetics are carried out in the extension ﬁeld F ql . For a Reed–Solomon code as a special case of primitive BCH codes with code word length n = q − 1 the calculations are executed in the ﬁnite ﬁeld Fq. Further details of Reed–Solomon decoders, including data about the implementation complexity which is measured by the number of gates needed to realise the corresponding integrated circuit module, can be found elsewhere  Lee, 2003, 2005 .  Erasure Correction  Erasures are deﬁned as errors with known error positions ij but unknown error values Yj . For the correction of these erasures, the algebraic decoding algorithm can be simpliﬁed. Since the error positions ij as well as the number of errors w are known, the error locators Xj = αij and the error locator polynomial λ z  can be directly formulated. Figure 2.63 illustrates the algebraic decoding algorithm for the correction of erasures.  2.4 Summary  In this chapter we have introduced the basic concepts of algebraic coding theory. Lin- ear block codes have been discussed which can be deﬁned by their respective generator and parity-check matrices. Several code construction techniques have been presented. As important examples of linear block codes we have treated the repetition code, parity-check code, Hamming code, simplex code and Reed–Muller code. So-called low density parity- check or LDPC codes, which are currently under research, will be presented in Section 4.1. Further information about other important linear and non-linear codes can be found else- where  Berlekamp, 1984; Bossert, 1999; Lin and Costello, 2004; Ling and Xing, 2004; MacWilliams and Sloane, 1998; McEliece, 2002; van Lint, 1999 .  By introducing further algebraic structures, cyclic codes were presented as an important subclass of linear block codes for which efﬁcient algebraic decoding algorithms exist. Cyclic codes can be deﬁned with the help of suitable generator or parity-check polynomials. Owing to their speciﬁc properties, efﬁcient encoding and decoding architectures are available, based on linear feedback shift registers. With the help of the zeros of the respective generator polynomial, BCH codes and Reed–Solomon codes were deﬁned. Further details about cyclic codes can be found elsewhere  Berlekamp, 1984; Bossert, 1999; Lin and Costello, 2004; Ling and Xing, 2004 .   94  ALGEBRAIC CODING THEORY  Algebraic decoding algorithm  Notation received word r z  ∈ Fq[z]  zn − 1 ; decoded code word ˆb z  ∈ Fq[z]  zn − 1 ; calculate syndromes Sj = r αj   = n−1  Algebraic decoding algorithm  if Sj = 0 for 1 ≤ j ≤ 2t then  ˆe z  = 0;  ri αij for 1 ≤ j ≤ 2t;  i=0  else  calculate syndrome polynomial S z  = 2t  Sj zj−1;  j=1  calculate relatively prime polynomials ω z , λ z  with deg ω z   ≤ t − 1, deg λ z   ≤ t and ω z  ≡ S z  λ z  modulo z2t using Euclid’s algorithm; −ij of λ z  for 1 ≤ j ≤ w with w = deg λ z  ;  = α  −1  determine zeros X j calculate error values Yj = − ω X  cid:1  calculate error positions ij from Xj = αij for 1 ≤ j ≤ w;  for 1 ≤ j ≤ w;  −1 −1  calculate error polynomial ˆe z  = w  Yj zij ;   X j  λ        j  j=1  end calculate code polynomial ˆb z  = r z  − ˆe z ;  Figure 2.61: Algebraic decoding algorithm for narrow-sense BCH codes   ALGEBRAIC CODING THEORY  95  Algebraic decoding algorithm  = α  ij  Xj −1   cid:1   λ   X  j     r z   Syndrome Calculation  Euclid's Algorithm  Chien Search  Forney´s Algorithm  Error  Correction  ˆb z   Sj  λ z   ω z   −1  j     ω X  Yj  Figure 2.62: Block diagram of algebraic decoding algorithm  The maximum likelihood decoding strategy has been derived in this chapter as an optimal decoding algorithm that minimises the word error probability  Bossert, 1999 . Instead of maximum likelihood decoding, a symbol-by-symbol MAP decoding algorithm can be implemented on the basis of the BCJR algorithm  Bahl et al., 1974 . This algorithm is derived by representing block codes with the help of trellis diagrams which will be discussed in detail in the context of convolutional codes in the next chapter. Furthermore, we have mainly discussed hard-decision decoding schemes because they prevail in today’s applications of linear block codes and cyclic codes. However, as was indicated for the decoding of Reed–Muller codes, soft-decision decoding schemes usually lead to a smaller error probability. Further information about the decoding of linear block codes, including hard-decision and soft-decision decoding algorithms, is given elsewhere  Bossert, 1999; Lin and Costello, 2004 .  The algebraic coding theory as treated in this chapter is nowadays often called the ‘classical’ coding theory. After Shannon’s seminal work  Shannon, 1948 , which laid the foundation of information theory, the ﬁrst class of systematic single-error correcting chan- nel codes was invented by Hamming  Hamming, 1950 . Channel codes that are capable of correcting more than a single error were presented by Reed and Muller, leading to the Reed–Muller codes which have been applied, for example, in space communications within the Mariner and Viking Mars mission  Costello et al., 1998; Muller, 1954 . Several years later, BCH codes were developed by Bose, Ray-Chaudhuri and Hocquenghem  Bose et al., 1960 . In the same year, Reed and Solomon introduced Reed–Solomon codes which found a wide area of applications ranging from space communications to digital video broadcasting to the compact disc  Reed and Solomon, 1960 . Algebraic decoding algo- rithms were found by Peterson for binary codes as well as by Gorenstein and Zierler for q-nary codes  Gorenstein and Zierler, 1961; Peterson, 1960 . With the help of efﬁciently implementable iterative decoding algorithms proposed by Berlekamp and Massey, these   96  ALGEBRAIC CODING THEORY  Algebraic decoding algorithm for erasure correction  Notation received word r z  ∈ Fq[z]  zn − 1 ; decoded code word ˆb z  ∈ Fq[z]  zn − 1 ; number of erasures w;  Algebraic decoding algorithm  calculate syndromes Sj = r αj   = n−1 calculate syndrome polynomial S z  = 2t calculate error locator polynomial λ z  = w$  calculate error locators Xj = αij ;  j=1  i=0  ri αij for 1 ≤ j ≤ 2t;  Sj zj−1;   1 − Xj z ;  j=1  calculate error evaluator polynomial ω z  ≡ S z  λ z  modulo z2t; calculate error values Yj = − ω X  cid:1   for 1 ≤ j ≤ w;  −1 −1     j     λ   X j  calculate error polynomial ˆe z  = w Yj zij ; calculate code polynomial ˆb z  = r z  − ˆe z ;  j=1  Figure 2.63: Algebraic decoding algorithm for erasure correction for narrow-sense BCH  codes  algebraic decoding algorithms became applicable for practical applications  Berlekamp, 1984; Massey, 1969 . Further details about important applications of these classical codes, as well as codes discussed in the following chapters, can be found elsewhere  Costello et al., 1998 .   3  Convolutional Codes  In this chapter we discuss binary convolutional codes. Convolutional codes belong, like the most practical block codes, to the class of linear codes. Similarly to linear block codes, a convolutional code is deﬁned by a linear mapping of k information symbols to n code symbols. However, in contrast to block codes, convolutional codes have a memory, i.e. the current encoder output of the convolutional encoder depends on the current k input symbols and on  possibly all  previous inputs.  Today, convolutional codes are used for example in Universal Mobile Telecommuni- cations System  UMTS  and Global System for Mobile communications  GSM  digital cellular systems, dial-up modems, satellite communications, 802.11 wireless Local Area Networks  LANs  and many other applications. The major reason for this popularity is the existence of efﬁcient decoding algorithms. Furthermore, those decoding methods can utilise soft-input values from the demodulator which leads to signiﬁcant performance gains compared with hard-input decoding.  The widespread Viterbi algorithm is a maximum likelihood decoding procedure that can be implemented with reasonable complexity in hardware as well as in software. The Bahl, Cocke, Jelinek, Raviv  BCJR  algorithm is a method to calculate reliability information for the decoder output. This so-called soft output is essential for the decoding of concatenated convolutional codes  turbo codes , which we will discuss in Chapter 4. Both the BCJR algorithm and the Viterbi algorithm are based on the trellis representation of the code. The highly repetitive structure of the code trellis makes trellis-based decoding very suitable for pipelining hardware implementations.  We will concentrate on rate 1 n binary linear time-invariant convolutional codes. These are the easiest to understand and also the most useful in practical applications. In Section 3.1, we discuss the encoding of convolutional codes and some of their basic properties.  Section 3.2 is dedicated to the famous Viterbi algorithm and the trellis representation of convolutional codes. Then, we discuss distance properties and error correction capabilities. The concept of soft-output decoding and the BCJR algorithm follow in Section 3.5.  Finally, we consider an application of convolutional codes for mobile communication channels. Section 3.6.2 deals with the hybrid Automatic Repeat Request  ARQ  protocols as deﬁned in the GSM standard for enhanced packet data services. Hybrid ARQ protocols  Coding Theory – Algorithms, Architectures, and Applications Andr´e Neubauer, J¨urgen Freudenberger, Volker K¨uhn  cid:148  2007 John Wiley & Sons, Ltd   98  CONVOLUTIONAL CODES  are a combination of forward error correction and the principle of automatic repeat request, i.e. to detect and repeat corrupted data. The basic idea of hybrid ARQ is that the error correction capability increases with every retransmission. This is an excellent example of an adaptive coding system for strongly time-variant channels.  3.1 Encoding of Convolutional Codes  In this section we discuss how convolutional codes are encoded. Moreover, we establish the notation for the algebraic description and consider some structural properties of binary linear convolutional codes.  3.1.1 Convolutional Encoder  The easiest way to introduce convolutional codes is by means of a speciﬁc example. Consider the convolutional encoder depicted in Figure 3.1. Information bits are shifted into a register of length m = 2, i.e. a register with two binary memory elements. The output sequence results from multiplexing the two sequences denoted by b 1  and b 2 . Each output bit is generated by modulo 2 addition of the current input bit and some symbols of the register contents. For instance, the information sequence u =  1, 1, 0, 1, 0, 0, . . .  will be encoded to b 1  =  1, 0, 0, 0, 1, 1, . . .  and b 2  =  1, 1, 1, 0, 0, 1, . . . . The generated code sequence after multiplexing of b 1  and b 2  is b =  1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, . . . .  A convolutional encoder is a linear sequential circuit and therefore a Linear Time- Invariant  LTI  system. It is well known that an LTI system is completely characterized  A rate R = 1 2 convolutional encoder with memory m = 2  u  b 1   b 2     Each output block is calculated by modulo 2 addition of the current input  block and some symbols of the register contents: = ui + ui−1 + ui−2, = ui + ui−2.  b 1  b 2   i  i  Figure 3.1: Example of a convolutional encoder   CONVOLUTIONAL CODES  99  by its impulse response. Let us therefore investigate the two impulse responses of this particular encoder. The information sequence u = 1, 0, 0, 0, . . . results in the output b 1  =  1, 1, 1, 0, . . .  and b 2  =  1, 0, 1, 0, . . . , i.e. we obtain the generator impulse responses g 1  =  1, 1, 1, 0, . . .  and g 2  =  1, 0, 1, 0, . . .  respectively. These generator impulse res- ponses are helpful for calculating the output sequences for an arbitrary input sequence  = =  b 1  b 2   i  i  m  l=0 ui−lg 1  l=0 ui−lg 2   m  l  l  ↔ b 1  = u ∗ g 1 , ↔ b 2  = u ∗ g 2 .  The generating equations for b 1  and b 2  can be regarded as convolutions of the input sequence with the generator impulse responses g 1  and g 2 . The code B generated by this encoder is the set of all output sequences b that can be produced by convolution of arbitrary input sequence u with the generator impulse responses. This explains the name convolutional codes. The general encoder of a rate R = k n convolutional code is depicted in Figure 3.2. Each input corresponds to a shift register, i.e. each information sequence is shifted into its own register. In contrast to block codes, the ith code block bi = b1 i of a convolutional code sequence b = b0, b1, . . . is a linear function of several information j with j ∈ {i − m, . . . , i} and not only of bi. The integer m blocks uj = u1  i , . . . , bn  j , . . . , uk  j , u2  i , b2  Convolutional encoder with k inputs and n outputs  u 1   u l   u k   ...  ...  Encoder  ...  ...  b 1   b j    b n     k and n denote the number of encoder inputs and outputs respectively.  Thus, the code rate is R = k  .  n    The current n outputs are linear combinations of the present k input bits and the previous k × m input bits, where m is called the memory of the convolutional code.    A binary convolutional code is often denoted by a three-tuple  n, k, m .  Figure 3.2: Convolutional encoder with k inputs and n outputs   100  CONVOLUTIONAL CODES  denotes the memory of the encoder. The k encoder registers do not necessarily have the same length. We denote the number of memory elements of the lth register by νl. The n output sequences may depend on any of the k registers. Thus, we require k × n impulse responses to characterise the encoding. If the shift registers are feedforward registers, i.e. they have no feedback, than the corresponding generator impulse responses g j   are limited to length νl + 1. For this reason, νl is often called the constraint length of the lth input sequence. The memory m of the encoder is  l  m = max  νl.  l  The memory parameters of a convolutional encoder are summarised in Figure 3.3. A binary convolutional code is often denoted by a three-tuple B n, k, m . For instance, B 2, 1, 2  represents the code corresponding to the encoder in Figure 3.1.  The reader is probably familiar with the theory of discrete-time LTI systems over the real or the complex ﬁeld, which are sometimes called discrete-time real or complex ﬁlters. Similarly, convolutional encoders can be regarded as ﬁlters over ﬁnite ﬁelds – in our case the ﬁeld F2. The theory of discrete-time LTI systems over the ﬁeld F2 is similar to the theory for real or complex ﬁelds, except that over a ﬁnite ﬁeld there is no notion of convergence of an inﬁnite sum. We will observe that a little linear system theory is helpful in the context of convolutional codes. For example, we know different methods to construct ﬁlters for a given transfer function from system theory. The structure depicted in Figure 3.4 is a canonical form of a transversal ﬁlter, where usually the forward coefﬁcients pl and the backward coefﬁcients ql are real numbers. In case of a convolutional encoder, these coefﬁcients are elements of the ﬁeld F2.  With convolutional codes it is possible to construct different circuits that result in the same mapping from information sequence to code sequence, i.e. a particular encoder can be implemented in different forms. However, as the particular implementation of an encoder plays a certain role in the following discussion, we will restrict ourselves to the canonical construction in Figure 3.4, the so-called controller canonical form. Our example encoder depicted in Figure 3.1 is also realised in controller canonical form. It has no feedback from the register outputs to the input. Therefore, this encoder has a ﬁnite impulse response. In  Memory parameters of a convolutional encoder  As the numbers of memory elements of the k encoder registers may differ, we deﬁne    the memory m = maxl νl of the encoder   the minimum constraint length νmin = minl νl, l=1 νl.    and the overall constraint length ν =  k  Figure 3.3: Memory parameters of a convolutional encoder   CONVOLUTIONAL CODES  Rational transfer functions    Shift register in controller canonical form  101  p0  p1  p2  u l   q1  q2  b j    pm  qm  . . .  . . .  . . .    A realisable rational function:  Gl,j  D  = Pl,j  D   Ql,j  D   = p0 + p1D + ··· + pmDm 1 + q1D + ··· + qmDm  .  Figure 3.4: Rational transfer functions  general, an encoder may have feedback according to Figure 3.4 and therefore an inﬁnite impulse response.  3.1.2 Generator Matrix in the Time Domain  Similarly to linear block codes, the encoding procedure can be described as a multiplication of the information sequence with a generator matrix b = uG. However, the information sequence u and the code sequence b are semi-inﬁnite. Therefore, the generator matrix of a convolutional code also has a semi-inﬁnite structure. It is constructed from k × n submatrices Gi according to Figure 3.5, where the elements of the submatrices are the coefﬁcients from the generator impulse responses.  For instance, for the encoder in Figure 3.1 we obtain 0   =  1 1 , 1   =  1 0 , 2   =  1 1 .  G0 =  g 1  G1 =  g 1  G2 =  g 1   0 , g 2  1 , g 2  2 , g 2    102  CONVOLUTIONAL CODES  Generator matrix in the time domain    The generator matrix is constructed from k × n submatrices     Gi =  g 1  1,i g 1  2,i ... g 1   k,i  g 2  1,i g 2  2,i ... g 2   k,i  . . . g n  1,i . . . g n  2,i ... . . . g n  k,i    for i ∈ [0, m].    The elements g  g j   , i.e. g l output j.   j   l,i   j    l,i are the coefﬁcients from the generator impulse responses is the ith coefﬁcient of the impulse response from input l to    The generator matrix is then    G0 G1  0 G0 G1 0  0 G0 G1 . . . 0  0  0  . . . Gm  0 . . . Gm  0 0  . . . . . . . . . Gm . . . . . . . . .    ,  G =  where a bold zero indicates an all-zero matrix.  Figure 3.5: Generator matrix in the time domain  Finally, the generator matrix is    G0 G1  0 G0 G1 0  0 G0 G1 . . . 0  0  0  . . . Gm  0 . . . Gm  0 0  . . . . . . . . . Gm . . . . . . . . .    =    11 10 11  00 11 10 00 00 11  00 00 00  G =    .  00 11 10 . . .  00 . . . 00 . . . 11 . . . . . .  With this generator matrix we can express the encoding of an information sequence, for instance u =  1, 1, 0, 1, 0, 0, . . . , by a matrix multiplication    11 10 11  00 11 10 00 00 11  00 00 00     00 11 10 . . .  00 . . . 00 . . . 11 . . . . . .  b = uG =  1, 1, 0, 1, 0, 0, . . .   =  1 1, 0 1, 0 1, 0 0, 1 0, 1 1, 0 0, . . . .   CONVOLUTIONAL CODES 3.1.3 State Diagram of a Convolutional Encoder  103  Up to now we have considered two methods to describe the encoding of a convolutional code, i.e. encoding with a linear sequential circuit, a method that is probably most suitable for hardware implementations, and a formal description with the generator matrix. Now we consider a graphical representation, the so-called state diagram. The state diagram will be helpful later on when we consider distance properties of convolutional codes and decoding algorithms.  The state diagram of a convolutional encoder describes the operation of the encoder. From this graphical representation we observe that the encoder is a ﬁnite-state machine. For the construction of the state diagram we consider the contents of the encoder registers as encoder state σ . The set S of encoder states is called the encoder state space.  Each memory element contains only 1 bit of information. Therefore, the number of encoder states is 2ν. We will use the symbol σi to denote the encoder state at time i. The state diagram is a graph that has 2ν nodes representing the encoder states. An example of the state diagram is given in Figure 3.6. The branches in the state diagram repre- sent possible state transitions, e.g. if the encoder in Figure 3.1 has the register contents  State diagram of a convolutional encoder  1   10  11  0   10  1   00  1   01   10  0   01  01  1   11  00  0   11  0   00    Each node represents an encoder state, i.e. the binary contents of the  memory elements.    Each branch represents a state transition and is labelled by the corres-  ponding k input and n output bits.  Figure 3.6: State diagram of the encoder in Figure 3.1   104 CONVOLUTIONAL CODES σi =  00  and the input bit ui at time i is a 1, then the state of the encoder changes from σi =  00  to σi+1 =  10 . Along with this transition, the two output bits bi =  11  are generated. Similarly, the information sequence u =  1, 1, 0, 1, 0, 0, . . .  corresponds to the state sequence σ =  00, 10, 11, 01, 10, 01, 00, . . . , subject to the encoder starting in the all-zero state. The code sequence is again b =  1 1, 0 1, 0 1, 0 0, 1 0, 1 1, 0 0, . . . . In gen- eral, the output bits only depend on the current input and the encoder state. Therefore, we label each transition with the k input bits and the n output bits  input output .  3.1.4 Code Termination  In theory, the code sequences of convolutional codes are of inﬁnite length, but for practical applications we usually employ ﬁnite sequences. Figure 3.7 gives an overview of the three different methods to obtain ﬁnite code sequences that will be discussed below.  Assume we would like to encode exactly L information blocks. The easiest procedure to obtain a ﬁnite code sequence is code truncation. With this method we stop the encoder after L information blocks and also truncate the code sequence after L code blocks. However, this straightforward approach leads to a substantial degradation of the error protection for the last encoded information bits, because the last encoded information bits inﬂuence only a small number of code bits. For instance, the last k information bits determine the last n code bits. Therefore, termination or tail-biting is usually preferred over truncation.  In order to obtain a terminated code sequence, we start encoding in the all-zero encoder state and we ensure that, after the encoding process, all memory elements contain zeros again. In the case of an encoder without feedback this can be done by adding k · m zero bits to the information sequence. Let L denote the number of information blocks, i.e. we  Methods for code termination  There are three different methods to obtain ﬁnite code sequences:    Truncation: We stop encoding after a certain number of bits without any additional effort. This leads to high error probabilities for the last bits in a sequence.    Termination: We add some tail bits to the code sequence in order to ensure a predeﬁned end state of the encoder, which leads to low error probabilities for the last bits in a sequence.    Tail-biting: We choose a starting state that ensures that the starting and  end states are the same. This leads to equal error protection.    Note, tail-biting increases the decoding complexity, and for termination  additional redundancy is required.  Figure 3.7: Methods for code termination   CONVOLUTIONAL CODES encode kL information bits. Adding k · m tail bits decreases the code rate to  105  where L  L + m  is the so-called fractional rate loss. But now the last k information bits have code constraints over n m + 1  code bits. The kL × n L + m  generator matrix now has a ﬁnite structure  Rterminated =  kL  n L + m   = R  L  L + m  ,    G0 G1  G =  0  . . . Gm  0 G0 G1 . . . 0 . . . G0 G1 0  . . . . . . Gm . . . . . . . . . . . . Gm  0 0  0 0  The basic idea of tail-biting is that we start the encoder in the same state in which it will stop after the input of L information blocks. For an encoder without feedback, this means that we ﬁrst encode the last m information blocks in order to determine the starting state of the encoder. Keeping this encoder state, we restart the encoding at the beginning of the information sequence. With this method the last m information blocks inﬂuence the ﬁrst code symbols, which leads to an equal protection of all information bits. The inﬂuence of the last information bits on the ﬁrst code bits is illustrated by the generator matrix of the tail-biting code. On account of the tail-biting, the generator matrix now has a ﬁnite structure. It is a kL × nL matrix      Gm ... G1     G =  G =  0  0 0  . . . Gm  G0 G1 0 G0 G1 . . . 0 . . . G0 G1 0 . . . 0  0 0  . . . . . . Gm . . . . . . . . . . . . Gm ... 0 G0 G1 0 G0 . . .  . . . 0  0 . . . 0 . . . Gm  0     .  11 10 00 11  11 10 . . . 00 00 00 00 . . .  11 00 10 11  00 00  00 11 . . . 11  00 . . .  . . . 00 . . . 00 ... 11 ... 11  10 . . . 00  For instance, the tail-biting generator matrix for the speciﬁc code B 2, 1, 2  is    .      .  From this matrix, the inﬂuence of the last two information bits on the ﬁrst four code bits becomes obvious.  Code termination and tail-biting, as discussed above, can only be applied to convolu- tional encoders without feedback. Such encoders are commonly used when the forward error correction is solely based on the convolutional code. In concatenated systems, as discussed   106  CONVOLUTIONAL CODES  Recursive encoder  u  b 2   b 1   Figure 3.8: Recursive encoder for the code B 2, 1, 2   in Chapter 4, usually recursive encoders, i.e. encoders with feedback, are employed. In the case of recursive encoders, we cannot simply add m zeros to the information sequence. The termination sequence for a recursive encoder depends on the encoder state. However, the termination sequence can be easily determined. Consider, for instance, the encoder in Figure 3.8. To encode the information bits, the switch is in the upper position. The tail bits are then generated by shifting the switch into the down position. This forces the encoder state back to the all-zero state in m transitions at most.  The method of tail-biting for recursive encoders is more complicated, because in this case the start state depends on the complete information sequence. An algorithm to calculate the encoder start state for tail-biting with recursive encoders is presented elsewhere  Weiss et al., 1998 .  3.1.5 Puncturing  In Figure 3.2 we have introduced the general convolutional encoder with k inputs and n outputs. In practical applications we will usually only ﬁnd encoders for rate 1 n convolu- tional codes, i.e. with one input. One reason for this is that there exists a simple method to construct high-rate codes from rate R = 1 n codes. This method is called puncturing, because a punctured code sequence is obtained by periodically deleting a part of the code bits of rate R = 1 n code  cf. Section 2.2.7 . However, the main reason is that the decoding of high-rate convolutional codes can be signiﬁcantly simpliﬁed by using punctured codes. This will be discussed in more detail in Section 3.4.2. Tables of punctured convolutional codes with a large free distance can be found elsewhere  Lee, 1997 .  Consider again the convolutional encoder shown in Figure 3.1. For each encoding step and b 2  . Now let us delete all bits of the sequence 2i+1 from each 4-bit output block. We obtain the  i  the encoder produces two code bits b 1  b 2  for odd times i, i.e. we delete b 2  encoder output b =  b 1   1 , b 1   2 , b 2   1 , b 2   0 , b 1   0 , b 2   i  2 , . . .  ⇒ bpunctured =  b 1   0 , b 2   0 , b 1   1 , b 1   2 , b 2   2 , . . . .   CONVOLUTIONAL CODES  Code puncturing  107    A punctured code sequence is obtained by periodically deleting a part of  the code bits of rate R = 1 n code.    The puncturing pattern is determined by an n × T puncturing matrix P, where the jth row corresponds to the jth encoder output and T is the period of the puncturing pattern. A zero element means that the corresponding  cid:7  code bit will be deleted, a code bit corresponding to a 1 will be submitted.    Example: Puncturing of the code B 2, 1, 2  according to P =   cid:6   1 1 1 0  results in a code Bpunctured 3, 2, 1 .  Figure 3.9: Code puncturing  We can express the periodic deletion pattern by an n × T puncturing matrix, where the jth row corresponds to the jth encoder output and T is the period of the puncturing pattern. For our example we have   cid:7    cid:6   P =  1 1 1 0  ,  where ‘0’ means that the corresponding code bit will be deleted; a code bit corresponding to a ‘1’ will be submitted. The puncturing procedure is summarised in Figure 3.9. Obviously, the original rate R = 1 2 mother code has become a rate R = 2 3 code after punctur- ing according to P. But what is the generator matrix of this code and what does the corresponding encoder look like?  To answer these questions, we consider the semi-inﬁnite generator matrix of the mother    11  00 00  code  00 11 10 . . . This matrix was constructed from the submatrices  10 11 11 10 00 11  G =  00 00  00  00 . . . 00 . . . 11 . . . . . .    .  However, we can also ﬁnd another interpretation of the generator matrix G. Considering the submatrices  G0 =  11 , G1 =  10  and G2 =  11 .  cid:6    cid:6    cid:7    cid:7   =   cid:1  G 0  1 1 1 0 0 0 1 1  and G  =   cid:1  1  1 1 0 0 1 0 1 1  we can interpret the matrix G as the generator matrix of a code with the parameters k = 2, n = 4 and m = 1. Note that other interpretations with k = 3, 4, . . . would also   108  CONVOLUTIONAL CODES  be possible. The generator matrix of the punctured code is obtained by deleting every column that corresponds to a deleted code bit. In our example, this is every fourth column, resulting in    11  cid:7   00 00  00  Gpunctured =   cid:6   1 11 1 10 0 11  0 00  0 1 1 . . .  00 . . . 00 . . . 11 . . . . . .   cid:6      and the submatrices   cid:1  G 0,punctured  =  1 1 1 0 0 1  and G   cid:1  1,punctured  =  1 1 0 1 0 1   cid:7   .  From these two matrices we could deduce the six generator impulse responses of the corres- ponding encoder. However, the construction of the encoder is simpliﬁed when we consider the generator matrix in the D-domain, which we will discuss in the next section.  3.1.6 Generator Matrix in the D-Domain  As mentioned above, an LTI system is completely characterised by its impulse response. However, it is sometimes more convenient to specify an LTI system by its transfer function, in particular if the impulse response is inﬁnite. Moreover, the fact that the input output relation of an LTI system may be written as a convolution in the time domain or as a multiplication in a transformed domain suggests the use of a transformation in the context of convolutional codes. We use the D-transform  x = xi , xi+1, xi+2, . . . ◦−  X D  =  xi Di , j ∈ Z.  +∞ cid:1   i=j  Using the D-transform, the sequences of information and code blocks can be expressed in terms of the delay operator D as follows  u D  = u0 + u1D + u2D 2 + ··· , b D  = b0 + b1D + b2D2 + ··· .  Moreover, inﬁnite impulse responses can be represented by rational transfer functions  Gl,j  D  = Pl,j  D   Ql,j  D   = p0 + p1D + ··· + pmDm 1 + q1D + ··· + qmDm  .  The encoding process that is described as a convolution in the time domain can be expressed by a simple multiplication in the D-domain  b D  = U D G D ,  where G D  is the encoding  or generator  matrix of the convolutional code. In general, a generator matrix G D  is a k × n matrix. The elements Gl,j  D  of this matrix are realisable rational functions. The term realisable reﬂects the fact that a linear sequential circuit always   CONVOLUTIONAL CODES  109  has a causal impulse response. Hence, we can only realise transfer functions that result in a causal impulse response. For the encoder in Figure 3.1 we obtain  G D  =  1 + D + D  2  , 1 + D  2   .  The generator matrix of the punctured convolutional code from Section 3.1.5 is   cid:6  Gpunctured D  = G  cid:1  0,punctured  cid:6    cid:7  + DG  cid:1  1,punctured + D 1 1 1 0 0 1 1 + D 1 + D   cid:6   =  1  1 1 0 1 0 1   cid:7   =   cid:7   D  0  1 + D  .  Considering the general controller canonical form of a shift register in Figure 3.4, it is now easy to construct the encoder of the punctured code as given in Figure 3.10. Usually an octal notation is used to specify the generator polynomials of a generator matrix. For example, the polynomial g D  = 1 + D + D2 + D4 can be represented by a binary vector g =  11101 , where the elements are the binary coefﬁcients of the polynomial  Encoder of a punctured code    Puncturing of the code B 2, 1, 2  according to P =  code Bpunctured 3, 2, 1  with the following encoder  1 1  1 0  results in a   cid:6    cid:7   b 1   b 2   b 3   u 1   u 2   Figure 3.10: Encoder of the punctured code Bpunctured 3, 2, 1    110  CONVOLUTIONAL CODES  g D  with the coefﬁcient of the lowest order in the leftmost position. To convert this vector to octal numbers, we use right justiﬁcation, i.e. g =  11101  becomes g =  011101 . Now, the binary vector g =  011101  is equivalent to  35 8 in octal notation. For example, the generator matrix of our canonical example code B 2, 1, 2  can be stated as  7 5 8, whereas the generator matrix  35 23 8 deﬁnes the code B 2, 1, 4 .  3.1.7 Encoder Properties  We have already mentioned that a particular encoder is just one possible realisation of a generator matrix. Moreover, there exist a number of different generator matrices that produce the same set of output sequences. Therefore, it is important to distinguish between properties of encoders, generator matrices and codes.  A convolutional code is the set of all possible output sequences of a convolutional encoder. Two encoders are called equivalent encoders if they encode the same code. Two encoding matrices G D  and G cid:1   D  are called equivalent generator matrices if they encode the same code. For equivalent generator matrices G D  and G cid:1    D  we have   cid:1  G   D  = T D G D ,  with T D  a non-singular k × k matrix.  We call G D  a polynomial generator matrix if Ql,j  D  = 1 for all submatrices Gl,j  D . Some polynomial generator matrices lead to an undesired mapping from information se- quence to code sequence. That is, an information sequence containing many  possibly inﬁnite  1s is mapped to a code sequence with only a few  ﬁnite  number of 1s. As a consequence, a small number of transmission errors can lead to a large  possibly inﬁnite  number of errors in the estimated information sequence. Such a generator matrix is called a catastrophic generator matrix. Note that the catastrophic behaviour is not a property of the code but results from the mapping of information sequence to code sequence. Hence, it is a property of the generator matrix. Methods to test whether a generator matrix is catastrophic can be found elsewhere  Bossert, 1999; Johannesson and Zigangirov, 1999 . The state dia- gram of the encoder provides a rather obvious condition for a catastrophic mapping. If there exists a loop  a sequence of state transitions  in the state diagram that produces zero output for a non-zero input, this is a clear indication of catastrophic mapping. For example, the generator matrix G D  =  1 + D, 1 + D2  is catastrophic. The corres- ponding encoder and the state diagram are depicted in Figure 3.11, where the critical loop in the state diagram is indicated by a dashed line. The generator matrix results from  G D  = T  D G  cid:1    D   =  1 + D  1, 1 + D  =  1 + D, 1 + D2 .  Note that in our example all elements of G D  have a common factor  1 + D . In general, a generator matrix G D  of a rate 1 n code is catastrophic if and only if all elements of G D  have a common factor other than D.   CONVOLUTIONAL CODES  111  Catastrophic encoder with state diagram    Catastrophic encoder:    State diagram with zero loop for non-zero input:  b 1   b 2   1   00  11  0   01  1   01  0   11  01  u  1   10  10  1   11  00  0   10  0   00  Figure 3.11: Encoder and state diagram corresponding to the catastrophic generator  matrix G D  =  1 + D, 1 + D2   A systematic encoding results in a mapping where the code sequence contains the unchanged information sequence. We call G D  a systematic generator matrix if it satisﬁes  G D  =     1  1  . . .  =  IkA D  .     G1,k+1 D   . . . G1,n D   ...  ...  1 Gk,k+1 D   . . . Gk,n D    112 CONVOLUTIONAL CODES That is, the systematic generator matrix contains an k × k identity matrix Ik. In general, the k unit vectors may be arbitrarily distributed over the n columns of the generator matrix. Systematic generator matrices will be of special interest in Chapter 4, because they are used to construct powerful concatenated codes. A systematic generator matrix is never catastrophic, because a non-zero input leads automatically to a non-zero encoder output. Every convolutional code has a systematic generator matrix. The elements of a systematic generator matrix will in general be rational functions. Codes with polynomial system- atic generator matrices usually have poor distance- and error-correcting capabilities. For instance, for the polynomial generator matrix G D  =  1 + D + D2, 1 + D2  we can write the two equivalent systematic generator matrices as follows  1 + D2   cid:1  G   D  =  1,  1 + D + D2 The encoder for the generator matrix G cid:1  cid:1  Figure 3.1 and Figure 3.8 encode the same code B 2, 1, 2 .  and G   cid:1  cid:1    D  =  1 + D + D2  1 + D2  , 1  .   D  is depicted in Figure 3.8, i.e. the encoders in   cid:6    cid:7    cid:6    cid:7   3.2 Trellis Diagram and the Viterbi Algorithm  Up to now we have considered different methods to encode convolutional codes. In this section we discuss how convolutional codes can be used to correct transmission errors. We consider possibly the most popular decoding procedure, the Viterbi algorithm, which is based on a graphical representation of the convolutional code, the trellis diagram. The Viterbi algorithm is applied for decoding convolutional codes in Code Division Multiple Access  CDMA   e.g. IS-95 and UMTS  and GSM digital cellular systems, dial-up modems, satellite communications  e.g. Digital Video Broadcast  DVB  and 802.11 wireless LANs. It is also commonly used in other applications such as speech recognition, keyword spotting and bioinformatics.  In coding theory, the method of MAP decoding is usually considered to be the opti- mal decoding strategy for forward error correction. The MAP decision rule can also be used either to obtain an estimate of the transmitted code sequence on the whole or to perform bitwise decisions for the corresponding information bits. The decoding is based on the received sequence and an a-priori distribution over the information bits. Figure 3.12 provides an overview over different decoding strategies for convolutional codes. All four formulae deﬁne decision rules. The ﬁrst rule considers MAP sequence estimation, i.e. we are looking for the code sequence ˆb that maximizes the a-posteriori probability Pr{br}. MAP decoding is closely related to the method of ML decoding. The difference is that MAP decoding exploits an a-priori distribution over the information bits, and with ML decoding we assume equally likely information symbols. Considering Bayes law Pr{r}Pr{br} = Pr{rb}Pr{b}, we can formulate the MAP rule as  ˆb = argmax  {Pr{br}} = argmax  b  b  Pr{rb}Pr{b}  Pr{r}  %  &  .  The constant factor Pr{r} is the same for all code sequences. Hence, it can be neglected and we obtain  ˆb = argmax  {Pr{rb}Pr{b}} .  b   CONVOLUTIONAL CODES  Decoding strategies    Maximum A-Posteriori  MAP  sequence estimation:  ˆb = argmax  {Pr{br}}    Maximum Likelihood  ML  sequence estimation: {Pr{rb}}  ˆb = argmax    Symbol-by-symbol MAP decoding: ˆut = argmax  {Pr{utr}}  ∀ t    Symbol-by-symbol ML decoding: ˆut = argmax  {Pr{rut}}  ∀ t  113   3.1    3.2    3.3    3.4   Figure 3.12: Decoding rules for convolutional codes  The term Pr{b} is the a-priori probability of the code sequence b. If all information sequences and therefore all code sequences are equally likely, then Pr{b} is also a constant. In this case we can neglect Pr{b} in the maximisation and have  ˆb = argmax  {Pr{rb}} ,  the so-called ML sequence estimation rule. Hence, ML decoding is equivalent to MAP decoding if no a-priori information is available. The Viterbi algorithm ﬁnds the best sequence ˆb according to the ML sequence estimation rule, i.e. it selects the code sequence ˆb that maximizes the conditional probability Pr{rb}  ˆb = argmax  {Pr{rb}} .  Therefore, implementations of the Viterbi algorithm are also called maximum likelihood decoders. In Section 3.5 we consider the BCJR algorithm that performs symbol-by-symbol decoding.  3.2.1 Minimum Distance Decoding  In Section 2.1.2 we have already considered the concept of maximum likelihood decoding and observed that for the Binary Symmetric Channel  BSC  we can actually apply minimum  b  b  ut  ut  b  b   114  CONVOLUTIONAL CODES  distance decoding in order to minimise the word error probability. To simplify the following discussion, we also consider transmission over the BSC. In later sections we will extend these considerations to channels with continuous output alphabets. It is convenient to revise some results concerning minimum distance decoding. Remem- ber that the binary symmetric channel has the input and output alphabet F2 = {0, 1}. The BSC complements the input symbol with probability ε < 1 2 . When an error occurs, a 0 is received as a 1, and vice versa. The received symbols do not reveal where the errors have occurred.  Let us ﬁrst consider a binary block code B of length n and rate R = k  n . Let r denote the received sequence. In general, if B is used for transmission over a noisy channel and all code words b ∈ B are a priori equally likely, then the optimum decoder must compute the 2k probabilities Pr{rb}. In order to minimise the probability of a decoding error, the decoder selects the code word corresponding to the greatest value of Pr{rb}. Pr{rb} = Pr{r − b}. Furthermore, the error process is memoryless which yields  For the BSC, the error process is independent of the channel input, thus we have  Pr{rb} = Pr{r − b} = n cid:20   Pr{ribi}.  i=1  Note that we have Pr{ribi} = ε if ri and bi differ. Similarly, we have Pr{ribi} = 1 − ε for ri = bi. Now, consider the Hamming distance dist r, b  between the received sequence r and a code word b ∈ B, i.e. the number of symbols where the two sequences differ. Using the Hamming distance, we can express the likelihood Pr{rb} as  Pr{rb} = n cid:20   i=1  Pr{ribi} = εdist r,b  1 − ε n−dist r,b ,  because we have dist r, b  bit positions where r and b differ and n − dist r, b  positions where the two sequences match. Hence, for the likelihood function we obtain   cid:2    cid:3 dist r,b  Pr{rb} = εdist r,b  1 − ε n−dist r,b   cid:3   =  1 − ε n  1−ε  .  ε   cid:2    cid:2    cid:3   ε  ε  1−ε  satisﬁes  The term  1 − ε n is a constant and is therefore independent of r and b. For ε < 0.5, the < 1. Thus, the maximum of Pr{rb} is attained for the code term word b that minimizes the distance dist r, b . Consequently, the problem of ﬁnding the most likely code word can be solved for the BSC by minimum distance decoding. A minimum distance decoder selects the code word ˆb that minimizes the Hamming distance to the received sequence  1−ε  ˆb = argmin b∈B  dist r, b .  The derivation of the minimum distance decoding rule is summarized in Figure 3.13. Viterbi‘s celebrated algorithm is an efﬁcient implementation of minimum distance decoding. This algorithm is based on the trellis representation of the code, which we will discuss next.   CONVOLUTIONAL CODES  115  Minimum distance decoding    In order to minimise the probability of a decoding error, the maximum likelihood decoder selects the code word corresponding to the greatest value of Pr{rb}.    The error process of the BSC is memoryless and independent of the  channel input  Pr{rb} = Pr{r − b} = n cid:20   i=1    We obtain the likelihood function  Pr{ribi} = εdist r,b  1 − ε n−dist r,b .   cid:6    cid:7 dist r,b   .  Pr{rb} =  1 − ε n  ε  1 − ε    For ε < 0.5, the maximum of Pr{rb} is attained for the code word b that  minimizes the distance dist r,b .    A minimum distance decoder selects the code word that minimizes the  Hamming distance to the received sequence  ˆb = argmin b∈B  dist r, b  = argmax  Pr{rb}  b  Figure 3.13: Minimum distance decoding  It is particularly efﬁcient for convolutional codes, because the trellises of these codes have a very regular structure.  3.2.2 Trellises  In Section 3.1.3 we have seen that the encoder of a convolutional code can be considered as a ﬁnite-state machine that is characterized by a state diagram. Alternatively, a convolutional encoder may be represented by a trellis diagram. The concept of the trellis diagram was invented by Forney  Forney, Jr, 1973b, 1974 . The trellis for the encoder in Figure 3.1 is depicted in Figure 3.14. A trellis is a directed graph, where the nodes represent encoder states. In contrast to the state diagram, the trellis has a time axis, i.e. the ith level of the trellis corresponds to all possible encoder states at time i. The trellis of a convolutional code has a very regular structure. It can be constructed from the trellis module, which is simply a state diagram, where the states σi at each time i and σi+1 at time i + 1 are   116  0 00  1 11 0 11  1 00 0 10  1 01 0 01  1 10 σj,i  Trellis for the encoder in Figure 3.1  CONVOLUTIONAL CODES  trellis module  σ0 = 00 σ1 = 01 σ2 = 10 σ3 = 11  0 00  1 11  0 00  1 11  0 10  1 01   cid:1   ,i+1  σj  trellis  0 00  1 11 0 11  1 00 0 10  1 01 0 01  1 10  0 00  1 11 0 11  1 00 0 10  1 01 0 01  1 10  0 00  1 11 0 11  1 00  0 10  1 01 0 01  1 10  0 00  0 00  0 11  0 11  0 10  0 01    The trellis module is the basic building block of the trellis. It is a state diagram, where the states σi at each time i and σi+1 at time i + 1 are depicted separately.    The transitions leaving a state are labelled with the corresponding input  and output bits  input output .  Figure 3.14: Trellis for the encoder in Figure 3.1  depicted separately. The transitions leaving a state are labelled with the corresponding input and output bits  input output .  As we usually assume that the encoder starts in the all-zero state, the trellis also starts in the zero state. Hence, the ﬁrst m levels differ from the trellis module. There are only initial state transitions that depart from the all-zero state. In Figure 3.14 we consider a terminated convolutional code with ﬁve information bits and two bits for termination. Therefore, the ﬁnal encoder state is always the all-zero state. All code words begin and end with the encoder state σ = 00. Moreover, each non-zero code word corresponds to a sequence of state transitions that depart from the all-zero state some number of times and return to the all-zero state. The trellis is a representation of the terminated convolutional codes, i.e. there is a one-to-one correspondence between the set of all paths through the trellis and the set of code sequences, i.e. the code B. Each path from the initial all-zero state to the ﬁnal all-zero state corresponds to a code word, and vice versa. Moreover, there is a one-to-one correspondence between the set of all paths through the trellis and the set of information sequences. Note that this one-to-one correspondence exists only if the encoder is non-catastrophic.  3.2.3 Viterbi Algorithm  We have seen in Section 3.2.1 that the problem of maximum likelihood decoding for the BSC may be reduced to minimum distance decoding. In 1967, Viterbi introduced an   CONVOLUTIONAL CODES  117  efﬁcient algorithm to ﬁnd the code word with minimum distance to the received sequence in a trellis  Viterbi, 1967 . This algorithm is currently the most popular decoding procedure for convolutional codes. The Viterbi algorithm is also applied to numerous other applications such as speech recognition and equalization for transmission over channels with memory. We will, however, concentrate on Viterbi decoding.  A brute-force approach to minimum distance decoding would be to calculate all the distances dist r, b  for all possible code words. The efﬁciency of Viterbi’s procedure comes from the fact that we can eliminate many code words from the list of possible candidates without calculating all the distances. Therefore, we will not immediately consider complete code sequences. Instead, we pass the trellis from the initial node to the terminating node and thereby calculate distances for partial code sequences.  The Viterbi algorithm is a dynamic programming algorithm. In computer science, dynamic programming is a method for reducing the runtime of algorithms exhibiting the properties of optimal substructures, which means that optimal solutions of subproblems can be used to ﬁnd the optimal solution of the overall problem. The subproblems of the Viterbi algorithm are ﬁnding the most likely path from the starting node to each node in the trellis. The optimal solution for the nodes of level i + 1 is, however, simple if we know the solutions for all nodes of level i. Therefore, the Viterbi algorithm traverses the trellis from left to right, ﬁnding the overall solution, the maximum likelihood estimate, when the terminating node is reached.  In this section we only consider terminated convolutional codes with L information blocks, i.e. kL information bits, and m blocks for termination. Let us ﬁrst consider an example of Viterbi decoding. Assume that we use the terminated convolutional code with the trellis depicted in Figure 3.14 for transmission over the BSC. We have sent the code word b =  11 01 01 00 01 01 11  and received the sequence r =  10 01 01 10 01 01 01 . How should we decode this sequence in order to correct possible transmission errors and deter- mine the transmitted code sequence? The Viterbi algorithm traverses the trellis from left to right and searches for the optimum path that is closest to the received sequence r, with the term closest referring to the smallest Hamming distance.  The decoding process for the ﬁrst three trellis levels is given in Figure 3.15. At the ﬁrst stage we only have two code segments 00 and 11; both segments differ from the received block 10 in one position. In the process of the Viterbi algorithm we store the corresponding distance values as the so-called node metric  cid:13  σj,i , where σj,i denotes the jth state at time i. We initialise the ﬁrst node σ0,0 with the metric  cid:13  σ0,0  = 0. For all other nodes the node metric is calculated from the node metric of the predecessor node and the distance of the currently processed code block. Hence, for the ﬁrst transition we have  cid:13  σ0,1  =  cid:13  σ0,0  + 1 = 1 and  cid:13  σ2,1  =  cid:13  σ0,0  + 1 = 1. At the second stage we consider the received block 01 and obtain the node metrics  cid:13  σ0,2  =  cid:13  σ0,1  + 1 = 2,  cid:13  σ1,2  =  cid:13  σ2,1  + 2 = 3,  cid:13  σ2,2  =  cid:13  σ0,1  + 1 = 2,  cid:13  σ3,2  =  cid:13  σ2,1  + 0 = 1.  At the third stage the situation changes. Now, two transitions arrive at each state. As we are interested in the path closest to the received sequence, we take the minimum of the   118  CONVOLUTIONAL CODES  Example of the Viterbi algorithm – forward pass  1     Received sequence r =  10 01 01 10 01 01 01 .   For the ﬁrst m = 2 transitions, the state metrics are the accumulated branch  metrics.  Level 1  0  0 00  1  Level 2  0  1 11  1  1  1  1  0 00  1 11  1  1  2  0 10 1  1 01  0  2  3  2  1  Figure 3.15: Example of the Viterbi algorithm – forward pass  1   metric values  distances    cid:4   cid:4   cid:4   cid:4    cid:13  σ0,3  = min  cid:13  σ1,3  = min  cid:13  σ2,3  = min  cid:13  σ3,3  = min   cid:13  σ0,2  + 1,  cid:13  σ1,2  + 1  cid:13  σ2,2  + 2,  cid:13  σ3,2  + 0  cid:13  σ0,2  + 1,  cid:13  σ1,2  + 1  cid:13  σ2,2  + 0,  cid:13  σ3,2  + 2   cid:5  = min{3, 4} = 3,  cid:5  = min{4, 1} = 1,  cid:5  = min{3, 4} = 3,  cid:5  = min{2, 3} = 2.  Obviously, the path with the larger metric value has a larger distance to the received sequence. This path can now be eliminated, because it cannot be part of the optimum code sequence. The decoder discards all transitions that do not correspond to the minimum metric value. The one remaining path for each state is called the survivor. It has to be stored. In Figure 3.16 the survivors are labelled by arrows. Note that the node metric is the smallest distance of any partial code sequence that starts at the initial node and ends at this particular node.  The above procedure is now continued until the terminating node of the trellis is reached. The resulting trellis is depicted in Figure 3.17. At the terminating node we will also have a single survivor. This ﬁnal survivor determines the best path through the trellis. The corresponding code word can be found by passing backwards through the trellis. Now, we traverse the trellis from right to left, following the stored survivors. In our example we   CONVOLUTIONAL CODES  119  Example of the Viterbi algorithm – forward pass  2     For each node, calculate the metric of the paths that enter the node  i  i + 1  dist ri , bi    dist ri , b cid:1  i     cid:13  σj,i+1    cid:13  σl,i     cid:1    cid:13  σl  ,i     cid:4    cid:13  σj,i+1  = min   cid:13  σl,i   + dist ri , bi  ,  cid:13  σl   Select the path with the smallest metric  survivor .   cid:1   ,i   + dist ri , b  cid:1  i    .    The third transition  survivors are labelled by arrows :   cid:5   Level 3  2  3  2  1  1 1 1 1 2 0  0  2  3  1  2  3  Figure 3.16: Example of the Viterbi algorithm – forward pass  2   obtain the estimated code word ˆb =  11 01 01 00 01 01 11 . From the node metric of the ﬁnal node  cid:13  σ0,7  = 3 we conclude that the distance to the received sequence is 3. Hence, we have corrected three errors.  A general description of the Viterbi algorithm is summarised in Figure 3.18. We will consider a version for channels with continuous alphabets and some implementation issues later in Section 3.4. Step 2 deﬁnes the so-called add–compare–select recursion of the Viterbi algorithm. Owing to the highly repetitive structure of the trellis, this recursion can be efﬁciently implemented in hardware. Further implementation issues will be discussed in Section 3.4, too.   120  CONVOLUTIONAL CODES  Example of the Viterbi algorithm – backward pass    Starting from the terminating node and going back to the initial node using the labelled survivors, we obtain the estimated code word ˆb =  11 01 01 00 01 01 11 .  1  1  1  1  1  1 2  0  1 1  2  1 3  2  1 2 0  0 1  2  3  1  3  2  1 1 1 1 0 2 2  0  2  3  2  2  1  1  2 0 0  1 1  2  1  1  2  0  3  2  3  2  3  3  2  1  1  Figure 3.17: Example of the Viterbi algorithm – backward pass  Viterbi algorithm  Step 1: Assign metric zero to the initial node  cid:13  σ0,0  = 0 and set time i = 1. Step 2: Each node σj,i of time instant i is processed as follows  forward pass : ,i−1 → σj,i that enter  cid:1  ,i−1  is the node ,i−1 and b cid:1  is the code  the node σj,i by  cid:13  σj metric of this branch’s predecessor node σj block corresponding to the branch σj  a. ADD: Calculate the metrics of all branches σj  ,i−1  + dist ri , b cid:1   cid:1   i  , where  cid:13  σj ,i−1 → σj,i.   cid:1    cid:1    cid:1   i  b. COMPARE: Assign the smallest branch metric among all branches  merging at node σj,i as the node metric  cid:13  σj,i  .  c. SELECT: Store the branch corresponding to the smallest branch  metric as the survivor.  Step 3: If i ≤ L + m, then continue with the next level, i.e. increment i by 1 and  go to step 2. Otherwise go to step 4.  Step 4: To ﬁnd the best path, we have to start from the terminating node σ0,L+m  and go to the initial node σ0,0, following the survivors  backward pass .  Figure 3.18: Viterbi algorithm   121  CONVOLUTIONAL CODES 3.3 Distance Properties and Error Bounds In this section we consider some performance measures for convolutional codes. The anal- ysis of the decoding performance of convolutional codes is based on the notion of an error event. Consider, for example, a decoding error with the Viterbi algorithm, i.e. the transmit- ted code sequence is b and the estimated code sequence is b cid:1   cid:7 = b. Typically, these code sequences will match for long periods of time but will differ for some code sequence seg- ments. An error event is a code sequence segment where the transmitted and the estimated code sequences differ. It is convenient to deﬁne an error event as a path through the trellis. Both sequences b and b cid:1  are represented by unique paths through the trellis. An error event is a code segment  path  that begins when the path for b cid:1  diverges from the path b, and ends when these two paths merge again. Now, remember that a convolutional code is a linear code. Hence, the error event b − b cid:1  is a code sequence. It is zero for all times where b coincides with b cid:1  . Therefore, we may deﬁne an error event as a code sequence segment that diverges from the all-zero code sequence and merges with the all-zero sequence at some later time. Furthermore, an error event can only start at times when b coincides with b cid:1  . That is, at times when the decoder is in the correct state. For the BSC, transmission errors occur statistically independently. Consequently, different error events are also statistically independent.  3.3.1 Free Distance  We would like to determine the minimum number of channel errors that could lead to a decoding error. Generally, we would have to consider all possible pairs of code sequences. However, with the notion of an error event, we can restrict ourselves to possible error events. Let the received sequence be r = b + e, i.e. e is the error sequence. With minimum distance decoding an error only occurs if  dist r, b  = wt r − b  ≥ dist r, b  cid:1     = wt r − b  cid:1      or  Therefore, the error event b − b cid:1   wt e  ≥ wt b − b   cid:1  + e  ≥ wt b − b  cid:1     − wt e . can only occur if wt e  ≥ wt b − b cid:1     2.  Remember that the error event b − b cid:1   is a code sequence. By analogy with the minimum Hamming distance of linear binary block codes, we deﬁne the free distance of a linear binary convolutional code  dfree =   cid:1  b,b cid:1 ∈B,b cid:7 =b cid:1  dist b, b  min       = wt b − b cid:1   as the minimum Hamming distance between two code sequences. Owing to the linearity, we have dist b, b cid:1  is a code sequence. We assume without loss of generality that an error event is a path that diverges from and remerges with the all- zero sequence. Therefore, we can determine the free distance by considering the Hamming weights of all non-zero code sequences   , where b − b cid:1   dfree = min b∈B,b cid:7 =0  wt b .   122  Free distance  CONVOLUTIONAL CODES    By analogy with the minimum Hamming distance of linear binary block  codes, we deﬁne the free distance of a linear binary convolutional code  dfree =   cid:1  b,b cid:1 ∈B,b cid:7 =b cid:1  dist b, b  min    = min b∈B,b cid:7 =0  wt b    3.5   as the minimum Hamming distance between two code sequences.    A convolutional code can correct all error patterns with weight up to  e =  cid:13  dfree − 1   cid:14 .  2  Figure 3.19: Free distance  e =  cid:13  dfree − 1   cid:14 .  2    The free distance is a code property. Convolutional codes with the best known free distance for given rate and memory are called optimum free distance codes  OFDs .  Consequently, the minimum number of transmission errors that could lead to a decoding error is e + 1 with  The deﬁnition of the free distance and our ﬁrst result on the error-correcting capabilities of convolutional codes are summarized in Figure 3.19. It is easy to verify that the code B 2, 1, 2  represented by the trellis in Figure 3.14 has free distance 5 and can correct all error patterns with weight up to 2.  The free distance of a convolutional code is an important performance measure. Con- volutional codes with the best known free distance for given rate and memory are called optimum free distance codes  OFDs . Tables of OFD codes for different rates can be found elsewhere  Lee, 1997 .  2  3.3.2 Active Distances A convolutional code with Viterbi decoding will in general correct much more than e =  cid:14  errors. In Section 3.2.3 we have seen that the code B 2, 1, 2  with free distance  cid:13  dfree−1 dfree = 5 corrected three errors. Actually, this code could correct hundreds of errors if larger code sequences were considered. With block codes, we observed that e + 1 channel errors  cid:14  could lead to a decoding error. In principle, these e + 1 errors could be with e =  cid:13  d−1 arbitrarily distributed over the n code symbols. With convolutional codes, e + 1 channel errors could also lead to an error event. Yet these e + 1 errors have to occur in close proximity, or, in other words, an error event caused by e + 1 channel errors will be rather limited in length. In this section we consider a distance measure that takes the length of error events into account. We will see that the error correction capability of a convolutional  2   CONVOLUTIONAL CODES  123  code increases with growing error event length. The rate of growth, the so-called slope α, will be an important measure when we consider the error correction capability of a concatenated convolutional code in Chapter 4.  We deﬁne the correct path trough a trellis to be the path determined by the encoded information sequence. This information sequence also determines the code sequence as well as a sequence of encoder states. The active distance measures are deﬁned as the minimal weight of a set of code sequence segments b[i1,i2] = bi1bi1+1 ··· bi2 which is given by a set of encoder state sequences according to Figure 3.20  H¨ost et al., 1999 .1 The set S0,0 [i1,i2] formally deﬁnes the set of state sequences that correspond to error events as discussed at the beginning of this section. That is, we deﬁne an error event as a code segment starting in a correct state σi1 and terminating in a correct state σi2+1. As the code is linear and an error event is a code sequence segment, we can assume without loss of generality that the correct state is the all-zero state. The error event differs at some, but not necessarily  Active burst distance    Let Sσs,σe  [i1,i2] denote the set of encoder state sequences σ[i1,i2] = σi1 σi1+1 ··· σi2 ∈ σs and terminate at depth i2 in ∈ σe and do not have all-zero state transitions along with  that start at depth i1 in some state σi1 some state σi2 all-zero information block weight in between: ∈ σs, σi2  = {σ[i1,i2] : σi1  Sσs,σe [i1,i2]  ∈ σe and not σi = 0, σi+1 = 0 with ui = 0, i1 ≤ i ≤ i2 − 1}   3.6   where σs and σe denote the sets of possible starting and ending states.    The jth-order active burst distance is def= min S0,0 [0,j+1]  ab j   cid:4    cid:5   wt b[0,j]   where j ≥ νmin.  channel error pattern e[i1,i2] has weight e ≥ ab i2−i1 2  .  Figure 3.20: Active burst distance    An error event b[i1,i2] can only occur with minimum distance decoding if the   3.7   1This deﬁnition of state sequences was presented elsewhere  Jordan et al., 1999 . It differs slightly from the original deﬁnition  H¨ost et al., 1999 . Here, all-zero to all-zero state transitions that are not generated by all-zero information blocks are included in order to consider partial  unit  memory codes.   124  CONVOLUTIONAL CODES  at all, states within the interval [i1, i2] from the correct path. The active burst distance ab as deﬁned in Figure 3.20 is the minimum weight of an error event of j + 1 code blocks. Therefore, we have  j  dfree = min {a   j  }.  b  j  Let e[i1,i2] denote an error pattern and e its weight, i.e. e[i1,i2] is a binary sequence with e 1s distributed over the interval [i1, i2]. According to the discussion in Section 3.3.1, a decoding error with minimum distance decoding could only occur if e = wt e[i1,i2]  ≥ wt b  2 for some code sequence b. Consequently, a minimum distance decoder of a convolutional code B can only output an error event b[i1,i2] if the corresponding error pattern e[i1,i2] has weight  e ≥ ab i2−i1 2  .  There exist some more active distance measures that are deﬁned in Figure 3.21. Those distances will be required in Chapter 4. They differ from the burst distance in the deﬁnition of the sets of starting and ending states. Generally, all active distances can be lower bounded  Other active distances    The jth-order active column distance is  ac j  def= min 0,σ [0,j+1]  S  wt b[0,j]   rc j  a  def= min Sσ,0 [0,j+1]  wt b[0,j]    cid:4    cid:4    cid:4    cid:5    cid:5    cid:5   where σ denotes any encoder state.    The jth-order active reverse column distance is  where σ denotes any encoder state.    The jth-order active segment distance is  s a j  def= min σ1 ,σ2 [m,m+j+1]  S  wt b[0,j]   where σ1 and σ2 denote any encoder state.  Figure 3.21: Other active distances   3.8    3.9    3.10    CONVOLUTIONAL CODES  125  by linear functions with the same slope α. Therefore, we can write  c  ≥ α · j + βb, ≥ α · j + β ≥ α · j + β ≥ α · j + βs,  rc  ,  ,  ab j c a j rc a j as j  where βb, βc, βrc and βs are rational constants. For example, the code B 2, 1, 2  has free distance dfree = 5. With the encoder  7 5 8, we obtain the parameters α = 1 2, β b = 9 2, βc = 3 2, βrc = 3 2 and βs = −1 2. With this code, three channel errors could lead to an error event. However, this error event would be limited to length j = 3, i.e. n j + 1  = 8 code bits. An error event of length j = 11 with 24 code bits would require at least  e ≥ ab j 2  ≥ αj + βb  = 11 · 1  + 9  2  = 5  2 2  2  channel errors.  From these deﬁnitions it follows that the active distances are encoder properties, not code properties.2 As a catastrophic encoder leads to a zero-loop in the state diagram, it is easy to see that a catastrophic encoding would lead to the slope α = 0.  Both parameters, α and dfree, characterise the distance properties of convolutional codes. The free distance determines the code performance for low channel error rates, whereas codes with good active distances, and high slope α, achieve better results for very high channel error rates. The latter case is important for code concatenation which we will consider in Chapter 4. However, convolutional codes with large slopes also yield better tail-biting codes  Bocharova et al., 2002 . A table with the free distances and slopes of the active distances for rate R = 1 2 OFD convolutional codes is given in Figure 3.22. We  Table of code parameters    The following table presents the free distances and slopes of the active dis- tances for rate R = 1 2 OFD convolutional codes with different memories. Memory Polynomial  Free distance dfree Slope α  2 3 4 5 6   7 5   17 15   35 23   75 53   171 133   5 6 7 8 10  0.5 0.5 0.36 0.33 0.31  Figure 3.22: Table of code parameters  2In fact the active distances are invariant over the set of minimal encoding matrices. For the deﬁnition of a  minimal encoding matrix, see elsewhere  Bossert, 1999; Johannesson and Zigangirov, 1999 .   126  CONVOLUTIONAL CODES  notice that with increasing memory the free distance improves while the slope α decreases. Tables of codes with good slope can be found elsewhere  Jordan et al., 2004b, 2000 .  3.3.3 Weight Enumerators for Terminated Codes  Up to now we have only considered code sequences or code sequence segments of minimum weight. However, the decoding performance with maximum likelihood decoding is also inﬂuenced by code sequences with higher weight and other code parameters such as the number of code paths with minimum weight. The complete error correction performance of a block code is determined by its weight distribution. Consider, for example, the weight distribution of the Hamming code B 7, 4 . This code has a total of 2k = 16 code words: the all-zero code word, seven code words of weight d = 3, seven code words of weight 4 and one code word of weight 7. In order to write this weight distribution in a compact way, we specify it as a polynomial of the dummy variable W , i.e. we deﬁne the Weight Enumerating Function  WEF  of a block code B n, k  as  AWEF W   = n cid:1   awW w,  w=0  where aw is the number of code words of weight w in B. The numbers a0, . . . , an are the weight distribution of B. For the Hamming code B 7, 4  we have  AWEF W   = 1 + 7W 3 + 7W 4 + W 7.  With convolutional codes we have basically code sequences of inﬁnite length. Therefore, we cannot directly state numbers of code words of a particular weight. To solve this problem, we will introduce the concept of path enumerators in the next section. Now, we discuss a method to evaluate weight distributions of terminated convolutional codes which are in fact block codes. Such weight enumerators are, for instance, required to calculate the expected weight distributions of code concatenations with convolutional component codes, as will be discussed in Chapter 4.  Consider the trellis of the convolutional code B 2, 1, 2  as given in Figure 3.14, where we labelled the branches with input and output bits corresponding to a particular state transition. Similarly, we can label the trellis with weight enumerators as in Figure 3.23, i.e. every branch is labelled by a variable W w with w being the number of non-zero code bits corresponding to this state transition. Using such a labelled trellis, we can now employ the forward pass of the Viterbi algorithm to compute the WEF of a terminated convolutional  W   for each node code. Instead of a branch metric, we compute a weight enumerator A  W   denotes the enumerator for state σj at level i. Initialising the in the trellis, where A 0  W   = 1, we could now traverse the trellis from left enumerator of the ﬁrst node with A 0  to right, iteratively computing the WEF for all other nodes. In each step of the forward i+1 W   at level i + 1 on the basis of the enumerators pass, we compute the enumerators A of level i and the labels of the corresponding transitions as indicated in Figure 3.23. That is, we multiply the enumerators of level i with the corresponding transition label and sum over all products corresponding to paths entering the same node. The enumerator A 0  L+m W   of the ﬁnal node is equal to the desired overall WEF.   j   i   j   i   j     CONVOLUTIONAL CODES  127  Calculating the weight enumerator function  i  i + 1  A 0   i  A 1   i  W 2  1 A 2  i+1  = W · A 0   i  + A 1   i  σ0 = 00 σ1 = 01 σ2 = 10 σ3 = 11  1  W  W 2  W W 2  1 W  W    A trellis module of the  7 5 8 convolutional code. Each branch is labelled  with a branch enumerator.    We deﬁne a 2ν × 2ν transition matrix T. The coefﬁcient τl,j of T is the weight enumerator of the transition from state σl to state σj , where we set τl,j = 0 for impossible transitions.    The weight enumerator function is evaluated iteratively  T  A0 W   =  1 0 ··· 0  Ai+1 W   = T · Ai  W  , AWEF W   =  1 0 ··· 0  · AL+m W  .  ,  Figure 3.23: Calculating the weight enumerator function  Using a computer algebra system, it is usually more convenient to represent the trellis module by a transition matrix T and to calculate the WEF with matrix operations. The trellis of a convolutional encoder with overall constraint length ν has the physical state space S = {σ0, . . . , σ2ν−1}. Therefore, we deﬁne a 2ν × 2ν transition matrix T so that the coefﬁcients τl,j of T are the weight enumerators of the transition from state σl to state σj , where we set τl,j = 0 for impossible transitions. The weight enumerators of level i can now be represented by a vector  with the special case  Ai  W   =  A 0   i A 1   i  ··· A 2ν−1   T     i  A0 W   =  1 0 ··· 0 T   128  CONVOLUTIONAL CODES  for the starting node. Every step of the Viterbi algorithm can now be expressed by a multiplication  Ai+1 W   = T · Ai  W  .  We are considering a terminated code with k · L information and n ·  L + m  code bits, and thus the trellis diagram contains L + m transitions and we have to apply L + m mul- tiplications. We obtain the WEF  AWEF W   =  1 0 ··· 0  ·    τ0,0  ...  . . .  τ1,1 ...  τ0,2ν−1  . . .  τ2ν−1,2ν−1   L+m  ·    ,    1  0 ... 0  where the row vector  1 0 ··· 0  is required to select the enumerator A 0  L+m W   of the ﬁnal node. The WEF may also be evaluated iteratively as indicated in Figure 3.23. An example of iterative calculation is given in Figure 3.24.  The concept of the weight enumerator function can be generalized to other enumerator  functions, e.g. the Input–Output Weight Enumerating Function  IOWEF   AIOWEF I, W   = k cid:1   n cid:1   i=0  w=0  ai,wI i W w,  where ai,w represents the number of code words with weight w generated by information words of weight i. The input–output weight enumerating function considers not only the weight of the code words but also the mapping from information word to code words. Therefore, it also depends on the particular generator matrix. For instance, the Hamming code B 7, 4  with systematic encoding has the IOWEF  AIOWEF I, W   = 1 + I  3W 3 + W 4  + I 2 3W 3 + 3W 4  + I 3 W 3 + 3W 4  + I 4W 7.  Note that by substituting I = 1 we obtain the WEF from AIOWEF I, W    AWEF W   = AIOWEF I, W    I=1 .  To evaluate the input–output weight enumerating function for a convolutional encoder, two transition matrices T and T cid:1  are required. The coefﬁcients τi,j of T are input–output enumerators, for example τ0,2 = I W 2 for the transition from state σ0 to state σ2 with the  7 5 8 encoder. The matrix T cid:1  regards the tailbits necessary for termination and therefore only contains enumerators for code bits, e.g. τ  = W 2. We obtain   cid:1  0,2  AIOWEF I, W   =  1 0 ··· 0  · T LT   cid:1 m ·  1 0 ··· 0   T  .   CONVOLUTIONAL CODES  129  Calculating the weight enumerator function – example    1 W 2  0 W 2 0  0  0 0 W W 1 0 0 W W  0    ·    For the  7 5 8 convolutional code we obtain  Ai+1 W   =  i+1 W   i+1 W   i+1 W   i+1 W      =    Consider the procedure for L = 2:  i=2−→    A 0    i=1−→    1  A 1  A 2  A 3     1    0 W 2 0    1  W 3 W 2 W 3 1 + 2W 5 + W 6 W 3 + 2W 4    i=3−→   .  W 2 + W 3 + W 4 + W 7  W 3 + 2W 4 + W 5  0 0 0  i=4−→    .  i  W   i  W   i  W   i  W    A 1  A 2  A 3     A 0       1 + W 5  W 3 + W 4 W 2 + W 3 W 3 + W 4    This results in the weight enumerating function  AWEF W   = A  0  L+m W   = A  0  4 W   = 1 + 2W  5 + W  6  .  Figure 3.24: Calculating the weight enumerator function – example  3.3.4 Path Enumerators  At the beginning of this section we have introduced the notion of an error event, i.e. if an error occurs, the correct sequence and the estimated sequence will typically match for long periods of time but will differ for some code sequence segments. An error event is a code sequence segment where the transmitted and the estimated code sequence differ. Without loss of generality we can assume that the all-zero sequence was transmitted. Therefore, we can restrict the analysis of error events to code sequence segments that diverge from the all- zero sequence and remerge at some later time. Such a code sequence segment corresponds to a path in the trellis that leaves the all-zero state and remerges with the all-zero state. A weight distribution for such code sequence segments is typically called the path enumerator. In this section we will discuss a method for calculating such path enumerators. Later on, we will see how the path enumerator can be used to estimate the decoding performance for maximum likelihood decoding.   130  CONVOLUTIONAL CODES  We will assume that the error event starts at time zero. Hence, all code sequence seg- ments under consideration correspond to state sequences  0, σ1, σ2, . . . , σj , 0, . . . . Note that j is greater than or equal to m, because a path that diverges from the all-zero state requires at least m + 1 transitions to reach the zero state again. Once more, we will discuss the procedure to derive the path enumerator for a particular example using the  7 5 8 con- volutional code. Consider, therefore, the signal ﬂow chart in Figure 3.25. This is basically the state diagram from Figure 3.6. The state transitions are now labelled with enumerators  Path enumerator function  W I  11  W  I  W I  W 2I  00  10  W  01  W 2  00    Path enumerator function can be derived from a signal ﬂow chart similar to the state diagram. State transitions are labelled with weight enumerators, and the loop from the all-zero state to the all-zero state is removed.    We introduce a dummy weight enumerator function for each non-zero state  and obtain a set of linear equations  A 1  I, W   = W A 2  I, W   + W A 3  I, W  , A 2  I, W   = I A 1  I, W   + I W A 3  I, W   = W I A 2  I, W   + W I A 3  I, W  ,  ,  2  AIOPEF I, W   = A 1  I, W  W 2.    Solving this set of equations yields the path enumerator functions  AIOPEF I, W   = W 5I   1 − 2W I    and  APEF I, W   = AIOPEF I, W    I=1 = W 5   1 − 2W    .  Figure 3.25: Path enumerator function   CONVOLUTIONAL CODES  131  for the number of code bits  exponent of W   and the number of information bits  expo- nent of I   that correspond to the particular transition. Furthermore, the loop from the all-zero state to the all-zero state is removed, because we only consider the state sequence  0, σ1, σ2, . . . , σj , 0, . . .  where only the ﬁrst transition starts in the all-zero state the last transition terminates in the all-zero state and there are no other transitions to the all-zero state in between. In order to calculate the Input–Output Path Enumerator Function  IOPEF , we introduce an enumerator for each non-zero state, comprising polynomials of the dummy variables W and I . For instance, A 2  I, W   denotes the enumerator for state σ2 =  10 . From the signal ﬂow chart we derive the relation A 2  I, W   = I A 1  I, W   + I W 2. Here, A 2  I, W   is the label of the initial transition I W 2 plus the enumerator of state σ1 multi- plied by I , because the transition from state σ1 to state σ2 has one non-zero information bit and only zero code bits. Similarly, we can derive the four linear equations in Figure 3.25, which can be solved for AIOPEF I, W  , resulting in  AIOPEF I, W   = W 5I   1 − 2W I    .  As with the IOWEF, we can derive the Path Enumerator Function  PEF  from the input– output path enumerator by substituting I = 1 and obtain  APEF W   = AIOPEF I, W    I=1 = W 5   1 − 2W    .  3.3.5 Pairwise Error Probability  In Section 3.3.1 and Section 3.3.2 we have considered error patterns that could lead to a decoding error. In the following, we will investigate the probability of a decoding error with minimum distance decoding. In this section we derive a bound on the so-called pairwise error probability, i.e. we consider a block code B = {b, b cid:1 } that has only two code words and estimate the probability that the minimum distance decoder decides on the code word b cid:1  when actually the code word b was transmitted. The result concerning the pairwise error probability will be helpful when we consider codes with more code words or possible error events of convolutional codes. Again, we consider transmission over the BSC. For the code B = {b, b cid:1 }, we can characterise the behaviour of the minimum distance decoder with two decision regions  cf. Section 2.1.2 . We deﬁne the set D as the decision region of the code word b, i.e. D is the set of all received words r so that the minimum distance decoder decides on b. Similarly, we deﬁne the decision region D cid:1  . Note that for the code with two code words we have D cid:1  = Fn\D.  for the code word b cid:1   Assume that the code word b was transmitted over the BSC. In this case, we can deﬁne  the conditional pairwise error probability as Peb =  Pr{rb}.  Similarly, we deﬁne  Peb cid:1  =  Pr{rb cid:1 }   cid:1   cid:1   r  ∈D  r  ∈D cid:1    132  CONVOLUTIONAL CODES  and obtain the average pairwise error probability  Pe = PebPr{b} + Peb cid:1 Pr{b  cid:1 }.  We proceed with the estimation of Peb. Summing over all received vectors r with r  ∈ D is usually not feasible. Therefore, it is desirable to sum over all possible received vectors r ∈ Fn. In order to obtain a reasonable estimate of Peb we multiply the term Pr{rb} with the factor   cid:9   % ≥ 1 for r  ∈ D  ≤ 1 for r ∈ D  This factor is greater than or equal to 1 for all received vectors r that lead to a decoding error, and less than or equal to 1 for all others. We have  Now, we can sum over all possible received vectors r ∈ Fn Pr{rb}Pr{rb cid:1 }.  Peb ≤   cid:1   ’  r  ∈D  Pr{rb}Pr{rb cid:1 }.  Pr{rb cid:1 } Pr{rb} =  cid:1  ’  r  Pr{rb cid:1 } Pr{rb}  cid:9  Pr{rb}   cid:1   r  ∈D  Peb ≤  + Pr{rbi}Pr{rb  }.   cid:1  i  r  r  r1  rn   cid:1  i   cid:1  i   cid:1  i  i=1  } +  +  The BSC is memoryless, and we can therefore write this estimate as   cid:1   } = n cid:20  Pr{ribi}Pr{rib ’ } is simply % √ ε2 + √ } = ε 1 − ε  for bi  cid:7 = b 2  cid:12 dist b,b cid:1   cid:11  ’   cid:1   Peb ≤  For the BSC, the term Pr{0bi}Pr{0b     * n cid:20   cid:1  ··· ’  i=1 Pr{rbi}Pr{rb + Pr{1bi}Pr{1b  cid:1  i +  cid:1  Peb ≤ n cid:20  Similarly, we obtain Peb cid:1  ≤ cid:2   Pr{rbi}Pr{rb  cid:3 dist b,b cid:1  √ ε 1 − ε  2  cid:11  ’ error probability is bounded by ε 1 − ε   Hence, we have   cid:12 dist b,b cid:1   ε 1 − ε   } =  Pe ≤  i=1  2  2   cid:1  i   cid:1  i  .  r        .   1 − ε 2 = 1 for bi = b   cid:1  i  .   . Thus, we can conclude that the pairwise  √ ε 1 − ε  the Bhat- This bound is usually called the Bhattacharyya bound, and the term 2 tacharyya parameter. Note, that the Bhattacharyya bound is independent of the total number of code bits. It only depends on the Hamming distance between two code words. Therefore,   CONVOLUTIONAL CODES  Bhattacharyya bound  133    The pairwise error probability for the two code sequences b and b cid:1   is  r  ∈D   cid:9   deﬁned as  with conditional pairwise error probabilities  Pe = PebPr{b} + Peb cid:1 Pr{b  cid:1 }  cid:1   cid:1   Pr{rb} and Peb cid:1  =  Peb =  Pr{rb cid:1 }  r  ∈D cid:1   and the decision regions D and D cid:1 .    To estimate Peb, we multiply the term Pr{rb} with the factor  % ≥ 1 for r  ∈ D Pr{rb cid:1 } Pr{rb}  cid:1  ’  ≤ 1 for r ∈ D and sum over all possible received vectors r ∈ Fn Pr{rb}Pr{rb cid:1 }.  cid:12 dist b,b cid:1   Peb ≤    For the BSC this leads to the Bhattacharyya bound  ’   cid:11   r     Pe ≤  ε 1 − ε   2  .  Figure 3.26: Bhattacharyya bound  it can also be used to bound the pairwise error probability for two convolutional code sequences. The derivation of the Bhattacharyya bound is summarised in Figure 3.26. For instance, consider the convolutional code B 2, 1, 2  and assume that the all-zero code word is transmitted over the BSC with crossover probability ε = 0.01. What is the probability that the Viterbi algorithm will result in the estimated code sequence ˆb =  11 10 11 00 00 . . . ? We can estimate this pairwise error probability with the Bhattacharyya ε 1 − ε  ≈ 0.2 and the bound Pe ≤ bound. We obtain the Bhattacharyya parameter 2 3.2 · 10 −4. In this particular case we can also calculate the pairwise error probability. The Viterbi decoder will select the sequence  11 10 11 00 00 . . .  if at least three channel errors occur in any of the ﬁve non-zero positions. This event has the probability  √   cid:7  εe 1 − ε    cid:6   5 e  5 cid:1   e=3  5−e ≈ 1 · 10 −5  .   134 3.3.6 Viterbi Bound  CONVOLUTIONAL CODES  We will now use the pairwise error probability for the BSC to derive a performance bound for convolutional codes with Viterbi decoding. A good measure for the performance of a convolutional code is the bit error probability Pb, i.e. the probability that an encoded information bit will be estimated erroneously in the decoder. However, it is easier to derive bounds on the burst error probability PB, which is the probability that an error event will occur at a given node. Therefore, we start our discussion with the burst error probability.3 We have already mentioned that different error events are statistically independent for the BSC. However, the burst error probability is not the same for all nodes along the correct path. An error event can only start at times when the estimated code sequence coincides with the correct one. Therefore, the burst error probability for the initial node  time i = 0  is greater than for times i > 0. We will derive a bound on PB assuming that the error event starts at time zero. This yields a bound that holds for all nodes along the correct path. Remember that an error event is a path through the trellis, that is to say, a code sequence segment. Let b denote the correct code sequence and E  b cid:1    denote the event that the code sequence segment b cid:1  causes a decoding error starting at time zero. A necessary condition for an error event starting at time zero is that the corresponding code sequence segment has a distance to the received sequence that is less than the distance between the correct path and r. This condition is not sufﬁcient, because there might exist another path with an even smaller distance. Therefore, we have  where the union is over all possible error events diverging from the initial trellis node. We b cid:1  Pr{E  b cid:1   } can now use the union bound to estimate the union of events Pr{∪b cid:1 E  b cid:1  and obtain   } ≤  Assume that dist b, b cid:1     = w. We can use the Bhattacharyya bound to estimate Pr{E  b cid:1    }  PB ≤ Pr{∪b cid:1 E  b  cid:1    },  PB ≤   cid:1   cid:11   b cid:1   Pr{E  b  } ≤  cid:1  ∞ cid:1   w=dfree   cid:12    }.  Pr{E  b  cid:1  ’ ε 1 − ε   cid:11  ’  2  w  .   cid:12   PB ≤  aw  2  ε 1 − ε   w  .  Let aw be the number of possible error events of weight w starting at the initial node. We have  Note, that the set of possible error events is the set of all paths that diverge from the all-zero path and remerge with the all-zero path. Hence, aw is the weight distribution of the convolutional code, and we can express our bound in terms of the path enumerator function A W     cid:11   ’  ∞ cid:1   w=dfree  PB ≤  aw  2  ε 1 − ε    cid:12  w = APEF W    √ W=2  ε 1−ε  .  This bound is called the Viterbi bound  Viterbi, 1971 .  3The burst error probability is sometimes also called the ﬁrst event error probability.   CONVOLUTIONAL CODES  Viterbi bound  135    The burst error probability PB is the probability that an error event will occur  at a given node.    For transmission over the BSC with maximum likelihood decoding the burst  error probability is bounded by   cid:11   ’  ∞ cid:1   w=dfree  PB ≤  aw  2  ε 1 − ε    cid:12  w = APEF W    √ W=2 ε 1−ε  .    The bit error probability Pb is the probability that an encoded information  bit will be estimated erroneously in the decoder.    The bit error probability for transmission over the BSC with maximum  likelihood decoding is bounded by  Pb ≤ 1  k  ∂AIOPEF I, W    ∂I  √ I=1;W=2  ε 1−ε   .  Figure 3.27: Viterbi bound  Consider, for example, the code B 2, 1, 2  with path enumerator APEF W   = W 5 1−2W .  For the BSC with crossover probability ε = 0.01, the Viterbi bound results in  PB ≤ APEF W    W≈0.2 ≈ 5 · 10 −4  .  In Section 3.3.5 we calculated the Bhattacharyya bound Pe ≤ 3.2 · 10 −4 on the pairwise error probability for the error event  11 10 11 00 00 . . . . We observe that for ε = 0.01 this path, which is the path with the lowest weight w = dfree = 5, determines the overall decoding error performance with Viterbi decoding.  Based on the concept of error events, it is also possible to derive an upper bound on the bit error probability Pb  Viterbi, 1971 . We will only state the result without proof and discuss the basic idea. A proof can be found elsewhere  Johannesson and Zigangirov, 1999 . The bit error probability for transmission over the BSC with maximum likelihood decoding is bounded by  Pb ≤ 1  k  ∂AIOPEF I, W    ∂I  √ I=1;W=2  ε 1−ε   .  The bound on the burst error probability and the bound on the bit error probability are summarized in Figure 3.27. In addition to the bound for the burst error probability, we require an estimate of the expected number of information bits that occur in the event   cid:17  cid:17  cid:17  cid:17    cid:17  cid:17  cid:17  cid:17    136  CONVOLUTIONAL CODES  √ of a decoding error. Here, the input–output path enumerator is required. Remember that, by substituting W = 2 ε 1 − ε  in the path enumerator, we evaluate the pairwise error probability for all possible Hamming weights w, multiply each pairwise error probability by the number of paths of the particular weight and ﬁnally sum over all those terms. In order to calculate the bit error probability, we have to take the expected number of information bits into account. Therefore, we derive the input–output path enumerator with respect to the variable I . Consequently, the pairwise error probabilities are weighted with the number of information bits corresponding to the particular error event.  Again, consider the code B 2, 1, 2  with the generator matrix  7 5 8 in octal notation.  With this generator matrix we have the input–output path enumerator  and the derivative  For the BSC with crossover probability ε = 0.01, the bound results in  AIOPEF I, W   = W 5I   1 − 2W I    ∂AIOPEF I, W    ∂I  =  W 5   1 − 2W I  2 .   cid:17  cid:17  cid:17  cid:17   Pb ≤  W 5   1 − 2W I  2  W≈0.2  ≈ 9 · 10 −4  .  3.4 Soft-input Decoding  Up to now, we have only considered so called hard-input decoding, i.e. in Section 3.2 we have assumed that the channel is a BSC which has only two output values. In this case we can employ minimum distance decoding with the Hamming metric as the distance measure. In general, the transmission channel may have a continuous output alphabet like, for example, the Additive White Gaussian Noise  AWGN  channel or fading channels in mobile communications. In this section we will generalise the concept of minimum distance decoding to channels with a continuous output alphabet. We will observe that we can still use the Viterbi algorithm, but with a different distance measure.  Later on in this section we consider some implementation issues. In particular, we  discuss the basic architecture of a hardware implementation of the Viterbi algorithm.  3.4.1 Euclidean Metric  As an example of a channel with a continuous output alphabet, we consider the AWGN channel, where we assume that Binary Phase Shift Keying  BPSK  is used for modu- lation. Hence, the code bits bi ∈ F2 are mapped to the transmission symbols xi ∈ {−1,+1} according to  If the receiver performs coherent demodulation, then the received symbols at the decoder input are  xi = 2bi − 1.  ri = xi + ni ,   CONVOLUTIONAL CODES  137  where we have assumed that the energy of the transmitted signal is normalised to 1. The Gaussian random variable ni represents the additive noise. Furthermore, we assume that the channel is memoryless, i.e. the noise samples ni are statistically independent. In this case the channel can be characterised by the following probability density function  p ribi   =  1√ 2π σ 2  exp   cid:6  −  ri − xi  2   cid:7   ,  2σ 2  where σ 2 is the variance of the additive Gaussian noise.  Again, we would like to perform ML sequence estimation according to Figure 3.12.  Note that, using Bayes’ rule, the MAP criterion can be expressed as p rb Pr{b}  ˆb = argmax  {Pr{br}} = argmax  ,  b  p r   where p r  is the probability density function of the received sequence and p rb  is the conditional probability density function given the code sequence b. Assuming, again, that the information bits are statistically independent and equally likely, we obtain the ML rule  %   cid:19  cid:20   ˆb = argmax  {Pr{rb}} = argmax  &  ,  .  b  b  For decoding we can neglect constant factors. Thus, we have   cid:19  cid:20   p ribi    cid:7 ,  .  b  i   cid:6  −  ri − xi  2  cid:19  cid:1   2σ 2  exp  ,  b  i  b  ˆb = argmax  cid:19  cid:1   Taking the logarithm and again neglecting constant factors, we obtain  ri − xi    = argmin  2  2  ˆb = argmax   b  i  − ri − xi    cid:9  cid:1   i  ri − xi  2 is the square of the Euclidean metric distE r, x  =  ri , xi ∈ R.   ri − xi  2,  Note that the term  ,  .  i  i  Therefore, it is called the squared Euclidean distance. Consequently, we can express the ML decision criterion in terms of a minimum distance decoding rule, but now with the squared Euclidean distance as the distance measure   cid:4    cid:5   ˆb = argmin  b  dist2  E r, x   .  3.4.2 Support of Punctured Codes Punctured convolutional codes as discussed in Section 3.1.5 are derived from a rate R = 1 n mother code by periodically deleting a part of the code bits. Utilising an appropriate   138  CONVOLUTIONAL CODES  metric, like the squared Euclidean distance, we can decode the punctured code using the trellis of the mother code.  The trick is that we compute the branch metric so that for each deleted bit the con- tribution to all branch metric values is constant. For example, with the squared Euclidean distance we can use a simple depuncturing unit before the Viterbi decoder that inserts the value zero for each punctured bit into the sequence of received values. Consequently, the missing information does not inﬂuence the decisions of the Viterbi algorithm, because the zero values do not alter the metric calculation. Yet, we can use the trellis of the original mother code for decoding.  The application of puncturing is motivated by the fact that Viterbi decoding of high-rate convolutional codes can be signiﬁcantly simpliﬁed by using punctured codes. The trellis of a convolutional code has 2ν nodes in each trellis section, where ν is the overall constraint length of the code. Furthermore, each node  accept for the m terminating trellis sections  has 2k outgoing branches. Hence, there are 2ν+k branches per trellis section and roughly  L + m 2ν+k branches in the terminated trellis. The number of branches determines the number of operations of the Viterbi algorithm and therefore the computational requirements. The complexity increases linearly with the length L of the trellis, but exponentially with the constraint length ν and with the dimension k. With punctured codes we always use the trellis of the rate R = 1 n mother code for decoding. Hence, each trellis section has only 2ν+1 branches. Of course, the addition of the zero values causes some computational overhead. However, the total number of additions is much smaller if we use puncturing and depuncturing. For instance, with a rate R = 2 3 code of overall constraint length ν = 6, we have to compute 3 · 2ν+2 = 768 additions for each trellis section or 384 additions per information bit. Using the trellis of the mother code, we have to compute 2 · 2ν+1 = 256 additions per information bit.  3.4.3 Implementation Issues  For applications with low data rates, the Viterbi algorithm can be implemented in software on a Digital Signal Processor  DSP . However, for high data rates the high computational requirements of maximum likelihood decoding demand hardware implementations in Very Large-Scale Integration  VLSI  technology or hybrid DSP architectures, where the Viterbi algorithm runs in a dedicated processor part or coprocessor.  Figure 3.28 provides the block diagram of a receiver structure with Viterbi decoding. The ﬁrst block is the so-called Automatic Gain Control  AGC . The AGC is an adaptive device that adjusts the gain of the received signal to an appropriate level for the analogue- to-digital  A D  conversion. For instance, in Section 3.4.1 we have assumed that the energy of the signal is normalised to 1. In this case the AGC should measure the average signal level and implement a scaling by 1√  .  The A D conversion provides a quantised channel output. It is important that the A D conversion reduces the word length to the required minimum, because the complexity of the Viterbi decoder depends strongly on the word length of the branch metrics. Massey pre- sented a procedure to calculate the quantisation thresholds for the AWGN channel  Massey, 1974 . Using this procedure, it was shown in  Johannesson and Zigangirov, 1999  that two quantisation levels would lead to roughly 2 dB loss in signal-to-noise ratio compared with the unquantised AWGN channel. This is essentially the difference between hard- and  Es   CONVOLUTIONAL CODES  139  Block diagram of a Viterbi decoder  AGC  A D  depuncturing  ˆu  decisions  branch metrics  SMU  ACSU  TMU  r  state metrics  latch  Viterbi decoder  Figure 3.28: Block diagram of a receiver structure with Viterbi decoding  soft-input decoding for the AWGN channel. In Section 1.2.4 we have already seen that, with respect to the channel capacity, the difference is actually up to 3 dB  cf. Figure 1.6 on page 8 . A 2-bit quantisation, i.e. four levels, would lead to approximately 0.57 dB loss, whereas with eight levels the loss is only about 0.16 dB. Hence, under ideal conditions a 3-bit quantisation should already provide sufﬁcient accuracy. However, the computed quantisation thresholds depend on the actual signal-to-noise ratio and require an ideal gain control. Therefore, in practice a larger word length for the channel values might be required. The block depuncturing in Figure 3.28 represents the depuncturing unit discussed in  the previous section and is only required if punctured codes are used.  The Viterbi decoder in Figure 3.28 depicts the structure of a hardware implementation. The Transition Metric Unit  TMU  calculates all possible branch metrics in the Viterbi algorithms. For a rate R = k n code, 2n different branch metric values have to be com- puted for each trellis section. The Add Compare Select Unit  ACSU  performs the forward recursion for all trellis nodes, i.e. the ACSU updates the state metric values. The ACSU is recursive which is indicated by the latch. Finally, the decisions for each trellis node, i.e. the local survivors, are stored in the Survivor Memory Unit  SMU  which also performs the backward recursion.  The ACSU performs the actual add–compare–select operation and is therefore the most complex part of the Viterbi decoder. In order to use the same unit for all trellis sections, a metric normalisation is required that ensures a ﬁxed word length. Note that we can subtract a common value from all state metric values in any level of the trellis, because these   140  CONVOLUTIONAL CODES  subtractions do not inﬂuence the decisions in the following decoding steps. Methods for metric normalisation are discussed elsewhere  Shung et al., 1990 .  3.5 Soft-output Decoding  Up to now, we have only considered Viterbi’s famous maximum likelihood decoding algo- rithm which estimates the best code sequence and outputs hard decisions for the estimated information bits. However, with respect to the residual bit error rate, Viterbi decoding does not ultimately provide the best performance.  Consider again Figure 3.12. The third and fourth decoding rules deﬁne the so-called symbol-by-symbol decision rules. For example, we have deﬁned the symbol-by-symbol MAP rule   cid:31     r}  ˆu l   t  = argmax  Pr{u l   t  ∀ t.  u   l  t  A decoder that implements this symbol-by-symbol MAP rule computes for all information bits the binary value ˆu l  r}.4 Hence, such a symbol-by-symbol MAP decoder actually minimizes the bit error rate. Nevertheless, the performance gain compared with Viterbi decoding is usually only marginal and commonly does not justify the higher decoding complexity of MAP decoding.  that maximizes the a-posteriori probability Pr{u l   t  t  However, for applications such as iterative decoding the hard bit decisions of the Viterbi algorithm are not sufﬁcient. It is essential that the decoder issues a soft output value and not merely provides the binary value of the estimated information or code bits  hard output . The term soft-output decoding refers to decoding procedures that additionally calculate a reliability information for each estimated bit, the so-called soft output. Consequently, the Viterbi algorithm is a hard-output decoder.  There are basically two types of Soft-Input Soft-Output  SISO  decoding methods. The ﬁrst type are soft-output extensions of the Viterbi algorithm. The most popular rep- resentative of this type is the Soft-Output Viterbi Algorithm  SOVA  proposed elsewhere  Hagenauer and Hoeher, 1989 . The SOVA selects the survivor path as the Viterbi algo- rithm. To calculate reliabilities, it utilises the fact that the difference in the branch metrics between the survivor and the discarded branches indicates the reliability of each deci- sion in the Viterbi algorithm. A similar reliability output algorithm was recently presented  Freudenberger and Stender, 2004 .  The second class of soft-output algorithms are based on the so-called BCJR algorithm which is named after its inventors Bahl, Cocke, Jelinek and Raviv  Bahl et al., 1974 . The BCJR algorithm is a symbol-by-symbol MAP decoding algorithm. A similar symbol-by- symbol MAP algorithm was also independently developed by McAdam, Welch and Weber  McAdam et al., 1972 . The soft outputs of the BCJR algorithm are the symbol-by-symbol a-posteriori probabilities Pr{u l  r}. Therefore, this kind of soft-output decoding is also called A-Posteriori Probability  APP  decoding.  t  APP decoding provides the best performance in iterative decoding procedures. How- ever, the SOVA has a signiﬁcantly lower complexity, with only slight degradation in the  4Note that if all information bits are a priori equally likely, the symbol-by-symbol MAP rule results in the  ML symbol-by-symbol rule.   CONVOLUTIONAL CODES  141  decoding performance. Therefore, the SOVA is also popular for practical decoder imple- mentations.  In this section we discuss APP decoding based on the BCJR algorithm for convolutional codes. We ﬁrst derive the original algorithm. Then, we consider a second version that is more suitable for implementations as it solves some numerical issues of the former version.  3.5.1 Derivation of APP Decoding  The BCJR algorithm is a symbol-by-symbol APP decoder, i.e. it calculates the a-posteriori probability for each information or code symbol. In the literature there exist many modiﬁca- tions of the original BCJR algorithm  see, for example, Hagenauer et al., 1996; Robertson et al., 1997; Schnug, 2002 . We will discuss a realisation with soft output as log-likelihood values  cf. Section 4.1.3 . For the sake of simplicity, we will assume a terminated convo- lutional code.  The log-likelihood ratio  L-value  of the binary random variable x is deﬁned as  L x  = ln  Pr{x = 0} Pr{x = 1} .  From this L-value we can calculate the probabilities  Pr{x = 0} =  1 1 + e −L x   and Pr{x = 1} =  1  1 + eL x   .  Hence, we can formulate the symbol-by-symbol MAP decoding rule in terms of the log- likelihood ratio  and obtain  t   = ln L ˆu l   cid:19   t  Pr{u l  Pr{u l  if L ˆu l  0 1 if L ˆu l   = 0r} = 1r} t   ≥ 0 t   < 0  t  ˆu l   t  =  .  We observe that the hard MAP decision of the symbol can be based on the sign of this L-value, and the quantity L ˆu l  t   is the reliability of this decision. The problem of APP decoding is therefore equivalent to calculating the log-likelihood ratios L ˆu l  t   for all t. In the following we discuss a method to compute the L-values L ˆu l   t   on the basis of the trellis representation of a convolutional code. First of all, note that the probabil- ity Pr{u l  = 0r} can be calculated from the a-posteriori probabilities Pr{br} of all code sequences b which correspond to an information bit u l  t  = 0 at position t. We have  t  Similarly, Pr{u l   t  = 1r} can be calculated from the APP of all code sequences b   cid:1    cid:1   Pr{u l   t  = 0r} =  Pr{br}.  b∈B,u  =0   l  t  Pr{u l   t  = 1r} =  Pr{br}.  b∈B,u  =1   l  t   142  This yields  CONVOLUTIONAL CODES  L ˆu l   t   = ln  Pr{u l  Pr{u l   t  t  = 0r} = 1r}  = ln  =0 Pr{br} =1 Pr{br} .  b∈B,u b∈B,u   l  t   l  t  Now, assume that the channel is memoryless and the information bits are statistically independent. Then we have      cid:20   Pr{br} = Pr{rb}Pr{b}  Pr{r}  Cancelling the terms Pr{ri}, we obtain   L ˆu l   b∈B,u b∈B,u  =0 =1   l  t   l  t  In particular, for depth t we obtain  .  i  Pr{ri}  Pr{ribi}Pr{bi}  = $ $ i Pr{ribi}Pr{bi} i Pr{ribi}Pr{bi} .  cid:20   Pr{ribi} =  Pr{ribi} · Pr{rtbt}Pr{bt} ·  Pr{ribi}.  t   = ln  cid:20   i<t   cid:20   i  i>t  Consider now Figure 3.29 which illustrates a segment of a code trellis. Each state transition to a state σt+1 at depth t + 1 corresponds to k σ  → σt+1 from a state σ   cid:1  t at depth t   cid:1  t  Trellis segment  u l  t  bt  ... ... ...  ... ... ...  σt  σt+1  Figure 3.29: Trellis segment of a convolutional code   CONVOLUTIONAL CODES  APP decoding    Initialisation: α σ0  = β σL+m  = 1.   Transition probabilities:    Forward recursion: starting from σ0, calculate  γ  σt , σ  t   =  cid:1   α σ  t+1  = Pr{rtbt}Pr{bt}  cid:1   cid:1   γ  σt−1, σ   cid:1  t  α σt−1     Backward recursion: starting from σL+m, calculate  t   =  cid:1   β σ   cid:1  t , σt+1 β σt+1   γ  σ  σt−1   cid:1   σt+1    Output:     L ˆut   = ln   cid:1  σ t  cid:1  σ t  →σt+1,u →σt+1,u   l  t   l  t  =0 α σ =1 α σ  t   · γ  σ  cid:1  t   · γ  σ  cid:1   t , σt+1  · β σt+1   cid:1  t , σt+1  · β σt+1   cid:1   Figure 3.30: APP decoding with probabilities  143   3.11    3.12    3.13    3.14   information bits ut and n code bits bt . Using the trellis to represent the convolutional code, we can write the L-value L ˆu l      t   as =0 Pr{σ  cid:1  t =1 Pr{σ  cid:1  t   cid:1  σ t  cid:1  σ t  →σt+1,u →σt+1,u   l  t   l  t  L ˆu l   t   = ln     → σt+1r} → σt+1r} = ln   cid:1  σ t  cid:1  σ t  →σt+1,u →σt+1,u   l  t   l  t  =0 Pr{σ =1 Pr{σ   cid:1  t  cid:1  t  → σt+1, r} → σt+1, r} .   cid:1  t  Pr{σ  Again, proceeding from the assumption of a memoryless channel and statistically indepen- dent information symbols, we can rewrite the transition probability Pr{σ → σt+1, r}. In particular, for depth t we obtain  cid:14  cid:15   cid:13   cid:14  cid:15   cid:16  → σt+1, r} = Pr{σ t , r[0,t−1]}  cid:1      cid:13   cid:16  · Pr{σt+1, r[t+1,L+m−1]}   cid:13  · Pr{σ  β σt+1    cid:14  cid:15   cid:16  → σt+1, rt}  cid:1  t ,σt+1  t   · γ  σ  cid:1  t   · γ  σ  cid:1   t , σt+1  · β σt+1   cid:1  t , σt+1  · β σt+1   cid:1   t   = ln  L ˆu l   =0 α σ =1 α σ  →σt+1,u →σt+1,u  and   cid:1  t    γ  σ   l  t  α σ   cid:1  t   cid:1  t  .   cid:1  σ t  cid:1  σ t   l  t    cid:1  t   to denote this value. Similarly, we use β σt+1  and γ  σ  CONVOLUTIONAL CODES 144 t , r[0,t−1]} is the joint probability of the event that the received sequence is r[0,t−1] up Pr{σ  cid:1  to time t − 1 and the transmitted sequence passes through state σ  cid:1  t . We use the abbreviated  cid:1  t , σt+1, rt   to denote notation α σ the probabilities Pr{σt+1, r[t+1,L+m−1]} and Pr{σ → σt+1, rt} respectively. Hence, β σt+1  is the joint probability of the event that the transmitted sequence passes through state σt+1  cid:1  t , σt+1  is the and we receive r[t+1,L+m−1] for the tail of the code sequence. Finally, γ  σ  cid:1  joint probability of the state transition σ  cid:1  t t   and β σt   recursively. During the forward recursion, starting with the initial node σ0, we compute  The BCJR algorithm is an efﬁcient method for calculating the joint probabilities α σ  → σt+1 and the n-tuple rt .   cid:1  t  t   =  cid:1   α σ  γ  σt−1, σ   cid:1  t  α σt−1   with the initial condition α σ0  = 1 and with the transition probabilities  t+1  = Pr{rtbt}Pr{bt} = Pr{rt , bt}.  cid:1   γ  σt , σ  Note that this forward recursion is similar to the forward recursion of the Viterbi algorithm, i.e. for each state we calculate a joint probability based on the previous states and state transitions. β σL+m  = 1. We compute  Then, we start the backward recursion at the terminating node σL+m with the condition  t   =  cid:1   β σ   cid:1  t , σt+1 β σt+1 .  γ  σ   cid:1   σt−1   cid:1   σt+1  This is essentially the same procedure as the forward recursion starting from the terminated end of the trellis. During the backward recursion we also calculate the soft-output L- values according to Equation  3.14 . All steps of the BCJR algorithm are summarised t+1  = Pr{rtbt}Pr{bt} =  cid:1  in Figure 3.30. Consider now the calculation of the values γ  σt , σ Pr{rtbt}Pr{ut}. As the channel is memoryless and the information bits are statistically } k cid:20  independent, we have  t+1  = Pr{rtbt}Pr{ut} = n cid:20   Pr{u l   Pr{r  γ  σt , σ  b   j   t   j   t  }.   cid:1   t  j=1  l=1   cid:1  t+1 , we can use the a-priori log-likelihood ratios for information In order to calculate γ  σt , σ bits and the L-values for the received symbols. Note that for an a-priori probability we have  Pr{u l   t  = 0} =  To calculate γ  σt , σ  1 −L u  = 1} =   l  t    1 + e  cid:1  t+1  it is sufﬁcient to use the term  t  and Pr{u l   cid:19   −L u   l   t =   l  t  u  e  1 −L u   l  t    e  , if u l  t , if u l  t  = 0 = 1  1 1 + eL u   l  t    −L u = e 1 + e   l  t   −L u  .   l  t      l   CONVOLUTIONAL CODES instead of the probabilities Pr{u l  −L u  145 = 1}, because the numerator terms 1 + t   will be cancelled in the calculation of the ﬁnal a-posteriori L-values. Similarly, we  cid:1  t+1   e can use e values no longer represent probabilities. Nevertheless, we can use  }. Certainly, the γ  σt , σ  = 0} or Pr{u l   instead of the probabilities Pr{r k cid:20   −L r   j    b t  b   j   t   j   t   j   t   j   t  b  t  t  t+1  = n cid:20    cid:1   e  j=1  γ  σt , σ  −L r  b   j   t   j   t   j    b t  −L u  e   l  t  u   l  t  l=1  in the BCJR algorithm and obtain the correct soft-output values.  3.5.2 APP Decoding in the Log Domain  The implementation of APP decoding as discussed in the previous section leads to some numerical issues, in particular for good channel conditions where the input L-values are large. In this section we consider another version of APP decoding, where we calculate logarithms of probabilities instead of the probabilities. Therefore, we call this procedure APP decoding in the log domain.  Basically, we will use the notation as introduced in the previous section. In particular,  we use  α σt   = ln  α σt    , β σt   = ln  β σt    , t , σt+1  = ln  cid:1   γ  σ   cid:2    cid:1  t , σt+1   .   cid:3   γ  σ  On account of the logarithm, the multiplications of probabilities in the update equations  cid:1  in Figure 3.30 correspond to sums in the log domain. For instance, the term γ  σt−1, σ t   t   + α σt−1  in the log domain. To calculate γ  σt−1, σ  cid:1   cid:1  α σt−1  corresponds to γ  σt−1, σ t  , we can now use  γ  σt−1, σ  b  L r   j   t   j   t   j    b t  L u l   t  u l   t  which follows from  t+1  = ln  cid:1   γ  σt , σ  −L r  b   j   t   j   t   j    b t  −L u   l  t  u   l  t  e   cid:2    cid:3   However, instead of the sum x1 + x2 of two probabilities x1 and x2, we have to calcu- ex1 + ex2 in the log domain. Consequently, the update equation for the forward late ln recursion is now    .   cid:1   t   = − n cid:1    n cid:20   j=1  Similarly, we obtain the update equation  t   = ln  cid:1   α σ  e γ  σt−1,σ   cid:1  t  +α σt−1    t   = ln  cid:1   β σ   cid:1  t ,σt+1 +β σt+1    e γ  σ  − k cid:1  k cid:20   l=1  l=1    .    e  j=1    cid:1    cid:1   σt−1  σt+1   146  CONVOLUTIONAL CODES  for the backward recursion and equation     L ˆu l   t   = ln   cid:1  σ t  cid:1  σ t  →σt+1,u →σt+1,u   l  t   l  t  =0 eα σ =1 eα σ   cid:1  t  +γ  σ  cid:1  t  +γ  σ   cid:1  t ,σt+1 +β σt+1   cid:1  t ,σt+1 +β σt+1    cid:2   for the soft output. On account of ln 1  = 0 the initialisation is now α σ0  = β σL+m  = 0. The most complex operation in the log domain version of the APP decoding algo- rithm corresponds to the sum of two probabilities x1 and x2. In this case, we have to . For this calculation we can also use the Jacobian logarithm which calculate ln yields ex1 + ex2   cid:3  = max{x1, x2} + ln  −x1−x2 cid:3   ex1 + ex2  1 + e   cid:3   cid:2    cid:2   ln  .  APP decoding in the log domain    Initialisation: α σ0  = β σL+m  = 0.   State transitions:  t  u l   t   3.16   or  γ  σt−1, σ  γ  σt , σ   cid:1   L r   j   t   j   t  b  j=1   j    b t  L u l   t   = − n cid:1   cid:2   t+1  = ln Pr{rtbt}  + ln Pr{bt}   cid:1  − k cid:1   cid:3  + fc ·   cid:3  + fc ·   t   + α σt−1   cid:1   γ  σt−1, σ  l=1  γ  σ   cid:2   t   = max  cid:1  σt−1  α σ    Backward recursion: starting from σL+m, calculate t , σt+1  + β σt+1   cid:1   β σ    Forward recursion: starting from σ0, calculate    Output:  L ˆu l   t   =  t   = max  cid:1  σt+1  cid:2   max  cid:1  →σt+1,u σ t −  =0   l  t  max →σt+1,u   cid:1  σ t  =1   l  t  α σ   cid:2   t   + γ  σ  cid:1  t   + γ  σ  cid:1   t , σt+1  + β σt+1   cid:1  t , σt+1  + β σt+1   cid:1   α σ   cid:3  + fc ·   cid:3  − fc ·   3.19    3.15    3.17    3.18   Figure 3.31: APP decoding in the log domain   147  1 + e  −x1−x2 cid:3    cid:2   cid:3  ≈ max{x1, x2} can  CONVOLUTIONAL CODES  To reduce the decoding complexity, this expression is usually implemented as   cid:2   ex1 + ex2  ln   cid:3  ≈ max{x1, x2} + fc x1, x2 ,   cid:2   . This where fc x1, x2  is a correction function that approximates the term ln function depends only on the absolute value of the difference x1 − x2 and can be imple- ex1 + ex2 mented with a look-up table, but the approximation with ln also be employed and usually leads only to a minor performance degradation. The latter approach is called the max-log approximation. Besides the signiﬁcantly lower complexity, the max-log approximation does not require exact input L-values. A constant factor com- mon to all input values does not inﬂuence the maximum operation. Hence, the max-log approximation has no need for a signal-to-noise ratio  SNR  estimation.  Note that the max-log approximation provides the same hard-decision output as the Viterbi algorithm  Robertson et al., 1997; Schnug, 2002 . Hence, it no longer minimises the bit error rate. The complete algorithm is summarised in Figure 3.31. The term fc ·  denotes the cor- rection function for all values included in the corresponding maximisation. If there are more than two terms, the algorithm can be implemented recursively where we always consider value pairs. For instance, in order to evaluate ln ex1 + ex2 + ex3 + ex4  , we ﬁrst calculate  cid:1  = ln ex1 + ex2   and x  x   cid:1  cid:1  = ln ex3 + ex4  . Then we have ln ex    = ln ex1 + ex2 + ex3 + ex4  .   cid:1  + ex   cid:1  cid:1   3.6 Convolutional Coding in Mobile Communications  In this section we will consider some examples for convolutional coding in mobile com- munications. With mobile communication channels it is necessary to use channel coding for transmission in order to avoid losses due to transmission errors. Mobile communication standards like GSM and UMTS distinguish between speech and data communication. The transmission of coded speech is a real-time service that allows only a small latency, but is rather impervious to residual errors in the decoded data stream, i.e. residual errors can usually be efﬁciently masked by the speech decoder. Data services, on the other hand, usu- ally require very low residual bit error rates, but allow for some additional delay caused by retransmissions due to transmission errors. We ﬁrst consider the transmission of speech sig- nals according to the GSM and the UMTS standards. In the subsequent sections we discuss how convolutional codes are employed in GSM to achieve very efﬁcient retransmission schemes that yield low residual error rates.  3.6.1 Coding of Speech Data  Usually, speech-coded data require a forward error correction scheme that allows for unequal error protection, i.e. there are some very important bits in the speech data stream that should be protected by robust channel coding. Then, there is a certain portion of data bits that is impervious to transmission errors. Consequently, speech coding and forward error correction should be designed together in order to get a good overall coding result. Both the GSM and UMTS standards deﬁne different speech coding types. We will, however, only consider the so-called full-rate and enhanced-full-rate codecs which illustrate   148  CONVOLUTIONAL CODES  how the speech coding and channel coding cooperate. The Full-Rate  FR  codec was the ﬁrst digital speech coding standard used in the GSM digital mobile phone system developed in the late 1980s. It is still used in GSM networks, but will gradually be replaced by Enhanced-Full-Rate  EFR  and Adaptive MultiRate  AMR  standards, which provide much higher speech quality.  With the GSM full-rate codec, the analogue speech signal is usually sampled with a sampling rate of 8000 samples per second and an 8-bit resolution of the analogue-to-digital conversion. This results in a data rate of 64 kbps. The FR speech coder reduces this rate to 13 kbps. To become robust against transmission errors, these speech-coded data are encoded with a convolutional code, yielding a transmission rate of 22.8 kbps. We will now focus on this convolutional encoding.  The speech coder delivers a sequence of blocks of data to the channel encoder. One such block of data is called a speech frame. With the FR coder, 13 000 bits per second from the encoder are transmitted in frames of 260 bits, i.e. a 260-bit frame every 20 ms. These 260 bits per frame are differentiated into classes according to their importance to the speech quality. Each frame contains 182 bits of class 1 which will be protected by channel coding, and 78 bits of class 2 which will be transmitted without protection. The class 1 bits are further divided into class 1a and class 1b. Class 1a bits are protected by a cyclic code and the convolutional code, whereas class 1b bits are protected by the convolutional code only. The 50 bits of class 1a are the most important bits in each frame, and the 132 bits of class 1b are less important than the class 1a bits, but more important than the 78 bits of class 2. The FR uses a Cyclic Redundancy Check  CRC  code  cf. Section 2.3.2  with the generator polynomial g x  = x3 + x + 1 for error detection. This code is only applied to the 50 bits in class 1a. As the generator polynomial indicates, the encoding of the CRC code results in three parity bits for error detection.  For the EFR coder, each block from the speech encoder contains only 244 information bits. The block of 244 information bits passes through a preliminary stage, applied only to EFR. This precoding produces 260 bits corresponding to the 244 input bits and 16 redundancy bits. For channel coding, those 260 bits are interpreted as FR data, i.e. as 182 bits of class 1 and 78 bits of class 2. Hence, the channel coding for EFR is essentially the same as for FR. For channel coding, the memory m = 4 convolutional encoder in Figure 3.32 is used. To terminate the code, four zero bits are added to the 185 class 1 bits  including the three CRC bits . Those 189 bits are then encoded with the rate R = 1 2 convolutional code, resulting in 378 bits. These code bits are transmitted together with the uncoded 78 less important bits from class 2, so that every speech frame corresponds to a total of 456 transmitted bits. The 50 speech frames per second yield a transmitted rate of 22.8 kbps.  With mobile communication channels, transmission errors usually occur in bursts of errors, i.e. the channel errors are correlated. This correlation occurs both in time and in frequency direction. We have already seen that convolutional codes can correct much more errors than indicated by the free distance of the code. However, this is only possible if the errors are spread over the code sequence. On the other hand, if a burst error occurs, and more than half the free distance of bits is altered, this may lead to a decoding error. The GSM standard takes different measures to cope with these correlated transmission errors.   CONVOLUTIONAL CODES  149  Convolutional encoder used in GSM  u  b 1   b 2   Figure 3.32: Rate R = 1 2 convolutional encoder memory m = 4  The ﬁrst method is called bit interleaving. Bit interleaving uses the fact that a burst of errors possibly contains a large number of errors within close proximity, but the occurrence of a burst is a relatively unlikely event. Therefore, the code bits are interleaved  permuted  and then transmitted. That way, the errors of a single burst are distributed over the complete block. It is then very likely that the distributed errors will only affect a correctable number of bits. To spread the transmitted bit over a longer period of time, GSM uses a Time Division Multiple Access  TDMA  structure with eight users per frequency band, where a single time slot is smaller than a speech frame. In the GSM standard such a slot is called a normal burst. It has a duration of 0.577 ms corresponding to 156.25-bit periods. However, as a normal burst is used to carry both signalling and user information over the GSM air interface, only 114 bits can by used for the coded speech data. Consequently, a speech frame is distributed over four normal bursts. This is illustrated in Figure 3.33.  If the mobile subscriber is moving, e.g. in a car, the errors in different normal bursts are usually uncorrelated. Therefore, the interleaving in GSM spreads adjacent code bits over different normal bursts. In some GSM cells, frequency hopping is used to provide uncorrelated normal bursts for slow-moving subscribers and as a means against frequency- selective fading and interference. With frequency hopping, the carrier of the transmitting radio signals is rapidly switched to another carrier frequency, i.e. with every normal burst. Figure 3.34 provides some simulation results for the transmission with the GSM FR codec. Obviously, the unprotected class 2 bits have a much higher bit error rate than the coded bits from class 1. The bits of class 1a have a slightly better error protection than class 1b. This results from the code termination and the positions in the interleaving. Owing to code termination, the encoder states at the start and at the end of the terminated code word are known. This leads to a slightly better protection for the information bits close to the code word ends. Moreover, the code bits corresponding to class 1a information bits are located close to the pilot sequence within every normal burst. The pilot symbols are bits known to the receiver that are used for channel estimation. This channel estimation is more reliable for nearby code bits.   150  CONVOLUTIONAL CODES  Normal burst structure  456 bits  114 bits  114 bits  114 bits  114 bits  1  2 3 4 5 6 7 8  1  2 3 4 5 6 7 8  1  2 3 4 5 6 7 8  1  2 3 4 5 6 7 8  Figure 3.33: Normal burst structure of the GSM air interface  The UMTS standard provides several speech and channel coding modes. The AMR speech codec deﬁnes coding methods for data rates from 4.75 to 12.2 kbps, where the 12.2 kbps mode is equivalent to the GSM EFR codec. Moreover, the basic procedure for the convolutional coding is also similar. However, UMTS employs a powerful convolutional code with memory m = 8 and code rate R = 1 3.  The 12.2 kbps speech coding mode uses 20 ms speech frames with 244 data bits per frame. In the UMTS system, those 244 bits are also partitioned into three classes according to their relevance to the speech decoder. Class A contains the 81 most important bits, while class B with 103 bits and class C with 60 bits are less vital. Only the bits of class A are protected with a CRC code. However, this code provides more reliability as eight redundancy bits are used for error detection. All three classes are encoded with the same rate R = 1 3 convolutional code, where each class is encoded separately. Hence, eight tail bits are added for each class. This results in 828 code bits. Then, puncturing is used to reduce the number of code bits to 688 for each speech frame.  3.6.2 Hybrid ARQ  In this section we discuss hybrid ARQ protocols as an application of convolutional codes for mobile communication channels. The principle of automatic repeat-request protocols is to detect and repeat corrupted data. This means that the receiver has to check whether the arriving information is correct or erroneous. Therefore, all ARQ schemes require an error detection mechanism. Usually, a small amount of redundancy is added to the information packet, e.g. with a CRC code.  If the received data packet is correct, the receiver sends an acknowledgement  ACK  to the transmitter, otherwise additional information for the erroneous packet is requested through a not acknowledgement  NACK  command. In the simplest case this retransmission      R E B     e t a R  r o r r E t i  B  100  10 cid:358 1  10 cid:358 2  10 cid:358 3  10 cid:358 4 0  hopping .  CONVOLUTIONAL CODES  Performance of FR coding  151  Class 1a Class 1b Class 2 undetected errors  5  10 C I  15  20    Channel model: Typical Urban  TU  channel with 50 km h  no frequency    Class 2 bits are transmitted without channel coding.    The curve for the undetected errors considers all errors of class 1a that are  not detected by the CRC code.  Figure 3.34: Simulation results for the full-rate coding scheme  request is answered by sending an exact copy of the packet. However, there exist more advanced retransmission schemes which will be discussed in this section.  Pure ARQ systems, where only error detection is used, are not suitable for time-variant mobile channels as successful transmissions become very unlikely for poor channel con- ditions. In order to improve the average throughput for such channels, ARQ systems are combined with additional forward error correction. This combination of ARQ and forward error correction is called hybrid ARQ.  There exist different types of hybrid ARQ. With type-I hybrid ARQ schemes the same encoded packet is sent for transmission and retransmission. Thus, the ARQ protocol does not inﬂuence the forward error correction. With type-II hybrid ARQ schemes, transmission and retransmission differ.  Packet data systems with ARQ schemes require some additional components compared with circuit-switched transmission systems. We have already mentioned the error detec- tion mechanism. The transmission of the acknowledgement  ACK  not acknowledgement  NACK  commands needs a reliable return channel. The transmitter has to store sent packets until an acknowledgement is received, thus transmitter memory is mandatory. Moreover,   152  CONVOLUTIONAL CODES  depending on the particular ARQ protocol, receiver memory may also be required. We will discuss these issues by considering a particular example, the ARQ schemes of the GSM Enhanced General Packet Radio Service  EGPRS .  A basic concept of channel coding and modulation in EGPRS is that the quality of the mobile channel is strongly time varying. Therefore, the modulation and error protection should be adjustable to varying channel conditions. Otherwise the worst-case scenario would determine the average data rate. In EGPRS this adaptation to the link quality is achieved through the deﬁnition of different modulation and coding schemes. In addition to the original GSM Gaussian Minimum Key Shifting  GMSK  modulation, eight-point Phase Shift Keying  8PSK  with 3 bits of data per modulated symbol has been introduced to enhance the throughput of the GSM data services for good channel conditions. The transmitted data are protected by convolutional coding with code rates from 0.37 to nearly 1.0. As the channel might be fast time varying, quick switching between the different modulation and coding schemes is possible.  Note that the same principle of adaptive modulation and coding is also included in the High-Speed Downlink Packet Access  HSDPA  mode deﬁned in the UMTS standard. In addition to the original UMTS four-point Phase Shift Keying  4PSK  modulation, 16-point Quadrature Amplitude Modulation  16QAM  has been introduced. Furthermore, several coding schemes are deﬁned that make it possible to adapt the required redundancy to the channel condition.  3.6.3 EGPRS Modulation and Coding  The well-established GSM standard originally only provided circuit-switched data services with low transmission rates of up to 9.6kbps. High-Speed Circuit-Switched Data  HSCSD  allows rates of up to 57.6 kbps  14.4 kbps time slot , and, with the General Packet Radio Service  GPRS , packet data services with gross data rates of 182 kbps  22.8 kbps time slot  become possible. The new Enhanced Data rates for GSM Evolution  EDGE  standard provides even higher data rates  up to 384 kbps . EDGE covers both ECSD for enhanced circuit-switched connections and EGPRS for enhanced packet data services. This section only deals with the latter standard. EGPRS is the GPRS evolutional upgrade. However, EGPRS preserves the most important GSM air interface features, such as the 200 kHz channelling and the TDMA scheme, i.e. every band of 200 kHz is subdivided into eight time slots. GSM originally used GMSK for modulation, which allowed for 1 bit of data per modulated symbol. In order to enhance the throughput of the GSM data services, a second modulation scheme, 8PSK  with 3 bits of data per modulated symbol  in addition to GMSK, has been introduced.  In order to ensure reliable packet data services with EGPRS, hybrid ARQ protocols will be employed. A link adaptation  type-I hybrid ARQ  algorithm adapts the modulation and coding scheme to the current channel condition. This should provide a mechanism to have a smooth degradation of the data rate for the outer cell areas. The more sophisticated incremental redundancy  type-II hybrid ARQ  schemes automatically adjust themselves to the channel condition by sending additional redundancy for not acknowledged data packets. Applied to time-variant channels, IR schemes allow higher throughputs compared with standard link adaptation schemes. However, for poor channel conditions the average delay may increase dramatically. Therefore, in EGPRS a combination of link adaptation   CONVOLUTIONAL CODES  Radio block structure  153  RLC MAC header  HCS  RLC data  TB  BCS    RLC MAC  radio link control medium access control  header contains  control ﬁelds.    HCS  header check sequence  for header error detection.    RLC data ﬁeld contains data payload.    TB  tail bits  for termination of the convolutional code.    BCS  block check sequence  for error detection in the data ﬁeld.  Figure 3.35: Radio block structure  and incremental redundancy is used. While Link Adaptation  LA  is mandatory in EGPRS, Incremental Redundancy  IR  is optional for the networks.  In Figure 3.35 the EGPRS radio block structure is presented. One radio block consists of one Radio Link Control  RLC  Medium Access Control  MAC  header and one or two RLC data blocks. The RLC MAC header contains control ﬁelds like a packet sequence number. The Header Check Sequence  HCS  ﬁeld is used for header error detection. The payload data are contained in the RLC data ﬁeld. Attached to each RLC data block there is a Tail Bits  TB  and a Block Check Sequence  BCS  ﬁeld. Figure 3.36 gives an overview of the EGPRS channel coding and interleaving structure. The ﬁrst step independently encodes data and header. For headers, rate R = 1 3 tail-biting convolutional codes are used. For data encoding, rate R = 1 3 terminated convolutional codes are employed. In order to adjust the code rates, encoding is followed by puncturing. After puncturing, the remaining bits are interleaved and mapped on four consecutive bursts. For some coding schemes, bit swapping is applied after the interleaving. Figure 3.36 also provides a detailed view of the RLC structure for the example of the modulation and coding scheme Modulation and Coding Scheme  MCS -9.  In contrast to GSM with pure GMSK modulation, EDGE will use both GMSK and 8PSK modulation. GMSK is used as a fall-back solution if 8PSK is not appropriate for the current channel condition. The combination of modulation type and code rate deﬁnes the MCS. A total of nine such modulation and coding schemes exists, partly GMSK and 8PSK modulated. These nine schemes are partitioned into three families  A,B,C , where the families correspond to different segmentations of the data stream from higher layers. Thus, switching without data resegmentation is only possible within one family.  The table in Figure 3.37 provides an overview. With coding schemes MCS 7, MCS 8, and MCS 9, two RLC blocks are transmitted with one radio block, while for all other schemes only one RLC data block per radio block is transmitted. The bit swapping   154  CONVOLUTIONAL CODES  Coding and interleaving structure    General coding and interleaving structure  conv. coding  puncturing  interleaving  bit swapping  burst N  burst N+1  burst N+2  burst N+3    Detailed structure for Modulation and Coding Scheme 9  MCS-9   45 bits  612 bits  612 bits  RLC MAC header HCS  RLC data=592 bits  TB  BCS  RLC data=592 bits  TB  BCS  136 bits  1836 bits  Rate 1 3 convolutional coding  1836 bits  puncturing  124 bits  612 bits  612 bits  612 bits  612 bits  612 bits  612 bits  burst N  burst N+1  burst N+2  burst N+3  Figure 3.36: Coding and interleaving structure for EGPRS  guarantees that, with MCS 8 and MCS 9, data interleaving is done over two bursts, while with MCS 7 the two RLC data blocks are interleaved over four bursts. The RLC MAC header is always interleaved over four normal bursts. Depending on the coding scheme, three different header types are deﬁned: type 1 for MCS 7, 8 and 9, type 2 for MCS 5 and 6 and type 3 for MCS 1, 2, 3 and 4.  For IR the possibility of mixed retransmission schemes MCS 5–7 and MCS 6–9 exists. For instance, the ﬁrst transmission is done with MCS 6 while the retransmissions are carried out with MCS 9. This is possible because MCS 6 and 9 correspond to the same mother   CONVOLUTIONAL CODES  155  EGPRS modulation and coding schemes  MCS  MCS-9 MCS-8 MCS-7 MCS-6  MCS-5 MCS-4 MCS-3  MCS-2 MCS-1  Data code rate 1.0 0.92 0.76 0.49  0.37 1.0 0.8  0.66 0.53  Header code rate 0.36 0.36 0.36 1 3  1 3 0.53 0.53  0.53 0.53  Modu- lation  Data within one radio block 2 × 592 2 × 544 8-PSK 2 × 448  592 544+48 448 352 296 272+24 224 176  GMSK  A A B A  Family Data rate  kbps  59.2 54.4 44.8 29.6 27.2 22.4 17.6 14.8 13.6 11.2 8.8  B C A  B C  Figure 3.37: EGPRS modulation and coding schemes  code and termination length  592 information bits . These retransmission schemes should be more efﬁcient than pure MCS 6 schemes.  3.6.4 Retransmission Mechanism  In terms of average data rate, the selective repeat technique is the most efﬁcient retransmis- sion mechanism. With Selective Repeat ARQ  SR-ARQ , the transmitter continuously sends packets. If the receiver detects an erroneous packet, a NACK command requests a retrans- mission. Upon reception of this NACK, the transmitter stops its continuous transmission, sends the retransmission and then goes on with the next packet.  For SR-ARQ, each packet requires a unique sequence number that allows the receiver to identify arriving packets and to send requests for erroneously received packets. Moreover, the receiver needs memory in order to deliver the data in correct order to the higher layers. In EGPRS a variation of the above selective repeat ARQ is employed. In order to use the return channel effectively, ACK NACK messages are collected and transmitted upon request  polling . If this polling is done periodically, we obtain a block-wise ACK NACK signalling, as indicated in Figure 3.38. Here, the polling period or Acknowledgement Period  AP  is 6, i.e. after six received packets the receiver sends a list of six ACK NACK messages. Of course, this polling scheme introduces an additional delay, so that there is a trade-off between delay and signalling efﬁciency.  The number of possible packet sequence numbers has to be limited, because the trans- mitter and receiver memories are limited. In order to take this fact into account, the transmitter is only allowed to send packets that have sequence numbers within a cer- tain interval  window . If an ACK is received, the window can be shifted to the next not   156  CONVOLUTIONAL CODES  SR-ARQ block-wise ACK NACK signalling  1  2  3  4  5  6  7  8  4  5  9 10  11 12 5  7 13 14  Transmitter  t  t  1  2  3  4  5  6  7  8  4  5  7 13 14  9 10 11 12 5 Receiver    EGPRS uses selective repeat ARQ with polling.    ACK NACK messages are collected and transmitted upon request.  Figure 3.38: SR-ARQ block-wise ACK NACK signalling  acknowledged sequence number. For the example in Figure 3.38, assume the window size  WS  is 10, i.e. for the ﬁrst transmissions sequence numbers within [1, . . . , 10] are allowed. After the ﬁrst ACK NACK signalling, the window can be shifted and we have [4, . . . , 13]. After the second ACK NACK signalling we obtain the window [5, . . . , 14]. A third erro- neous reception of packet 5 would lead to a so-called stalled condition, i.e. the receiver would not be allowed to send new data packets until packet 5 is received successfully. To prevent stalled conditions, window size and acknowledgement period have to be chosen carefully. In EGPRS the maximum window size depends on the multislot class, i.e. on the number of time slots that are assigned to one user. The window size is limited to 192 if only one time slot per carrier is used.  3.6.5 Link Adaptation  The ARQ scheme should be adjustable to varying channel conditions. Otherwise the worst-case scenario would determine the average throughput. For this link adaptation  LA , different coding schemes with different code rates and a switching mechanism are required. In Figure 3.39, simulation results for BPSK modulation and a Gaussian channel are depicted. The ordinate is the average throughput normalised to the maximum through- put. Three different code rates, R = 1, R = 1 2 and R = 1 3, have been used. For good channel conditions the code rate limits the throughput, i.e. pure ARQ  R = 1  outperforms the hybrid schemes. However, for signal-to-noise ratios below 12.5 dB, successful trans- missions become very unlikely with pure ARQ. At this point the hybrid scheme with code rate R = 1 2 still provides optimum performance. For very poor channel conditions the rate R = 1 3 scheme performs best. Each scheme in Figure 3.39 outperforms the others over a certain Es N0 region. Outside this range, either the throughput falls rapidly or the   CONVOLUTIONAL CODES  157  Throughput of type-I ARQ  Pure ARQ TypeI 1 2 TypeI 1 3  1  0.9  0.8  0.7  t u p h g u o r h t  0.6  0.5  0.4  0.3  0.2  0.1  0   5  10  15  20  E  N s  0    Each coding scheme outperforms the others over a certain Es N0 region.    A link adaptation scheme should choose the coding scheme with best  possible average throughput for the current channel condition.    The maximum possible throughput with link adaptation is the envelope of  all throughput curves.  Figure 3.39: Throughput of type-I ARQ for a Gaussian channel  coding overhead limits the maximum throughput. Hence, as long as the channel is static around a speciﬁc operational point, type-I schemes perform well.  In mobile channels the signal-to-noise ratio is time variant. Fast power control can provide nearly static channel conditions. However, there are limits to power control, e.g. transmit power limits or for high mobile velocity. A link adaptation scheme should choose the coding scheme with best possible average throughput for the current channel condition. Therefore, we can state the maximum possible throughput with link adapta- tion, i.e. the envelope of all throughput curves, without actually considering the switch- ing algorithm.  3.6.6 Incremental Redundancy  The basic property of type-II hybrid ARQ is that the error correction capability increases with every retransmission. This can be done by storing erroneous packets  soft values  to   158  CONVOLUTIONAL CODES  use them as additional redundancy for later versions of the same packet. The amount of redundancy is automatically adjusted to the current channel condition. As the redundancy increases with each retransmission, this technique is called incremental redundancy  IR . If the retransmission request is answered by sending an exact copy of the packet  as with pure ARQ , we call the IR technique diversity combining. Here the soft values of different transmissions are combined. For this technique, additional receiver memory for the soft values is required. Moreover, the soft combining increases receiver complexity. However, the error correction capability increases with every retransmission.  Another possibility is to encode the data packets before transmission, e.g. with a con- volutional code, but to send only a segment of the complete code sequence with the ﬁrst transmission. If retransmissions are necessary, additional code segments are sent.  Throughput of type-II hybrid ARQ  M=unlimited M=0  no IR  IR without coding gain  ] s p b k [   t  u p h g u o r h  t  60  50  40  30  20  10  0 0  5  10  [dB] 0   N  E s  15  20    With diversity combining, we notice remarkable gains compared with pure  ARQ.  tional coding gain.  condition.    Forward error correction with maximum ratio combining achieves an addi-    The amount of redundancy is automatically adjusted to the current channel  Figure 3.40: Throughput of type-II hybrid ARQ for a Gaussian channel   CONVOLUTIONAL CODES  159  Consider, for example, the coding scheme MCS-9 which is presented in Figure 3.36. Two data packets share a common packet header which contains a unique identiﬁer for each data packet. The header and the data packets are encoded with a rate 1 3 convolutional code with memory m = 6. For the data part, code termination is applied. The shorter header block is encoded with a tail-biting code. The code word for the header is punctured and the resulting 124 bits are equally distributed over one frame, i.e. four time slots. The code bits of the two data blocks are subdivided into three equally large code segments of 612 bits. Such a code segment is interleaved over two bursts. For every data block of 592 bits, only one code segment of 612 bits is transmitted per frame. Additional code segments are transmitted on request.  Of course, the number of different code segments is limited. If more retransmissions are necessary, copies of already sent code segments will be transmitted. If these copies are only exchanged for the already received versions and not combined, we call the receiver technique pure code combining. If copies are combined, we have maximum ratio combining. In Figure 3.40, simulation results for a Gaussian channel with the EGPRS coding scheme MCS-9 are depicted. We compare pure ARQ  no IR  with IR. For the incremental redundancy schemes we have no memory constraints. Here, we use diversity combining and maximum ratio combining.  Performance for a mobile communication channel  no IR IR  60  50  40  30  20  10  ] s p b k [   t u p h g u o r h t  0 0  5  10  15 20 C I [dB]  25  30  35    Performance for the typical urban  TU  channel model with a mobile velocity  of 3 km h  throughput versus carrier-to-interference ratio .    MCS-9 with incremental redundancy and without code combining.    Incremental redundancy achieves a coding gain of up to 10 dB without  explicit link adaptation.  Figure 3.41: Throughput of type-II hybrid ARQ for a mobile communication channel   160  CONVOLUTIONAL CODES  With diversity combining, we already notice remarkable gains compared with pure ARQ. However, maximum ratio combining utilises an additional coding gain. This coding gain is the reason for the plateau within the curve. In this region of signal-to-noise ratios it becomes very unlikely that the ﬁrst transmission is successful. Yet, the ﬁrst retransmission increases the redundancy, so that the need for further retransmission also becomes unlikely. Figure 3.41 presents the performance of coding scheme MCS-9 for a mobile commu- nication channel. For the mobile channel we used the typical urban  TU  channel model as deﬁned in the GSM standard with a mobile velocity of 3 km h and a single interfere. Owing to the time-varying nature of the mobile channel, the curve for the IR scheme shows no plateau as for the stationary gaussian channel. Compared with the transmission without code combining, the performance is now improved for all data rates below 50 kbps, where the gain in terms of carrier-to-interference ratio increases for low data rates up to 10 dB. Incremental redundancy schemes with convolutional coding are usually based on rate- compatible punctured convolutional codes. With these punctured codes, a rate compatibility restriction on the puncturing tables ensures that all code bits of high rate codes are used by the lower-rate codes. These codes are almost as good as the best known general convo- lutional codes of the respective rates. Tables of good rate-compatible punctured codes are given elsewhere  Hagenauer, 1988 .  3.7 Summary  This chapter should provide a basic introduction to convolutional coding. Hence, we have selected the topics that we think are of particular relevance to today’s communication systems and to concatenated convolutional codes which will be introduced in Chapter 4. However, this chapter is not a comprehensive introduction to convolutional codes. We had to omit much of the algebraic and structural theory of convolutional codes. Moreover, we had to leave out many interesting decoding algorithms. In this ﬁnal section we summarise the main issues of this chapter and make references to some important publications.  Convolutional codes were ﬁrst introduced by Elias  Elias, 1955 . The ﬁrst decoding method for convolutional codes was sequential decoding which was introduced by Wozen- craft  Wozencraft 1957; see also Wozencraft and Reiffen 1961 . Sequential decoding was further developed by Fano  Fano, 1963 , Zigangirov  Zigangirov, 1966  and Jelinek  Jelinek, 1969 .  The widespread Viterbi algorithm is a maximum likelihood decoding procedure that is based on the trellis representation of the code  Viterbi, 1967 . This concept, to represent the code by a graph, was introduced by Forney  Forney, Jr, 1974 . Owing to the highly repetitive structure of the code trellis, trellis-based decoding is very suitable for pipelining hardware implementations. Consequently, maximum likelihood decoding has become much more popular for practical applications than sequential decoding, although the latter decoding method has a longer history.  Moreover, the Viterbi algorithm is very impervious to imperfect channel identiﬁcation. On the other hand, the complexity of Viterbi decoding grows exponentially with the overall constraint length of the code. Today, Viterbi decoders with a constraint length of up to 9 are found in practical applications. Decoding of convolutional codes with a larger constraint length is the natural domain of sequential decoding, because its decoding complexity is determined by the channel condition and not by the constraint length. Sequential decoding is   CONVOLUTIONAL CODES  161  therefore applied when very low bit error rates are required. Recently, sequential decoding has attracted some research interest for applications in hybrid ARQ protocols  Kallel, 1992; Orten, 1999 . A good introduction to sequential decoding can be found in the literature  Bossert, 1999; Johannesson and Zigangirov, 1999 .  The BCJR algorithm is a method for calculating reliability information for the decoder output  Bahl et al., 1974 . This so-called soft output is essential for the decoding of con- catenated convolutional codes  turbo codes , which we will discuss in Chapter 4. Like the Viterbi algorithm, the BCJR algorithm is also based on the code trellis which makes it suitable for hardware as well as for software implementations.  Early contributions to the algebraic and structural theory of convolutional codes were made by Massey and Sain  Massey and Sain, 1967, 1968  and by Forney  Forney, Jr, 1970, 1973a , and more recently by Forney again  Forney, Jr, 1991; Forney, Jr et al., 1996 . Available books  Bossert, 1999; Johannesson and Zigangirov, 1999  are good introductory texts to this subject. McEliece provides a comprehensive exposition of the algebraic theory of convolutional codes  McEliece, 1998 .  As mentioned above, this chapter has focused on the application of convolutional codes in today’s communication systems. In Section 3.6 we have considered some examples for convolutional coding in mobile communications. In particular, we have discussed the speech coding and the hybrid ARQ protocols as deﬁned in the GSM standard. Similar concepts are also applied in UMTS mobile communications. Some further results about the GSM link adaptation and incremental redundancy schemes can be found in the literature  Ball et al., 2004a,b . We would like to acknowledge that the simulation results in Section 3.6 are published by courtesy of Nokia Siemens Networks.5  5Nokia Siemens Networks, COO RA RD System Architecture, RRM, Radio and Simulations, GSM EDGE &  OFDM Mobile Radio, Sankt-Martinstrasse 76, 81617 Munich, Germany.    4  Turbo Codes  In this chapter we will discuss the construction of long powerful codes based on the concatenation of simple component codes. The ﬁrst published concatenated codes were the product codes introduced by Elias,  Elias, 1954 . The concatenation scheme according to Figure 4.1 was introduced and investigated by Forney  Forney, Jr, 1966  in his PhD thesis. With this serial concatenation scheme the data are ﬁrst encoded with a so-called outer code, e.g. a Reed–Solomon code. The code words of this outer code are then encoded with a second, so-called inner code, for instance a binary convolutional code.  After transmission over the noisy channel, ﬁrst the inner code is decoded, usually using soft-input decoding. The inner decoding results in smaller bit error rates at the output of the inner decoder. Therefore, we can consider the chain of inner encoder, channel and inner decoder as a superchannel with a much smaller error rate than the original channel. Then, an outer, usually algebraic, decoder is used for correcting the residual errors. This two-stage decoding procedure has a much smaller decoding complexity compared with the decoding of a single code of the same overall length. Such classical concatenation schemes with an outer Reed–Solomon code and an inner convolutional code  Justesen et al., 1988  are used in satellite communications as well as in digital cellular systems such as the Global System for Mobile communications  GSM .  During recent years, a great deal of research has been devoted to the concatenation of convolutional codes. This research was initiated by the invention of the so-called turbo codes  Berrou et al., 1993 . Turbo codes are a class of error-correcting codes based on a parallel concatenated coding scheme, where at least two systematic encoders are linked by an interleaver. In the original paper, Berrou, Glavieux and Thitimasjshima showed by simulation that turbo codes employing convolutional component codes are capable of −5 at a code rate of R = 1 2 and a signal-to-noise achieving bit error rates as small as 10 ratio Eb N0 of just 0.7 dB above the theoretical Shannon limit.  However, in order to achieve this level of performance, large block sizes of several thousand code bits are required. The name turbo reﬂects a property of the employed iterative decoding algorithm: the decoder output of one iteration is used as the decoder input of the next iteration. This cooperation of the outer and inner decoder is indicated by the feedback link in Figure 4.1.  Coding Theory – Algorithms, Architectures, and Applications Andr´e Neubauer, J¨urgen Freudenberger, Volker K¨uhn  cid:148  2007 John Wiley & Sons, Ltd   164  TURBO CODES  Basic concept of code concatenation  superchannel  outer encoder  inner encoder  outer decoder  inner decoder  channel  feedback  Figure 4.1: Basic concept of code concatenation  The concept of iterative decoding is, however, much older than turbo codes. It was introduced by Gallager  Gallager, 1963 , who also invented the class of Low-Density Par- ity Check  LDPC  codes. In fact, turbo codes can be considered as a particular construction of low-density parity-check codes. Yet, LDPC codes were largely forgotten soon after their invention. There are several reasons why LDPC codes were initially neglected. First of all, the computational power to exploit iterative decoding schemes was not available until recently. In the early years of coding theory, Reed–Solomon codes with algebraic decoding were much more practical. Furthermore, the concatenated Reed–Solomon and convolutional codes were considered perfectly suitable for error control coding.  The reinvention of iterative decoding by Berrou, Glavieux and Thitimasjshima also led to the rediscovery of LDPC codes  see, for example,  MacKay, 1999 . LDPC codes have a remarkably good performance; for example, LDPC codes with a code length of 1 million were constructed with a signal-to-noise ratio of only 0.3 dB above the Shannon limit  Richardson et al., 2001 .  Today, there exist numerous publications on the construction of LDPC codes and turbo codes that also show the excellent performance of these code constructions. It is not possible to cover even a modest fraction of these publications in a single book chapter. Moreover, we have to leave out much of the interesting theory. Hence, this text is not a survey. Nevertheless, we hope that this chapter provides an interesting and useful introduction to the subject. Our discussion focuses on the basic properties of concatenated convolutional codes. We start with an introduction to LDPC codes and Tanner graphs in Section 4.1. In Section 4.2 we introduce product codes as a ﬁrst example of code concatenation that also illustrates the connections between concatenated codes and LDPC codes. We discuss the   TURBO CODES  165  encoding and decoding of turbo-like codes in Section 4.3. Then, we present three different methods to analyse the code properties of concatenated convolutional codes.  In Section 4.4 we consider the analysis of the iterative decoding algorithm for con- catenated convolutional codes. Therefore, we utilise the extrinsic information transfer characteristics as proposed by ten Brink  ten Brink, 2000 .  One of the ﬁrst published methods to analyse the performance of turbo codes and serial concatenations was proposed by Benedetto and Montorsi  Benedetto and Montorsi, 1996, 1998 . With this method we calculate the average weight distribution of an ensemble of codes. Based on this weight distribution it is possible to bound the average maximum likelihood performance of the considered code ensemble. This method will be discussed in Section 4.5. Later on, in Section 4.6, we will focus our attention on the minimum Hamming distance of the concatenated codes. By extending earlier concepts  H¨ost et al., 1999 , we are able to derive lower bounds on the minimum Hamming distance. Therefore, we discuss the class of woven convolutional codes that makes it possible to construct codes with large minimum Hamming distances.  4.1 LDPC Codes  LDPC codes were originally invented by Robert Gallager in his PhD thesis  Gallager, 1963 . They were basically forgotten shortly after their invention, but today LDPC codes are among the most popular topics in coding theory.  Any linear block code can be deﬁned by its parity-check matrix. If this matrix is sparse, i.e. it contains only a small number of 1s per row or column, then the code is called a low-density parity-check code. The sparsity of the parity-check matrix is a key property of this class of codes. If the parity-check matrix is sparse, we can apply an efﬁcient iterative decoding algorithm.  4.1.1 Codes Based on Sparse Graphs  Today, LDPC codes are usually deﬁned in terms of a sparse bipartite graph, the so-called Tanner graph  Tanner, 1981 .1 Such an undirected graph has two types of node, the message nodes and check nodes.2  Figure 4.2 provides the Tanner graph of the Hamming code B 7, 4, 3   cf. Section 2.2.8 where an equivalent Hamming code is used . The n nodes on the left are the message nodes. These nodes are associated with the n symbols b1, . . . , bn of a code word. The r nodes c1, . . . , cr on the right are the so-called check nodes and represent parity-check equations. For instance, the check node c1 represents the equation b1 ⊕ b4 ⊕ b5 ⊕ b7 = 0. Note that in this section we use the symbol ⊕ to denote the addition modulo 2 in order to distinguish it from the addition of real numbers. The equation b1 ⊕ b4 ⊕ b5 ⊕ b7 = 0 is determined by the edges connecting the check node c1 with the message nodes b1, b4, b5  1A graph is the basic object of study in a mathematical discipline called graph theory. Informally speaking, a graph is a set of objects called nodes connected by links called edges. A sparse graph is a graph with few edges, and a bipartite graph is a special graph where the set of all nodes can be divided into two disjoint sets A and B such that every edge has one end-node in A and one end-node in B.  2In an undirected graph the connections between nodes have no particular direction, i.e. an edge from node i  to node j is considered to be the same thing as an edge from node j to node i.   166  TURBO CODES  Tanner graph of the Hamming code    The parity-check matrix of the Hamming code B 7, 4, 3     1 0 0 1 1 0 1  0 1 0 1 0 1 1 0 0 1 0 1 1 1     H =  can be considered as adjacency matrix of the following Tanner graph  b1  b2  b3  b4  b5  b6  b7  c1 : b1 ⊕ b4 ⊕ b5 ⊕ b7 = 0  c2 : b2 ⊕ b4 ⊕ b6 ⊕ b7 = 0  c3 : b3 ⊕ b5 ⊕ b6 ⊕ b7 = 0  Figure 4.2: Tanner graph of the Hamming code  and b7. Note that edges only connect two nodes not residing in the same class. A vector b =  b1, . . . , bn  is a code word if and only if all parity-check equations are satisﬁed, i.e. for all check nodes the sum of the neighbouring positions among the message nodes is zero. Hence, the graph deﬁnes a linear code of block length n. The dimension is at least k = n − r. It might be larger than n − r, because some of the check equations could be linearly dependent. Actually, the r × n parity-check matrix can be considered as the adjacency matrix of the Tanner graph.3 The entry hj i of the parity-check matrix H is 1 if and only if the jth check node is connected to the ith message node. Consequently, the jth row of the parity- check matrix determines the connections of the check node cj , i.e. cj is connected to all message nodes corresponding to 1s in the jth row. We call those nodes the neighbourhood of cj . The neighbourhood of cj is represented by the set Pj = {i : hj i = 1}. Similarly, the 1s in the ith column of the parity-check determine the connections of the message node 3In general, the adjacency matrix A for a ﬁnite graph with N nodes is an N × N matrix where the entry aij is the number of edges connecting node i and node j . In the special case of a bipartite Tanner graph, there exist no edges between check nodes and message nodes. Therefore, we do not require a square  n + r  ×  n + r  adjacency matrix. The parity-check matrix is sufﬁcient. For sparse graphs an adjacency list is often the preferred representation because it uses less space.   TURBO CODES  167  Tanner graph of a regular code    All message nodes have the degree 2.    All check nodes have the degree 3.  b1  b2  b3  b4  b5  b6  b7  b8  b9  b1 ⊕ b2 ⊕ b5 = 0 b1 ⊕ b3 ⊕ b7 = 0 b2 ⊕ b4 ⊕ b8 = 0 b3 ⊕ b4 ⊕ b6 = 0 b5 ⊕ b6 ⊕ b9 = 0 b7 ⊕ b8 ⊕ b9 = 0  Figure 4.3: Tanner graph of a regular code  bi. We call all check nodes that are connected to bi the neighbourhood of bi and denote it by the set Mi = {j : hj i = 1}. For instance, we have P1 = {1, 4, 5, 7} and M7 = {1, 2, 3} for the Tanner graph in Figure 4.2.  The Tanner graph in Figure 4.2 deﬁnes an irregular code, because the different message nodes have different degrees  different numbers of connected edges .4 A graph where all message nodes have the same degree and all check nodes have the same degree results in a regular code. An example of a regular code is given in Figure 4.3. The LDPC codes as invented by Gallager were regular codes. Gallager deﬁned the code with the parity-check matrix so that every column contains a small ﬁxed number dm of 1s and each row contains a small ﬁxed number dc of 1s. This is equivalent to deﬁning a Tanner graph with message node degree dm and check node degree dc.  4In graph theory, the degree of a node is the number of edges incident to the node.   168 4.1.2 Decoding for the Binary Erasure Channel  TURBO CODES  Let us now consider the decoding of an LDPC code. Actually there is more than one such decoding algorithm. There exists a class of algorithms that are all iterative procedures where, at each round of the algorithm, messages are passed from message nodes to check nodes, and from check nodes back to message nodes. Therefore, these algorithms are called message-passing algorithms. One important message-passing algorithm is the belief propagation algorithm which was also presented by Robert Gallager in his PhD thesis  Gallager, 1963 . It is also used in Artiﬁcial Intelligence  Pearl, 1988 . In order to introduce this message passing, we consider the Binary Erasure Channel  BEC . The input alphabet of this channel is binary, i.e. F2 = {0, 1}. The output alphabet consists of F2 and an additional element, called the erasure. We will denote an erasure by a question mark. Each bit is either transmitted correctly or it is erased where an erasure occurs with probability ε. Note that the capacity of this channel is 1 − ε. Consider, for instance, the code word b =  1, 0, 1, 0, 1, 1, 0, 0, 0  of the code deﬁned by the Tanner graph in Figure 4.3. After transmission over the BEC we may receive the vector r =  1, ?, 1, ?, ?, 1, 0, 0, ? .  How can we determine the erased symbols? A simple method is the message passing illustrated in Figure 4.4. In the ﬁrst step we assume that all message nodes send the received values to the check nodes. In the second step we can evaluate all parity-check equations.  Message passing for the Binary Erasure Channel  BEC   1st iteration 1  2nd iteration 1  ?  1  ?  ?  1  0  0  ?  b2 = b1 ⊕ b5 =? b1 = b3 ⊕ b7 = 1 b2 = b4 ⊕ b8 =? b4 = b3 ⊕ b6 = 0 b5 = b6 ⊕ b9 =? b9 = b7 ⊕ b8 = 0  ?  1  0  ?  1  0  0  0  Figure 4.4: Message passing for the BEC  b2 = b4 ⊕ b8 = 0  b5 = b6 ⊕ b9 = 1   TURBO CODES 169 If a single symbol in one of the equations is unknown, e.g. b4 in b3 ⊕ b4 ⊕ b6, then the parity-check equation determines the value of the erased symbol. In our example, b4 = b3 ⊕ b6 = 0. However, if more than one erasure occurs within a parity-check equation, we cannot directly infer the corresponding values. In this case we assume that evaluation of the parity-check equation results in an erasure, e.g. b2 = b1 ⊕ b5 =?. All results from the parity-check equations are then forwarded to the message nodes. Basically, every check node will send three messages. In Figure 4.4 we have only stated one equation per check node. The connection for this message is highlighted. With this ﬁrst iteration of the message- passing algorithm we could determine the bits b4 = 0 and b9 = 0. Now, we run the same procedure for a second iteration, where the message nodes b4 and b9 send the corrected values. In Figure 4.4 we have highlighted the two important parity-check equations and connections that are necessary to determine the remaining erased bits b2 = 0 and b5 = 1.  4.1.3 Log-Likelihood Algebra  To discuss the general belief propagation algorithm, we require the notion of a log-likelihood ratio. The log-likelihood algebra, as introduced in this section, was developed by Hagenauer  cid:1 } denote the  Hagenauer et al., 1996 . Let x be a binary random variable and let Pr{x = x . The log-likelihood ratio of x probability that the random variable x takes on the value x is deﬁned as   cid:1   L x  = ln  Pr{x = 0} Pr{x = 1} ,  where the logarithm is the natural logarithm. The log-likelihood ratio L x  is also called the L-value of the binary random variable x. From the L-value we can calculate the probabilities  Pr{x = 0} =  1 1 + e −L x   and Pr{x = 1} =  1  1 + eL x   .  If the binary random variable x is conditioned on another random variable y, we obtain the conditional L-value  L xy  = ln  Pr{x = 0y} Pr{x = 1y} = ln  Pr{yx = 0} Pr{yx = 1} + ln  Pr{x = 0} Pr{x = 1} = L yx  + L x .  Consider, for example, a binary symbol transmitted over a Gaussian channel. With binary phase shift keying we will map the code bit bi = 0 to the symbol −1 and bi = 1 to +1. Let ri be the received symbol. For the Gaussian channel with variance σ and signal-to-noise ratio Es N0  2σ 2 , we have the probability density function  = 1  p ribi   = 1√ 2π σ  exp   cid:6 − ri −  1 − 2bi   2   cid:7   2σ 2  .  Thus, for the conditional L-values we obtain L ribi   = ln = ln  Pr{ribi = 0} Pr{ribi = 1} p ribi = 0  p ribi = 1    170  TURBO CODES   cid:12   cid:12   N0   ri − 1 2  ri + 1 2  − Es − Es  ri − 1 2 −  ri + 1 2  N0   cid:3    cid:11   cid:11   cid:2   exp  = ln  exp = − Es N0  = 4  Es N0  ri .  As L ribi   only depends on the received value ri and the signal-to-noise ratio, we will usually use the shorter notation L ri   for L ribi  . The a-posteriori L-value L biri   is therefore  L biri   = L ri   + L bi   = 4  ri + L bi  .  Es N0  %  The basic properties of log-likelihood ratios are summarised in Figure 4.5. Note that the hard decision of the received symbol can be based on this L-value, i.e.  ˆbi =  if L biri   > 0 ,i.e. Pr{bi = 0ri} > Pr{bi = 1ri} if L biri   < 0 ,i.e. Pr{bi = 0ri} < Pr{bi = 1ri}  .  0 1  Furthermore, note that the magnitude L biri   is the reliability of this decision. To see this, assume that L biri   > 0. Then, the above decision rule yields an error if the transmitted bit was actually bi = 1. This happens with probability  Pr{bi = 1} =  1 1 + eL bi  ri    , L biri   > 0.  Now, assume that L biri   < 0. A decision error occurs if actually bi = 0 was transmitted. This event has the probability Pr{bi = 0} =  ri   , L biri   < 0.  1 −L bi  1 L bi  ri    =  1 + e  1 + e  Hence, the probability of a decision error is  Pr{bi  cid:7 = ˆbi} =  1 L bi  ri    1 + e  and for the probability of a correct decision we obtain ri   L bi Pr{bi = ˆbi} = e 1 + e L bi  ri   .  Up to now, we have only considered decisions based on a single observation. In the following we deal with several observations. The resulting rules are useful for decoding. If the binary random variable x is conditioned on two statistically independent random variables y1 and y2, then we have Pr{x = 0y1, y2} Pr{x = 1y1, y2} = ln L xy1, y2  = ln = L y1x  + L y2x  + L x .  Pr{y2x = 0} Pr{y2x = 1} + ln  Pr{y1x = 0} Pr{y1x = 1} + ln  Pr{x = 0} Pr{x = 1}   171   4.1    4.2   TURBO CODES  Log-likelihood ratios    The log-likelihood ratio of the binary random variable x is deﬁned as  L x  = ln  Pr{x = 0} Pr{x = 1}    From the L-value we can calculate the probabilities and Pr{x = 1} =  Pr{x = 0} =  1 1 + e −L x   1  1 + eL x     If the binary random variable x is conditioned on another random variable y, or on two statistically independent random variables y1 and y2, we obtain the conditional L-values  L xy  = L yx  + L x  or L xy1, y2  = L y1x  + L y2x  + L x    4.3     For the Gaussian channel with binary phase shift keying and signal-to-noise  ratio Es N0  we have the a-posteriori L-value  L biri   = L ribi   + L bi   = 4  ri + L bi    Es N0   4.4   where bi is the transmitted bit and ri is the received symbol.  Figure 4.5: Log-likelihood ratios  This rule is useful whenever we have independent observations of a random variable, for example for decoding a repetition code. Consider, for instance, the code B = { 0, 0 ,  1, 1 }. We assume that the information bit u is equally likely to be 0 or 1. Hence, for a memoryless symmetrical channel we can simply sum over the received L-values to obtain L ur  = L r1  + L r2  with the received vector r =  r1, r2 . Consider now two statistically independent random variables x1 and x2. Let ⊕ denote the addition modulo 2. Then, x1 ⊕ x2 is also a binary random variable with the L-value L x1 ⊕ x2 . This L-value can be calculated from the values L x1  and L x2  Pr{x1 = 0}Pr{x2 = 0} + Pr{x1 = 1}Pr{x2 = 1} Pr{x1 = 1}Pr{x2 = 0} + Pr{x1 = 0}Pr{x2 = 1} .  Using Pr{x1 ⊕ x2 = 0} = Pr{x1 = 0}Pr{x2 = 0} +  1 − Pr{x1 = 0}  1 − Pr{x2 = 0}  and Equation  4.2 , we obtain  L x1 ⊕ x2  = ln  Pr{x1 ⊕ x2 = 0} =  1 + eL x1 eL x2    1 + eL x1   1 + eL x2    .   TURBO CODES  172  Similarly, we have  which yields  Pr{x1 ⊕ x2 = 1} =  eL x1  + eL x2    1 + eL x1   1 + eL x2    L x1 ⊕ x2  = ln  1 + eL x1 eL x2  eL x1  + eL x2   .  This operation is called the boxplus operation, because the symbol  cid:3  is usually used for notation, i.e.  L x1 ⊕ x2  = L x1   cid:3  L x2  = ln  1 + eL x1 eL x2  eL x1  + eL x2   .  Later on we will see that the boxplus operation is a signiﬁcant, sometimes dominant portion of the overall decoder complexity with iterative decoding. However, a ﬁxed-point Digital Signal Processor  DSP  implementation of this operation is rather difﬁcult. Therefore, in practice the boxplus operation is often approximated. The computationally simplest estimate is the so-called max-log approximation  L x1   cid:3  L x2  ≈ sign L x1  · L x2   · min{L x1 ,L x2 } .  The name expresses the similarity to the max-log approximation introduced in Section 3.5.2. Both approximations are derived from the Jacobian logarithm.  Besides a low computational complexity, this approximation has another advantage, i.e. the estimated L-values can be arbitrarily scaled, because constant factors can be cancelled. Therefore, an exact knowledge of the signal-to-noise ratio is not required. The max-log approximation is illustrated in Figure 4.6 for a ﬁxed value of L x2  = 2.5. We observe that maximum deviation from the exact solution occurs for L x1 − L x2  = 0.  We now use the boxplus operation to decode a single parity-check code B 3, 2, 2  after transmission over the Additive White Gaussian Noise  AWGN  channel with a signal-to- noise ratio of 3 dB  σ = 0.5 . Usually, we assume that the information symbols are 0 or 1 with a probability of 0.5. Hence, all a-priori L-values L bi   are zero. Assume that the code word b =  0, 1, 1  was transmitted and the received word is r =  0.71, 0.09,−1.07 . = 8. To obtain the corresponding channel L-values, we have to multiply r by 4 Es Hence, we have L r0  = 5.6, L r1  = 0.7 and L r2  = −8.5. In order to decode the code, N0 we would like to calculate the a-posteriori L-values L bir . Consider the decoding of the ﬁrst code bit b0 which is equal to the ﬁrst information bit u0. The hard decision for the information bit ˆu0 should be equal to the result of the modulo addition ˆb1 ⊕ ˆb2. The log- likelihood ratio of the corresponding received symbols is L r1   cid:3  L r2 . Using the max-log approximation, this can approximately be done by  = 2 σ 2  Le u0  = L r1   cid:3  L r2   ≈ sign  L r1  · L r2   · min{L r1 ,L r2 } ≈ sign 0.7 ·  −8.5   · min{0.7, − 8.5} ≈ −0.7.   TURBO CODES  Boxplus operation  173    For x1 and x2, two statistically independent binary random variables, x1 ⊕ x2 is also a binary random variable. The L-value L x1 ⊕ x2  of this random variable is calculated with the boxplus operation  L x1 ⊕ x2  = L x1   cid:3  L x2  = ln  1 + eL x1 eL x2  eL x1  + eL x2    4.5     This operation can be approximated by  L x1   cid:3  L x2  ≈ sign L x1  · L x2   · min{L x1 ,L x2 }   4.6   as illustrated in the following ﬁgure for L x2  = 2.5  exact value approximation  3  2  1  0   cid:358 1   cid:358 2    2 x   L  cid:3     1 x   L   cid:358 3  cid:358 10  cid:358 8  cid:358 6  cid:358 4  cid:358 2  0  2  4  6  8 10  L x1   Figure 4.6: Illustration of the boxplus operation and its approximation. Reprinted with  permission from  cid:148  2001 IEEE.  The value Le u0  is called an extrinsic log-likelihood ratio. It can be considered as the information that results from the code constraints. Note that this extrinsic information is statistically independent of the received value r0. Therefore, we can simply add Le u0  and L u0r0  = L r0  to obtain the a-posteriori L-value  L u0r  = L r0  + Le u0  ≈ 4.9.  For the two other bits we calculate the extrinsic L-values Le u1  = −5.6 and Le b2  = 0.7, as well as the a-posteriori L-values L u1r  = −4.9 and L b2r  = −7.7. The hard decision results in ˆb =  0, 1, 1 .   174 4.1.4 Belief Propagation  TURBO CODES  The general belief propagation algorithm is also a message-passing algorithm similar to the one discussed in Section 4.1.2. The difference lies in the messages that are passed between nodes. The messages passed along the edges in the belief propagation algorithm are log-likelihood values. Each round of the algorithm consists of two steps. In the ﬁrst half-iteration, a message is sent from each message node to all neighbouring check nodes. In the second half-iteration, each check node sends a message to the neighbouring message nodes. Let Ll[bi → cj ] denote the message from a message node bi to a check node cj in the lth iteration. This message is computed on the basis of the observed channel value ri and some of the messages received from the neighbouring check nodes except cj according to Equation  4.7  in Figure 4.7. It is an important aspect of belief propagation that the message sent from a message node bi to a check node cj must not take into account the message sent in the previous round from cj to bi. Therefore, this message is explicitly excluded in the update Equation  4.7 . Remember that Mi denotes the neighbourhood of the node bi. The message Ll[cj → bi] from the check node cj to the message node bi is an extrinsic log-likelihood value based on the parity-check equation and the incoming messages from all neighbouring message nodes except bi. The update rule is given in Equation  4.8 . The symbol    cid:3  denotes the sum with respect to the boxplus operation.  Consider now the code deﬁned by the Tanner graph in Figure 4.3. We consider transmis- sion over a Gaussian channel with binary phase shift keying. Suppose the transmitted code word is b =  +1,−1,−1,−1,−1,+1,−1,+1,−1 . For this particular code word we may · r =  5.6,−10.2, 0.7, 0.5,−7.5, 12.2, obtain the following channel L-values L r  = 4 Es −8.5, 6.9,−7.7 . In the ﬁrst step of the belief propagation algorithm the message nodes pass these received values to the neighbouring check nodes. At each check node we calcu- late a message for the message nodes. This message takes the code constraints into account. For the ﬁrst check node this is the parity-check equation b1 ⊕ b2 ⊕ b5 = 0. Based on this  N0  Update equations for belief propagation    Messages from message nodes to check nodes  %  L ri   +  L ri    Ll[bi → cj ] =   cid:1 ∈M  i  j  \{j} Ll−1[cj   cid:1  → bi]  if l = 1 if l > 1   cid:1    Messages from check nodes to message nodes  cid:3  Ll−1[bi \{i}  Ll[cj → bi] =   cid:1 ∈P  i  j   cid:1  → cj ]   4.7    4.8   Figure 4.7: Update equations for the message passing with belief propagation   TURBO CODES  175  Example of belief propagation  1st iteration  2nd iteration  L2[b1 → c1] ≈ 4, 6  L2[b1 → c2] ≈ 13, 1  5.6 −10.2  0.7  0.5 −7.5  12.2 −8.5  6.9 −7.7  L1[c1 → b2] ≈ −5.6 L1[c2 → b1] ≈ −0.7 L1[c3 → b2] ≈ 0.5 L1[c4 → b4] ≈ 0.7 L1[c5 → b5] ≈ −7.7 L1[c6 → b9] ≈ −6.9  12.4 −15.3 −4.4 −5.7 −20.8  20.2 −14.7  14.1 −22.1  equation, we calculate three different extrinsic messages for the three neighbouring message nodes  Figure 4.8: Example of belief propagation  Le b1  = L b2 ⊕ b5  = L b2   cid:3  L b5  ≈ 7.5, Le b2  = L b1 ⊕ b5  = L b1   cid:3  L b5  ≈ −5.6, Le b5  = L b1 ⊕ b2  = L b1   cid:3  L b2  ≈ −5.6.  Similarly, we can calculate the messages for the remaining check nodes. Some of these messages are provided in Figure 4.8, where the corresponding connections from check node to message node are highlighted.  The extrinsic L-values from the check nodes are based on observations that are sta- tistically independent from the received value. Therefore, we can simply add the received channel L-value and the extrinsic messages for each message node. For instance, for the node b1 we have L b1  = L r1  + L b2 ⊕ b5  + L b3 ⊕ b7  ≈ 12.4. If we do this for all message nodes we obtain the values given in Figure 4.8. These are the L-values for the message nodes after the ﬁrst iteration.  As mentioned above, it is an important aspect of belief propagation that we only pass extrinsic information between nodes. In particular, the message that is sent from a message   176  TURBO CODES  node bi to a check node cj must not take into account the message sent in the previous round from cj to bi. For example, the L-value L b1  ≈ 12.4 contains the message Le b1  = L b2 ⊕ b5  = 7.5 from the ﬁrst check node. If we continue the decoding, we have to subtract this value from the current L-value to obtain the message L b1  − L b2 ⊕ b5  = L r1  + L b3 ⊕ b7  ≈ 4.6 from the message node to the ﬁrst check node. Similarly, we obtain L b1  − L b3 ⊕ b7  = L r1  + L b2 ⊕ b5  ≈ 13.1 for the message to the second check node. These messages are indicated in Figure 4.8.  The reason for these particular calculations is the independence assumption. For the ﬁrst iteration, the addition at the message node of the received L-value and the extrinsic L-values from check nodes is justiﬁed, because these values resulted from statistically independent observations. What happens with further iterations? This depends on the neighbourhood of the message nodes. Consider, for example, the graph in Figure 4.9 which illustrates the neighbourhood of the ﬁrst message node in our example. This graph represents the message ﬂow during the iterative decoding process. In the ﬁrst iteration we receive extrinsic  The independence assumption    The independence assumption is correct for l iterations of the algorithm if  the neighbourhood of a message node is a tree up to depth l.    The graph below is a tree up to depth l = 1.   At depth l = 2 there are several nodes that represent the same code bit.  This causes cycles as indicated for the node b4.    The shortest possible cycle has a length of 4.   a  neighbourhood of b1   b  shortest possible cycle  b1  b2  b5  b3  b7  b4  b8  b6  b9  b4  b6  b8  b9  Figure 4.9: The independence assumption   TURBO CODES 177 information from all nodes of depth l = 1. In the second iteration we receive messages from the nodes of depth l = 2, and so on.  The messages for further rounds of the algorithm are only statistically independent if the graph is a so-called tree.5 In particular, if all message nodes are unique. In our simple example this holds only for l = 1. At depth l = 2 there are several nodes that represent the same code bit. This causes cycles6 as indicated for the node b4. However, for longer codes we can apply signiﬁcantly more independent iterations. In general, if the neighbourhood of all message nodes is a tree up to depth l, then the incoming messages are statistically independent and the update equation correctly calculates the corresponding log-likelihood based on the observations.  In graph theory, we call the length of the shortest cycle the girth of a graph. If the graph has no cycles, its girth is deﬁned to be inﬁnite. According to ﬁgure  b  in Figure 4.9, the girth is at least 4. In our example graph the girth is 8. The girth determines the maximum number of independent iterations of belief propagation. Moreover, short cycles in the Tanner graph of a code may lead to a small minimum Hamming distance. Therefore, procedures to construct Tanner graphs maximising the girth were proposed. However, in practice it is not clear how the girth actually determines the decoding performance. Most methods to analyse the performance of belief propagation are based on the independence assumption, and little is known about decoding for graphs with cycles.  4.2 A First Encounter with Code Concatenation  In this section we introduce the notion of code concatenation, i.e. the concept of constructing long powerful codes from short component codes. The ﬁrst published concatenated codes were the product codes introduced by Elias  Elias, 1954 . This construction yields a block code of length n × n based on component codes of length n. We will consider product codes to introduce the basic idea of code concatenation and to discuss the connection to Tanner graphs and LDPC code. Later, in Section 4.6, we will consider another kind of product code. These so-called woven convolutional codes were ﬁrst introduced by H¨ost, Johannesson and Zyablov  H¨ost et al., 1997  and are product codes based on convolutional component codes.  4.2.1 Product Codes  Code concatenation is a method for constructing good codes by combining several simple codes. Consider, for instance, a simple parity-check code of rate R = 2 3. For each 2-nit information word, a parity bit is attached so that the 3-bit code word has an even weight, i.e. an even number of 1s. This simple code can only detect a single error. For the Binary Symmetric Channel  BSC  it cannot be used for error-correction. However, we can con- struct a single error-correcting code combining several simple parity-check codes. For this construction, we assume that the information word is a block of 2 × 2 bits, e.g.   cid:6   u =  0 1  1 0   cid:7   .  5A tree is a graph in which any two nodes are connected by exactly one path, where a path in a graph is a  sequence of edges such that from each of its nodes there is an edge to the next node in the sequence.  6A cycle is a path where the start edge and the end edge are the same.   178  TURBO CODES  We encode each row with a single parity-check code and obtain  Next, we encode column-wise, i.e. encode each column of Bo with a single parity-check code and have the overall code word  The overall code is called a product code, because the code parameters are obtained from the products of the corresponding parameters of the component codes. For instance, let ki and ko denote the dimension of the inner and outer code respectively. Then, the overall dimension is k = kiko. Similarly, we obtain the overall code length n = nino, where ni and no are the lengths of the inner code and outer code respectively. In our example, we have k = 2 · 2 = 4 and n = 3 · 3 = 9. Moreover the overall code can correct any single transmission error. By decoding the parity-check codes, we can detect both the row and the column where the error occurred. Consider, for instance, the received word   cid:6   0 1  1 1 0 1  Bo =   0 1 1  b =  1 0 1 1 1 0   cid:7   .    .    0 1 1  0 0 1 1 1 0    .  r =  We observe that the second row and the ﬁrst column have odd weights. Hence, we can correct the transmission error at the intersection of this row and column.  The construction of product codes is not limited to parity-check codes as component codes. Basically, we could construct product codes over an arbitrary ﬁnite ﬁeld. However, we will restrict ourselves to the binary case. In general, we organise the k = koki informa- tion bits in ko columns and ki rows. We apply ﬁrst row-wise outer encoding. The code bits of the outer code words are then encoded column-wise. It is easy to see that the overall code is linear if the constituent codes are linear. Let Go and Gi be the generator matrices of the outer and the inner code respectively. Using the Kronecker product, we can describe the encoding of the ki outer codes by Go ⊗ Iki, where Iki is a ki × ki identity matrix. Similarly, the encoding of the ko inner codes can be described by Iko ⊗ Gi with the ko × ko identity matrix Iko. The overall generator matrix is then  G =  Go ⊗ Iki   Iko ⊗ Gi   .  The linearity simpliﬁes the analysis of the minimum Hamming distance of a product code. Remember that the minimum Hamming distance of a linear code is equal to the minimum weight of a non-zero code word. The minimum weight of a non-zero code word of a product code can easily be estimated. Let d o and di denote the minimum Hamming distance of the outer and inner code. First, consider the encoding of the outer codes. Note that any non- zero information word leads to at least one non-zero outer code word. This code word has at least weight do. Hence, there are at least d o non-zero columns. Every non-zero column results in a non-zero code word of the inner code with a weight of at least d i. Therefore, a non-zero code word of the product code consists of at least d o non-zero columns each   TURBO CODES  Product codes  179    A binary product code B n = noni, k = koki, d = dodi  is a concatenation of ki binary outer codes Bo no, ko, do  and ko binary inner codes Bi ni, ki, di .   To encode a product code, we organise the k = koki information bits in ko columns and ki rows. We apply ﬁrst row-wise outer encoding. The code bits of the outer code words are then encoded column-wise. This encoding is illustrated in the following ﬁgure.  information  block  horizontal  parity  vertical parity  parity of parity  Figure 4.10: Encoding of product codes  of at least di non-zero symbols. We conclude that the minimum Hamming distance of the overall code  d ≥ d  o  i  .  d  We can also use product codes to discuss the concept of parallel concatenation, i.e. of turbo codes. We will consider possibly the most simple turbo code constructed from systematically encoded single parity-check codes B 3, 2, 2 . Take the encoding of the product code as illustrated in Figure 4.10. We organise the information bits in ko columns and ki rows and apply ﬁrst row-wise outer and then column-wise inner encoding. This results in the parity bits of the outer code  horizontal parity  and the parity bits of the inner code, which can be divided into the parity bits corresponding to information symbols, and parity of outer parity bits. Using two systematically encoded single parity-check codes B 3, 2, 2  as above, this results in code words of the form    u0  b =    ,  u2  0 p p  − u1 p − 0 u3 p   1 1 p 2  i denotes a vertical parity bit.  where p  − i denotes a horizontal parity bit and p   180  TURBO CODES  A product code is a serial concatenation as we ﬁrst encode the information with the outer code s  and apply the outer code bits as information to the inner encoder s . With the parallel concatenated construction, we omit the parity bits of parity. That is, we simply encode the information bits twice, once in the horizontal and once in the vertical direction. For our example, we obtain    u0  b =  − u1 p − 0 u3 p u2   1 0 p p 1    .    u0  b =  − u1 p − 0 u3 p   1 1 p 2  u2  p 0 p    ,  Note that the encoding in the horizontal and vertical directions is independent and can therefore be performed in parallel, hence the name.  4.2.2 Iterative Decoding of Product Codes  We will now use the log-likelihood algebra to discuss the turbo decoding algorithm. We will use this algorithm to decode a product code B 9, 4, 4  with code words  as described in the previous section. However, now we will use soft decoding, i.e. soft chan- nel values. For this particular product code, we could use the belief propagation algorithm as discussed in Section 4.1.4. In fact, the Tanner graph given in Figure 4.3 on page 167 is a representation of the code B 9, 4, 4 . To see this, note that the ﬁrst check node in the Tanner graph represents the parity equation of the ﬁrst row of the product code. Similarly, c4 and c6 represent the second and third row. The column constraints are represented by the check nodes c2, c3 and c5.  In practice, a so-called turbo decoding algorithm is used to decode concatenated codes. This is also an iterative message-passing algorithm similar to belief propagation. The itera- tive decoding is based on decoders for the component codes that use reliability information at the input and provide symbol-by-symbol reliability information at the output. In the case of a simple single parity-check code, this is already performed by executing a single boxplus operation. When we use more complex component codes, we apply in each itera- tion a so-called soft-in soft-out  SISO  decoding algorithm. Such an algorithm is the Bahl, Cocke, Jelinek, Raviv  BCJR  algorithm discussed in Section 3.5. Commonly, all symbol reliabilities are represented by log-likelihood ratios  L-values . In general, a SISO decoder, such as an implementation of the BCJR algorithm, may expect channel L-values and a- priori information as an input and may produce reliabilities for the estimated information symbols L uir , the extrinsic L-value for code Le bjr  and information bits Le uir   see Figure 4.11 . The turbo decoding algorithm determines only the exchange of sym- bol reliabilities between the different component decoders. In the following we discuss this turbo decoding, considering the parallel concatenated code with single parity-check component codes. The same concept of message passing applies to more complex codes, as we will see later on.   TURBO CODES  Soft-in soft-out decoder  181    In general, a soft-in soft-out  SISO  decoder, such as an implementation of the BCJR algorithm, may expect channel L-values and a-priori informa- tion as an input and may produce reliabilities for the estimated information symbols L uir , the extrinsic L-value for code Le bjr  and information bits Le uir   L ri    La ui    SISO  decoder  L uir  Le uir  Le bjr   Suppose that we encode the information block u =  0, 1, 1, 1  and use binary phase  shift keying with symbols from {+1,−1}. The transmitted code word is  Figure 4.11: Soft-in soft-out decoder    +1 −1 −1 −1 −1 +1 −1 +1    .  b =    5.6 −10.2 −7.5 0.7 −8.5  0.5 6.9  12.2    .  · r =  4  Es N0  For this particular code word we may obtain the following channel L-values after trans- mission over a Gaussian channel  Again, we assume that no a-priori information is available. Hence, we have La ui   = 0 for all information bits. Let us start with decoding the ﬁrst row. On account of the parity-check 0   ≈ −5.6. − equation we have L Similarly, we can evaluate the remaining extrinsic values of L  e  u0  = L u1   cid:3  L p 0   ≈ 7.5 and L e  u1  = L u0   cid:3  L p − − −  cid:7   cid:6  − e  u  and obtain 7.5 −5.6 0.7 0.5  e  u  = −  L  .  Now, we use this extrinsic information as a-priori knowledge for the column-wise decoding, i.e. we assume La u  = L − e  u . As we use systematic encoded component codes and the log-likelihood values are statistically independent, we can simply add up the channel and extrinsic values for the information bits e  u  · r = −    13.1 −15.8 −7.5 1.2 −8.5  Lch · r + L    .  1.2 6.9  12.2    e ˆu  as a-priori information for a  182  TURBO CODES  These values are the basis for the second decoding step, i.e. column-wise decoding. For the  0  ≈ ﬁrst column we have L −8.5. Hence, we obtain  u2  = L u0   cid:3  L p  .    L   cid:7   e u  =    0  ≈ −1.2 and Le    e u0  = L u2   cid:3  L p   cid:6  −1.2 −1.2 −8.5 −6.9 If we wish to continue the iterative decoding, we use L  new iteration. Hence, we calculate  4.4 −11.4 −7.5 −7.8 −6.4 −8.5 6.9 as input for the next iteration of row-wise decoding.  Lch · r + L  When we stop the decoding, we use L ˆu  = Lch · r + L  cid:7   iterative decoder. After the ﬁrst iteration we have  e u  · r =    cid:6   12.2     L ˆu  =  11.9 −17 −7.3 −5.7  .  4.3 Concatenated Convolutional Codes   e u  + L  − e  u  as output of the  To start with concatenated convolutional codes, it is convenient to consider their encoding scheme. We introduce three different types of encoder in this section: the original turbo encoder, i.e. a parallel concatenation, and serially and partially concatenated codes.  4.3.1 Parallel Concatenation  As mentioned above, turbo codes result from a parallel concatenation of a number of systematic encoders linked by interleavers. The general encoding scheme for turbo codes is depicted in Figure 4.12. All encoders are systematic. The information bits are interleaved after encoding the ﬁrst component code, but the information bits are only transmitted once. With two component codes, the code word of the resulting code has the structure  b =  u, uA1, π u A2  ,  where π ·  denotes a permutation of the information bits and Aj denotes a submatrix of the systematic generator matrix Gj =  IkAj  .  As we see, the parallel concatenation scheme can easily be generalised to more than two component codes. If not mentioned otherwise, we will only consider turbo codes with two equal component codes in the following discussion. The rate of such a code is  R = Rc 2 − Rc  ,  where Rc is the rate of the component codes.  Although the component codes are usually convolutional codes, the resulting turbo code is a block code. This follows from the block-wise encoding scheme and the fact that convolutional component codes have to be terminated.   TURBO CODES  183  Encoder scheme for turbo codes  u  b 1  1 b 2  1  b 1  2 b 2  2  b 1   j  b 2   j  encoder1  encoder2  encoderj  interleaver1  interleaverj−1    Most commonly, turbo codes are constructed from two equal component  codes of rate Rc.    In this case, the rate of the overall code is R = Rc 2 − Rc   4.9   Figure 4.12: Encoder scheme for turbo codes  4.3.2 The UMTS Turbo Code   cid:12   As an example of a turbo code we will consider the code deﬁned in the Universal Mobile Telecommunications System  UMTS  standard. The corresponding encoder is shown in  cid:11  Figure 4.13. The code is constructed from two parallel concatenated convolutional codes of memory m = 3. Both encoders are recursive and have the generator matrix G D  = 1, 1+D+D3 . In octal notation, the feedforward generator is  15 8 and the feedback gen- 1+D2+D3 erator is  13 8. The information is encoded by the ﬁrst encoder in its original order. The second encoder is applied after the information sequence is interleaved. The information is only transmitted once, therefore the systematic output of the second decoder is omitted in the ﬁgure. Hence, the overall code rate is R = 1 3. According to the UMTS standard, the length K of the information sequence will be in the range 40 ≤ K ≤ 5114. After the information sequence is encoded, the encoders are forced back to the all-zero state. However, unlike the conventional code termination, as discussed in Section 3.1.4, the recursive encoders cannot be terminated with m zero bits. The termination sequence for a recursive encoder depends on the encoder state. Because the   184  TURBO CODES  Encoder for the UMTS turbo codes  u  b 1   b 2   b 3   interleaver  Figure 4.13: Encoder for the UMTS turbo codes  states of the two encoders will usually be different after the information has been encoded, the tails for each encoder must be determined separately. An example of this termination is given in Figure 3.8 on page 106. The tail bits are then transmitted at the end of the encoded code sequence. Hence, the actual code rate is slightly smaller than 1 3.  4.3.3 Serial Concatenation  At high signal-to-noise ratio  SNR , turbo codes typically reveal an error ﬂoor, which means that the slope of the bit error rate curve declines with increasing SNR  see, for example, Figure 4.18 on page 189 . In order to improve the performance of parallel con- catenated codes for higher SNR, Benedetto and Montorsi applied iterative decoding to serially concatenated convolutional codes  SCCC  with interleaving  Benedetto and Mon- torsi, 1998 . The corresponding encoders  see Figure 4.14  consist of the cascade of an outer encoder, an interleaver permuting the outer code word and an inner encoder whose input words are the permuted outer code bits. The information sequence u is encoded by a rate Ro = ko no outer encoder. The outer code sequence bo is interleaved and then fed into the inner encoder. The inner code has rate Ri = ki ni. Hence, the overall rate is  R = R  i  o  .  R   TURBO CODES  185  Serial concatenation with interleaving  u  outer encoder  bo  ui  π  inner encoder  b  Figure 4.14: Serial concatenation with interleaving  4.3.4 Partial Concatenation  We will consider the partially concatenated convolutional encoder as depicted in Figure 4.15. Such an encoder consists of one outer and one inner convolutional encoder. In between there are a partitioning device and an interleaver, denoted by P and π respectively. The information sequence u is encoded by a rate Ro = ko no outer encoder. The outer code sequence bo is partitioned into two so-called partial code sequences bo, 1  and bo, 2 . The partial code sequence bo, 1  is interleaved and then fed into the inner encoder. The other symbols of the outer code sequence  bo, 2 , dashed lines in Figure 4.15  are not encoded by the inner encoder. This sequence, together with the inner code sequence bi, constitutes the overall code sequence b. Similarly to the puncturing of convolutional codes, we describe the partitioning of the outer code sequence by means of a partitioning matrix P. Consider a rate Ro = ko no outer convolutional code. P is a tp × no matrix with matrix elements ps,j ∈ {0, 1}, where tp ≥ 1 is an integer determining the partitioning period. A matrix element ps,j = 1 means that the corresponding code bit will be mapped to the partial code sequence bo, 1 , while a code bit corresponding to ps,j = 0 will appear in the partial code sequence bo, 2 . We deﬁne the partial rate Rp as the fraction of outer code bits that will be encoded by the inner encoder. s,j ps,j , the number of 1s in the partitioning matrix, and with the total number of With elements notp in P, we have      Finally, we obtain the rate of the overall code  Rp =  s,j ps,j notp  .  R =  RoRi  Rp + Ri 1 − Rp   ,  where Ri is the inner code rate. Clearly, if Rp = 1, this construction results in a serially concatenated convolutional code  Benedetto and Montorsi, 1998 . If we choose Rp = Ro, and use systematic outer and inner encoding, such that we encode the information sequence with the inner encoder, we obtain a parallel  turbo  encoder. Besides these two classical cases, partitioning provides a new degree of freedom for code design.  Throughout the following sections we give examples where the properties of different concatenated convolutional codes are compared. Therefore, we will refer to the codes   186  TURBO CODES  Partially concatenated convolutional encoder  u  outer encoder  bo  bo, 1   π  inner encoder  bi  b  P   4.10    4.11   P  bo, 2     Rp =  s,j ps,j notp    The partial rate Rp is the fraction of outer code bits that will be encoded by  the inner encoder    The rate of the overall code is R =  RoRi  Rp + Ri 1 − Rp   where Ri and Ro are the rate of the inner code and outer code respectively.    Rp = 1 results in a serially concatenated convolutional code.   Rp = Ro and systematic outer and inner encoding result in a parallel  concatenated convolutional code.  Figure 4.15: Partially concatenated convolutional encoder  constructed in Figure 4.16. Those codes are all of overall rate R = 1 3, but with different partial rates. There are classical parallel Rp = 1 2 and serially concatenated convolutional codes Rp = 1, as well as partially concatenated convolutional codes.  4.3.5 Turbo Decoding  All turbo-like codes can be decoded iteratively with a message-passing algorithm similar to belief propagation. The iterative decoding is based on symbol-by-symbol soft-in soft-out  SISO  decoding of the component codes, i.e. algorithms that use reliability information at the input and provide reliability information at the output, like the BCJR algorithm which evaluates symbol-by-symbol a-posteriori probabilities. For the following discussion, all symbol reliabilities are represented by log-likelihood ratios  L-values .  We consider only the decoder structure for partially concatenated codes as given in Figure 4.17. Decoders of parallel and serial constructions can be derived as special cases from this structure. The inner component decoder requires channel L-values as well as   TURBO CODES  187  Example of concatenated convolutional codes    We construct different concatenated codes of overall rate R = 1 3. For the inner encoding we always employ rate Ri = 1 2 codes with generator matrix Gi D  =  1,  1+D2 1+D+D2  .    For outer encoding we employ the mother code with generator matrix G D =  1 + D + D2, 1 + D2 , but we apply puncturing and partitioning to obtain the desired overall rate.    We consider:  – Two parallel concatenated codes: both with partial rate Rp = Ro = 1 2, but with different partitioning schemes  P =  1, 0 , P =  0, 1   . – A serially concatenated code with Rp = 1, Ro = 2 3. – Two partially concatenated constructions with Ro = 3 5, Rp = 4 5 and P =  0, 1, 1, 1, 1  and with Ro = 4 7, Rp = 5 7 and P =  1, 1, 0, 1, 1, 0, 1 . Note that here the partitioning matrices are obtained by optimising the partial distances  see Section 4.6.2 .  Figure 4.16: Example of concatenated convolutional codes  a-priori information for the inner information symbols as an input. The output of the inner decoder consists of channel and extrinsic information for the inner information symbols. The outer decoder expects channel L-values and provides extrinsic L-values Le bo  for the outer code symbols and estimates ˆu for the information symbols. Initially, the received sequence r is split up into a sequence r 1  which is fed into the inner decoder and a sequence r 2  which is not decoded by the inner decoder. Moreover, the a-priori values for the inner decoder La ui  are set to zero.  For each iteration we use the following procedure. We ﬁrst perform inner decoding. The −1 . This de-interleaved output symbols L ui  of the inner decoder are de-interleaved  π sequence and the received symbols r 2  are multiplexed according to the partitioning scheme P. The obtained sequence L ro  is regarded as the channel values of the outer code, i.e. it is decoded by the outer decoder. The outer decoder provides extrinsic L-values for the outer code bits Le bo  and an estimated information sequence ˆu. For the next round of iterative decoding, the extrinsic output Le bo  is partitioned according to P into Le bo, 1   and Le bo, 2  , whereas the sequence Le bo, 2   is not regarded any more, because bo, 2  was not encoded by the inner encoder. Le bo, 1   is interleaved again such that the resulting sequence La ui  can be used as a-priori information for the next iteration of inner decoding.  Finally, we give some simulation results for the AWGN channel with binary phase shift keying. All results are obtained for ten iterations of iterative decoding, where we utilise the   188  Decoding structure  TURBO CODES  For each iteration we use the following procedure:    Perform inner decoding. The output symbols L ui  of the inner decoder −1 , and the received symbols r 2  are multiplexed  are de-interleaved  π according to the partitioning scheme P.    The obtained sequence L bo  is decoded by the outer decoder. The extrinsic output Le bo  is partitioned according to P into Le bo, 1   and Le bo, 2  , where Le bo, 2   is discarded.    Le bo, 1   is interleaved again such that the resulting sequence La bi  can  be used as a priori information for the next iteration of inner decoding.  La ui   Le bo, 1    π  Le bo, 2    Le bo   inner decoder  L ui   −1  π  P  L ro   outer decoder  ˆu  P  Figure 4.17: Decoding structure  r  r 1   r 2   P  max-log version of the BCJR algorithm as discussed in Section 3.5.2. Simulation results are depicted in Figure 4.18, where the ﬁgures presents the Word Error Rate  WER  for codes of length n = 600 or n = 3000 respectively. The component codes are as given in Figure 4.16. The two partially concatenated codes outperform the serial concatenated code for the whole considered region of WER. At high SNR we notice the typical error ﬂoor of the turbo code  P =  1, 0  . Here, the partially concatenated codes achieve signiﬁcantly better performance. Simulation results regarding the bit error rate are similar.  4.4 EXIT Charts  For the analysis of the iterative decoding algorithm we utilise the EXtrinsic Information Transfer  EXIT  characteristics as proposed by ten Brink  ten Brink, 2000; see also ten Brink, 2001 . The basic idea of the EXIT chart method is to predict the behaviour of   TURBO CODES  Simulation results  189    The ﬁgure depicts word error rates  WER  for n = 600 and n = 3000 respect-  WER  WER  ively.  100  10 cid:358 1  10 cid:358 2  10 cid:358 3  10 cid:358 4 0  100  10 cid:358 1  10 cid:358 2  10 cid:358 3  10 cid:358 4 0  R =1 2 p =5 7 R p R =4 5 p =1 R p  R =1 2 p =5 7 R p R =4 5 p =1 R p  1  2 E  N b  0  3  0.5  1  N E b  0  1.5  2  Figure 4.18: Simulation results  the iterative decoder by looking at the input output relations of the individual constituent decoders.  4.4.1 Calculating an EXIT Chart  We assume that a-priori information can be modelled by jointly independent Gaussian random variables. The decoder can then be characterised by its transfer function where the mutual information at the output is measured depending on the a-priori information and the channel input. All considerations are based on the assumptions in Figure 4.19. These assumptions are motivated by simulation results, which show that with large inter- leavers the a priori values remain fairly uncorrelated from the channel values. More- over, the histograms of the extrinsic output values are Gaussian-like. This is indicated in Figure 4.20 which depicts the distribution of the L-values at the input and output of the Soft-Input Soft-Output  SISO  decoder. For this ﬁgure we have simulated a transmitted   190  TURBO CODES  Basic assumptions of the EXIT charts method   1  The a-priori values are independent of the respective channel values.  2  The probability density function pe ·  of the extrinsic output values is the  Gaussian density function.   3  The probability density function pe ·  fulﬁls the symmetry condition  Divsalar  and McEliece, 1998   pe ξX = x  = pe −ξX = x  · exξ   4.12  where the binary random variable X denotes a transmitted symbol  either outer code symbol or inner information symbol  with x ∈ {±1}.  Figure 4.19: Basic assumptions of the EXIT charts method  Distribution of L-values before and after decoding  decoder input decoder output  y c n e u q e r f  e v i t a e r  l  0.1  0.05  0  cid:358 5  0  5  10  L-value  15  20  25  Figure 4.20: Distribution of the L-values at the decoder input and output   TURBO CODES  191  all-zero code sequence of the  7 5  convolutional code over a Gaussian channel with binary phase shift keying. The Gaussian distribution of the input values is a consequence of the channel model. The distribution of the output L-values of the SISO decoder is also Gaussian-like. Note that the area under the curve on the right of the axis L = 0 determines the bit error rate. Obviously, this area is smaller after decoding, which indicates a smaller bit error rate.  4.4.2 Interpretation Let La denote the a-priori L-value corresponding to the transmitted symbol X . Based on the assumptions in Figure 4.19, the a-priori input of the SISO decoders is modelled as independent random variables  La = µa · X + Na,  where µa is a real constant and Na is a zero-mean Gaussian distributed random variable with variance σ 2 a . Thus, the conditional probability density function of the a-priori L-values is  pa ξX = x  =  −  ξ−µax 2  2σ 2 a  .  1√ 2π σa  e  Moreover, the symmetry condition  4.12  implies µa = σ 2 a 2  .  The mutual information Ia = I  X; La  between the transmitted symbol X and the corres- ponding L-value La is employed as a measure of the information content of the a-priori knowledge  x∈{−1,1} × log2 With Equation  4.13  and µa = σ 2  a   cid:1   - ∞ −∞ pa ξX = x   Ia = 1 2  dξ.  2 · pa ξX = x   pa ξX = −1  + pa ξX = 1  - ∞  2 we obtain  −  ξ− σ 2 2 x 2 a 2σ 2 a  log2  1 + e  −ξ  dξ.  Ia = 1 −  1√ 2π σa  −∞ e  Ie = I  X; Le  = 1 2   cid:1   - ∞ −∞ pe ξX = x   x∈{−1,1} 2 · pe ξX = x   × log2  pe ξX = −1  + pe ξX = 1   dξ.  Note that Ia is only a function of σa. Similarly, for the extrinsic L-values we obtain   192  TURBO CODES  Calculating an EXIT chart    The a-priori  input of the SISO decoders is modelled as independent a . The conditional probability  Gaussian random variables La with variance σ 2 density function of the a-priori L-values is  pa ξX = x  =  −  ξ− σ 2 2 x 2 a 2σ 2 a  1√ 2π σa  e   4.13     The mutual  information Ia = I  X; La  between the transmitted symbol X and the corresponding L-value La is employed as a measure of the information content of the a-priori knowledge. Ia is only a function of σa  - ∞  −∞ e  Ia = 1 −  1√ 2π σa  −  ξ− σ 2 2 x 2 a 2σ 2 a  log2  1 + e  −ξ  dξ   4.14     The mutual information Ie = I  X; Le  between the transmitted symbol X and the corresponding extrinsic L-value Le is employed as a measure of the information content of the code correlation  Ie = I  X; Le  = 1 2   cid:1   - ∞ −∞ pe ξX = x   x∈{−1,1} 2 · pe ξX = x   × log2  pe ξX = −1  + pe ξX = 1   dξ   4.15   where pe ·  is estimated by means of Monte Carlo simulations.  Figure 4.21: Calculating an EXIT chart  Note that it is not possible to express Ia = I  X; La  and Ie = I  X; Le  in closed form. However, both quantities can be evaluated numerically. In order to evaluate Equation  4.15 , we estimate pe ·  by means of Monte Carlo simulations. For this purpose, we generate independent random variables La according to Equation  4.13  and apply them as a-priori input to the SISO decoder. We obtain a similar distribution pe ·  to Figure 4.20. With this distribution we can calculate Ie = I  X; Le  numerically according to Figure 4.21. Ia = I  X; La  is also evaluated numerically according to Equation  4.14 . In order to analyse the iterative decoding algorithm, the resulting extrinsic information transfer  EXIT  characteristics Ie = T  Ia, Eb N0  of the inner and outer decoder are plotted into a single diagram, where for the outer decoder the axes of the transfer characteristic are swapped. An example of an EXIT chart is given in Figure 4.22. This EXIT chart can be   TURBO CODES  193  Interpretation of an EXIT chart  inner code outer code  ie I =  oa I  0.4  1  0.8  0.6  0.2  0 0  0.2  0.4 I i a  = I o 0.6 e  0.8  1    The resulting extrinsic information transfer  EXIT  characteristics Ie = T  Ia, Eb N0  of the inner and outer decoder are plotted into a single diagram, where for one decoder the axes of the transfer characteristic is swapped.    During the iterative decoding procedure the extrinsic output values of one decoder become the a-priori values of the other decoder. This is indicated by the line between the transfer functions.    Each line indicates a single decoding step of the iterative decoding proce-  dure.  Figure 4.22: Interpretation of an EXIT chart  interpreted as follows. During the iterative decoding procedure, the extrinsic output values of one decoder become the a-priori values of the other decoder. This is indicated by the line between the transfer functions, where each line indicates a single decoding step of one of the constituent decoders  a half-iteration of the iterative decoding procedure .  EXIT charts provide a visualisation of the exchange of extrinsic information between the two component decoders. Moreover, for very large interleaver sizes, EXIT charts make it possible to predict the convergence of the iterative decoding procedure accord- ing to Figure 4.23. Convergence is only possible if the transfer characteristics do not intersect. In the case of convergence, the average number of required decoding steps can be estimated. Furthermore, the EXIT chart method make it possible to predict the so-called waterfall region of the bit error rate performance. This means that we observe   194  TURBO CODES  Convergence of iterative decoding  Eb N0=0.1dB  Eb N0=0.3dB  Ei  I  =  Ao  I  Ei  I  =  Ao  I  0 0  1  0.8  0.6  0.4  0.2  1  0.8  0.6  0.4  0.2  0 0  0.5 i =I o I E A  Eb N0=0.8dB  0.5 i =I o I E A  1  0.8  0.6  0.4  0.2  0 0  100  10  2  104  0  Ei  I  =  Ao  I  1  1  → R E B  0.5 i =I o I E A  1  0.2  N E b 0  0.4  These EXIT charts can be interpreted as follows:    Pinch-off region: the region of low signal-to-noise ratios where the bit  error rate is hardly improved with iterative decoding.    Bottleneck region: here the transfer characteristics leave a narrow tunnel. During the iterative decoding the convergence towards low bit error rates is slow, but possible.    Wide-open region: region of fast convergence.  Figure 4.23: EXIT charts and the simulated bit error rate  BER  for a serially  concatenated code with overall rate R = 1 3  an Eb N0-decoding threshold that corresponds to the region of the EXIT charts where the transfer characteristics leave a narrow tunnel. For this signal-to-noise ratio, the iter- ative decoding algorithm converges towards low bit error rates with a large number of iterations. Therefore, this region of signal-to-noise ratios is also called the bottleneck region. The region of low signal-to-noise ratios, the pinch-off region, is characterised by an intersection of the transfer characteristics of the two decoders. Here, the bit error rate is hardly improved with iterative decoding. Finally, we have the region of fast conver- gence, where the signal-to-noise ratio is signiﬁcantly larger than the decoding threshold. In   TURBO CODES  195  the so-called wide-open region the decoding procedure converges with a small number of iterations. For instance, we construct a serially concatenated code. For the inner encoding we employ a rate Ri = 1 2 code with generator matrix Gi D  =  1, 1+D2 1+D+D2  . For the outer encoding we employ the same code, but we apply puncturing and partitioning to obtain the outer rate Ro = 2 3 and the desired overall rate R = 1 3. For this example, the EXIT charts for different values of Eb N0 are depicted in Figure 4.23. The bottleneck region corresponds to a signal-to-noise ratio Eb N0 ≈ 0.3 dB. For the code of rate R = 1 3 and length n = 100 000, the required signal-to-noise ratio in order to obtain a bit error rate −4 is less than 0.1 dB away from the predicted threshold value, where the iterative of 10 decoding procedure was run for 30 iterations. Now, consider our running example with the codes from Figure 4.16. The corres- ponding EXIT charts for a signal-to-noise ratio Eb N0 = 0.1 dB can be found in Figure 4.24. The solid line is the transfer characteristic of the inner code. The two other charts correspond to the parallel concatenation  Rp = 1 2  with the partitioning matrix P =  1, 0  and the partially concatenated code with Rp = 5 7. For the parallel concatenation the EXIT chart leaves a narrow tunnel. Thus, the iterative decoding should converge towards low bit error rates. Similarly, with Rp = 5 7 the convergence of the iterative decoding is  EXIT charts for different partial rates  I  Ei = Ao  I  0.5  1  0.9  0.8  0.7  0.6  0.4  0.3  0.2  0.1  0 0  E  N b  =0.1dB 0  Ro=4 7, R =5 7 p  R =1 2, P= 1 0  p  0.5 i =I o I E A  0.1  0.2  0.3  0.4  0.6  0.7  0.8  0.9  1  Figure 4.24: EXIT charts for the codes from Figure 4.16 for Eb N0 = 0.1 dB   196 TURBO CODES slow, but possible. In comparison with P =  1, 0 , we observe a wider tunnel for the Rp = 5 7 construction. Therefore, the iterative decoding should converge with a smaller number of iterations. Note that the choice of the partitioning pattern is also important. The parallel construction with P =  0, 1  gets stuck at values > 0.5. Likewise, the EXIT charts for the serially concatenated code as well as for Rp = 4 5 get instantly stuck, so no bit error rate reduction is expected at this signal-to-noise ratio. These curves are omitted in Figure 4.24.  For large interleaver sizes the EXIT chart method allows accurate prediction of the waterfall region. For interleavers of short or moderate length bounding techniques that include the interleaving depth are more appropriate, particularly if we also consider the region of fast convergence, i.e. if we wish to determine possible error ﬂoors.  4.5 Weight Distribution  We have seen in Section 3.3.6 that the performance of a code with maximum likelihood decoding is determined by the weight distribution, of the code. If we know the weight distribution, we can bound the word error rate using, for example, the union bound. However, for concatenated codes, even of moderate length, it is not feasible to evaluate the complete weight distribution. It is usually not even possible to determine the mini- mum Hamming distance of a particular code. A common approach in coding theory to overcome this issue, and to obtain at least some estimate of the code performance with maximum likelihood decoding, is to consider not particular codes but an ensemble of codes.  In the context of turbo codes, this approach was introduced by Benedetto and Mon- torsi  Benedetto and Montorsi, 1998 . They presented a relatively simple method for calculating the expected weight distribution of a concatenated convolutional code from the weight distributions of the component codes. The expected weight distribution is the average over the weight distributions of all codes in the considered ensemble, where the ensemble is deﬁned by the set of all possible interleavers. In this section we will utilise the concept introduced by Benedetto and Montorsi.  Benedetto and Montorsi, 1998  to derive the expected weight distribution Aw for partially concatenated convolutional codes. Then, we will use Aw to bound the expected code performance with maximum likelihood decoding.  4.5.1 Partial Weights  For further consideration of partially concatenated convolutional codes it is necessary not only to regard the overall weight of the outer encoder output but also the partial weights. These partial weights distinguish between the weight w1 of the code sequence being fed into the inner encoder and the weight w2 of the code sequence not being encoded by the inner encoder.  By analogy with the extended weight enumerator function discussed in Section 3.3.3, we introduce the labels W1 and W2 instead of W so that we can determine the partial weight enumerator function APEF W1, W2  as described in Section 3.3.4.   TURBO CODES  197  For further calculations, assuming b1 is fed into the inner encoder and b2 is not encoded,  the matrix A from the above examples is modiﬁed as    1  0 0  W1W2  ˜A =    .  0 W1W2 0 W1 W2  1 0 0  0 0 W2 W1  The modiﬁcation of A can be done by introducing labels V1, . . . , Vn which represent the output weight of each output of the encoder so that A contains polynomials Aij  V1, . . . , Vn . Then the polynomials ˜Aij  W1, W2  can be easily obtained by replacing Vk  k = 1, . . . , c  with W1 or W2 according to the partitioning scheme. In this context, Aij marks the element in row i and column j of matrix A.  4.5.2 Expected Weight Distribution  Now we derive the expected weight distribution Aw for partially concatenated convolu- tional codes, where Aw denotes the expected number of code words with weight w. We restrict ourselves to randomly chosen permutations of length N and terminated convolu- } tional component codes. First, we evaluate the expected number of code words E{Ai w2, ˜w with weight ˜w at the output of the inner encoder conditioned on the weight w2 of the partial outer code sequence vo, 2 . Let Ai w1, ˜w denote the number of code words generated by the inner encoder with input weight w1 and output weight ˜w  Benedetto and Montorsi, 1998 . Let Ao w1,w2 denote the number of code words with partial weights w1 and w2 corresponding to the outer encoder and its partitioning. We have  } = N cid:1   w1=0  Ao  w1,w2   cid:2    cid:3  · Ai  N w1  w1, ˜w  .  E{A  i w2, ˜w   cid:2    cid:3   This formula becomes plausible if we note that the term Ai is the probability that a random input word ui to the inner encoder of weight w1 will produce an output word bi of weight ˜w. With w = ˜w + w2 we obtain Aw = E{Aw} =   cid:1   E{A  w1, ˜w   N w1  }.  i w2, w−w2   w2≤w  This is summarized in Figure 4.25. formula  4.16  can be used for bounding the average maximum likelihood performance of the code ensemble given by all possible permutations. Let P W denote the expected word error rate and n the overall code length. Using the standard union bound for the additive white Gaussian noise  AWGN  channel with binary phase shift keying, we have  P W ≤ n cid:1   w=1  −w·R·Eb N0 .  Awe  We consider the codes as given in Figure 4.16. However, for inner encoding we use the inner rate Ri = 1 2 code with generator matrix Gi D  =  1,  1+D  . With code length n = 300  1   198  TURBO CODES  Expected weight distribution    The expected weight distribution Aw for a partially concatenated convolu-  tional code is  Aw = E{Aw} =  E  i w2, w−w2  A   4.16    cid:4    cid:1   w2≤w   cid:5   where E{Ai } is the expected number of code words with weight ˜w at the output of the inner encoder conditioned on the weight w2 of the partial outer code sequence.  w2, ˜w    Using the standard union bound for the AWGN channel with Binary Phase  Shift Keying  BPSK , we can bound the expected word error rate  P W ≤ n cid:1   w=1  −w·R·Eb N0  Awe   4.17   Figure 4.25: Expected weight distribution  we obtain similar overall rates R ≈ 1 3. The results for bounding the word error rate by the union bound in inequality  4.17  are depicted in Figure 4.26. Rp = 1 corresponds to a serially concatenated convolutional code, and Rp = 1 2 to a turbo code. For a partially concatenated code with Rp = 4 5 we use a rate Ro = 3 5 punctured outer code. With the partitioning period tp = 1 there exist ﬁve different partitioning schemes. The results for the schemes with best and worst performance are also given in Figure 4.26. The per- formance difference between P =  1, 1, 1, 1, 0  and P =  0, 1, 1, 1, 1  indicates that the particular choice of the partitioning scheme is important. In this example the partially con- catenated code with P =  0, 1, 1, 1, 1  outperforms the serially and parallel concatenated constructions.  4.6 Woven Convolutional Codes  In this section we concentrate on the minimum Hamming distance of the constructed con- catenated code. With respect to the minimum Hamming distance of the concatenated code, especially for codes of short lengths, the choice of the particular interleaver is very impor- tant. With turbo-like codes, the use of designed interleavers is motivated by the asymptotic coding gain, which is the gain in terms of transmit power that can be achieved with coding compared with the uncoded case for very low residual error rates. For unquantised channels the asymptotic coding gain is  Clark and Cain, 1988   Ga = 10 log10 R · d dB,   TURBO CODES  199  Union bound on the word error rate  100  10 cid:358 1  10 cid:358 2  R E W  10 cid:358 3  10 cid:358 4  10 cid:358 5  10 cid:358 6 2  R =4 5, P= 1,1,1,1,0  p R =4 5, P= 0,1,1,1,1  p R =1 2, P= 1,0  p R =1, P= 1,1,1  p  3  4  6  7  8  5  N E b  0  Figure 4.26: Union bound on the word error rate for rate R = 1 3 codes  code length  n = 300  with different partial rates  cf. Figure 4.16   where R is the rate and d is the minimum Hamming distance of the code. This formula implies that for ﬁxed rates the codes should be constructed with minimum Hamming dis- tances as large as possible, in order to ensure efﬁcient performance for high signal-to-noise ratios.  We start our discussion by introducing the class of woven convolutional codes. Woven code constructions yield a larger designed minimum Hamming distance than ordinary serial or parallel constructions. In the original proposal  H¨ost et al., 1997 , two types of woven convolutional code are distinguished: those with outer warp and those with inner warp. In this section we consider encoding schemes that are variations of woven codes with outer warp. We propose methods for evaluating the distance characteristics of the considered codes on the basis of the active distances of the component codes. With this analytical bounding technique, we derive lower bounds on the minimum  or free  distance of the concatenated code. These considerations also lead to design criteria for interleavers.  Note that some of the ﬁgures and results of this section are reprinted, with permission,  from Freudenberger et al.  2001 ,  cid:148  2001 IEEE.   200 4.6.1 Encoding Schemes  TURBO CODES  With binary woven convolutional codes, several convolutional encoders are combined in such a way that the overall code is a convolutional code. The basic woven convolutional encoder with outer warp is depicted in Figure 4.27. It consists of lo outer convolutional encoders which have the same rate Ro = ko no and a single inner encoder.7 The informa- l with l = 1, . . . , lo. These subsequences tion sequence u is divided into lo subsequences uo uo l are encoded with the outer encoders. The resulting outer code sequences bo lo are written row-wise into a buffer of lo rows. The binary code bits are read column-wise from this buffer. The resulting sequence constitutes the input sequence ui of the single inner rate Ri = ki ni convolutional encoder. After inner encoding, we obtain the ﬁnal code sequence b of the Woven Convolutional Code  WCC . The resulting woven convolutional code has overall rate R = RiRo.  1, . . . , bo  Woven encoder with outer warp  u  ui  inner encoder  b  uo 1  uo 2  uo lo  outer encoder1  outer encoder2  ...  outer encoderlo  bo 1  bo 2  bo lo    A woven convolutional encoder with outer warp consists of lo outer convo-  lutional encoders that have the same rate Ro = ko no.    The information sequence u is divided into lo subsequences uo  l with l =  1, . . . , lo.    The outer code sequences bo  are written row-wise into a buffer of lo rows. The binary code bits are read column-wise and the resulting sequence constitutes the input sequence ui of the single inner rate Ri = ki ni convolutional encoder.  1, . . . , bo  lo    The resulting woven convolutional code has the overall rate  R = RiRo.  Figure 4.27: Woven encoder with outer warp  7In contrast, a woven encoder with inner warp has a single outer and several inner encoders.   TURBO CODES  Woven turbo encoder  201  u  uo 1  uo 2  uo lo  outer encoder1  outer encoder2  ...  outer encoderlo  bo, 1  1 bo, 1  2  bo, 2  1  bo, 1  lo  P  P  P  bo, 2  2  bo, 2  lo  bo, 2   ui  inner encoder  bi  b    The overall woven turbo code has the rate  R =  RoRi  Rp + Ri 1 − Rp   ,  where Rp is the fraction of outer code bits that will be encoded by the inner encoder.    The partitioning of  sequences bo, 1   l  the outer code sequences into two partial code is described by means of a partitioning matrix P.  and bo, 2   l  Figure 4.28: Woven turbo encoder  The concept of partial concatenation as discussed in Section 4.3.4 was ﬁrst introduced by Freudenberger et al.  Freudenberger et al., 2000a, 2001  in connection with woven turbo codes. Woven turbo codes belong to the general class of woven convolutional codes. Figure 4.28 presents the encoder of a Woven Turbo Code  WTC . Like an ordinary woven encoder, a woven turbo encoder consists also of lo outer convolutional encoders and one inner convolutional encoder. The information sequence u is subdivided into lo sequences which are the input sequences to the lo rate Ro = ko no outer encoders.  Parts of the symbols of the outer code sequences  bo, 1    , which are located in the same bit positions, are multiplexed to the sequence ui. The sequence ui is the input sequence of the inner encoder. The other symbols of the outer code sequences  bo, 2  , dashed lines in Figure 4.28  are not encoded by the inner encoder. These sequences bo, 2  form the sequence bo, 2  which, together with the inner code sequence bi, constitutes the overall code sequence b.  l  l  l  As with partially concatenated codes, the partitioning of the outer code sequences into is described by means of a partitioning matrix  and bo, 2   two partial code sequences bo, 1  l P. Furthermore, the overall code rate is R =  l  RoRi  Rp + Ri 1 − Rp   ,   202  u  Woven encoder with row-wise interleaving  TURBO CODES  uo 1  uo 2  uo lo  outer encoder1  outer encoder2  bo 1  bo 2  π1  π2  ...  outer encoderlo  bo lo  πlo  ui  inner encoder  b  Figure 4.29: Woven encoder with row-wise interleaving  where Rp is the fraction of outer code bits that will be encoded by the inner encoder as deﬁned in Section 4.3.4.  Up to now, we have only considered woven constructions without interleavers. These constructions lead to overall convolutional codes. Usually, when we employ interleaving, we use block-wise interleavers and terminated convolutional codes so that the resulting concatenated code is actually a block code. This is possibly also the case with woven codes for most applications.  However, there are a number of interesting aspects of the construction without inter- leaving, e.g. a sliding window decoding algorithm for WCC can be introduced  Jordan et al., 2004a . This algorithm requires no code termination and might be interesting for applications where low-latency decoding is required. In the following we will mainly con- sider constructions with interleaving, in particular row-wise interleaving as indicated in Figure 4.29. Here, each outer code sequence bo l is interleaved by arbitrary and independent interleavers. The interleaved sequences are fed into the inner encoder. Of course, the same interleaver concept can also be applied to woven turbo codes.  4.6.2 Distance Properties of Woven Codes  Now, we investigate the distance properties of the considered codes on the basis of the active distances of the component codes. With this analytical bounding technique, we derive lower bounds on the minimum  or free  distance of the concatenated code.  Below, we deﬁne the generating tuples of an input sequence u D  of a convolutional encoder. Then, we show that each generating tuple generates at least dg non-zero bits in the encoded sequence b D , where dg is some weight in the region βb ≤ dg ≤ dfree.  Consider a convolutional encoder and its active burst distance  cf. Section 3.3.2 on page 122 . We call a segment of a convolutional code sequence burst if it corresponds to an encoder state sequence starting in the all-zero state and ending in the all-zero states and having no consecutive all-zero states in between which correspond to an all-zero input   203   4.18   TURBO CODES  Generating tuples    Let dg be an integer with βb ≤ dg ≤ dfree. We deﬁne the generating length  for dg as   cid:30    cid:29   jg =  2dg − βb  α  i.e. jg is the minimum j for which the lower bound on the active burst distance satisﬁes αj + βb ≥ 2dg.    Let t1 be the time index of the ﬁrst non-zero tuple ut =  u 1   , . . . , u k    of the sequence u. Let t2 be the time index of the ﬁrst non-zero tuple with t2 ≥ t1 + jg, and so on. We call the information tuples ut1 , ut2 , . . . generating tuples.  , u 2   t  t  t    Let u be the input sequence of a convolutional encoder with Ng generating tuples with generating length jg. Then the weight of the corresponding code sequence b satisﬁes  wt b  ≥ Ngdg   4.19   Figure 4.30: Deﬁnition of generating tuples  tuple of the encoder. Note that a burst of length j + 1 has at least weight ab j  , where length is deﬁned in n-tuples and the corresponding number of bits is equal to n j + 1 . For an encoder characterised by its active burst distance, we will now bound the weight of the generated code sequence given the weight of the corresponding information sequence. Of course, this weight of the code sequence will depend on the distribution of the 1s in the input sequence. In order to consider this distribution, we introduce the notion of generating tuples as deﬁned in Figure 4.30. Let dg be an integer satisfying βb ≤ dg ≤ dfree. Remember that dfree is the free distance of the code and β b is a constant in the lower bound of the active burst distance deﬁned in Equation  3.11 . We deﬁne the generating length for dg as   cid:29   jg =  2dg − βb  α   cid:30   ,  where α is the slope of the lower bound on the active distances. The generating length jg is the minimum length j for which the lower bound on the active burst distance satisﬁes αj + βb ≥ 2dg, i.e. jg is the length of a burst that guarantees that the burst has at least weight 2dg. Now consider an arbitrary information sequence u. We call the ﬁrst non-zero k-tuple ut =  u 1    a generating tuple, because it will generate a weight of at least dfree in the code sequence b. But what happens if there are more non-zero input bits? Now the deﬁnition of the generating tuples comes in handy. Let t1 be the time index of the ﬁrst  , . . . , u k   , u 2   t  t  t   204 generating tuple, i.e. of the ﬁrst non-zero tuple ut =  u 1    in the sequence u. Moreover, let t2 be the time index of the ﬁrst non-zero tuple with t2 ≥ t1 + jg, and so on. We call the information tuples ut1 , ut2 , . . . generating tuples. If the number of generating tuples is Ng, then the weight of the code sequence is at least Ng · dg.  TURBO CODES  , . . . , u k   , u 2   t  t  t  Why is this true? Consider an encoder with generating length jg according to Equation  4.18 . The weight of a burst that is started by a generating tuple of the encoder will be at least dfree if the next generating tuple enters the encoder in a new burst, and at least 2dg if the next generating tuple enters the encoder inside the burst. This approach can be generalised. Let Ni denote the number of generating tuples corresponding to the ith burst. For Ni = 1 the weight of the ith burst is greater or equal to dfree, which is greater or equal to dg . The length of the ith burst is at least  Ni − 1 jg + 1 for Ni > 1 and we obtain  wt bursti   ≥ ˜ab   cid:2    cid:3   Ni − 1 jg ≥ α Ni − 1 jg + β  b  .  With Equation  4.18  we have αjg ≥ 2dg − βb and it follows that  wt bursti   ≥  Ni − 1  2dg − β    + β  b  b  ≥ Ni dg +  Ni − 2  dg − βb .  Taking into account that dg ≥ βb, i.e. dg − βb ≥ 0, we obtain wt bursti   ≥ Ni dg ∀ Ni ≥ 1.  cid:1   Finally, with Ng =   cid:1   i Ni we obtain wt b  ≥  wt bursti   ≥  Ni dg = Ngdg.  i  i  We will now use this result to bound the free distance of a WCC. Consider the encoder of a woven convolutional code with outer warp as depicted in Figure 4.29. Owing to the linearity of the considered codes, the free distance of the woven convolutional code is given by the minimal weight of all possible inner code sequences, except the all-zero sequence. free non- zero bits in the inner information sequence. Can we guarantee that there are at least d o free generating tuples? In fact we can if we choose a large enough lo. Let dg be equal to the free distance di free of the inner code. We deﬁne the effective length of a convolutional encoder as  If one of the outer code sequences bo l  is non-zero, then there exist at least d o   cid:30    cid:29   leff = k  2dfree − βb  .  α  eff be the effective length of the inner encoder. If lo ≥ li is non-zero, then there exist at least d o  Let li outer code sequences bo l the inner information sequence that generate a weight greater or equal to d i quently, it follows from inequality  4.19  that dWCC free in Figure 4.31.  eff holds and one of the free generating tuples in free. Conse- free. This result is summarised  ≥ do  freedi   TURBO CODES  205  Free distance of woven convolutional codes    We deﬁne the effective length of a convolutional encoder as   cid:29    cid:30   leff = k  2dfree − βb  α  WCC free  d  ≥ d  o freed  i free   4.20    4.21     Let li  eff be the effective length of the inner encoder. The free distance of the  eff outer convolutional encoders satisﬁes the inequality  WCC with lo ≥ li  Figure 4.31: Free distance of woven convolutional codes  For instance, let us construct a woven encoder employing lo outer codes. For outer codes we use the punctured convolutional code deﬁned in Section 3.1.5 with the generator matrix   cid:6    D  =  Go  1 + D 1 + D  D  0  1  1 + D   cid:7   .  This code has free distance d o code is Gi D  =  1, of the inner encoder is given by ˜ab,i j   = 0.5j + 4 and we have li lo = li R = RiRo = 1 3 .  = 3. The generator matrix of the inner convolutional = 5. The lower bound on the active burst distance = 12. Then, with = 15. The rate of the overall code is  = 12, we obtain the free distance d WCC  1+D2 1+D+D2  , i.e. d i  free  free  free  eff  eff  4.6.3 Woven Turbo Codes  free.  We will now analyse the free distance of a woven turbo code. In Section 4.6.2 we have used the concept of generating tuples and the fact that any non-zero outer code sequence in a woven convolutional encoder has at least weight d o  With woven turbo codes, however, the number of non-zero bits encoded by the inner encoder will be smaller than do free. We will now deﬁne a distance measure that enables us to estimate the free distance in the case of partitioned outer code sequences. We call this distance measure the partial distance.  The partitioning matrix P determines how a code sequence b is partitioned into the two sequences b 1  and b 2 . Note that, using the notion of a burst, we could deﬁne the free distance as the minimum weight of a burst. Analogously, we deﬁne the partial distance as the minimum weight of partial code sequence b 2  [0,j]. However, we condition this value on the weight of the partial code sequence b 1  [0,j]. The formal deﬁnition is given in Figure 4.32.   206  TURBO CODES  Partial distance and free distance of a WTC    Let b[0,j] denote a burst of length j + 1.   Considering all possible bursts, we deﬁne the partial distance with respect to a partitioning matrix P as the minimum weight of the partial code sequence b 2  [0,j], where we ﬁx the weight of the partial code sequence b 1  [0,j]  dp w  =  min b[0,j ],wt b 1  [0,j ]  =w    The free distance of the WTC with lo ≥ li w · di  ≥ min   cid:31   dWTC free  free  w  eff outer codes satisﬁes     + do  p  w    4.22    4.23    cid:31    cid:11    cid:12    wt  b 2  [0,j]  Figure 4.32: Deﬁnition of the partial distance and lower bound on the free distance of a  woven turbo code  WTC   For instance, the rate R = 1 2 convolutional code with generator matrix G D  =  1 +  D + D2, 1 + D2  has free distance dfree = 5. With partitioning matrix   cid:6    cid:7   P =  1 0   cid:6   we obtain the partial distances dp w  = 4, 2, 2, 2, 2, . . . with w = 2, 3, 4, . . . . Note that  cid:7  dp w = 1  does not exist. With partitioning matrix 0 1   cid:1  = P  we obtain the partial distances d distances exist only for even values of w.  p w  = 3, 2, 2, 2, 2, . . . with w = 2, 4, 6, . . . . These partial  cid:1  Using the partial distance, we can now bound the free distance of a WTC. Owing to the linearity of the considered codes, the free distance of the woven turbo code is given by the minimal weight of all possible overall code sequences b, except the all-zero sequence. If lo ≥ li has weight w, then there exist at least w generating tuples in the inner information sequence.  eff holds and one of the outer partial code sequences bo, 1   l  Thus, with inequality  4.19  we have wt b  ≥ w · di corresponding partial code sequence bo, 2  distance of the outer code. Hence, we have wt b  ≥ w · d i   free distance of the WTC with lo ≥ li free eff outer codes satisﬁes  is at least do  l  WTC free  d  ≥ min  w  + d  i free  o p  w   ,   cid:31  w · d  free. However, the weight of the p  w  denotes the partial p  w . Consequently, the  p  w , where do + do   TURBO CODES  207  free is the free distance of the inner code and li  where di eff denotes the effective length of the inner encoder according to Equation  4.20 . For instance, we construct a WTC with equal inner and outer rate Ri = Ro = 1 2 codes with generator matrices Gi D  = Go D  =  1, 1+D2 1+D+D2   similar to the examples given in Figure 4.16. After outer encoding, we apply the partitioning matrix P as given in the last example. Then, the overall rate is RWTC = 1 3. The lower bound on the active burst distance of the inner encoder is given by ˜ab,i j   = 0.5j + 4 and we have li ≥ 14 according to inequality  4.23 . Note that partitioning of the outer code sequences according ≥ 13. For an ordinary rate R = 1 3 turbo code with the same to P cid:1  inner and outer generator matrices as given above, and with a randomly chosen block interleaver, we can only guarantee a minimum Hamming distance d ≥ 7.  = 12. Thus, with lo ≥ 12 outer codes we obtain an overall code with free distance d WTC  would lead to dWTC free  free  eff  We should note that the results from inequalities  4.21  and  4.23  also hold when we employ row-wise interleaving and terminate the component codes. In this case, the overall code is a block code and the bounds become bounds for the overall minimum Hamming distance.  How close are those bounds? Up to now, no tight upper bounds on the minimum or free distance have been known. However, we can estimate the actual minimum Hamming distance of a particular code construction by searching for low-weight code words. For this purpose we randomly generate information sequences of weight 1, 2, and 3. Then we encode these sequences and calculate the weight of the corresponding code words. It is obvious that the actual minimum distance can be upper bounded by the lowest weight found.  In Figure 4.33 we depict the results of such a simulation. This histogram shows the distribution of the lowest weights, where we have investigated 1000 different random interleavers per construction. All constructions are rate R = 1 3 codes with memory m = 2 component codes, as in the examples above. The number of information symbols was K = 100. Note that for almost 90 percent of the considered turbo codes  TC  we get dTC ≤ 10, and we even found one example where the lowest weight was equal to the lower bound 7. However, with woven turbo codes with row-wise interleaving the lowest weight found was 15, and therefore dWTC ≤ 15. For the serial concatenated code  SCC  with random interleaving and for the WCC with row-wise interleaving, the smallest lowest weights were dSCC ≤ 7 and dWCC ≤ 17 respectively. For K = 1000 we have investigated 100 different interleavers per construction and obtained dWTC ≤ 18, dTC ≤ 10, dWCC ≤ 17 and dSCC ≤ 9. Thus, for these examples we observe only small differences between analytical lower bounds and the upper bounds obtained by simulation.  In Figure 4.34 we give some simulation results for the AWGN channel with binary phase shift keying. We employ the iterative decoding procedure discussed in Section 4.3.5. For the decoding of the component codes we use the suboptimum symbol-by-symbol a-posteriori probability algorithm from Section 3.5.2. All results are obtained for ten iterations of itera- tive decoding. We compare the performance of a rate R = 1 3 turbo code and woven turbo code as discussed above. Both codes have dimension K = 1000.  Because of the row-wise interleaving, the WTC construction possesses less randomness than a turbo code. Nevertheless, this row-wise interleaving guarantees a higher minimum Hamming distance. We observe from Figure 4.34 that both constructions have the same   208  TURBO CODES  Distribution of lowest-weight code words  TC WTC  SCC WCC  s e d o c   f  o    r e b m u n  900  800  700  600  500  400  300  200  100  0 0  250  200  s e d o c   f  o    r e b m u n  150  100  50  0 0  20 10 lowest weight  30  20 40 lowest weight  60  Figure 4.33: Distribution of lowest-weight code words for different rate R = 1 3 code  constructions  performance at low and moderate Bit Error Rate  BER . Owing to the higher minimum distance, the performance of the WTC becomes better at high SNR. Furthermore, the WTC outperforms the Turbo Code  TC  for the whole considered region of WER and achieves signiﬁcantly better performance at high SNR.  4.6.4 Interleaver Design  Now we apply the concept of generating tuples to serially concatenated codes as introduced in Section 4.3.3. A serially concatenated encoder consists of a cascade of an outer encoder, an interleaver and an inner encoder, where we have assumed that the interleaver is randomly chosen. The aim of this section is an interleaver design for this serial construction that guarantees a minimum Hamming distance similar to that of woven codes.  Let us ﬁrst consider a random interleaver. The minimum Hamming distance of the con- catenated code is given by the minimal weight of all possible inner code sequences, except the all-zero sequence. A non-zero outer code sequence has at least weight d o free. Owing to the random structure of the interleaver, those do free non-zero bits may occur in direct free  ki cid:19 . sequence after interleaving and therefore may be encoded as a burst of length  cid:18 d o   TURBO CODES  209  Simulation results for turbo and woven turbo codes  WTC TC  WTC TC  100  10−1  R E W  10−2  10−3  → R E B  10−3  10−4  100  10  1  10−2  10−5  10−6  10−7 0  1  E  N b  0  2  10−4 0  3  1   N E b 0  2  3  Figure 4.34: Simulation results for rate R = 1 3 turbo and woven turbo codes for the  AWGN channel  Such a burst has at least weight ab,i  cid:18 do free  ki cid:19  − 1 , where ab,i ·  is the active burst dis- tance of the inner code. However, the weight of the inner code sequence cannot be less then di free. We conclude that the minimum Hamming distance of the Serially Concatenated % Code  SCC  with a randomly chosen interleaver satisﬁes the inequality   cid:6  cid:29    cid:30   dSCC ≥ max  di free, ab,i  do free ki   cid:7 & − 1  .  Of course, we have considered a worst-case scenario, and other interleavers may lead to a much higher minimum Hamming distance. For the rate R = 1 3 SCC with memory m = 2 component codes as discussed in the previous section, this results in the estimate d SCC ≥ 7. However, we have seen in Figure 4.33 that it is actually not too difﬁcult to ﬁnd interleavers that lead to such a low minimum Hamming distance.  Nevertheless, the above worst-case considerations lead to a criterion for good inter- leavers with respect to the overall minimum distance. Obviously, the bits of an outer code sequence should be separated after interleaving. Formally, we can specify this characteris- tic of the interleaver as in the deﬁnition of Figure 4.35. This criterion guarantees that an  l1, l2 -interleaver separates bits that are in close proximity before the interleaving by at least l2 positions after the interleaving.   210   l1, l2 -interleaver  TURBO CODES    Let t and t    We consider all possible pairs of indices  t, t   cid:1  denote two time indices of bits before interleaving.  cid:1  with t − t   cid:1  < l1.   denote the corresponding indices after interleaving.   , t  cid:7 = t   cid:1    cid:1     Let π t   and π t    We call an interleaver  l1, l2 -interleaver if for all such pairs  t, t    it satisﬁes  π t   − π t   cid:1     ≥ l2   cid:1    4.24   Figure 4.35: Deﬁnition of  l1, l2 -interleaving  Such an  l1, l2 -interleaver can be realised by an ordinary block interleaver. A block interleaver is a two-dimensional array where we write the bits into the interleaver in a row-wise manner, whereas we read the bits column-wise. However, such a simple block interleaving usually leads to a signiﬁcant performance degradation with iterative decoding. To introduce some more randomness in the interleaver design, we can permute the symbols in each row randomly, and with a different permutation for each row. When we read the bits from the interleaver, we still read them column-wise, but we ﬁrst read the bits from the even-numbered rows, then from the odd-numbered rows. This still achieves an  l1, l2 - structure. Such an interleaver is called an odd even interleaver and was ﬁrst introduced for turbo codes. The turbo coding scheme in the UMTS standard  see Section 4.3.2  uses a similar approach.  A more sophisticated approach uses search algorithms for pseudorandom permutations, guaranteeing that the conditions in Figure 4.35 are fulﬁlled  H¨ubner and Jordan, 2006; H¨ubner et al., 2004 . We will not discuss the particular construction of an  l1, l2 -interleaver, but only investigate whether such an interleaver ensures a minimum Hamming distance that is at least the product of the free distances of the component codes. This result was ﬁrst published by Freudenberger et al.  Freudenberger et al. 2000b; see also Freudenberger et al. 2001 .  But how do we choose the parameters l1 and l2 in order to achieve a larger minimum Hamming distance? In the case of l2 the answer follows from the investigations in the previous sections. In order to bound the minimum Hamming distance of the concatenated code, we are again looking for the minimum weight sequence among all possible inner code sequences. If the outer code sequence has only weight d o free, those successive bits in the outer code sequence should be sufﬁciently interleaved to belong to independent generating tuples. Hence, we require l2 = li eff is the effective length of the inner encoder. The parameter l1 has to consider the distribution of the code bits in the outer code sequence. Again, we can use the active distances to determine this parameter. In Figure 4.36, we deﬁne the minimum length of a convolutional encoder on the basis of the active column distance and the active reverse column distance  see Section 3.3.2 . Both distance measures  eff, where li   TURBO CODES  Minimum length    Consider a rate R = k n convolutional encoder and its active distances.   Let jc denote the minimum j for which ac j   ≥ dfree holds  jc = argmin {a  c   j   ≥ dfree}    Let jrc denote the minimum j for which arc j   ≥ dfree holds  jrc = argmin  {arc j   ≥ dfree}  j  j    We deﬁne the minimum length as  lmin = min{n jc + 1 , n jrc + 1 }  Figure 4.36: Minimum length  211   4.25    4.26    4.27   consider the weight growth of a burst with increasing segment length. The column distance considers the weight growth in the direction from the start to the end of the burst, while the reverse column distance regards the opposite direction.  The minimum length is an estimate of the positions of the 1s in a code sequence. A burst may have dfree or more non-zero bits. However, when we consider a span of lmin positions at the start or end of the burst, this span includes at least dfree 1s.  Let us summarise these two results. The deﬁnition of the minimum length ensures that we have to consider at most lo min code bits of a burst to obtain an outer code segment with at least do free non-zero bits. The deﬁnition of the effective length guarantees that those bits in the outer code sequence are sufﬁciently interleaved to belong to independent generating tuples.  Consequently, using an  l1, l2 -block interleaver with l1 ≥ lo  eff, there exist at least do free generating tuples in each non-zero input sequence to the inner encoder. With the results from Figure 4.30, it follows that the minimum Hamming distance of the SCC with an  l1, l2 -block interleaver with l1 ≥ lo SCC ≥ d  eff satisﬁes the inequality  min and l2 ≥ li  min and l2 ≥ li  d  o freed  i free.  It should be clear from this discussion that the concept of  l1, l2 -interleaving can also be applied to partially concatenated codes as introduced in Section 4.3.4. In this case we obtain the same bound as for woven turbo codes in Figure 4.32.  Furthermore, we should note that the concept of designing interleavers on the basis of the active distances is not limited to the product distance. Freudenberger et al. presented an interleaver design for woven codes with row-wise interleaving that resulted in a minimum   212  TURBO CODES  Hamming distance of about twice the product of the distances of the component codes  Freudenberger et al., 2001 . A similar result was obtained by H¨ubner and Richter, but with a design that enabled much smaller interleaver sizes  H¨ubner and Richter, 2006 .  4.7 Summary  There exist numerous possible code constructions for turbo-like concatenated convolutional codes. Most of these constructions can be classiﬁed into the two major classes of parallel  Berrou et al., 1993  and serially  Benedetto and Montorsi, 1998  concatenated codes. However, other classes such as multiple concatenations  Divsalar and Pollara, 1995  and hybrid constructions  Divsalar and McEliece, 1998  are known, i.e. combinations of parallel and serial concatenations.  Therefore, we have concentrated our discussion on some, as we think, interesting code classes. In Section 4.3.4 we introduced the concept of partial concatenation  Freudenberger et al., 2004 . Partially concatenated convolutional codes are based on the idea of parti- tioning the code sequences of the outer codes in a concatenated coding system. Partially concatenated convolutional codes provide a general framework to investigate concatenated convolutional codes. For example, parallel and serially concatenated convolutional codes can be regarded as special cases of this construction.  The concept of partial concatenation was ﬁrst introduced  Freudenberger et al., 2001  in connection with woven turbo codes which belong to the general class of woven convo- lutional codes. The woven code construction was ﬁrst introduced and investigated by H¨ost, Johannesson and Zyablov  H¨ost et al., 1997 . A series of papers on the asymptotic behaviour of WCC show their distance properties  Zyablov et al., 1999a  and error-correcting capa- bilities  Zyablov et al., 1999b, 2001 . The characteristics of woven codes were further investigated  Freudenberger et al., 2001; H¨ost, 1999; H¨ost et al., 2002, 1998; Jordan et al., 2004a .  In the context of turbo codes, the idea of looking at code ensembles rather than individ- ual codes was introduced by Benedetto and Montorsi. Methods for estimating the weight distribution of turbo codes and serial concatenations with randomly chosen interleavers were presented  Benedetto and Montorsi, 1996; Perez et al., 1996; Benedetto and Montorsi, 1998 . These weight distributions can be used for bounding the average maximum likeli- hood performance of the considered code ensemble. Based on this technique, Benedetto and Montorsi derived design rules for turbo codes as well as for serially concatenated codes.  The analysis of the iterative decoding algorithm is the key to understanding the remark- ably good performance of LDPC and turbo-like codes. The ﬁrst analysis for a special type of belief propagation was made by Luby et al.  Luby et al., 1998 . This analysis was applied to hard decision decoding of LDPC codes  Luby et al., 2001  and generalised to belief propagation over a large class of channels  Richardson and Urbanke, 2001 .  The analysis for turbo-like codes was pioneered by ten Brink  ten Brink, 2000 . In Section 4.4 we considered the analysis of the iterative decoding algorithm for concatenated convolutional codes. Therefore, we utilised the extrinsic information transfer characteristics as proposed by ten Brink. The idea is to predict the behaviour of the iterative decoder by looking at the input output relations of the individual constituent decoders.  In Section 4.6 the minimum Hamming distance was the important criterion for the code search and code construction. We derived lower bounds on the minimum Hamming   TURBO CODES  213  distance for different concatenated code constructions. Restricting the interleaver to the class of  l1, l2 -permutations made it possible to improve the lower bounds.  Naturally, the reader will be interested in design guidelines to construct good con- catenated convolutional codes. However, giving design guidelines remains a subtle task, because none of the three considered methods for analysing the code properties gives a complete picture of the code performance with iterative decoding.  A possible approach could be based on the results of Section 4.5, where we considered the expected weight distribution. However, bounds on performance based on the expected weight distribution usually assume maximum likelihood decoding.  EXIT charts are a suitable method for investigating the convergence behaviour of the iterative decoding for long codes. For codes of moderate length it becomes important to take the interleaving depth into account.  With respect to the minimum Hamming distance of the concatenated code, especially for codes of short length, the choice of the particular interleaver is very important. The use of designed interleavers is motivated by the asymptotic coding gain. In order to ensure efﬁcient performance for high signal-to-noise ratios, the concatenated code should have a large minimum Hamming distance, which motivates the use of designed interleavers.  Considering the convergence behaviour of the iterative decoding or the overall dis- tance spectrum, a connection with the active distances or partial distances is not obvious. Anyhow, we would like to note that, in all presented examples, optimising the slope of the active distances or optimising the partial distances led to the best results. Tables of convolutional codes and of punctured convolutional codes with good active distances can be found elsewhere  Jordan et al., 2004b .  Certainly, it would be desirable to predict the absolute code performance with sub- optimum iterative decoding. First results on the ﬁnite-length analysis of iterative coding systems are available  Di et al., 2002 . However, these results are at present limited to the class of low-density parity-check codes and to the binary erasure channel.    5  Space–Time Codes  5.1 Introduction  Today’s communication systems approach more and more the capacity limits predicted by Shannon. Sophisticated coding and signal processing techniques exploit a physical radio link very efﬁciently. Basically, the capacity is limited by the bandwidth restriction. In the course of the deployment of new services and applications, especially multimedia ser- vices, the demand of high data rates is currently increasing and will continue in the near future. Since bandwidth is an expensive resource, network providers are looking for efﬁ- cient opportunities to increase the system capacity without requiring a larger portion of the spectrum. In this context, multiple antennas represent an efﬁcient opportunity to increase the spectral efﬁciency of wireless communication systems by exploiting the resource space. They became popular a decade ago as a result of fundamental work  Alamouti, 1998; Fos- chini, 1996; Foschini and Gans, 1998; K¨uhn and Kammeyer, 2004; Seshadri and Winters, 1994; Seshadri et al., 1997; Tarokh et al., 1998; Telatar, 1995; Wittneben, 1991; Wolni- ansky et al., 1998; and many others . After initial research activities, their large potential has been widely recognised, so that they have been incorporated into several standards. As an example, very simple structures can already been found in Release 99 of Universal Mobile Telecommunications System  UMTS  systems  Holma and Toskala, 2004 . More sophisticated methods are in discussion for further evolutions  3GPP, 2007; Hanzo et al., 2002 .  Basically, two different categories of how to use multiple antennas can be distinguished. In the ﬁrst category, the link reliability is improved by increasing the instantaneous signal- to-noise ratio  SNR  or by reducing the variations in the fading SNR. This leads to lower outage probabilities. The ﬁrst goal can be accomplished by beamforming, i.e. the main lobe of the antenna pattern steers in distinct directions. A reduced SNR variance is obtained by increasing the system’s diversity degree, e.g. by space–time coding concepts. Moreover, spatially separable interferers can be suppressed with multiple antennas, resulting in a signal-to-interference-plus-noise-ratio  SINR .  In the second category of multiple-input multiple-output  MIMO  techniques, the data rate is multiplied by transmitting several independent data streams simultaneously, one  Coding Theory – Algorithms, Architectures, and Applications Andr´e Neubauer, J¨urgen Freudenberger, Volker K¨uhn  cid:148  2007 John Wiley & Sons, Ltd   216  SPACE–TIME CODES  over each antenna. If all data streams belong to a single user, this approach is denoted as Space Division Multiplexing  SDM , otherwise it is termed Space Division Multiple Access  SDMA . It will be shown in Section 5.3 that the potential capacity gain of multiple- antenna systems is much larger than the gain obtained by simply increasing the transmit power. Certainly, diversity and multiplexing techniques do not exclude each other but can be combined. An interesting example are multistratum codes  B¨ohnke et al., 2004a,b,c .  This chapter introduces two MIMO strategies, orthogonal space–time block codes and spatial multiplexing, and is therefore not comprehensive. Well-known coding techniques such as space–time trellis codes  B¨aro et al., 2000a,b; Naguib et al., 1997, 1998; Seshadri et al., 1997; Tarokh et al., 1997, 1998  and non-orthogonal space–time block codes  Bossert et al., 2000, 2002; Gabidulin et al., 2000; Lusina et al., 2001, 2003, 2002  are not consid- ered here. Moreover, detection strategies such as the sphere detector  Agrell et al., 2002; Fincke and Pohst, 1985; Schnoor and Euchner, 1994  achieving maximum likelihood per- formance or lattice-reduction-based approaches  K¨uhn, 2006; Windpassinger and Fischer, 2003a,b; W¨ubben, 2006; W¨ubben et al., 2004a,b  will not be presented. A comprehensive overview of space–time coding is available elsewhere  Liew and Hanzo, 2002 .  The ﬁrst four chapters introduced coding and decoding techniques of error-correcting codes. In order to focus on the main topic, they simpliﬁed the communication system very much and hid all system components that were not actually needed for the coding itself in a channel model with appropriate statistics. This is not possible for multiple-antenna techniques because they directly inﬂuence the channel model. Although we try to keep the system as simple as possible, some more components are required.  Therefore, this introduction contains two sections describing brieﬂy digital modulation schemes and the principle of diversity. Section 5.2 extends the scalar channel used in pre- vious chapters to a MIMO channel with multiple inputs and outputs. Some information about standardisation issues for MIMO channels are also presented. Section 5.3 derives different performance measures for MIMO transmission strategies. In Section 5.4, orthog- onal space–time block codes are introduced. Owing to their simplicity, they have already found their way into existing mobile radio standards. Section 5.5 explains spatial multi- plexing, and Section 5.6 gives a short overview of currently discussed MIMO techniques for UMTS.  5.1.1 Digital Modulation Schemes  Before we start to describe MIMO channels and speciﬁc space–time coding techniques, we will review linear modulation schemes and the principle of diversity. We will abandon a detailed analysis and refer instead to the rich literature  Benedetto and Biglieri, 1999; Kammeyer, 2004; Proakis, 2001; Schulze and L¨uders, 2005; Sklar, 2003 . According to Figure 5.1, the modulator located at the transmitter maps an m-bit tuple  b[ cid:6 ] = cid:2   b1[ cid:6 ], . . . , bm[ cid:6 ]   cid:3 T = cid:2 ˜b[i], . . . , ˜b[i + m − 1]  cid:3 T   cid:21    cid:22   i  m  with   cid:6  =  onto one of M = 2m possible symbols Sµ ∈ S, where M = S denotes the size of the alphabet S. The average symbol energy is given in Equation  5.1 , where the last equality holds if all symbols and, hence, all bit tuples are equally likely. The mapping M of b[ cid:6 ] onto s[ cid:6 ] = M b[ cid:6 ]  can be performed in different ways. While the symbol error rate only depends on the geometrical arrangement of the symbols   SPACE–TIME CODES  Digital modulation  217  ˜b[i]  S P  s[ cid:6 ] = M b[ cid:6 ]   signal mapper  b[ cid:6 ] 1  m    m-bit tuple b[ cid:6 ] obtained from serial-to-parallel conversion.   m-bit tuple is mapped onto one of M = 2m symbols Sµ ∈ S.   Mapping strategies:  – Gray mapping: neighbouring symbols differ only in a single bit – Natural mapping: counting symbols counterclockwise – Anti-Gray mapping: neighbouring symbols differ in many bits    Average symbol energy for equiprobable symbols Sµ  Es = Ts · E{Sµ2} = Ts · M−1 cid:1   Pr{Sµ} · Sµ2 =  i.i.d.  Ts M  µ=0  Sµ2   5.1   · M−1 cid:1   µ=0  Figure 5.1: Principles of linear digital modulation  as well as the signal-to-noise ratio, the bit error rate is also affected by the speciﬁc mapping of the m-tuples onto the symbols Sµ. In uncoded systems, Gray mapping delivers the lowest bit error probability because neighbouring symbols differ only in one bit, leading to single- bit errors when adjacent symbols are mixed up. In the context of concatenated systems with turbo detection, different strategies such as anti-Gray mapping should be preferred because they ensure a better convergence of the iterative detection scheme  Sezgin et al., 2003 . At the receiver, the demodulator has to deliver an estimate for each bit in b[ cid:6 ] on the basis of the received symbol r[ cid:6 ] = h[ cid:6 ]s[ cid:6 ] + n[ cid:6 ]. The factor h[ cid:6 ] represents the complex-valued channel coefﬁcient which is assumed to be perfectly known or estimated at the receiver. A look at Figure 5.2 shows two different approaches. The ML symbol detector chooses the hypothesis ˆs[ cid:6 ] that minimizes the smallest squared Euclidean distance between h[ cid:6 ]ˆs[ cid:6 ] and the received symbol r[ cid:6 ]. It therefore performs a hard decision, and the bits bµ[ cid:6 ] are obtained by the inverse mapping procedure ˆb[ cid:6 ] = M−1 ˆs[ cid:6 ] . In practical implementations, thresholds are deﬁned between adjacent symbols and a quantisation with respect to these thresholds delivers the estimates ˆs[ cid:6 ].  Especially in concatenated systems, hard decisions are not desired because subsequent decoding stages may require reliability information, e.g. log-likelihood values, on the bit level. In these cases, the best way is to apply the bit-by-bit Maximum A-Posteriori  MAP  detector which delivers an LLR for each bit bµ[ cid:6 ] in b[ cid:6 ] according to Equation  5.3 .   218  SPACE–TIME CODES  Digital demodulation    Received signal for ﬂat fading channels and AWGN  r[ cid:6 ] = h[ cid:6 ] · s[ cid:6 ] + n[ cid:6 ].    Maximum Likelihood  ML  symbol detector decides in favour of the symbol  Sµ closest to r[ cid:6 ]  ˆs[ cid:6 ] = argmin˜s  r[ cid:6 ] − h[ cid:6 ] · ˜s   cid:17  cid:17    cid:17  cid:17 2  and  ˆb[ cid:6 ] = M−1 ˆs[ cid:6 ]    5.2     A-Posteriori Probability  APP  bit detector delivers a Log-Likelihood Ratio   LLR  for each bit in b[ cid:6 ]  L bµ[ cid:6 ]  = ln  Pr{bµ = 0  r[ cid:6 ]} Pr{bµ = 1  r[ cid:6 ]} = ln    ˜s∈S,bµ=0 p r[ cid:6 ]  ˜s  · Pr{˜s} ˜s∈S,bµ=1 p r[ cid:6 ]  ˜s  · Pr{˜s}   5.3   Figure 5.2: Principles of digital demodulation  The signal alphabet S = {S0, . . . , SM−1} strongly depends on the type of modulation. Generally, amplitude, phase and frequency modulation are distinguished. Since we are conﬁning ourselves in this chapter to linear schemes, only the ﬁrst two techniques will be considered.  Amplitude Shift Keying  ASK   ASK is a modulation scheme that maps the information onto the amplitude of real-valued symbols. Two examples, a binary signalling scheme  2-ASK  and 4-ASK, are depicted in the upper row of Figure 5.3. Generally, the amplitudes are chosen from a signal alphabet  S = cid:4    2µ + 1 − M  · e  0 ≤ µ < M   cid:5   consisting of M odd multiples of a constant e. The parameter e is adjusted such that the energy constraint in Equation  5.1  is fulﬁlled. For equally likely symbols, the result in Equation  5.4  is obtained, leading to the minimum normalised squared Euclidean distance given in Equation  5.5 .  Quadrature Amplitude Modulation  QAM   QAM is deployed in many modern communication systems such as Wireless Local Area Network  WLAN  systems  ETSI, 2001; Hanzo et al., 2000  and the High-Speed Down- link Packet Access  HSDPA  in UMTS  Holma and Toskala, 2004 . It differs from the   SPACE–TIME CODES  219  Amplitude shift keying  ASK   2-ASK Im  4-ASK  Im  -e  e  Re  -3e  -e  e  3e  Re    Energy normalisation   cid:9    2 != Es ⇒ e =   2µ + 1 − M e  M−1 cid:1   .  µ=0  Ts M    Normalised minimum squared Euclidean distance  3  M2 − 1  · Es Ts   5.4   2  cid:19  0  =  2e 2 Es Ts  = 12  M2 − 1   5.5   Figure 5.3: ASK modulation: symbol alphabets for 2-ASK  e = √  Es Ts  and 4-ASK  Es Ts 5  and main modulation parameters   e = √  real-valued ASK in that the symbols’ imaginary parts are also used for data transmis- sion. Figure 5.4 shows examples of 4-QAM and 16-QAM. Real and imaginary parts of s[ cid:6 ] can be chosen independently from each other so that the number of bits per symbol, and hence the spectral efﬁciency, is doubled compared with ASK schemes. Fur- thermore, both real and imaginary parts contribute equally to the total symbol energy, resulting in Equation  5.6 . Accordingly, the minimum squared Euclidean distance is given in Equation  5.7 .  Phase Shift Keying  PSK   √ The Phase Shift Keying arranges all symbols on a circle with radius Es Ts, resulting in identical symbol energies. This is an important property, especially in mobile radio communications because the mobile unit has a limited battery lifetime and an energy- efﬁcient power ampliﬁer is essential. They are generally designed for a certain operat- ing point so that an energy-efﬁcient transmission is obtained if the transmitted signal has a constant complex envelope. Within the Global System for Mobile communica- tions  GSM  extension Enhanced Data rates for GSM Evolution  EDGE   Olofsson and Furusk¨ar, 1998; Schramm et al., 1998 , 8-PSK is used in contrast to Gaussian Mini- mum Key Shifting  GMSK  as in standard GSM systems. This enlarges the data rate   220  SPACE–TIME CODES  Quadrature amplitude modulation  QAM   4-QAM Im  √ Es Ts  16-QAM Im  3e  e  -3e   cid:9   Re  -3e  -e  -e  e  3e  Re    Energy normalisation with M   cid:1  = √  M   cid:1 −1 cid:1   M  µ=0  2  2e   2µ + 1 − M   cid:1      2 != Es Ts  ⇒  e =  3  2 M − 1   · Es Ts   5.6     Normalised minimum squared Euclidean distance  2  cid:19  0  =  2e 2 Es Ts  = 6  M − 1   5.7   Figure 5.4: QAM modulation: symbol alphabets for 4-QAM  e = 1  and 16-QAM  Es Ts 10  and main modulation parameters   e = √  signiﬁcantly since 3 bits are transmitted per symbol compared with only a single bit for GMSK.  The bits of the m-tuples b[ cid:6 ] determine the symbols’ phases which are generally mul- tiples of 2π M. Alternatively, an offset of π M can be chosen, as shown in Figure 5.5 for Quaternary Phase Shift Keying  QPSK , 8-PSK and 16-PSK. Binary Phase Shift Keying for M = 2 and QPSK for M = 4 represent special cases because they are identical to 2- ASK and 4-QAM respectively. For M > 4, real and imaginary parts are not independent from each other and have to be detected simultaneously. The normalised minimum squared Euclidean distance is given in Equation  5.8 .  Error Rate Performance  Unfortunately, the exact symbol error probability cannot be expressed in closed form for all considered modulation schemes. However, a common tight approximation exists. Assuming that most error events mix up adjacent symbols, the average error probability is dominated by this event. For the Additive White Gaussian Noise  AWGN  channel, we obtain the   8-PSK Im  √ Es Ts  16-PSK Im  √ Es Ts  SPACE–TIME CODES  Phase shift keying  PSK   QPSK Im  √ Es Ts  Re    Normalised minimum squared Euclidean distance  Re   cid:11    cid:12   π  M  = 4 sin2  2  cid:19  0  Figure 5.5: PSK modulation: symbol alphabets and main modulation parameters  error probability approximation  K¨uhn, 2006   221  Re   5.8    5.9   In expression  5.9  the parameter a depends on the modulation scheme and amounts to   cid:9  cid:6          cid:7 2   cid:19 0 2  Es N0  Ps ≈ a · erfc    M − 1  M  2 ·   1  √  a =  √ M − 1   M for QAM  for ASK  .  for PSK   cid:8  cid:9    cid:10   P  BPSK b  · erfc  = 1 2  Es N0  The well-known result for Binary Phase Shift Keying  BPSK   is obtained by setting M = 2 for ASK.  For ﬂat fading channels, the instantaneous signal-to-noise ratio depends on the actual  channel coefﬁcient h[ cid:6 ]. Hence, the SNR, and, consequently, the error probability   cid:2    cid:3  ≈ a · erfc  Ps  h[ cid:6 ]   cid:9  cid:6       cid:7 2 h[ cid:6 ]2 Es  N0   cid:19 0 2      5.10    222  SPACE–TIME CODES  are random variables. The average error probability is obtained by calculating the expec- tation of expression  5.10  with respect to h[ cid:6 ]. For the Rayleigh fading channel, we obtain   cid:9    cid:4   Ps = E  Ps h[ cid:6 ]    1 −   cid:5  ≈ a ·      5.11     cid:19 0 2 2Es N0 1 +   cid:19 0 2 2Es N0  From the results above, we can conclude that the error probability obviously increases with growing alphabet size M. For a ﬁxed average transmit power, large alphabets S lead to small Euclidean distances and, therefore, to a high probability of a detection error. On the other hand, the spectral efﬁciency of a modulation scheme measured in bits per symbol increases if S is enlarged. Hence, communication engineers have to ﬁnd a trade-off between high spectral efﬁciencies and low error probabilities. Some numerical results that conﬁrm this argumentation are presented in Figure 5.6. Astonishingly, 16-QAM performs much better than 16-PSK and nearly as well as 8-PSK because QAM schemes exploit the two-dimensional signal space much more efﬁciently than their PSK counterparts, leading to larger squared Euclidean distances.  While the error probabilities show an exponential decay when transmitting over the AWGN channel, the slope is asymptotically only linear for the Rayleigh fading channel. From this observation we can conclude that a reliable uncoded transmission over fading channels requires a much higher transmit power in order to obtain the same error proba- bility as for the white Gaussian noise channel. Instead of increasing the transmit power, appropriate coding represents a powerful alternative.  Error probabilities for digital modulation schemes  AWGN  Rayleigh fading  →  s  P  100  10 cid:358 1  10 cid:358 2  10 cid:358 3  10 cid:358 4  10 cid:358 5  10 cid:358 6 0  100  10 cid:358 1  10 cid:358 2  10 cid:358 3  10 cid:358 4  10 cid:358 5  →  s  P  BPSK QPSK 8-PSK 16-PSK 16-QAM 64 cid:358 QAM  10  5 15 Eb N0 in dB →  10 cid:358 6 0  20  20  10 30 Eb N0 in dB →  40  Figure 5.6: Error rate performance for digital modulation schemes:   a  AWGN channel,  b  ﬂat Rayleigh fading channel   SPACE–TIME CODES 5.1.2 Diversity  223  The detrimental effect of fading on the error rate performance can be overcome if sources of diversity are available. Having diversity means that a symbol s[ cid:6 ] is transmitted over different independent propagation paths. The risk that all available paths experience simul- taneously a deep fade is much smaller than the probability that a single channel has a small gain. Therefore, variations in the signal-to-noise ratio owing to fading can be signiﬁcantly reduced, leading to much smaller error probabilities. If, however, correlations between the propagation paths exist, the potential diversity gain is smaller.  Diversity can originate from different sources which are listed in Figure 5.7. The general principle will be explained for the simple example of receive diversity. More sophisticated techniques are described later in subsequent sections. Let us assume for the moment that a symbol x[ cid:6 ] arrives at the receiver via D independent parallel propagation paths with chan- nel coefﬁcients hµ[ cid:6 ], 1 ≤ µ ≤ D, as depicted in Figure 5.8. These paths may stem from  Sources of diversity    Frequency diversity:  Frequency-selective channels provide frequency diversity. The signal is transmitted over different propagation paths that differ in strength and delay. They are often modelled time varying and statistically independent  uncorrelated scattering assumption . With appropriate receiver structures such as linear FIR ﬁlters, the Viterbi equaliser or the Rake receiver for Code Division Multiple Access  CDMA  systems, frequency diversity can be exploited.    Time diversity:  The application of Forward Error Correction  FEC  coding yields time diversity if the channel varies signiﬁcantly during one code word or coded frame. In this case, the decoder performs a kind of averaging over good and bad channel states.    Space diversity:  In this chapter, systems using multiple antennas at transmitter or receiver are deployed to use spatial diversity. If the antenna elements are more than several wavelengths apart from each other, the channels can be assumed independent and diversity is obtained.    Polarisation diversity:  If antennas support different polarisations, this can be used for polarisation diversity.  Figure 5.7: List of different diversity sources   224  SPACE–TIME CODES  Receive Diversity and Maximum Ratio Combining  x[ cid:6 ]  n1[ cid:6 ]  n2[ cid:6 ]  y1[ cid:6 ]  y2[ cid:6 ]  nD[ cid:6 ]  yD[ cid:6 ]  h1[ cid:6 ]  h2[ cid:6 ]  hD[ cid:6 ]  Maximum  Ratio  Combiner  ˜x[ cid:6 ]    Looking for combiner w[ cid:6 ] that maximises SNR  ˜x[ cid:6 ] = wH[ cid:6 ] · y[ cid:6 ] = wH[ cid:6 ] · h[ cid:6 ]x[ cid:6 ] + wH[ cid:6 ] · n[ cid:6 ]   5.12     SNR after combining  γ [ cid:6 ] = wH[ cid:6 ]h[ cid:6 ]2 · σ 2X  cid:30 w[ cid:6 ] cid:30 2 · σ 2N   5.13     Optimisation leads to Maximum Ratio Combining  MRC  with w[ cid:6 ] = h[ cid:6 ]  ˜x[ cid:6 ] =  cid:30 h[ cid:6 ] cid:30 2x[ cid:6 ] + hH[ cid:6 ] · n[ cid:6 ] with γ [ cid:6 ] =  cid:30 h[ cid:6 ] cid:30 2 · Es   5.14   N0  Figure 5.8: Illustration of receive diversity and optimum Maximum Ratio Combining  MRC . Reproduced by permission of John Wiley & Sons, Ltd  the deployment of D receive antennas collecting the emitted signal at different locations. The received samples  yµ[ cid:6 ] = hµ[ cid:6 ] · x[ cid:6 ] + nµ[ cid:6 ] ,  1 ≤ µ ≤ D   5.15   are disturbed by independent noise contributions nµ[ cid:6 ] resulting in instantaneous signal-to- noise ratios  γµ[ cid:6 ] = hµ[ cid:6 ]2 · Es  N0  = σ 2  with expectations γ µ µEs N0. The samples have to be appropriately combined. We will conﬁne ourselves in this chapter to the optimal Maximum Ratio Combining  MRC  technique which maximises the signal-to-noise ratio at the combiner’s output. A description of further approaches such as equal gain combining, square-law combining and selection combining can be found elsewhere  K¨uhn, 2006; Simon and Alouini, 2000 .   SPACE–TIME CODES  Using vector notations, the received vector y[ cid:6 ] = cid:2   y[ cid:6 ] = h[ cid:6 ] · x[ cid:6 ] + n[ cid:6 ]  y1[ cid:6 ], . . . , yD[ cid:6 ]   cid:3 T has the form  225   5.16   writing the coefﬁcients of the combiner into a vector w leads to Equation  5.12  and the corresponding SNR in Equation  5.13 . Since the product wHh describes the projection of w onto h, the SNR in Equation  5.13  becomes largest if w and h are parallel, e.g. if w = h holds. In this case, the received samples yµ[ cid:6 ] are weighted by the corresponding complex µ[ cid:6 ] and combined to the signal ˜x[ cid:6 ]. This procedure is called ∗ conjugate channel coefﬁcient h MRC and maximises the signal-to-noise ratio  SNR . For independent channel coefﬁcients and noise samples, it amounts to  γ [ cid:6 ] =  cid:30 h[ cid:6 ] cid:30 2 · Es  N0  hµ[ cid:6 ]2 · Es  .  N0  = D cid:1   µ=1  Essentially, the maximum ratio combiner can be interpreted as a matched ﬁlter that also maximises the SNR at its output. If the channel coefﬁcients in all paths are identically Rayleigh distributed with average power σ 2H = 1, the sum of their squared magnitudes is chi-squared distributed with 2D degrees of freedom  Bronstein et al., 2000; Simon and Alouini, 2000   The achievable gain due to the use of multiple antennas at the receiver is twofold. First, the hµ2 equals D. Hence, the average SNR is increased mean of the new random variable by a factor D and amounts to  µ   5.17   p  hµ2  ξ   = ξ D−1  D − 1 !   −ξ  · e  γ = E{γ [ cid:6 ]} = D · Es  .  N0  This enhancement is termed array gain and originates from the fact that the D-fold signal energy is collected by the receive antennas. The second effect is called diversity gain and can be illuminated best by normalising γ [ cid:6 ] to unit mean. Using the relation  Y = a · X  ⇒  pY  y  = 1  a · pX  we can transform the Probability Density Function  PDF  in Equation  5.18  with a = Es N0 D into that of the normalised SNR in Equation  5.17 . Numerical results are shown in Figure 5.9. With growing diversity degree D, the instantaneous signal-to-noise ratio concentrates more and more on Es N0 and the SNR variations become smaller. Since very low signal-to-noise ratios occur less frequently, the average error probability will decrease. For D → ∞, the SNR does not vary any more and the AWGN channel without fading is obtained.   cid:11    cid:12   y  a  ,  Ergodic Error Probability for MRC  Regarding the average error probability, the solution in expression  5.10  can be reused. hµ[ cid:6 ]2. Second, First, the squared magnitude h[ cid:6 ]2 has to be replaced by the sum    µ   226  SPACE–TIME CODES  SNR distribution for diversity reception    Probability density function of SNR normalised to unit mean  !  " D ·  p ξ   =  D  Es N0  ξ D−1  D − 1 !  · e  −ξ D  Es N0    5.18   D = 1 D = 2 D = 4 D = 10 D = 20 D = 100  →    ξ   p  5  4  3  2  1  0 0  0.5  1  2  2.5  3  1.5 ξ →  Figure 5.9: Probability density function of SNR for D-fold diversity, Rayleigh distributed coefﬁcients and Maximum Ratio Combining  MRC . Reproduced by  permission of John Wiley & Sons, Ltd  the expectation with respect to the chi-square distribution with 2D degrees of freedom has to be determined. For BPSK and i.i.d. diversity branches, an exact solution is given in Equation  5.21   Proakis, 2001 . The parameter γ denotes the average SNR in each branch. A well-known approximation for γ   1 provides a better illustration of the result. In this case,  1 + α  2 ≈ 1 holds and the sum in Equation  5.21  becomes  D−1 cid:1    cid:6  D − 1 +  cid:6    cid:7    cid:6   cid:7  2D − 1  =   5.19  Furthermore, the Taylor series yields  1 − α  2 ≈ 1  4γ  . With these results, the symbol error rate can be approximated for large signal-to-noise ratios by   cid:6 =0  D   cid:6    cid:6    cid:7  D ·   cid:6   cid:7  2D − 1  D  Ps ≈  1 4γ   cid:6   cid:7  2D − 1  D  =  4Es N0   −D ·   5.20   Obviously, Ps is proportional to the Dth power of the reciprocal signal-to-noise ratio. Since error rate curves are scaled logarithmically, their slope will be dominated by the diversity   SPACE–TIME CODES  227  Error rate performance for diversity reception    Error probability for Binary Phase Shift Keying  BPSK , Rayleigh density  and D-fold diversity   cid:6   Ps =  1 − α 2   cid:7  D · D−1 cid:1    cid:6 =0   cid:6  D − 1 +  cid:6    cid:7    cid:6   ·   cid:6   1 + α 2   cid:7    cid:6    cid:9   with α =  γ  1 + γ   5.21   →  s  P  100  10 cid:358 2  10 cid:358 4  10 cid:358 6 0   a  SNR per antenna D = 1 D = 2 D = 4 D = 10   b  SNR per symbol D = 1 D = 2 D = 4 D = 10 D = 20 D = 100  100  10 cid:358 2  10 cid:358 4  →  s  P  20  10 30 Eb N0 in dB →  40  10 cid:358 6 0  20  10 30 DEb N0 in dB →  40  Figure 5.10: Error probability for D-fold diversity, BPSK, Rayleigh distributed coefﬁcients and MRC:  a  SNR measured per receive antenna  dashed lines =  approximation from expression  5.20  ,  b  SNR per combined symbol  bold dashed line  = AWGN reference   degree D at high SNRs. This is conﬁrmed by the results depicted in Figure 5.10. In diagram  a  the symbol error probability is plotted versus the signal-to-noise ratio Es N0 per receive antenna. Hence, we observe the array gain as well as the diversity gain. The array gain amounts to 3 dB if D is doubled, e.g. from D = 1 to D = 2. The additional gap is caused by the diversity gain. It can be illuminated by the curves’ slopes at high SNRs, as indicated by the dashed lines.  In diagram  b , the same error probabilities are plotted versus the SNR after the com- biner. Therefore, the array gain is invisible and only the diversity gain can be observed. The gains are largest at high signal-to-noise ratios and if the diversity degree increases from a low level. Asymptotically for D → ∞, the AWGN performance without any fading is reached.   228  Outage probability  SPACE–TIME CODES  For slowly fading channels, the average error probability is often of minor interest. Instead, operators are particularly interested in the probability that the system will not be able to guarantee a speciﬁed target error rate. This probability is called the outage probability Pout. Since the instantaneous error probability Ps γ [ cid:6 ]  depends directly on the actual signal-to- noise ratio γ [ cid:6 ], the probability of an outage event when maximum ratio combining i.i.d. diversity paths is obtained by integrating the distribution in Equation  5.17   Pout = Pr{Ps γ [ cid:6 ]  > Pt} =  p ξ   dξ   5.22   The target SNR γt corresponds to the desired error probability Pt = Ps γt . Figure 5.11 shows numerical results for BPSK. The left-hand diagram shows Pout versus Eb N0 for a ﬁxed target error rate of Pt = 10 −3. Obviously, Pout decreases with growing signal-to-noise ratio as well as with increasing diversity degree D. At very small values of Es N0, we observe the astonishing effect that high diversity degrees provide a worse performance. This can be explained by the fact that the variations in γ [ cid:6 ] become very small for large D. As a consequence, instantaneous signal-to-noise ratios lying above the average Es N0 occur less frequently than for low D, resulting in the described effect. Diagram  b  shows  γt-  0  Outage probabilities for diversity reception   a  Pt = 10 −3 D = 1 D = 2 D = 4 D = 10 D = 20 D = 100   b  10 log10 Eb N0  = 12 dB Pt = 10 −1 Pt = 10 −2 Pt = 10 −3 Pt = 10 −4 Pt = 10 −5  →  t u o P  100  10 cid:358 1  10 cid:358 2  10 cid:358 3  10 cid:358 4 0  →  t u o P  100  10 cid:358 2  10 cid:358 4  10 cid:358 6  10 cid:358 8  100  20  10 30 Eb N0 in dB →  40  101  D →  102  103  Figure 5.11: Outage probabilities for BPSK and i.i.d. Rayleigh fading channels:  a  a target error rate of Pt = 10  −3 and  b  10 log10 Eb N0  = 12 dB. Reproduced by  permission of John Wiley & Sons, Ltd   SPACE–TIME CODES 229 the results for a ﬁxed value 10 log10 Eb N0  = 12 dB. Obviously, diversity can reduce the outage probability remarkably.  5.2 Spatial Channels 5.2.1 Basic Description  Contrary to scalar channels with a single input and a single output introduced in Chapter 1, spatial channels resulting from the deployment of multiple transmit and receive antennas are vector channels with an additional degree of freedom, namely the spatial dimension. A general scenario is depicted in Figure 5.12. It illustrates a two-dimensional view that considers only the azimuth but not the elevation. For the scope of this chapter, this two- dimensional approach is sufﬁcient and will be used in the subsequent description. An extension to a truly three-dimensional spatial model is straightforward and differs only by an additional parameter, the elevation angle.  As shown in Figure 5.12, the azimuth angle θR denotes the DoA of an impinging waveform with respect to the orientation of the antenna array. Equivalently, θT represents  Channel with multiple transmit and receive antennas  Simple model with discrete DoAs and DoDs:  scatterer  θT,1  θT,2  θT,LoS  θR,LoS  θR,1  θR,2  transmitter  scatterers  receiver    Multiple direction of arrivals  DoAs  θR,µ at receiver.    Direction of departure  DoD  θT,µ at transmitter depends on Direction of  Arrival  DoA  θR,µ.  ⇒ Channel impulse response h t, τ, θ R  is a three-dimensional function.  Figure 5.12: Channel with multiple transmit and receive antennas   230  SPACE–TIME CODES  the DoD of a waveform leaving the transmitting antenna array. Obviously, both angles depend on the orientation of the antenna arrays at transmitter and receiver as well as on the location of the scatterers. For the purpose of this book it is sufﬁcient to presuppose a one- to-one correspondence between θR and θT. In this case, the DoD is a function of the DoA and the channel impulse response has to be parameterised by only one additional param- eter, the azimuth angle θR. Hence, the generally time-varying channel impulse response h t, τ   known from Chapter 1 is extended by a third parameter, the direction of arrival θR.1 Therefore, the augmented channel impulse response h t, τ, θR  bears information about the angular power distribution.  Principally, Line of Sight  LoS  and non-line of sight  NLoS  scenarios are distin- guished. In Figure 5.12, an LoS path with azimuth angles θT,LoS and θR,LoS exists. Those paths are mainly modelled by a Ricean fading process and occur for rural outdoor areas. NLoS scenarios typically occur in indoor environments and urban areas with rich scatter- ing, and the corresponding channel coefﬁcients are generally modelled as Rayleigh fading processes.  Statistical Characterisation  As the spatial channel represents a stochastic process, we can follow the derivation in Chapter 1 and describe it by statistical means. According to Figure 5.13, we start with the autocorrelation function φHH  cid:19 t, τ, θR  of h t, τ, θR  which depends on three parameters, the temporal shift  cid:19 t, the delay τ and the DoA θR. Performing a Fourier transformation with respect to  cid:19 t delivers the three-dimensional Doppler delay-angle scattering function deﬁned in Equation  5.24   Paulraj et al., 2003 . It describes the power distribution with respect to its three parameters. If  cid:21 HH fd, τ, θR  is narrow, the angular spread is small, while a broad function with signiﬁcant contributions over the whole range −π < θR ≤ π indicates a rather diffuse scattering environment. Hence, we can distinguish between space-selective and non-selective environments.  Similarly to scalar channels, integrations over undesired variables deliver marginal spectra. As an example, the delay-angle scattering function is shown in Equation  5.25 . Similarly, we obtain the power Doppler spectrum  the power delay proﬁle  or the power azimuth spectrum   cid:21 HH fd, τ, θR  dτ dθR ,  0  −π   cid:21 HH fd  = π- ∞-  cid:21 HH τ   = π- ∞-  −π   cid:21 HH θ   =   cid:21 HH τ, θR  dθR   cid:21 HH τ, θ   dτ .  0  1In order to simplify notation, we will use in this subsection a channel representation assuming that all  parameters are continuously distributed.   SPACE–TIME CODES  231  Statistical description of spatial channels    Extended channel impulse response h t, τ, θR .    Autocorrelation function of h t, τ, θR   φHH  cid:19 t, τ, θR  =   t +  cid:19 t, τ, θR  dt   5.23     Doppler delay-angle scattering function   cid:21 HH fd, τ, θR  =  −j2πfdξ dξ   5.24     Delay-angle scattering function  ∗  - ∞ −∞ h t, τ, θR  · h - ∞ −∞ φHH ξ, τ, θR  · e fd max-   cid:21 HH τ, θR  =  −fd max   cid:21 HH fd, τ, θR dfd   5.25   Figure 5.13: Statistical description of spatial channels  Typical Scenarios  Looking for typical propagation conditions, some classiﬁcations can be made. First, outdoor and indoor environments have to be distinguished. While indoor channels are often affected by rich scattering environments with low mobility, the outdoor case shows a larger variety of different propagation conditions. Here, macro-, micro- and picocells in cellular networks show a signiﬁcantly different behaviour. Besides different cell sizes, these cases also differ as regards the number and spatial distribution of scatterers, mobile users and the position of the base station or access point. At this point, we will not give a comprehensive overview of all possible scenarios but conﬁne ourselves to the description of an exemplary environment and some settings used for standardisation.  A typical scenario of a macrocell environment is depicted in Figure 5.14. Since the uplink is being considered, θR denotes the DoA at the base station while θT denotes the DoD at the mobile unit. The latter is often assumed to be uniformly surrounded by local scatterers, resulting in a diffuse ﬁeld of impinging waveforms arriving from nearly all directions. Hence, the angular spread  cid:19 θT that deﬁnes the azimuthal range within which signal paths depart amounts to  cid:19 θT = 2π. By contrast, the base station is generally elevated above rooftops. Scatterers are not as close to the base station as for the mobile device and are likely to occur in clusters located at discrete azimuth angles θR. Depending on the size and the structure of a cluster, a certain angular spread  cid:19 θR ! 2π is associated with it. Moreover, the propagation delay τi corresponding to   232  SPACE–TIME CODES  Spatial channel properties in a typical macrocell environment  θR   cid:19 θR  base station  mobile unit    Example for uplink transmission from mobile to base station.    More or less uniform distributed scatterers local to mobile unit.    Distinct preferred directions of arrival and departure at base station.    Cluster of scatterers leads to preferred direction θR with certain angular  spread  cid:19 θR.  Figure 5.14: Spatial channel properties in a typical macrocell environment  cluster i depends on the length of the path from transmitter over the scatterer towards the receiver.  In order to evaluate competing architectures, multiple access strategies, etc., of future telecommunication systems, their standardisation requires the deﬁnition of certain test cases. These cases include a set of channel proﬁles under which the techniques under consider- ation have to be evaluated. Figure 5.15 shows a set of channel proﬁles deﬁned by the third-Generation Partnership Project 3GPP  3GPP, 2003 . The Third-Generation Partner- ship Project  3GPP  is a collaboration agreement between the following telecommunications standards bodies and supports the standardisation process by elaborating agreements on technical proposals:  – ARIB: Association of Radio Industries and Business,  http:  www.arib.or.jp english index.html  – CCSA: China Communications Standards Association,  http:  www.ccsa.org.cn english  – ETSI: European Telecommunications Standards Institute,  http:  www.etsi.org  – ATIS: Alliance for Telecommunications Industry Solutions,  http:  www.atis.org  – TTA: Telecommunications Technology Association, http:  www.tta.or.kr e index.htm   SPACE–TIME CODES  233  – TTC: Telecommunications Technology Committee,  http:  www.ttc.or.jp e index.html  In Figure 5.15,  cid:19 d represents the antenna spacing in multiples of the wavelength λ. For the mobile unit, two antennas are generally assumed, while the base station can deploy more antennas. Regarding the mobile, NLoS and LoS scenarios are distinguished for the modiﬁed pedestrian A power delay proﬁle. With a line-of-sight  LoS  component, the scattered paths are uniformly distributed in the range [−π, π] and have a large angular spread. Only the LoS component arrives and departs from a preferred direction. In the absence of an LoS component and for all other power delay proﬁles, the power azimuth spectrum is assumed to be Laplacian distributed  p cid:22  θ   = K · e  −  √ 2θ− ¯ cid:22  σ cid:22   · G θ    with mean ¯ cid:22  and root mean square σ cid:22 . The array gain G θ   certainly depends on the angle θ, and K is a normalisation constant ensuring that the integral over p cid:22  θ   equals unity. Here, the base station array, generally mounted above rooftops, is characterised by small angular spreads and distinct DoDs and DoAs. In the next subsection, it will be shown how to classify and model spatial channels.  3GPP SCM link level parameters  3GPP, 2003   Model  Case 1  power delay proﬁle mod. pedestrian  Case 2 vehicular A  Case 3 pedestrian B  Mobile  station  Base station   cid:19 d  θR, θT  σθR   cid:19 d θT θR σθT,R  A  0.5λ 1  LoS on:  LoS path 22.5◦, rest uniform  2  LoS off: 67.5◦,   Laplacian   1  LoS on:  uniform for NLoS paths 2  LoS off: 35◦  0.5λ  67.5◦  35◦  Laplacian  or uniform  0.5λ  Odd paths: 22.5◦, paths: Even −67.5◦  35◦  Laplacian   Uniform linear array with 0.5λ, 4λ or 10λ  50◦ 20◦  2◦ for DoD ,  5◦ for DoA  Figure 5.15: 3GPP SCM link level parameters  3GPP, 2003    234 5.2.2 Spatial Channel Models General Classiﬁcation  SPACE–TIME CODES  In Chapter 1, channels with a single input and a single output have been introduced. For a frequency-selective channel with Lh taps, their output was described by  r[k] = Lh−1 cid:1   κ=0  h[k, κ] · x[k − κ] + n[k] .  x[k] = cid:2  r[k] = cid:2   x1[k]  r1[k]   cid:3 T ··· xNT[k]  cid:3 T  rNR[k]  ···  .  In this chapter, we extend the scenario to multiple-input and multiple-output  MIMO  systems as depicted in Figure 5.16. The MIMO channel has NT inputs represented by the signal vector  and NR outputs denoted by  General structure of a frequency-selective MIMO channel  h1,1[k, κ]  x1[k]  hNR,1[k, κ]  Tx  h1,NT[k, κ]  hNR,NT[k, κ]  xNT[k]  n1[k]  r1[k]  nNR[k]  Rx  rNR[k]  hµ,ν[k, κ] · xν[k − κ] + nµ[k]   5.26     Received signal at antenna µ  rµ[k] = NT cid:1   Lh−1 cid:1   ν=1  κ=0    Entire received signal vector  r[k] = Lh−1 cid:1   κ=0  H[k, κ] · x[k − κ] + n[k] = H[k] · xLh[k] + n[k]   5.27   Figure 5.16: General structure of a frequency-selective MIMO channel   SPACE–TIME CODES  235  Each pair  ν, µ  of transmit and receive antennas is connected by a generally frequency- selective single-input single-output channel hµ,ν[k, κ], the properties of which have already been described in Chapter 1. At each receive antenna, the NT transmit signals and additive noise superpose, so that the νth output at time instant k can be expressed as in Equation  5.26  in Figure 5.16. The parameter Lh represents the largest number of taps among all contributing channels.  A more compact description is obtained by using vector notations. According to  Equation  5.27 , the output vector  can be described by the convolution of a sequence of channel matrices  ···  r1[k]  r[k] = cid:2    h1,1[k, κ]  ...  hNR,1[k, κ]   cid:3 T  rNR[k]    ··· h1,NT[k, κ] . . . ··· hNR,NT[k, κ]  ...  H[k, κ] =  with 0 ≤ κ < Lh and the input vector x[k] plus the NR dimensional noise vector n[k]. Each row of the channel matrix H[k, κ] contains the coefﬁcients corresponding to a speciﬁc receive antenna, and each column comprises the coefﬁcients of a speciﬁc transmit antenna, all for a certain delay κ at time instant k. Arranging all matrices H[k, κ] side by side to an overall channel matrix  and stacking all input vectors x[k − κ] on top of each other  H[k] = cid:2  xLh[k] = cid:2   H[k, 0]  x[k]T   cid:3  ··· H[k, Lh − 1]  cid:3 T  ··· x[k − Lh − 1]T  r[k] = H[k] · x[k] + n[k]  leads to the expression on the right-handside of Equation  5.27  in Figure 5.16. The general MIMO scenario of Figure 5.16 contains some special cases. For the frequency-non-selective MIMO case, H[k, κ] = 0NR×NT holds for κ > 0 and the model simpliﬁes to with H[k] = H[k, 0]. Many space–time coding concepts have been originally designed especially for this ﬂat fading case where the channel does not provide frequency diversity. In Orthogonal Frequency Division Multiplexing  OFDM  systems with multiple transmit and receive antennas, the data symbols are spread over different subcarriers each experiencing a ﬂat fading MIMO channel. Two further cases depicted in Figure 5.17 are obtained if the transmitter or the receiver deploys only a single antenna. The channel matrix of these Single-input Multiple-Output  SIMO  systems reduces in this case to a column vector  h[k, κ] = cid:2  h[k, κ] = cid:2   h1,1[k, κ]  h1,1[k, κ]   cid:3 T ··· hNR,1[k, κ]  cid:3  ··· hNR,1[k, κ]  .  .  By Contrast, the Multiple-Input Single-Output  MISO  system has multiple inputs but only a single receive antenna and is speciﬁed by the row vector  In order to distinguish between column and row vectors, the latter is underlined. Appro- priate transmission strategies for MIMO, SIMO and MISO scenarios are discussed in later sections.   236  SPACE–TIME CODES  Special MIMO channel    Single-Input Multiple-Output  SIMO  channel with NT = 1  h1[k, κ]  x1[k]  hNR[k, κ]  n1[k]  r1[k]  nNR[k]  Rx  rNR[k]    Multiple-Input Single-Output  MISO  channel with NR = 1  h1[k, κ]  hNT[k, κ]  x1[k]  Tx  xNT[k]  n1[k]  r1[k]  Figure 5.17: Special MIMO channel  Modelling Spatial Channels  In many scientiﬁc investigations, the elements in H are assumed to be independent and identically Rayleigh distributed  i.i.d. . This case assumes a rich scattering environment without an LoS connection between transmitter and receiver. While this special situation represents an interesting theoretical point of view and will also be used in this chapter for benchmarks, real-world scenarios often look different. In many situations, the chan- nel coefﬁcients in H are highly correlated. This correlation may be caused by a small distance between neighbouring antenna elements, which is desired for beamforming pur- poses. For statistically independent coefﬁcients, this distance has to be much larger than half the wavelength. A second reason for correlations is the existence of only a few dominant scatterers, leading to preferred directions of departure and arrival and small angular spreads.   SPACE–TIME CODES  237  Modelling spatial channels    General construction of MIMO channel matrix h[k, ξ, θR,ν] · a[k, θR,ν] · bT  H[k, κ] =   cid:1   κmax cid:1   .    · g[κ − ξ]  ξ=0  ν   cid:4   k, θT,µ   cid:5     Correlation matrix   cid:21 HH = E  vec H vec H H    Construction of correlated channel matrix from matrix Hw with i.i.d. ele-  ments  vec H  =  cid:21   1 2HH · vec Hw     Simpliﬁed model with separated transmitter and receiver correlations  H =  cid:21   · Hw ·  cid:21   1 2 T  1 2 R  where  cid:21 R   cid:21 T  are assumed to be the same for all transmit  receive  antennas.    Relationship between  cid:21 HH,  cid:21 T and  cid:21 R  cid:21 HH =  cid:21 T  ⊗  cid:21 R  T   5.28    5.29    5.30    5.31    5.32   Figure 5.18: Modelling spatial channels  In practice, only a ﬁnite number of propagation paths can be considered. Therefore, the continuously distributed channel impulse response h t, τ, θR  will be replaced by a discrete form h[k, κ, θR,ν].2  A suitable MIMO channel model represented by a set of matrices H[k, κ] can be constructed by using Equation  5.28  in Figure 5.18. The vector a[k, θR,ν] denotes the steering vector at the receiver which depends on the DoA θR,ν as well as the array geometry. The corresponding counterpart at the transmitter is bT[k, θT,µ], where the direction of departure θT,µ is itself a function of θR,ν and the delay κ. Finally, g[κ] represents the joint impulse response of transmit and receive ﬁlters. With the assumption that A · , a[·] and b[·] are sufﬁciently known, an accurate model of the space–time channel can be constructed by using correlation matrices as summarised in  2The time and delay parameters k and κ are generally aligned to the sampling grid on the entire model. However, we have a ﬁnite number of directions of arrival and departure that are not aligned onto a certain grid. In order to indicate their discrete natures, they are indexed by subscripts ν and µ respectively.   238 SPACE–TIME CODES Figure 5.18. The exact NTNR × NTNR correlation matrix  cid:21 HH is given in Equation  5.29 , where the operator vec A  stacks all columns of a matrix A on top of each other. The channel matrix H can be constructed from a matrix Hw of the same size with i.i.d. elements according to Equation  5.30 . To be exact,  cid:21 HH should be determined for each delay κ. However, in most cases  cid:21 HH is assumed to be identical for all delays.  A frequently used simpliﬁed but less general model is obtained if transmitter and receiver correlation are separated. In this case, we have a correlation matrix  cid:21 T describing the correlations at the transmitter and a matrix  cid:21 R for the correlations at the receiver. The channel model is now generated by Equation  5.31 , as can be veriﬁed by3  E  HHH  1 2 R Hw cid:21   1 2 T  cid:21   H 2 T HH  w cid:21   H H 2 R   cid:21   and  E  1 2 R Hw cid:21  A relationship between the separated correlation approach and the optimal one is obtained with the Kronecker product ⊗, as shown in Equation  5.32 .  H 2 T HH  H 2 R  cid:21   HHH  1 2 T  w cid:21    cid:21   This simpliﬁcation matches reality only if the correlations at the transmit side are identical for all receive antennas, and vice versa. It cannot be applied for pinhole  or keyhole  channels  Gesbert et al., 2003 . They describe scenarios where transmitter and receiver may be located in rich scattering environments while the rays between them have to pass a keyhole. Although the spatial fading at transmitter and receiver is mutually independent, we obtain a degenerated channel of unit rank that can be modelled by   cid:4   cid:4    cid:4   cid:4    cid:5  = E  cid:5  = E   cid:5  =  cid:21 R  cid:5  =  cid:21 T .  In order to evaluate the properties of a channel, especially its correlations, the Singular Value Decomposition  SVD  is a suited means. It decomposes H into three matrices: a unitary NR × NR matrix U, a quasi-diagonal NR × NT matrix  cid:23  and a unitary NT × NT matrix V. While U and V contain the eigenvectors of HHH and HHH respectively,  H = hR · hH T .   σ1   cid:23  =     . . .  σr  0r×NT−r  0NR−r×NT−r  contains on its diagonal the singular values σi of H. The number of non-zero singular values is called the rank of a matrix. For i.i.d. elements in H, all singular values are identical in the average, and the matrix has full rank as pointed out in Figure 5.19. The higher the correlations, the more energy concentrates on only a few singular values and the rank of the matrix decreases. This rank deﬁciency can also be expressed by the condition number deﬁned in Equation  5.34  as the ratio of largest to smallest singular value. For unitary  orthogonal  matrices, it amounts to unity and becomes larger for growing correlations. The rank of a matrix will be used in subsequent sections to quantify the diversity degree and the spatial degrees of freedom. The condition number will be exploited in the context of lattice reduced signal detection techniques.  3Since Hw consists of i.i.d. elements, E{Hw cid:21 HH  } = I holds.  w   SPACE–TIME CODES  239  Analysing MIMO channels    Singular Value Decomposition  SVD  for ﬂat channel matrix  H = U ·  cid:23  · VH    Condition number  κ H  = σmax  =  cid:30 H cid:30 2 ·  cid:30 H  −1 cid:30 2 ≥ 1  σmin   cid:30 H cid:30 2 = sup x cid:7 =0   cid:30 Ax cid:30   cid:30 x cid:30   –  cid:6 2 norm of a matrix  –  cid:30 H cid:30 2 = σmax –  cid:30 H−1 cid:30 2 = σ −1 min   5.33    5.34    5.35     Rank of a matrix rank H  denotes the number of non-zero singular values.   Full rank: rank H  = min{NT, NR}.  Figure 5.19: Analysing MIMO channels  5.2.3 Channel Estimation  At the end of this section, some principles of MIMO channel estimation are introduced. Generally, channel knowledge is necessary in order to overcome the disturbing inﬂuence of the channel. This holds for many space–time coding and multilayer transmission schemes, especially for those discussed in the next sections. However, there exist some exceptions similar to the concepts of differential and orthogonal modulation schemes for single-input single-output channels which allow an incoherent reception without Channel State Informa- tion  CSI . These exceptions are unitary space–time codes  Hochwald and Marzetta, 2000; Hochwald et al., 2000  and differentially encoded space–time modulation  Hochwald and Sweldens, 2000; Hughes, 2000; Schober and Lampe, 2002  which do not require any channel knowledge, either at the transmitter or at the receiver.  While perfect CSI is often assumed for ease of analytical derivations and ﬁnding ulti- mate performance limits, the channel has to be estimated in practice. The required degree of channel knowledge depends on the kind of transmission scheme. The highest spectral efﬁciency is obtained if both transmitter and receiver have channel knowledge. However, this is the most challenging case. In Time Division Duplex  TDD  systems, reciprocity of the channel is often assumed, so that the transmitter can use its own estimate obtained in the uplink to adjust the transmission parameters for the downlink. By contrast, Frequency Division Duplex  FDD  systems place uplink and downlink in different frequency bands   240  SPACE–TIME CODES  so that reciprocity is not fulﬁlled. Hence, the transmitter is not able to estimate channel parameters for the downlink transmission directly. Instead, the receiver transmits its esti- mates over a feedback channel to the transmitter. In many systems, e.g. UMTS  Holma and Toskala, 2004 , the data rate of the feedback channel is extremely low and error correction coding is not applied. Hence, the channel state information is roughly quantised, likely to be corrupted by transmission errors and often outdated in fast-changing environments.  Many schemes do not require channel state information at the transmitter but only at the receiver. The loss compared with the perfect case is small for medium and high signal- to-noise ratios and becomes visible only at very low SNRs. Next, we describe the MIMO channel estimation at the receiver.  Principally, reference-based and blind techniques have to be distinguished. The former techniques use a sequence of pilot symbols known to the receiver to estimate the channel. They are inserted into the data stream either as one block at a predeﬁned position in a frame, e.g. as preamble or mid-amble, or they are distributed at several distinct positions in the frame. In order to be able to track channel variations, the sampling theorem of Shannon has to be fulﬁlled so that the time between successive pilot positions is less than the coherence time of the channel. By contrast, blind schemes do not need a pilot overhead. However, they generally have a much higher computational complexity, require rather large block lengths to converge and need an additional piece of information to overcome the problem of phase ambiguities. Figure 5.20 gives a brief overview starting with the pilot-assisted channel estimation. The transmitter sends a sequence of NP pilot symbols over each transmit antenna repre- sented by the  NT × NP  matrix Xpilot. Each row of Xpilot contains one sequence and each column represents a certain time instant. The received  NR × NP  pilot matrix is denoted by Rpilot and contains in each row a sequence of one receive antenna. Solving the optimisation problem  leads to the maximum likelihood estimate ˆHML as depicted in Equation  5.37 . The Moore–Penrose inverse X†  pilot is deﬁned  Golub and van Loan, 1996  by  4442  444Rpilot − ˜H · Xpilot  cid:3 −1 · cid:2   XpilotXH  pilot  .  ˆHML = argmin˜H  X†  pilot  = XH  pilot  Depending on the condition number of Xpilot, the inversion of XpilotXH  From the last equation we can conclude that the matrix XpilotXH pilot has to be invertible, which is fulﬁlled if rank Xpilot  = NP holds, i.e. Xpilot has to be of full rank and the number of pilot symbols in Xpilot has to be at least as large as the number of transmit antennas NT. pilot may lead to large values in X† pilot, signiﬁcant noise ampliﬁcations can occur. This drawback can be circumvented by choosing long train- ing sequences with NP >> NT, which leads to a large pilot overhead and, consequently, to a low overall spectral efﬁciency. Another possibility is to design appropriate training sequences. If Xpilot is unitary, Xpilot · XH  pilot. Since the noise N is also multiplied with X†  = XH  ⇒  = INT  X†  pilot  pilot  pilot  holds and no noise ampliﬁcation disturbs the transmission.   SPACE–TIME CODES  241  MIMO channel estimation    Pilot-assisted channel estimation:  – Received pilot signal  Rpilot = H · Xpilot + Npilot  – Pilot-assisted channel estimation ˆH = Rpilot · X†  pilot  = H + Npilot · X†  pilot  – Pilot matrix with unitary Xpilot    Blind channel estimation based on second-order statistics  pilot  ˆH = Rpilot · XH  cid:4   HxxHHH + nnH  = H + N · XH  pilot   cid:5  = σ 2X ·  cid:21 R + σ 2N · INR  ˆH = E{rrH} = E   5.36    5.37    5.38    5.39   Figure 5.20: MIMO channel estimation  A blind channel estimation approach based on second-order statistics is shown in Equation  5.39 . The right-hand side of this equation holds under the assumption of sta- tistically independent transmit signals E{xxH} = σ 2X INT, white noise E{nnH} = σ 2N INR and the validity of the channel model H =  cid:21   cf. Equation  5.31  . It can be observed that this approach does not deliver phase information. Moreover, the estimate only depends on the covariance matrix at the receiver, i.e. the receiver cannot estimate cor- relations at the transmit antenna array with this method. The same holds for the transmitter in the opposite direction. However, long-term channel characteristics such as directions of arrival that are incorporated in  cid:21 R can be determined by this approach.  · Hw ·  cid:21   1 2 T  1 2 R  5.3 Performance Measures  5.3.1 Channel Capacity  In order to evaluate the quality of a MIMO channel, different performance measures can be used. The ultimate performance limit is represented by the channel capacity indicating the maximum data rate that can be transmitted error free. Fixing the desirable data rate by choosing a speciﬁc modulation scheme, the resulting average error probability determines how reliable the received values are. These quantities have been partly introduced for the   242  SPACE–TIME CODES  scalar case and will now be extended for multiple transmit and receive antennas. We start our analysis with a short survey for the scalar case.  Channel Capacity of Scalar Channels  Figure 5.21 starts with the model of the scalar AWGN channel in Equation  5.40  and recalls the well-known results for this simple channel. According to Equation  5.41 , the mutual information between the input x[k] and the output r[k] is obtained from the difference in the output and noise entropies. For real continuously Gaussian distributed signals, the differential entropy has the form Idiff X   = E    cid:5  = 0.5 · log2 2π eσ 2X   ,   cid:4  − log2  pX  x   .  while it amounts to  Idiff X   = log2 π eσ 2X    for circular symmetric complex signals where real and imaginary parts are statistically independent and identically distributed. Inserting these results into Equation  5.41  leads to the famous formulae  5.42b  and  5.42a . It is important to note that the difference between mutual information and capacity is the maximisation of I  X;R  with respect to the input statistics pX  x . For the considered AWGN channel, the optimal continuous distribution of the transmit signal is Gaussian. The differences between real and complex cases can be  AWGN channel capacity    Channel output    Mutual information  r[k] = x[k] + n[k]   5.40   I  X;R  = Idiff R  − Idiff R  X   = Idiff R  − Idiff N     5.41     Capacity for complex Gaussian input and noise  equivalent baseband   . I  X;R   C = sup pX  x    cid:7    cid:6  1 + Es  cid:7   N0    = log2  cid:6  1 + 2  C = 1 2  · log2  Es N0  Figure 5.21: AWGN channel capacity   5.42a    5.42b     Capacity for real Gaussian input  imaginary part not used    SPACE–TIME CODES  243  explained as follows. On the one hand, we do not use the imaginary part and waste half of the available dimensions  factor 1 2 in front of the logarithm . On the other hand, the imaginary part of the noise does not disturb the real x[k], so that the effective SNR is doubled  factor 2 in front of Es N0 . The basic difference between the AWGN channel and a frequency-non-selective fading channel is its time-varying signal-to-noise ratio γ [k] = h[k]2Es N0 which depends on the instantaneous fading coefﬁcient h[k]. Hence, the instantaneous channel capacity C[k] in Equation  5.43  is a random variable itself and can be described by its statistical properties. For fast-fading channels, a coded frame generally spans over many different fading states so that the decoder exploits diversity by performing a kind of averaging. Therefore, the average capacity ¯C among all channel states, termed ergodic capacity and deﬁned in Figure 5.22, is an appropriate means. In Equation  5.44 , the expectation is deﬁned as   cid:4   f  X    E   cid:5  =  - ∞ −∞ f  x  · pX  x dx .  Channel Capacity of Multiple-Input and Multiple-Output Channels  The results in Figure 5.22 can be easily generalised to MIMO channels. The only difference is the handling of vectors and matrices instead of scalar variables, resulting in multivariate distributions of random processes. From the known system description  r = H · x + n  of Subsection 5.2.1 we know that r and n are NR dimensional vectors, x is NT dimen- sional and H is an NR × NT matrix. As for the AWGN channel, Gaussian distributed  Scalar fading channel capacities    Single-Input Single-Output  SISO  fading channel r[k] = h[k] · x[k] + n[k]    Instantaneous channel capacity C[k] = log2 %    Ergodic channel capacity  ¯C = E{C[k]} = E   cid:7    cid:6  1 + h[k]2 · Es  cid:6  1 + h[k]2 · Es  N0  log2  N0   cid:7 &  Figure 5.22: Scalar fading channel capacities   5.43    5.44    244  SPACE–TIME CODES  input alphabets achieve capacity and are considered below. The corresponding multivari- ate distributions for real and complex random processes with n dimensions are shown in Figure 5.23. The n × n matrix  cid:21 AA denotes the covariance matrix of the n-dimensional process A and is deﬁned as   cid:4    cid:5  = UA ·  cid:13 A · UHA .   cid:21 AA = E  aaH  The right-handside of the last equation shows the eigenvalue decomposition  see deﬁnition  B.0.7  in Appendix B  which decomposes the Hermitian matrix  cid:21 AA into the diagonal matrix  cid:13 A with the corresponding eigenvalues λA,i, 1 ≤ i ≤ n, and the square unitary matrix UA. The latter contains the eigenvectors of A.  Multivariate distributions and related entropies    Joint probability density for a real multivariate Gaussian process    Joint probability density for complex multivariate Gaussian process  pA a  =  1  √ det 2π  cid:21 AA   · exp  5   cid:21   −aT 5  6 −1AAa 2 6 −1AAa .  1  √ det π  cid:21 AA   · exp  −aH   cid:21   -   cid:5  = −  pA a  =  cid:4   .  An  pA a  · log2 · n cid:1    = 1  log2   cid:2   2  i=1   cid:2     = n cid:1   i=1  .  Idiff A  = log2   cid:3       cid:3     Joint entropy of a multivariate process with n dimensions  Idiff A  = − E  log2[pA a ]  pA a   da   5.45     Joint entropy of a multivariate real Gaussian process  Idiff A  = 1 2  · log2  det 2π e cid:21 AA   2π eλA,i   5.46a     Joint entropy of a multivariate complex Gaussian process  det π e cid:21 AA   log2  π eλA,i   5.46b   Figure 5.23: Multivariate distributions and related entropies   SPACE–TIME CODES  245  Following Figure 5.23, we recognize from Equation  5.45  that the differential entropy Idiff X   is deﬁned in exactly the same way as for scalar processes except for the expec- tation  integration  over an NT-dimensional space. Solving the integrals by inserting the multivariate distributions for real and complex Gaussian processes, we obtain the differ- ential entropies in Equations  5.46a  and  5.46b . They both depend on the covariance matrix  cid:21 AA. Using its eigenvalue decomposition and the fact that the determinant of a matrix equals the product of its eigenvalues, we obtain the right-hand-side expressions of Equations  5.46a  and  5.46b . If the elements of A are statistically independent and identically distributed  i.i.d. , all with variance λA,i = σ 2A, the differential entropy becomes  7 n cid:20   i=1  8  = n cid:1   i=1   cid:2   Idiff A  = log2   π eλA,i   log2  π eλA,i   cid:3  =  i.i.d.   cid:2    cid:3   n · log2  π eσ  2A  for the complex case. An equivalent expression is obtained for real processes.  Using the results of Figure 5.23, we can now derive the capacity of a MIMO channel. A look at Equation  5.47  in Figure 5.24 illustrates that the instantaneous mutual information  Mutual Information of MIMO systems   5.47     Mutual information of a MIMO channel  I  X;R  H  = Idiff R  H  − Idiff N   = log2   Inserting SVD H = UH cid:23 HVHH and  cid:21 NN = σ 2N INR  det  cid:21 RR  det  cid:21 NN     cid:7    cid:6   INR  + 1 σ 2N  I  X;R  H  = log2 det  cid:6     Perfect channel knowledge only at receiver  I  X;R  H  = log2 det  INR  + σ 2X σ 2N   cid:23 H cid:23 HH    Perfect channel knowledge at transmitter and receiver  I  X;R  H  = log2 det  INR  + 1 σ 2N   cid:23 H cid:13 X  cid:23 HH  log2   cid:6    cid:23 HVHH cid:21 XX VH cid:23   HH   5.48    cid:7   = NT cid:1   i=1   cid:6  1 + σ 2H,i  log2  · σ 2X σ 2N   cid:7    cid:7   = NT cid:1   i=1   cid:8  1 + σ 2H,i   5.49    cid:10   · σ 2X ,i σ 2N  5.50   Figure 5.24: Mutual Information of MIMO systems   246  SPACE–TIME CODES  for a speciﬁc channel H is similarly deﬁned as the mutual information of the scalar case  Figure 5.21 . With the differential entropies Idiff R  H  = log2  cid:2   det π e cid:21 RR   and   cid:2    cid:3    cid:3   Idiff N   = log2  det π e cid:21 NN    ,  we obtain the right-handside of Equation  5.47 . With the relation r = Hx + n, the covari- ance matrix  cid:21 RR of the channel output r becomes   cid:21 RR = E{rrH} = H cid:21 XX HH +  cid:21 NN .  Moreover, mutually independent noise contributions at the NR receive antennas are often assumed, resulting in the noise covariance matrix   cid:21 N N = σ  2N · INR .  Inserting these covariance matrices into Equation  5.47  and exploiting the singular value decomposition of the channel matrix H = UH cid:23 HVHH delivers the result in Equation  5.48 . It has to be mentioned that the singular values σH,i of H are related to the eigenvalues λH,i of HHH by λH,i = σ 2H,i. Next, we distinguish two cases with respect to the available channel knowledge. If only the receiver has perfect channel knowledge, the best strategy is to transmit independent data streams over the antenna elements, all with average power σ 2X . This corresponds to the transmit covariance matrix  cid:21 XX = σ 2X · INT and leads to the result in Equation  5.49 . Since the matrix  cid:23  in Equation  5.49  is diagonal, the whole argument of the determinant is a diagonal matrix. Hence, the determinant equals the product of all diagonal elements which is transformed by the logarithm into the sum of the individual logarithms. We recognize from the right-handside of Equation 5.49  that the MIMO channel has been decomposed into a set of parallel  independent  scalar channels with individual signal-to-noise ratios σ 2H,i σ 2X  σ 2N . Therefore, the total capacity is simply the sum of the individual capacities of the contributing parallel scalar channels.  If the transmitter knows the channel matrix perfectly, it can exploit the eigenmodes of the channel and, therefore, achieve a higher throughput. In order to accomplish this advantage, the transmitter covariance matrix has to be chosen as   cid:21 XX = VH ·  cid:13 X · VHH ,  i.e. the eigenvectors have to equal those of H. Inserting the last equation into Equation  5.48 , we see that the eigenvector matrices VH eliminate themselves, leading to Equation  5.50 . Again, all matrices are diagonal matrices, and we obtain the right-handside of Equation  5.50 . The question that still has to be answered is how to choose the eigenvalues λH,i = σ 2H,i of  cid:21 XX , i.e. how to distribute the transmit power over the parallel data streams. Following the procedure described elsewhere  Cover and Thomas, 1991; K¨uhn, 2006  using Lagrangian multipliers, the famous waterﬁlling solution is obtained. It is illustrated in Figure 5.25, where each bin represents one of the scalar channels. We have to imagine the   SPACE–TIME CODES  Waterﬁlling solution    Waterﬁlling solution   θ − σ 2N  σ 2H,i  0  =  2X ,i  σ  σ 2N σ 2H,i  for θ >  else  NT cid:1   i=1  2X ,i  σ  != NT · Es  N0    Total transmit power constraint  = 0  σ 2X ,2  θ  σ 2X ,1  σ 2N  σ 2H,2  σ 2N  σ 2H,1  σ 2X ,3  σ 2X ,4  σ 2N  σ 2H,3  σ 2N  σ 2H,4  = 0  σ 2X ,5  σ 2X ,6  σ 2N  σ 2H,5  σ 2N  σ 2H,6  247   5.51    5.52   channel ν  Figure 5.25: Waterﬁlling solution. Reproduced by permission of John Wiley & Sons, Ltd  diagram as a vessel with a bumpy ground where the height of the ground is proportional to the ratio σ 2N  σ 2H,i. Pouring water into the vessel is equivalent to distributing transmit power onto the parallel scalar channels. The process is stopped when the totally available transmit power is consumed. Obviously, good channels with a low σ 2N  σ 2H,i obtain more transmit power than weak channels. The worst channels whose bins are not covered by the water level θ do not obtain any power, which can also be seen from Equation  5.51 . Therefore, we can conclude that much power is spent on good channels transmitting high data rates, while little power is given to bad channels transmitting only very low data rates. This strategy leads to the highest possible data rate.   248  SPACE–TIME CODES  Channel capacity and receive diversity   a  SNR per receive antenna  NR = 1 NR = 2 NR = 3 NR = 4  → ¯C  12  10  8  6  4  2   b  SNR after combining NR = 1 NR = 2 NR = 3 NR = 4  → ¯C  12  10  8  6  4  2  0 0  0 0 20 Es N0 in dB per receive antenna  15  10  5  10  5 15 Es N0 in dB →  20  Figure 5.26: Channel capacity and receive diversity. Reproduced by permission of John  Wiley & Sons, Ltd  Figure 5.26 illuminates array and diversity gains for a system with a single transmit and several receive antennas. In the left-hand diagram, the ergodic capacity is plotted versus the signal-to-noise ratio at each receive antenna. The more receive antennas employed, the more signal energy can be collected. Hence, doubling the number of receive antennas also doubles the SNR after maximum ratio combining, resulting in a 3 dB gain. This gain is denoted as array gain.4 Additionally, a diversity gain can be observed, stemming from the fact that variations in the SNR owing to fading are reduced by combining independent diversity paths. Both effects lead to a gain of approximately 6.5 dB by increasing the number of receive antennas from NR = 1 to NR = 2. This gain reduces to 3.6 dB by going from NR = 2 to NR = 4. The array gain still amounts to 3 dB, but the diversity gain is getting smaller if the number of diversity paths is already high.  The pure diversity gains become visible in the right-hand diagram plotting the ergodic capacities versus the SNR after maximum ratio combining. This normalisation removes the array gain, and only the diversity gain remains. We observe that the ergodic capacity increases only marginally owing to a higher diversity degree. Figure 5.27 shows the ergodic capacities for a system with NT = 4 transmit antennas versus the signal-to-noise ratio Es N0. The MIMO channel matrix consists of i.i.d. com- plex circular Gaussian distributed coefﬁcients. Solid lines represent the results with perfect channel knowledge only at the receiver, while dashed lines correspond to the waterﬁlling solution with ideal CSI at transmitter and receiver. Asymptotically for large signal-to-noise ratios, we observe that the capacities increase linearly with the SNR. The slope amounts  4Certainly, the receiver cannot collect a higher signal power than has been transmitted. However, the channels’  path loss has been omitted here so that the average channel gains are normalised to unity.   SPACE–TIME CODES  249  Ergodic capacity of MIMO systems  NR = 1 NR = 2 NR = 3 NR = 4  → C  35  30  25  20  15  10  5  0 0  5  10  15  Es N0 in dB →  20  25  30  Figure 5.27: Ergodic capacity of MIMO systems with NT = 4 transmit antennas and varying NR  solid lines: CSI only at receiver; dashed lines: waterﬁlling solution with  perfect CSI at transmitter and receiver   to 1 bit s Hz for NR = 1, 2 bit s Hz for NR = 2, 3 bit s Hz for NR = 3 and 4 bit s Hz for NR = 4, and therefore depends on the rank r of H. For ﬁxed NT = 4, the number of non- zero eigenmodes r grows with the number of receive antennas up to rmax = rank H  = 4. These data rate enhancements are called multiplexing gains because we can transmit up to rmax parallel data streams over the MIMO channel. This conﬁrms our theoretical results that the capacity increases linearly with the rank of H while it grows only logarithmically with the SNR.  Moreover, we observe that perfect CSI at the transmitter leads to remarkable improve- ments for NR < NT. For these conﬁgurations, the rank of our system is limited by the number of receive antennas. Exploiting the non-zero eigenmodes requires some kind of beamforming which is only possible with appropriate channel knowledge at the transmit- ter. For NR = 1, only one non-zero eigenmode exists. With transmitter CSI, we obtain an array gain of 10 log10 4  ≈ 6 dB which is not achievable without channel knowledge. For NR = NT = 4, the additional gain due to channel knowledge at the transmitter is vis- ible only at low SNR where the waterﬁlling solution drops the weakest eigenmodes and concentrates the transmit power only on the strongest modes, whereas this is impossible without transmitter CSI. At high SNR, the water level in Figure 5.25 is so high that all eigenmodes are active and a slightly different distribution of the transmit power has only a minor impact on the ergodic capacity.   250 5.3.2 Outage Probability and Outage Capacity  SPACE–TIME CODES  As we have seen from Figure 5.22, the channel capacity of fading channels is a random variable itself. The average capacity is called the ergodic capacity and makes sense if the channel varies fast enough so that one coded frame experiences the full channel statistics. Theoretically, this assumes inﬁnite long sequences due to the channel coding theorem. For delay-limited applications with short sequences and slowly fading channels, the ergodic capacity is often not meaningful because a coded frame is affected by an incomplete part of the channel statistics. In these cases, the ‘short-term capacity’ may vary from frame to frame, and network operators are interested in the probability that a system cannot support a desired throughput R. This parameter is termed the outage probability Pout and is deﬁned in Equation  5.53  in Figure 5.28.  Equivalently, the outage capacity Cp describes the capacity that cannot be achieved in p percent of all fading states. For the case of a Rayleigh fading channel, outage probability and capacity are also presented in Figure 5.28. The outage capacity Cout is obtained by resolving the equation for Pout with respect to R = Cout. For MIMO channels, we sim- ply have to replace the expression of the scalar capacity with that for the multiple-input multiple-output case.  Outage probability of fading channels    Outage probability of a scalar channel  – Outage probability for scalar Rayleigh fading channels  K ¨uhn, 2006   Pout = Pr{C[k] < R} = Pr{h[k]2  <   5.53   }  Es N0  2R − 1  cid:7    cid:6   1 − 2R  Es N0  Pout = 1 − exp  Cout = log2   cid:3  – Outage capacity for scalar Rayleigh fading channel 1 − Es N0 · log 1 − Pout  8   cid:2  Pout = Pr{C[k] < R} = Pr{ r cid:1   1 + σ  7  log2  2H,i    Outage probability for MIMO channel with singular values σH,i  < R}  .  · σ 2X ,i σ 2N  µ=1   5.54   Figure 5.28: Outage probability of fading channels   SPACE–TIME CODES  251  Capacity and outage probability of Rayleigh fading channels   a    b   C1 C5 C10 C20 C50 AWGN ¯C  → C  12  10  8  6  4  2  0  cid:358 10  1  0.8  0.6  0.4  0.2  0dB  5dB  10dB15dB20dB25dB30dB  →  t u o P  0  10  20  Es N0 in dB →  30  40  0 0  2  4  6 R →  8  10  Figure 5.29: Capacity and outage probability of Rayleigh fading channels  The left-hand diagram in Figure 5.29 shows a comparison between the ergodic capac- ities of AWGN and ﬂat Rayleigh fading channels  bold lines . For sufﬁciently large SNR, the curves are parallel and we can observe a loss due to fading of roughly 2.5 dB. Com- pared with the loss of approximately 17 dB at a bit error rate  BER  of Pb = 10 −3 in the uncoded case, the observed difference is rather small. This discrepancy can be explained by the fact that the channel coding theorem presupposes inﬁnite long code words allowing the decoder to exploit a high diversity gain. Therefore, the loss in capacity com- pared with the AWGN channel is relatively small. Astonishingly, the ultimate limit of 10 log10 Eb N0  = −1.59 dB is the same for AWGN and Rayleigh fading channels.  Additionally, the left-hand diagram shows the outage capacities for different values of Pout. For example, the capacity C50 can be ensured with a probability of 50% and is close to the ergodic capacity ¯C. The outage capacities Cp decrease dramatically for smaller Pout,  i.e. the higher the requirements, the higher is the risk of an outage event. At a spectral efﬁciency of 6 bit s Hz, the loss compared with the AWGN channel in terms of Eb N0 amounts to nearly 8 dB for Pout = 0.1 and roughly 18 dB for Pout = 0.01.  The right-hand diagram depicts the outage probability versus the target throughput R for different values of Es N0. As expected for large signal-to-noise ratios, high data rates can be guaranteed with very low outage probabilities. However, Pout grows rapidly with decreasing Es N0. The asterisks denote the outage probability of the ergodic capacity R = ¯C. As could already be observed in the left-hand diagram, it is close to a probability of 0.5. Finally, Figure 5.30 shows the outage probabilities for 1 × NR and 4 × NR MIMO systems at an average signal-to-noise ratio of 10 dB. From ﬁgure  a  it becomes obvious   252  SPACE–TIME CODES  Outage probability of MIMO fading channels   a  NT = 1   b  NT = 4  →  t u o P  1  0.8  0.6  0.4  0.2  0 0  1  0.8  0.6  0.4  0.2  →  t u o P  NR = 1 NR = 2 NR = 3 NR = 4 12  4  8 R →  16  0 0  4  8 R →  12  16  Figure 5.30: Outage probability of MIMO fading channels  that an increasing number of receive antennas enlarges the diversity degree and, hence, minimises the risk of an outage event. However, there is no multiplexing gain with only a single transmit antenna, and the gains for additional receive antennas become smaller if the number of receiving elements is already large. This is a well-known effect from diversity, assuming an appropriate scaling of the signal-to-noise ratio. In ﬁgure  b , the system with NT = 4 transmit antennas is considered. Here, we observe larger gains with each additional receive antenna, since the number of eigenmodes increases so that we obtain a multiplexing gain besides diversity enhancements.  5.3.3 Ergodic Error Probability  Having analysed MIMO systems on an information theory basis, we will now have a look at the error probabilities. The following derivation was ﬁrst introduced for space–time codes  Tarokh et al., 1998 . However, it can be applied to general MIMO systems. It assumes an optimal maximum likelihood detection and perfect channel knowledge at the receiver and a block-wise transmission, i.e. L consecutive vectors x[k] are written into the NT × L transmit matrix  X =.  x[0] x[1] ··· x[L − 1]    x1[0]  x2[0]  ...    =  x1[1] x2[1]  ··· x1[L − 1] x2[L − 1] ··· . . . ··· xNT[L − 1]  ...  xNT[0] xNT[1]    .   SPACE–TIME CODES  253  The MIMO channel is assumed to be constant during L time instants so that we receive an NR × L matrix R    r1[0]  r2[0] ...    =  r1[1] r2[1]  ··· ··· . . . ···  r1[L − 1] r2[L − 1] rNR[L − 1]  ...  rNR[0]  rNR[1]    = H · X + N ,  R =.  r[0] r[1] ··· r[L − 1]  where N contains the noise vectors n[k] within the considered block. The set of all possible matrices X is termed X. In the case of a space–time block code, certain constraints apply to X, limiting the size of X  see Section 5.4 . By contrast, for the well-known Bell Labs Layered Space–Time  BLAST  transmission, there are no constraints on X, leading to a size X = M NTL, where M is the size of the modulation alphabet. Calculating the average error probability generally starts with the determination of the pairwise error probability Pr{B → ˜B  H}. Equivalently to the explanation in Chapter 3, it denotes the probability that the detector decides in favour of a code matrix ˜X although X was transmitted. Assuming uncorrelated noise contributions at each receive antenna, the optimum detector performs a maximum likelihood estimation, i.e. it determines that code matrix ˜X which minimises the squared Frobenius distance  cid:30 R − H ˜X cid:30 2 F  see Appendix B on page 312 . Hence, we have to consider not the difference  cid:30 X − ˜X cid:30 2 F, but the difference in the noiseless received signals  cid:30 HX − H ˜X cid:30 2 F, as done in Equation  5.55  in Figure 5.31. In order to make the squared Frobenius norm independent of the average power Es of a symbol, we normalise the space–time code words by the average power per symbol to  If the µth row of H is denoted by hµ, the squared Frobenius norm can be written as  ˜X√  .  Es Ts  X√ Es Ts  B = 442  F  44H B − ˜B   =44hµ B − ˜B   and  ˜B = 442 = NR cid:1   µ=1  hµ B − ˜B  B − ˜B HhH  µ .  This rewriting and the normalisation lead to the form given in Equation  5.56 . We now apply the eigenvalue decomposition on the Hermitian matrix   B − ˜B  B − ˜B   H = U cid:13 UH  .  The matrix  cid:13  is diagonal and contains the eigenvalues λν of  B − ˜B  B − ˜B H, while U is  unitary and consists of the corresponding eigenvectors. Inserting the eigenvalue decompo- sition into Equation  5.56  yields  44H B − ˜B   442  F  = NR cid:1   µ=1  hµU ·  cid:13  · UHhH  µ  β   cid:13 βH  µ  µ  = NR cid:1   µ=1  = [βµ,1, . . . , βµ,L]. Since  cid:13  is diagonal, its multiplication with β  with β the left- and the right-handside respectively reduces to  µ  and βH µ  µ  from  44H B − ˜B   442  F  L cid:1   = NR cid:1   µ=1  ν=1  βµ,ν2 · λν = NR cid:1   r cid:1   µ=1  ν=1  βµ,ν2 · λν .   254  SPACE–TIME CODES  Pairwise error probability    Pairwise error probability for code matrices X and ˜X      *44HX − H ˜X   442  F     4σ 2N   5.55   hµ ·  B − ˜B  B − ˜B H · hH  µ   5.56     Normalisation to unit average power per symbol  Pr{X → ˜X  H} = 1 2  · erfc  · NR cid:1   µ=1  = Es Ts  442  F  44H ·  X − ˜X  = hµU 442  µ  = Es Ts  · NR cid:1   r cid:1   · NR cid:1     Substitution of β  44H ·  X − ˜X   ·  cid:13  · βH  β  = Es Ts  βµ2 · λµ   5.57   F  µ=1 ν=1   With σ 2N = N0 Ts, pairwise error probability becomes  µ=1  µ  µ  Pr{X → ˜X  H} = 1 2  · erfc  βµ,ν2 · λν   5.58         * Es  4N0  r cid:1   · NR cid:1   µ=1  ν=1     Figure 5.31: Pairwise error probability for space–time code words  The last step in our derivation starts with the application of the upper bound erfc   Assuming that the rank of  cid:13  equals rank{ cid:13 } = r ≤ L, i.e. r eigenvalues λµ are non-zero, the inner sum can be restricted to run from ν = 1 only to ν = r because λν>r = 0 holds. The last equality is obtained because  cid:13  is diagonal. Inserting the new expression for the squared Frobenius norm into the pairwise error probability of Equation  5.55  delivers the result in Equation  5.58 . −x on the complementary error function. Rewriting the double sum in the exponent into the e product of exponential functions leads to the result in inequality  5.59  in Figure 5.32. In order to obtain a pairwise error probability averaged over all possible channel observations, the expectation of Equation  5.58  with respect to H has to be determined. This expectation is calculated over all channel coefﬁcients hµ,ν of H. At this point it has to be mentioned that the multiplication of a vector hµ with U performs just a rotation in the NT-dimensional  √ x  <   SPACE–TIME CODES  255  Determinant and rank criteria    Upper bound on pairwise error probability by erfc   Pr{B → ˜B  H} ≤ 1 2    Expectation with respect to H yields  Pr{B → ˜B} ≤ 1 2  ·  ·  Es 4N0    Rank criterion: maximise the minimum rank of  r cid:20   ν=1  · NR cid:20  7  µ=1  −x  "  Es 4N0  √ x  < e  ! −βµ2λµ 8−rNR  cid:3    cid:12 1 r  λν  exp   cid:11  r cid:20   ν=1   cid:2    cid:2  B − ˜B  cid:10 1 r  B − ˜B  cid:3  $  gd = NR · min ˜B   B,  cid:8   gc = min ˜B   B,  rank  r cid:20   ν=1  λν    Determinant criterion: maximise the minimum of    r  ν=1 λν  1 r  Figure 5.32: Determinant and rank criteria   5.59    5.60    5.61    5.62   space. Hence the coefﬁcients βµ,ν of the vector β coefﬁcients hµ,ν  Naguib et al., 1997 .  µ  have the same statistics as the channel  Assuming the frequently used case of independent Rayleigh fading, the coefﬁcients hµ,ν, and, consequently also βµ,ν, are complex rotationally invariant Gaussian distributed random variables with unit power σ 2H = 1. Hence, their squared magnitudes are chi-squared distributed with two degrees of freedom, i.e.  holds. The expectation of inequality  5.59  now results in  pβµ,ν  ξ   = e  −ξ   cid:31    Pr{B → ˜B} = EH Pr{B → ˜B  H} 5 · NR cid:20  r cid:20   %  Eβ  exp  ≤ 1 2  µ=1  ν=1  6&  − λν · βµ,ν2 · Es 4N0   256  SPACE–TIME CODES  6  − ξ · λν  Es 4N0  dξ  - ∞  0  · NR cid:20  r cid:20  7 r cid:20   µ=1  ν=1  e  −ξ · exp 8  NR  1  1 + λν · Es  4N0  ν=1  5  .  ≤ 1 2  ·  ≤ 1 2  The upper bound can be relaxed a little by dropping the +1 in the denominator. It is still tight for large SNR. Slight rewriting of the last inequality leads to inequality  5.60 .  We can draw some important conclusions from the result in inequality  5.60 . It resem- bles the error probability of a transmission with D-fold diversity which is proportional to   cid:6    cid:7 −D  .  Ps ∝  4Es N0  8  7 cid:6   A comparison shows that the exponent rNR in inequality 5.60  is equivalent to the diversity degree D . Hence, in order to achieve the maximum possible diversity degree, the minimum rank r among all pairwise differences B − ˜B should be maximised, leading to the rank criterion presented in Equation  5.61 . The maximum diversity degree equals NTNR. Since the error probabilities are generally scaled logarithmically when depicted versus the SNR, the diversity degree in the exponent determines the slope of the error probability curves due to  "−1   cid:7 −D  cid:3 1 r multiplies the signal-to-noise ratio Es N0 in inequal-  = D · log  By contrast, the product ity 5.60 . The logarithm turns this multiplication into an addition and, therefore, into a horizontal shift of the corresponding error probability curve. By analogy with coding the- ory, this shift is called the coding gain. The largest coding gain is obtained if the minimum of all possible products is maximised as stated in Equation  5.62 . If the design of B ensures a full rank r = rank{B − ˜B} = NT, the product of the eigenvalues equals the determinant det B − ˜B    cid:2 $   cid:8 !  4Es N0  4Es N0  ν=1 λν   cid:10   log  r   cid:8   NT cid:20   ν=1   cid:10 1 NT = min  ˜B   B,   cid:11   λν   cid:12 1 NT  .  det B − ˜B   gc = min ˜B   B,  Therefore, the criterion is termed the determinant criterion, as presented on Figure 5.32.  Some comments have to be made on these criteria. If no constraints are imposed on the matrix B, i.e. it contains arbitrary symbols as in the case of Bell Labs Layered Space–Time  BLAST -like layered space–time transmissions, the maximum minimal rank equals NR and only receive diversity is gained. Consequently, no coding gain can be expected. However, NT data streams can be transmitted in parallel  one over each antenna  boosting the achievable data rate. If appropriate space–time encoding is applied at the transmitter, the code design may ensure a higher minimum rank, at most r = NTNR, as well as a coding gain. However, the data rate is lower, as in the ﬁrst case. Obviously, a trade-off between reliability and data rate can be achieved  Zheng and Tse, 2003 . In the next section, a general description of space–time coding for the cases just discussed will be introduced.   SPACE–TIME CODES 5.4 Orthogonal Space–Time Block Codes  257  In this chapter we pursue the goal of obtaining spatial diversity by deploying several anten- nas at the transmitter but only a single antenna at the receiver. However, a generalisation to multiple receive antennas is straightforward  K¨uhn, 2006 . Furthermore, it is assumed that the average energy Es per transmitted symbol is constant and, in particular, independent of NT and the length of a space–time block. Since aiming for diversity is mostly beneﬁcial if the channel between one transmit and one receive antenna provides no diversity, we con- sider frequency-non-selective channels. Moreover, the channel is assumed to be constant during one space–time code word.  r[ cid:6 ] = NT cid:1   We saw from Section 5.1 that spatial receive diversity is simply achieved by maximum ratio combining the received samples, resulting in a coherent  constructive  superposi- tion, i.e. the squared magnitudes have been summed. Unfortunately, transmitting the same symbol s[ cid:6 ] from all NT transmit antennas generally leads to an incoherent superposition hν[ cid:6 ] + n[ cid:6 ] = s[ cid:6 ] · ˜h[ cid:6 ] + n[ cid:6 ]  + n[ cid:6 ] = s[ cid:6 ] · √ at the receive antenna. The factor 1  NT ensures that the average transmit power per symbol is independent of NT. In the case of i.i.d. Rayleigh fading coefﬁcients, the new channel coefﬁcient ˜h[ cid:6 ] has the same distribution as each single coefﬁcient hν[ cid:6 ] and  hν[ cid:6 ] · s[ cid:6 ]√  · NT cid:1   1√ NT  ν=1  ν=1  NT  nothing has been won. If the transmitter knows the channel coefﬁcients, it can predistort the symbols so that the superposition at the receiver becomes coherent. This strategy is known as beamforming and is not considered in this book. Therefore, more sophisticated signalling schemes are required in order to achieve a diversity gain.  Although orthogonal space–time block codes do not provide a coding gain, they have the great advantage that decoding simply requires some linear combinations of the received symbols. Moreover, they provide the full diversity degree achievable with a certain number of transmit and receive antennas. In order to have an additional coding gain, they can be easily combined with conventional channel coding concepts, as discussed in the previous chapters.  5.4.1 Alamouti’s Scheme  Before we give a general description of space–time block codes, a famous but simple example should illustrate the basic principle. We consider the approach introduced by Alamouti  Alamouti, 1998  using two transmit antennas and a single receive antenna. The original structure is depicted in Figure 5.33. As we will see, each symbol s[ cid:6 ] is transmitted twice. In order to keep the transmit power per symbol constant, each instance of the symbol is normalised by the factor 1  Two consecutive symbols s[ cid:6  − 1] and s[ cid:6 ] are collected from the input sequence. They are denoted as s1 = s[ cid:6  − 1] and s2 = s[ cid:6 ] respectively and are mapped onto the √ NT = 2 transmit antennas as follows. At the ﬁrst time instant, x1[ cid:6 ] = s1  √ 2 is sent over the ﬁrst antenna and x2[ cid:6 ] = s2  2 over the second one. At the receiver, we obtain the superposition  √  2.  · cid:2  h1s1 + h2s2   cid:3  + n[ cid:6 ] .  r[ cid:6 ] = h1x1[ cid:6 ] + h2x2[ cid:6 ] + n[ cid:6 ] = 1√ 2   258  SPACE–TIME CODES  Alamouti’s space–time block code  s1 = s[ cid:6  − 1]  s1 −s  ∗ 2  x1[ cid:6 ]    Received block of two consecutive symbols  s[ cid:6 ]  demux  s2 = s[ cid:6 ]   cid:7    cid:6  r[ cid:6 ] r[ cid:6  + 1] ∗  ˜r =  h1  h2  n[ cid:6 ]  r[ cid:6 ]   cid:7    cid:6  n[ cid:6 ] n[ cid:6  + 1] ∗  cid:7   = ˜H · s + ˜n   5.63    5.64   ∗ 1  s  s2   cid:6   x2[ cid:6 ]   cid:7    cid:7    cid:6   ·  +  s1 s2  ·  = 1√ 2  h1 ∗ h 2  h2  −h ∗ 1   cid:6 h12 + h22  0    Estimated symbol vector after matched ﬁltering  y = ˜HH · ˜r = 1 2  ·  0  h12 + h22  · s + ˜HH · ˜n  Figure 5.33: Orthogonal space–time block code by Alamouti with NT = 2 transmit  antennas  √ Next, both symbols are exchanged and the ﬁrst antenna transmits x1[ cid:6  + 1] = −s ∗ √ 2   while the second antenna emits x2[ cid:6  + 1] = s ∗ 1   r[ cid:6  + 1] = h1x2[ cid:6  + 1] + h2x2[ cid:6  + 1] + n[ cid:6  + 1] = 1√ 2  2. This leads to h1 − s  ∗ 1  ∗ 2  2  + h2s  · cid:2   cid:3  + n[ cid:6  + 1] .  T respectively. This yields the  Using vector notations, we can write the two received symbols and the two noise samples  r[ cid:6 ] r[ cid:6  + 1]  into vectors r =.  compact description   T and n =.  cid:7   cid:6  −s  s1  s2 ∗ s 1  ∗ 2  ·  n[ cid:6 ] n[ cid:6  + 1]  cid:6    cid:7   h1 h2  r = 1√ 2  ·  + n = X2 · h + n  cid:7   cid:6  s1 −s ∗ ∗ 2 1  s2  ·  2  s   5.65    5.66   The columns of the space–time code word x[ cid:6 ] x[ cid:6  + 1]  X2 = cid:2    cid:3  = 1√  represent the symbols transmitted at a certain time instant, while the rows represent the symbols transmitted over a certain antenna. The entire set of all code words is denoted by X2. Since K = 2 symbols s1 and s2 are transmitted during L = 2 time slots, the rate of this   SPACE–TIME CODES 259 code is R = K L = 1. It is important to mention that the columns in X2 are orthogonal, so that Alamouti’s scheme does not provide a coding gain.  Taking the conjugate complex of the second line in  5.65 , we can rewrite this equation and obtain Equation  5.63  in Figure 5.33. Obviously, this slight modiﬁcation has trans- formed the Multiple-Input Single-Output  MISO  channel h into an equivalent MIMO channel  ˜H = H[X2] = 1√  ·  2   cid:7    cid:6  h2 −h ∗ ∗ 2 h 1  h1  .  This matrix has orthogonal columns, so that the matched ﬁlter ˜H represents the optimum receive ﬁlter according to Section 5.1. The matched ﬁlter output is given in Equation  5.64 . A comparison of the diagonal elements with the results of the receive diversity concept on page 224 illustrates the equivalence of both concepts. The multiple antenna side has simply been moved from the receiver to the transmitter, leading to similar results. In both cases, the squared magnitudes of the contributing channel coefﬁcients hν are summed. Hence, the full diversity degree of D = 2 is obtained which is the largest possible degree for NT = 2.  However, there exists a major difference between transmit and receive diversity which can be illuminated by deriving the signal-to-noise ratio. For Alamouti’s scheme, the signal power after matched ﬁltering becomes S = 1 4  2S  while  · cid:2 h12 + h22  cid:3 2 · σ · cid:2 h12 + h22  cid:3  · σ · cid:2 h12 + h22  cid:3  · σ 2S  2N  . = 1 2  N = 1 2  · cid:2 h12 + h22   cid:3  · Es  N0  σ 2N √  instead of  h12 + h22 Es N0 for the receive diversity concept. Comparing both SNRs, we observe that they differ by the factor 1  2. This corresponds to an SNR loss of 3 dB because no array gain is possible without channel knowledge at the transmitter. The reason for this difference is that we assumed perfect channel knowledge at the receiver, so that the matched ﬁlter delivered an array gain of 10 log10 NR  ≈ 3 dB. However, we have no channel knowledge at the transmitter, and hence no array gain. With perfect channel knowledge at the transmitter, both results would have been identical.  Moreover, the off-diagonal elements are zero, so that no interference between s1 and s2 disturbs the decision. Additionally, the noise remains white when multiplied with a matrix consisting of orthogonal columns. Hence, the symbol-by symbol detection   cid:17  cid:17    cid:17  cid:17 2  ˆsµ = argmin a∈S  yµ −  h12 + h22   a  .   5.68   holds for the received noise power. Consequently, we obtain  γ = S  N  = 1 2   5.67   is optimum.    5.69   260  Application to UMTS  SPACE–TIME CODES  In the UMTS standard  release 99   3GPP, 1999 , a slightly different implementation was chosen because the compatibility with one-antenna devices should be preserved. This mod- iﬁcation does not change the achievable diversity gain. Instead of setting up the space–time code word according to Equation  5.66 , the code matrix has the form  X2 = cid:2   x[ cid:6  − 1] x[ cid:6 ]   cid:6  −s  s1  ∗ 2  ·  2   cid:7   s2 ∗ s 1   cid:3  = 1√ 1 and −s ∗  This implementation transmits both original symbols s1 and s2 over the ﬁrst antenna, ∗ 2 are emitted over the second antenna. whereas the complex conjugate symbols s If a transmitter has only a single antenna, we simply have to remove the second row of X2; the signalling in the ﬁrst row is not affected at all. On the other hand, switching from NT = 1 to NT = 2 just requires the activation of the second antenna without inﬂuencing the data stream x1[ cid:6 ]. The original approach of Alamouti would require a complete change of X2.  5.4.2 Extension to More than Two Transmit Antennas Figure 5.34 shows the basic structure of a space–time block coding system for NR = 1 receive antenna. The space–time encoder collects a block of K successive symbols sµ and assigns them onto a sequence of L consecutive vectors  x[k] = cid:2   x1[k]   cid:3 T ··· xNT[k]  ∗ 1 , . . . , s  with 0 ≤ k < L. Therefore, the code matrix XNT consists of K symbols s1, . . . , sK as ∗ well as their conjugate complex counterparts s K that are arranged in NT rows and L columns. Since the matrix occupies L time instants, the code rate is given in Equation  5.70 . When comparing different space–time coding schemes, one important parameter is the spectral efﬁciency η in Equation  5.71 . It equals the product of R and the number m of bits per modulation symbol s[ cid:6 ]. Therefore, it determines the average number of bits that are transmitted per channel use. In order to obtain orthogonal space–time block codes, the rows in XNT have to be orthogonal, resulting in the orthogonality constraint given in Equation 5.72 .  The factor in front of the identity matrix ensures that the average transmit power per XNT is a square NT × NT  symbol equals Es Ts and is independent of NT and L. Since XH NT matrix, the equality   cid:4   tr  XNTXH  NT   cid:5  = K · Es  Ts   5.73   holds. In order to illustrate the condition in Equation  5.73 , we will ﬁrst look at Alamouti’s scheme. Each of the two symbols is transmitted twice during one block, once as the original √ version and a second time as the complex conjugate version. As the total symbol power 2 in front of X2 is required, as already should be ﬁxed to Es Ts, a scaling factor of 1  used on page 258. For general codes where a symbol is transmitted N times  either the N is original symbol or its complex conjugate  with full power, the scaling factor 1  obtained. As we will see, some schemes attenuate symbols in XNT differently, i.e. they are not transmitted each time with full power. Consequently, this has to be considered when determining an appropriate scaling factor.  √   SPACE–TIME CODES  261  Orthogonal space–time block codes  s[ cid:6 ]  demux  s1  sK  XNT  x1[k]   NT × L  matrix  h1  hNT  n[k]  r[k]    Code rate    Spectral efﬁciency    Orthogonality constraint  xNT[k]  R = K  L  η = m · R = m · K  L  XNTXH NT  = K NT  · Es Ts  · INT   5.70    5.71    5.72   Figure 5.34: General structure of orthogonal space–time block codes with NT transmit  antennas  Unfortunately, Alamouti’s scheme is the only orthogonal space–time code with rate R = 1. For NT > 2, orthogonal codes exist only for rates R < 1. This loss in spectral efﬁciency can be compensated for by using larger modulation alphabets, e.g. replacing Quaternary Phase Shift Keying  QPSK  with 16-QAM, so that more bits per symbol can be transmitted. However, we know from Subsection 5.1.1 that increasing the modulation size M leads to higher error rates. Hence, we have to answer the question as to whether the achievable diversity gain will be larger than the SNR loss due to a change of the modulation scheme. Half-rate codes exist for an arbitrary number of transmit antennas. Code matrices for NT = 3 and NT = 4 are presented and summarised in Figure 5.35. Both codes achieve the full diversity degree that equals NT. For NT = 3, each of the K = 4 symbols occurs 6 times √ 6. With NT = 4 transmit antennas, again four in X3, resulting in a scaling factor of 1  symbols are mapped onto L = 8 time slots, where each symbol is used 8 times, leading to a factor 1  Figure 5.36 shows two codes with NT = 3 and NT = 4  Tarokh et al., 1999a,b . Both codes again have the full diversity degree and map K = 3 symbols onto L = 4 time slots, leading to a rate of R = 3 4. In order to distinguish them from the codes presented so far, we use the notation T3 and T4. For NT = 3, the orthogonal space–time code word is presented in Equation  5.76 . Summing the squared magnitudes for each symbol in T3  √  8.   262  SPACE–TIME CODES  Orthogonal space–time block codes for R = 1 2  ·  s2  s1  X3 = 1√ 6     NT = 3 transmit antennas  L = 8, K = 4  s1 −s2 −s3 −s4 s4 −s3 s3 −s4  s1 −s2 −s3 −s4    NT = 4 transmit antennas  L = 8, K = 4   s4 −s3  X4 = 1√ 8  s3 −s4  s1  s2  s2  s1  s1  ·  s3 −s2  s4  s2 s1  ∗ 1 ∗ 2 ∗ 3  s  s  s  ∗ 1 ∗ 2 ∗ 3 ∗ 4  s  s  s  s        −s ∗ −s 4 ∗ 3 ∗ 2  s  −s ∗ −s 4 ∗ 3 ∗ 2 ∗ 1  s  s  −s ∗ 2 ∗ −s 1  ∗ 4  s  s  −s ∗ 2 ∗ −s 1 ∗ 4 ∗ 3  s  −s ∗ 3 ∗ 4 ∗ 1  s  s  s  −s ∗ 3 ∗ 4 ∗ −s 1  ∗ 2  s   5.74    5.75   Figure 5.35: Half-rate orthogonal space–time block codes  Tarokh et al., 1999a   Orthogonal space–time block codes for R = 3 4    NT = 3 transmit antennas  L = 4, K = 3  √ ∗ ∗ √ 2s 2 3 ∗ ∗ √ 2s 2s 1 3 2s3 −s1 − s + s2 − s ∗ 1    2s1 −2s  T3 = 1√ 12  √ 2s2 2s3  ·  ∗ 2  √ ∗ −√ 2s 3 2s s1 − s + s2 + s  ∗ 3  ∗ 1  ∗ 2    NT = 4 transmit antennas  L = 4, K = 3    √  2s1 −2s ∗ ∗ √ 2s 2 3 ∗ ∗ √ √ 2s2 2s 2s 1 3 2s3 −s1 − s + s2 − s ∗ 2s3 −√ √ 2s3 1 − s2 − s 2s3 −s1 − s ∗ 1  ∗ 2 ∗ 2  T4 = 1 4  √ ∗ −√ 2s 3 2s s1 − s + s2 + s ∗ ∗ 1 2 + s2 + s − s1 + s ∗ 1  ∗ 3  ∗ 2       5.76      5.77   Figure 5.36: Orthogonal space–time block codes with rate R = 3 4  Tarokh et al.,  1999a    SPACE–TIME CODES  √  263  results in a value of 12. Hence, the scaling factor amounts to 1  summation yields a value of 16, and thus a scaling factor of 0.25.  12. For the code T4, the  The detection at the receiver works in the same way as for Alamouti’s scheme. K¨uhn derived the equivalent channel matrices H[XNT] for each space–time block code  K¨uhn, 2006 . They make it possible to describe the received vector as the product of H[XNT] and the transmit vector x.5 Since the columns of these matrices are orthogonal, a simple multiplication with their Hermitian delivers the desired data symbol estimates ˜sµ.  5.4.3 Simulation Results  After the introduction of several orthogonal space–time block codes, we now analyse their error rate performance. A ﬁrst comparison regards all discussed schemes with BPSK modulation. Hence, the spectral efﬁciencies are different, as shown in Figure 5.37. In the left-hand diagram, the error rates are depicted versus Es N0. Obviously, codes with identical diversity degrees such as X3 and T3 achieve the same error rates because their differing code rates are not considered. This somehow leads to an unfair comparison. Instead, one  Performance of orthogonal space–time block codes for BPSK   a  error rate versus Es N0 X2 X3 X4 T3 T4   b  error rate versus Eb N0  X2 X3 X4 T3 T4  100  10 cid:358 1  10 cid:358 2  10 cid:358 3  10 cid:358 4  → R E B  10  5 15 Es N0 in dB →  20  10 cid:358 5 0  10  5 15 Eb N0 in dB →  20  Figure 5.37: Bit error rates for different orthogonal STBCs with BPSK and code rates R X2  = 1, R X3  = R X4  = 0.5 and R T3  = R T4  = 0.75  AWGN reference: bold  line . Reproduced by permission of John Wiley & Sons, Ltd  5For the code matrices T3 and T4, a real-valued description is required, i.e. the linear equation system does  not contain complex symbols and their conjugates, but only real and imaginary parts of all components.  → R E B  100  10 cid:358 1  10 cid:358 2  10 cid:358 3  10 cid:358 4  10 cid:358 5 0   264 SPACE–TIME CODES should depict the error rates versus Eb N0, where Es = REb holds. First, the corresponding results in the right-hand diagram illustrate that the slopes of all curves are still the same because using Eb N0 does not change the diversity degree. However, horizontal shifts can be observed, so that T4 now yields the best results. The higher diversity degree of 4 overcompensates for the lower code rate compared with Alamouti’s scheme. On the other hand, the half-rate codes lose most, and Alamouti’s code outperforms X3 over a wide range of Eb N0 values. As a reference, the AWGN curve is plotted as a bold line. Certainly, it cannot be reached by any of the codes owing to a maximum diversity degree of only 4.  Next, we would like to compare different space–time coding schemes under the con- straint of identical spectral efﬁciencies. Therefore, the modulation schemes have to be adapted. In order to achieve a spectral efﬁciency η = 2 bit s Hz, Alamouti’s scheme has to employ QPSK, while the codes X3 and X4 with R = 1 2 have to use 16-QAM or 16- PSK. Owing to the better performance of 16-QAM, we conﬁne ourselves to that modulation scheme. For η = 3 bit s Hz, 8-PSK is chosen for X2 and 16-QAM for T3 and T4.  The results are depicted in Figure 5.38. On account of to the higher robustness of QPSK compared with 16-QAM the code X2 performs better than X3 and X4 for low and medium signal-to-noise ratios  left-hand diagram . Asymptotically, the diversity gain becomes dominating owing to the larger slope of the curves, so that X3 and X4 are bet- ter for large SNR. The error rates of all space–time codes are signiﬁcantly better than simple QPSK transmission without diversity, although the AWGN performance is not reached.  Performance of orthogonal space–time block codes   a  η = 2 bit s Hz   b  η = 3 bit s Hz  X2, QPSK X3, 16-QAM X4, 16-QAM  X2, 8-PSK T3, 16-QAM T4, 16-QAM  → R E B  100  10 cid:358 1  10 cid:358 2  10 cid:358 3  10 cid:358 4  10 cid:358 5 0  → R E B  100  10 cid:358 1  10 cid:358 2  10 cid:358 3  10 cid:358 4  10 cid:358 5 0  5  15  10 20 Eb N0 in dB →  25  30  5  15  10 20 Eb N0 in dB →  25  30  Figure 5.38: Bit error rates for different orthogonal STBCs with η = 2 bit s Hz and η = 3 bit s Hz  AWGN reference: bold solid line; Rayleigh reference: bold dashed line    SPACE–TIME CODES  265 Diagram  b  illustrates results for a spectral efﬁciency η = 3 bit s Hz. Owing to the relative high code rate R = 0.75 for T3 and T4, only four instead of three bits per symbol have to be transmitted, leading to the modulation schemes 8-PSK and 16-QAM. As we know from Subsection 5.1.1, 16-QAM performs nearly as well as 8-PSK. Hence, the larger diversity gain for T3 and T4 is visible over the whole range of SNRs, and the weaker performance of 8-PSK compared with 16-QAM is negligible.  5.5 Spatial Multiplexing  5.5.1 General Concept  In the previous section, coding in space and time has been discussed, mainly for systems with multiple transmit but only a single receive antenna. This approach aims to increase the diversity degree and, hence, the reliability of a communication link. The achievable data rate is not changed or even decreased by code rates R < 1. By Contrast, multiple antennas at transmitter and receiver make it possible to increase the data rate by exploiting the resource space without increasing the bandwidth. This multiplexing gain is possible by transmitting independent data streams over spatially separated channels. We will see later that diversity and multiplexing gains can be obtained simultaneously by using a more general description such as linear dispersion codes.  In this section, we assume frequency-non-selective fading channels between each pair of transmit and receive antennas that can be modelled by complex-valued coefﬁcients whose real and imaginary parts are independent and Gaussian distributed. Linear modulation schemes such as M-PSK or M-QAM are used at the transmitter. The improved throughput by increasing M comes along with a larger sensitivity to noise and interference. Moreover, the computational complexity of some detection schemes grows exponentially with the number of the possible transmit vectors and, thus, with M. As stated in Section 5.2, the general system model can be described by  r = H · x + n   5.78   Principally, two cases have to be distinguished.  Exploiting Eigenmodes of Spatial Channel  If channel knowledge is available at transmitter and receiver side, we saw in Subsec- tion 5.3.1 that capacity can be achieved by exploiting the channel’s eigenmodes and applying the waterﬁlling principle. The maximum number of non-zero eigenmodes equals the rank of H and is upper bounded by min{NT, NR}. However, it has to be mentioned that the number of parallel data streams that can be supported also depends on the available transmit power σ 2X > 0 and may be smaller than the rank of H. The transmission strategy is accomplished by performing a singular value decomposition of the channel matrix H = UH cid:23 HVHH. If the waterﬁlling solution distributes the total transmit power onto r data streams with powers λi, the transmit vector is constructed by  x = VX ·  cid:13   1 2X · s,   266 SPACE–TIME CODES where  cid:13 X = diag[λ1 ··· λr] is a diagonal matrix containing the different power levels. The transmit ﬁlter VX comprises those columns of VH that correspond to the r largest singular values of H. Equivalently, the receive ﬁlter UX contains those columns of UH that are associated with the used eigenmodes. Using VX and UX leads to  y = UHX · r =  cid:23 H ·  cid:13 X · s + ˜n .  Since  cid:23 H and  cid:13 X are diagonal matrices, the data streams are perfectly separated into parallel channels whose signal-to-noise ratios amount to  γi = σ 2H,i  · λi · σ 2X σ 2N  = σ 2H,i  · λi · Es  .  N0  They depend on the squared singular values σ 2H,i and the speciﬁc transmit power levels λi. For digital communications, the input alphabet is not Gaussian distributed as assumed in Section 5.3 but consists of discrete symbols. Hence, ﬁnding the optimal bit and power allo- cation is a combinatorial problem and cannot be found by gradient methods as discussed on 241. Instead, algorithms presented elsewhere  Fischer and Huber, 1996; Hughes-Hartogs, 1989; Krongold et al., 1999; Mutti and Dahlhaus, 2004  have to be used. Different opti- misation strategies are possible. One target may be to maximise the throughput at a given average error rate. Alternatively, we can minimise the error probability at a given total throughput or minimise the transmit power for target error and data rates.  No Channel Knowledge at Transmitter  In Section 5.3 it was shown that the resource space can be used even in the absence of channel knowledge at the transmitter. The loss compared with the optimal waterﬁlling solution is rather small. However, the price to be paid is the application of advanced signal processing tools at the receiver. A famous example is the Bell Labs Layered Space–Time  BLAST  architecture  Foschini, 1996; Foschini and Gans, 1998; Foschini et al., 1999 . Since no channel knowledge is available at the transmitter, the best strategy is to transmit independent equal power data streams called layers. At least two BLAST versions exist. The ﬁrst, termed diagonal BLAST, distributes the data streams onto the transmit antennas according to a certain permutation pattern. This kind of interleaving ensures that the symbols within each layer experience more different fading coefﬁcients, leading to a higher diversity gain during the decoding process.  In this section, we focus only on the second version, termed vertical BLAST, which is shown in Figure 5.39. Hence, no interleaving between layers is performed and each data stream is solely assigned to a single antenna, leading to x = s. The name stems from the vertical arrangement of the layers. This means that channel coding is applied per layer. Alternatively, it is also possible to distribute a single coded data stream onto the transmit antennas. There are slight differences between the two approaches, especially concerning the detection at the receiver. As we will soon see, per-layer encoding makes it possible to include the decoder in an iterative turbo detection, while this is not directly possible for a single code stream.  From the mathematical description in Equation  5.78  we see that a superposition  rµ = NT cid:1   ν=1  hµ,ν · xν + nµ   SPACE–TIME CODES  Spatial multiplexing with V-BLAST  encoder  modulator  u1  b1  x1  encoder  modulator  xNT  uNT  bNT  h1,1  hNR,1  h1,NT  hNR,NT  n1  r1  nNR  rNR  space-time detector  267  ˜x1  ˜xNT  V-BLAST  space  x1  x2  xNT  time  encoding  Figure 5.39: Multilayer transmission with vertical BLAST  V-BLAST  and per-layer  of all transmitted layers is obtained at each receive antenna 1 ≤ µ ≤ NR. The task of the space–time detector is to separate the data streams again. In coded systems, the space–time detector and decoder are generally implemented separately for complexity reasons. However, both components can exchange iteratively information equivalent to the turbo principle for concatenated codes discussed in Chapter 4. We will continue the discussion with the optimum bit-by-bit space–time detector that delivers LLRs. Since the computational complexity becomes demanding for high modulation levels and many layers, alternative detection strategies are discussed subsequently.  As mentioned above, we will now consider a receiver structure as depicted in Figure 5.40.  5.5.2 Iterative APP Preprocessing and Per-layer Decoding A joint preprocessor calculates LLRs L ˆbν  r  for each layer ν on the basis of the received samples r1 up to rNR. As we assume a memoryless channel H, the preprocessor can work in a symbol-by-symbol manner without performance loss. After de-interleaving, the FEC decoders deliver independently for each layer estimates ˆuν of the information bits as well as LLRs L ˆbν   of the coded bits. From the LLRs, the extrinsic parts, i.e. the parts generated   268  SPACE–TIME CODES  Turbo detection for V-BLAST  La b1   n1  r1  nNR  rNR  APP  processor  L b1  r    r   L bNT  La bNT     cid:25   −1   cid:25   −1   cid:25    cid:25   L b1   decoder  decoder  L bNT    ˆu1  ˆuNT  Figure 5.40: Iterative turbo detection of V-BLAST for per-layer encoding  from the code’s redundancy, are extracted, interleaved and fed back as a-priori information to the preprocessor. Now, a second iteration can start with an improved output of the joint preprocessor owing to the available side information from the previous iteration. As already described in Chapter 4, an iterative detection process is obtained that aims to approach the maximum likelihood solution  Liew and Hanzo, 2002 .  We will now have a more thorough look at the joint preprocessor. The major steps are summarised in Figure 5.41. From Equation  5.79  we see that the LLR depends on the ratio of conditional probabilities Pr{bν = ξ  r} which are deﬁned as  Since p r  is identical in numerator and denominator of the log likelihood ratio, it can be skipped, leading to the right-hand side of Equation  5.79 . The joint probability densities have to be further processed to obtain a tractable form. For multilevel modulation schemes such as QAM and PSK, each symbol s is assigned to several bits bν with 1 ≤ ν ≤ ld M . The probability that a certain bit bν takes a value ξ can be calculated by summing the probabilities Pr{s} over those symbols s whose νth bit equals ξ  Pr{bν = ξ  r} = p bν , r  p r   .  Pr{bν = ξ} =  Pr{s}.   cid:1   s,bν=ξ  At this point it has to be emphasised that r comprises corrupted versions of all NT trans- mitted symbols. Therefore, it is not sufﬁcient to consider a single symbol s. Instead, the set   SPACE–TIME CODES  269  APP-preprocessor for V-BLAST  p bν = 0, r  p bν = 1, r    5.79   p bν = ξ, r  =  p r  s  · Pr{s}   5.80     LLR of APP preprocessor  L bν  r  = log    Joint probability density  Pr{bν = 0  r} Pr{bν = 1  r} = log  cid:1   cid:1   p s, r  =  s∈Sν  ξ    s∈Sν  ξ      Conditional probability density function p r  s  ∝= exp  "  !  − cid:30 r − Hs cid:30 2  σ 2N    A-priori probability  Pr{s} = NT ld M  cid:20   ν=1  Pr{bν  s }   5.81    5.82   Figure 5.41: APP calculation steps for the joint preprocessor of V-BLAST systems  of NT symbols and, correspondingly, all bits bν with 1 ≤ ν ≤ NT ld M  have to be taken into account. For this purpose, we deﬁne a kind of vector modulation that maps a vector  of NT ld M  bits onto a vector   cid:3 T  b1  b = cid:2  s = cid:2   ··· bNT ld M   cid:3 T  ···  sNT  b2  s1  s2  of NT symbols. The set of vectors is divided into subsets Sν  ξ   containing those symbol ensembles for which the νth bit of their binary representation b takes the value bν = ξ, leading to Equation  5.80 . This transforms the generally unknown joint probability p bν , r  into an expression depending on the conditional densities p r  s  and the a-priori probabilities Pr{s}. The former determine the channel statistics and are proportional to the expression in Equation  5.81 . Assuming that all bits bν are statistically independent, the a-priori probability Pr{s} can be factorised into the marginal probabilities according to Equation  5.82  where the bits bν  s  are determined by the speciﬁc symbol vector s.  From the above equations we can already conclude that the computational complexity grows linearly with the number of possible hypotheses and, hence, exponentially with the   270  SPACE–TIME CODES  number of layers as well as the number of bits per symbol. Thus, this optimal approach is feasible only for small systems and small modulation alphabets.  Turbo Iterations  In the ﬁrst iteration, no a-priori information from the decoders is available. Hence, the a-priori probabilities are constant, Pr{s} = 2 −NT ld M , and can be dropped. The insertion of Equation  5.80  and Equation  5.81  into Equation  5.79  then leads to Equation  5.83  in Figure 5.42. We observe that the numerator and denominator only differ by the sub- sets Sν  ξ   which distinguish the two hypotheses bν = 0 and bν = 1. Subsequent per-layer soft-output decoding according to Bahl, Cocke, Jelinek, Raviv  BCJR  or max-log MAP algorithms  rf. to Section 3.4  provides LLRs for each code bit and layer.  APP preprocessor and turbo iterations    LLR of APP preprocessor in ﬁrst iteration  ! !     s∈Sν  0  exp  s∈Sν  1  exp  − cid:30 r−Hs cid:30 2  − cid:30 r−Hs cid:30 2  σ 2N  σ 2N  " "  L bν  r  = log    Calculating a-priori probabilities from decoder LLRs    A-priori information per symbol  Pr{bν = ξ} = exp[−ξ La bν  ] 1 + exp[−La bν  ] → NT ld M  cid:20   exp[−bν  s La bν  ] 1 + exp[−La bν  ]    s∈Sν  0  exp  ! ! − cid:30 r−Hs cid:30 2 − cid:30 r−Hs cid:30 2  s∈Sν  1  exp  σ 2N  σ 2N  exp[−bν  s La bν  ] " "  bν  s La bν    NT ld M  ν=1  NT ld M  ν=1  bν  s La bν    ν=1  − −  Pr{s} = NT ld M  cid:20   ν=1  L bν  r  = log    LLR of APP preprocessor after ﬁrst iteration   5.83    5.84    5.85    5.86   Figure 5.42: APP preprocessor and turbo iterations for the V-BLAST system   SPACE–TIME CODES  271  They are fed back into the APP processor and have to be converted into a-priori proba- bilities Pr{s}. Using the results from the introduction of APP decoding given in Section 3.4, we can easily derive Equation  5.83  where ξ ∈ {0, 1} holds. Inserting the expression for the bit-wise a-priori probability in Equation  5.84  into Equation  5.82  directly leads to Equation  5.85 . Since the denominator of Equation  5.84  does not depend on the value of bν itself but only on the decoder output L bν  , it is independent of the speciﬁc symbol vector s represented by the bits bν. Hence, it becomes a constant factor regarding s and can be dropped, as done on the right-hand side. Replacing the a-priori probabilities in Equation  5.79  with the last intermediate results leads to the ﬁnal expression in Equation  5.86 . On account of bν ∈ {0, 1}, the a-priori LLRs only contribute to the entire result if a symbol vector s with bν = 1 is considered. For these cases, the L bν   contains the correct information only if it is negative, otherwise its information is wrong. Therefore, true a-priori information increases the exponents in Equation  5.86  which is consistent with the negative squared Euclidean distance which should also be maximised.  Max-Log MAP Solution  As already mentioned above, the complexity of the joint preprocessor still grows expo- nentially with the number of users and the number of bits per symbol. In Section 3.4 a suboptimum derivation of the BCJR algorithm has been introduced. This max-log MAP approach works in the logarithmic domain and uses the Jacobian logarithm   cid:2   ex1 + ex2  log   cid:3  = log  . emax{x1,x2} cid:2   = max{x1, x2} + log  −x1−x2 cid:3   . −x1−x2  1 + e 1 + e  Obviously, the right-hand side depends on the maximum of x1 and x2 as well as on the absolute difference. If the latter is large, the logarithm is close to zero and can be dropped. We obtain the approximation  Applying approximation  5.87  to Equation  5.86  leads to   cid:2   log  ex1 + ex2  cid:19    cid:3  ≈ max{x1, x2} + NT ld M  cid:1  + NT ld M  cid:1   ν=1  ν=1   cid:30 r − Hs cid:30 2  cid:19   σ 2N  cid:30 r − Hs cid:30 2  σ 2N  − min s∈Sν  0   ,  ,  bν  s La bν    L bν  r  ≈ min s∈Sν  1   bν  s La bν     5.88    5.87   We observe that the ratio has become a difference and the sums in numerator and denom- inator have been exchanged by the minima searches. The latter can be performed pairwise between the old minimum and the new hypothesis. It has to be mentioned that the ﬁrst minimisation runs over all s ∈ Sν  0 , while the second uses s ∈ Sν  1 .   272  SPACE–TIME CODES  Using Expected Symbols as A-Priori Information  Although the above approximation slightly reduces the computational costs, often they may be still too high. The prohibitive complexity stems from the sum over all possible hypotheses, i.e. symbol vectors s. A further reduction can be obtained by replacing the explicit consideration of each hypothesis with an average symbol vector ¯s. To be more precise, the sum in Equation  5.86  is restricted to the M hypotheses of a single symbol sµ in s containing the processed bit bν. This reduces the number of hypotheses markealy from M NT to M. For the remaining NT − 1 symbols in s, their expected values   cid:1   ξ∈S   cid:1   ξ∈S  ξ · ld M  cid:20   ν=1  ¯sµ =  ξ · Pr{ξ} ∝  −bν  ξ  La bν    e   5.89   are used. They are obtained from the a-priori information of the FEC decoders. As already mentioned before, the bits bν are always associated with the current symbol ξ of the sum. In the ﬁrst iteration, no a-priori information from the decoders is available, so that no expectation can be determined. Assuming that all symbols are equally likely would result in ¯sµ ≡ 0 for all µ. Hence, the inﬂuence of interfering symbols is not considered at all and the tentative result after the ﬁrst iteration can be considered to be very bad. In many cases, convergence of the entire iterative process cannot be achieved. Instead, either the full-complexity algorithm or alternatives that will be introduced in the next subsections have to be used in this ﬁrst iteration.  5.5.3 Linear Multilayer Detection  A reduction in the computational complexity can be achieved by separating the layers with a linear ﬁlter. These techniques are already well known from multiuser detection strategies in CDMA systems  Honig and Tsatsanis, 2000; Moshavi, 1996 . In contrast to optimum maximum likelihood detectors, linear approaches do not search for a solution in the ﬁnite signal alphabet but assume continuously distributed signals. This simpliﬁes the combinatorial problem to an optimisation task that can be solved by gradient methods. This leads to a polynomial complexity with respect to the number of layers instead of an exponential dependency. Since channel coding and linear detectors can be treated separately, this section considers an uncoded system. The MIMO channel output can be expressed by the linear equation system  r = Hs + n  which has to be solved with respect to s.  Zero-Forcing Solution  The zero-forcing ﬁlter totally suppresses the interfering signals in each layer. It delivers the vector sZF ∈ CNT which minimises the squared Euclidean distance to the received vector r  5.90   44r − H˜s 442  sZF = argmin ˜s∈CNT  Although Equation  5.90  resembles the maximum likelihood approach, it signiﬁcantly dif- fers from it by the unconstraint search space CNT instead of SNT. Hence, the result sZF is   SPACE–TIME CODES  273  Linear detection for the V-BLAST system  demodulator  ˜s1  ˜sNT  linear  ZF MMSE  detector  n1  r1  nNR  rNR    zero-forcing ﬁlter    Minimum mean-squared error ﬁlter  WZF = H · cid:2  WMMSE = H · cid:2   demodulator   cid:3 −1  HHH   cid:3 −1  HHH + σ 2N σ 2S  INT  ˆu1  ˆuNT  Figure 5.43: Linear detection for the V-BLAST system  ∂  ∂˜sH  r − H˜s  yields the solution  44r − H˜s  generally not an element of SNT. However, this generalisation transforms the combinatorial problem into one that can be solved by gradient methods. Hence, the squared Euclidean dis- tance in Equation  5.90  is partially differentiated with respect to ˜sH. Setting this derivative to zero  442 = ∂ ∂˜sH  cid:2  sZF = WH The ﬁlter matrix WZF = H† = H and can be expressed by the right-hand side of Equation  5.92  if H has full rank. In this case, the inverse of HHH exists and the ﬁlter output becomes   cid:3 H · cid:2   cid:2  · r = H† · r = cid:2   cid:3 −1 is called the Moore–Penrose, or pseudo, inverse −1HH · cid:2    cid:3  = −HHr + HHH˜s != 0  cid:3 −1 · HH · r   cid:3  = s + WH  sZF =  HHH   Hs + n  r − H˜s  HHH   5.92    5.93    5.91   HHH  · n  ZF  ZF  Since the desired data vector s is only disturbed by noise, the ﬁnal detection is obtained by a scalar demodulator sZF = Q sZF  of the ﬁlter outputs sZF. The non-linear function Q ·  represents the hard-decision demodulation.  Although the ﬁlter output does not suffer from interference, the resulting signal-to-noise ratios per layer may vary signiﬁcantly. This effect can be explained by the fact that the total suppression of interfering signals is achieved by projecting the received vector into the null space of all interferers. Since the desired signal may have only a small component   274  SPACE–TIME CODES  lying in this subspace, the resulting signal-to-noise ratio is low. It can be expressed by the error covariance matrix      cid:31  cid:2   cid:31  cid:2   cid:21 ZF = E = E = σ 2N WH   cid:3  cid:2   cid:3 H sZF − s sZF − s  cid:3  cid:2  s + WH ZFn − s ZFn − s s + WH  cid:2   cid:3 −1 ZFWZF = σ 2N  HHH      cid:3 H   cid:4    cid:5   = WH  ZF E  nnH  WZF   5.94   which contains on its diagonal the mean-squared error for each layer. The last row holds if the covariance matrix of the noise equals a diagonal matrix containing the noise power σ 2N . The main drawback of the zero-forcing solution is the ampliﬁcation of the background noise. If the matrix HHH has very small eigenvalues, its inverse may contain very large values that enhance the noise samples. At low signal-to-noise ratios, the performance of the Zero-Forcing ﬁlter may be even worse than a simple matched ﬁlter. A better solution is obtained by the Minimum Mean-Square Error  MMSE  ﬁlter described next.  Minimum Mean-Squared Error Solution  Looking back to Equation  5.90 , we observe that the zero-forcing solution sZF does not consider that the received vector r is disturbed by noise. By contrast, the MMSE detector WMMSE does not minimise the squared Euclidean distance between the estimate and the r, but between the estimate   cid:31 44WHr − s  442      5.95   sMMSE = WH  MMSE  · r with WMMSE = argmin W∈CNT×NR  E  and the true data vector s. Similarly to the ZF solution, the partial derivative of the squared Euclidean distance with respect to W is determined and set to zero. With the relation ∂WH ∂W = 0  Fischer, 2002 , the approach  .  WHr − x  WHr − x H    cid:5  = WH cid:21 RR −  cid:21 S R != 0   5.96    cid:4   tr  E  ∂ ∂W  leads to the well-known Wiener solution  WH  MMSE  =  cid:21 S R ·  cid:21   −1RR  The covariance matrix of the received samples has the form   cid:21 RR = E{rrH} = H cid:21 X X HH +  cid:21 N N  while the cross-covariance matrix becomes   cid:21 S R = E{srH} =  cid:21 S SHH +  cid:21 S N  Assuming that noise samples and data symbols are independent of each other and identically distributed, we obtain the basic covariance matrices   cid:21 N N = E{nnH} = σ 2N · INR  cid:21 S S = E{ssH} = σ 2S · INT  cid:21 S N = E{snH} = 0   5.97    5.98    5.99    5.100a    5.100b    5.100c    SPACE–TIME CODES  Inserting them into Equations  5.98  and  5.99  yields the ﬁnal MMSE ﬁlter matrix   cid:6  HHH + σ 2N σ 2S  INR   cid:7 −1  WMMSE = H  275   5.101   The MMSE detector does not suppress the multiuser interference perfectly, and some residual interference still disturbs the transmission. Moreover, the estimate is biased. The error covariance matrix, with Equation  5.101 , now becomes   cid:31  cid:2    cid:3  cid:2    cid:21 MMSE = E  sMMSE − s  sMMSE − s      cid:3 H   cid:2   = σ  2N  HHH + σ 2N σ 2S  INT   cid:3 −1   5.102   From Equations  5.101  and  5.102  we see that the MMSE ﬁlter approaches the zero- forcing solution if the signal-to-noise ratio tends to inﬁnity.  5.5.4 Original BLAST Detection  We recognized from Subsection 5.5.3 that the optimum APP preprocessor becomes quickly infeasible owing to its computational complexity. Moreover, linear detectors do not exploit the ﬁnite alphabet of digital modulation schemes. Therefore, alternative solutions have to be found that achieve a close-to-optimum performance at moderate implementation costs. Originally, a procedure consisting of a linear interference suppression stage and a subsequent detection and interference cancellation stage was proposed  Foschini, 1996; Foschini and Gans, 1998; Golden et al., 1998; Wolniansky et al., 1998 . It detects the layers successively as shown in Figure 5.44, i.e. it uses already detected layers to cancel their interference onto remaining layers.  In order to get a deeper look inside, we start with the mathematical model of our MIMO  system  r = Hx + n  with  H = cid:2   h1   cid:3   ··· hNT  and express the channel matrix H by its column vectors hν. A linear suppression of the interference can be performed by applying a Zero Forcing  ZF  ﬁlter introduced on page 272. Since the ZF ﬁlter perfectly separates all layers, it totally suppresses the interference, and the only disturbance that remains is noise  K¨uhn, 2006 .  ˜sZF = WH  ZFr = s + WH ZFn  −1 which may contain However, the Moore-Penrose inverse incorporates the inverse  HHH  large values if H is poorly conditioned. They would lead to an ampliﬁcation of the noise n and, thus, to small signal-to-noise ratios.  Owing to the successive detection of different layers, this procedure suffers from the risk of error propagation. Hence, choosing a suited order of detection is crucial. Obvi- ously, one should start with the layer having the smallest error probability because this minimises the probability of error propagation. Since no interference disturbs the deci- sion after the ZF ﬁlter, the best layer is the one with the largest SNR. This measure can be determined by the error covariance matrix deﬁned in Equation  5.104 . Its diagonal   SPACE–TIME CODES  Original V-BLAST detector  276  r  w1  h1  ˆs1  r1 w2  h2  ˆs2  r2 w3  ˆs3  h3   5.103     Zero Forcing  ZF  ﬁlter matrix equals Moore–Penrose inverse   cid:2  WZF = H† = H  HHH   cid:3 −1  cid:5  = σ 2N · WH    Error covariance matrices   cid:4    cid:21 ZF = E  ZFr − x  WH  ZFr − x H   WH  ZFWZF   5.104   Figure 5.44: Structure of V-BLAST detector  Foschini, 1996; Foschini and Gans, 1998   with Zero Forcing  ZF  interference suppression  elements determine the squared Euclidean distances between the separated layers and the true symbols and equal the squared column norm of WZF. Therefore, it sufﬁces to deter- mine WZF; an explicit calculation of  cid:21 ZF is not required. We have to start with the layer that corresponds to the smallest column norm in WZF. This leads to the algorithm presented in Figure 5.45.  The choice of the layer to be detected next in step 2 ensures that the risk of error propagation is minimised. Assuming that the detection in step 4 delivers the correct symbol ˆsλν , the cancellation step 5 reduces the interference so that subsequent layers can be detected more reliably. Layers that have already been cancelled need not be suppressed by the ZF ﬁlter any more. Hence, the ﬁlter has more degrees of freedom and exploits more diversity  K¨uhn, 2006 . Mathematically, this behaviour is achieved by removing columns from H which increases the null space for linear interference suppression.  In order to determine the linear ﬁlters in the different detection steps, the system matrices describing the reduced systems have to be inverted. This causes high implementation costs. However, a much more convenient way exists that avoids multiple matrix inversions. This approach leads to identical results and is presented in the next section.   SPACE–TIME CODES  277  ZF-V-BLAST detection algorithm    Initialisation: set r1 = r, H1 = H   The following steps are repeated for ν = 1, . . . , NT until all layers have  been detected: 1. Calculate the ZF ﬁlter matrix WZF = H† ν. 2. Determine the ﬁlter wν as the column in WZF with the smallest squared  norm, e.g. the λνth column.  3. Use wν to extract the λνth layer by  ˜sλν  = wH  ν  · rν .  4. Detect the interference-free layer either by soft or hard decision  ˆsλν  = QS ˜sλν  . 5. Subtract the decided symbol ˆsλν from rν rν+1 = rν − hλν  ˆsλν  and delete the λνth column from Hν, yielding Hν+1.  Figure 5.45: V-BLAST detection algorithm  Foschini, 1996; Foschini and Gans, 1998   with Zero Forcing  ZF  interference suppression  MMSE Extension of V-BLAST  As mentioned above, the perfect interference suppression of the ZF ﬁlter comes at the expense of a severe noise ampliﬁcation. Especially at low signal-to-noise ratios, it may happen that the ZF ﬁlter performs even worse than the simple matched ﬁlter. The noise ampliﬁcation can be avoided by using the MMSE ﬁlter derived on page 274. It provides the smallest mean-squared error and represents a compromise between residual interfer- ence power that cannot be suppressed and noise power. Asymptotically, the MMSE ﬁlter approaches the matched ﬁlter for very low SNRs, and the ZF ﬁlter for very high SNRs.  From the optimisation approach   cid:31 44WHr − s  442     WMMSE = argmin W∈CNT×NR  E  we easily obtain the MMSE solution presented in Equation  5.105  in Figure 5.46. A con- venient way to apply the MMSE ﬁlter to the BLAST detection problem without new derivation is provided in Equation  5.106 . Extending the channel matrix H in the way   278  SPACE–TIME CODES  MMSE extension of V-BLAST detection    Minimum Mean-Square Error  MMSE  ﬁlter matrix   cid:2  WMMSE = H  HHH + σ 2N σ 2N  INT    Relationship between ZF and MMSE ﬁlter WMMSE H  = WZF H  with    Error covariance matrices  cid:21 MMSE = E   cid:4    cid:6   WMMSEr − x  WMMSEr − x    cid:7 −1  H  = σ 2N ·  HHH + σ 2N σ 2N  INT   cid:3 −1   cid:6   H =  H  σN σX INT   cid:7    cid:5    5.105    5.106    5.107   Figure 5.46: Procedure of V-BLAST detection  Foschini, 1996; Foschini and Gans,  1998  with Minimum Mean-Square Error  MMSE  interference suppression  presented yields the relation   cid:11   HHH =  HH  σN σX INT   cid:6    cid:12   ·   cid:7    cid:6  HHH + σ 2N σ 2X   cid:7   INT  .  H  σN σX INT  =  Obviously, applying the ZF solution to the extended matrix H delivers the MMSE solution of the original matrix H  Hassibi, 2000 . Hence, the procedure described on page 275 can also be applied to the MMSE-BLAST detection. There is only a slight difference that has to be emphasised. The error covariance matrix  cid:21 MMSE given in Equation  5.107  cannot be expressed by the product of the MMSE ﬁlter WMMSE and its Hermitian. Hence, the squared column norm of WMMSE is not an appropriate measure to determine the optimum detection order. Instead, the error covariance matrix has to be explicitly determined.  5.5.5 QL Decomposition and Interference Cancellation  The last subsection made obvious that multiple matrix inversions have to be calculated, one for each detection step. Although the size of the channel matrix is reduced in each detection step, this procedure is computationally demanding. K¨uhn proposed a different implementation  K¨uhn, 2006 . It is based on the QL decomposition of H, the complexity of which is identical to a matrix inversion. Since it has to be computed only once, it   SPACE–TIME CODES  279  saves valuable computational resources. The QR decomposition of H is often used in the literature  W¨ubben et al., 2001 .  ZF-QL Decomposition by Modiﬁed Gram–Schmidt Procedure  We start with the derivation of the QL decomposition for the zero-forcing solution. An extension to the MMSE solution will be presented on page 284. In order to illuminate the basic principle of the QL decomposition, we will consider the modiﬁed Gram–Schmidt algorithm  Golub and van Loan, 1996 . Nevertheless, different algorithms that are based on Householder reﬂections or Givens rotations  see Appendix B  may exhibit a better behaviour concerning numerical robustness. Basically, the QL decomposition decomposes an NR × NT matrix H = QL into an NR × NT matrix  q1 and a lower triangular NT × NT matrix  ··· qNT−1 qNT  Q = cid:2    L1,1  L =  L2,1 L2,2 ...  ···  LNT,1  . . .  LNT,NT   cid:3     .  For NT ≤ NR, which is generally true, we can imagine that the columns in H span an NT- dimensional subspace from CNR. The columns in Q span exactly the same subspace and represent an orthogonal basis. They are determined successively from µ = NT to µ = 1, as described by the pseudocode in Figure 5.47.  First, the matrices L and Q are initialised with the all-zero matrix and the channel matrix H respectively. In the ﬁrst step, the vector qNT is chosen such that it points in the direction of hNT  remember the initialisation . After its normalisation to unit length by  =  cid:30 hNT   cid:30  ⇒ qNT  = hNT  LNT,NT ,  LNT,NT  the projections of all remaining vectors qν<NT onto qNT are subtracted from qν<NT. The resulting vectors  qν − LNT,νqNT  point in directions that are perpendicular to qNT and build the basis for the next step. Con- tinuing with qNT−1, the procedure is repeated down to q1. The projection and subtraction ensure that the remaining vectors q1 to qµ are perpendicular to the hyperplane spanned by the already ﬁxed vectors qµ+1 to qNT. Hence, the columns of Q are mutually orthogonal. After NT steps, Q and L are totally determined.  Successive Interference Cancellation  SIC   Inserting the QL decomposition into our system model yields  r = QLx + n   5.108    280  SPACE–TIME CODES  Modiﬁed Gram–Schmidt algorithm    Columns qµ of Q are determined from right to left.    Elements of triangular matrix L are determined from lower right corner to  upper left corner  Step  Task  Initialisation: L = 0, Q = H for µ = NT, . . . , 1  set diagonal element Lµ,µ =  cid:30 qµ cid:30  normalise qµ = qµ Lµ,µ to unit length for ν = 1, . . . , µ − 1 · qν calculate projections Lµ,ν = qH qν = qν − Lµ,ν · qµ  µ   1   2   3   4   5   6   7   8   9   end  end  Figure 5.47: Pseudocode of modiﬁed Gram–Schmidt algorithm for QL decomposition of  channel matrix H  Golub and van Loan, 1996   The advantage of this representation becomes obvious when it is multiplied from the left- hand side by QH. On account of the to orthogonal columns in Q, QHQ = INT holds and the multiplication results in  y = QH · r = Lx + QHn =   5.109     L1,1  L2,1 ...  0 L2,2 ...  0  . . . ··· LNT,NT  LNT,1 LNT,2    · x + ˜n  The modiﬁed noise vector ˜n still represents white Gaussian noise because Q consists of orthogonal columns. However, the most important property is the triangular structure of L. Multiplication with QH suppresses the interference partly, i.e. x1 is totally free of interference, x2 only suffers from x1, x3 from x1 and x2 and so on. This property allows a successive detection strategy as depicted in Figure 5.48. It starts with the ﬁrst layer, whose received sample of which y1 = L1,1x1 + ˜n1 can be characterised the signal-to-noise ratio γ1 = L2  cid:12  hard decision Q ·  delivers the estimate  1,1Es N0. After appropriate scaling, a   cid:11   ˆs1 = Q  −1 1,1  · y1  L   5.110    SPACE–TIME CODES  Successive interference cancellation  L2,1  1 L2,2  Q ·   L3,1  LNT,1  L3,2  LNT,2  Q ·   y1  1 L1,1 y2  yNT  281  ˆs1  ˆs2  LNT,NT−1  ˆsNT  Q ·   1 L4,4  Figure 5.48: Illustration of interference cancellation after multiplying r with QH.  Reproduced by permission of John Wiley & Sons, Ltd  The obtained estimate can be used to remove the interference in the second layer which is then only perturbed by noise. Generally ,the µth estimate is obtained by   cid:8   ˆsµ = Q  1  Lµ,µ  6 cid:10   Lµ,ν · ˆsν  5  yµ − µ−1 cid:1   ·  ν=1   5.111   Supposing that previous decisions have been correct, the signal-to-noise ratio amounts to γµ = L2 µ,µEs N0. This entire procedure consists of the QL decomposition and a subse- quent successive interference cancellation and is therefore termed ‘QL-SIC’. Obviously, the matrix inversions of the original BLAST detection have been circumvented, and only a single QL decomposition has to be performed.  Optimum Post-Sorting Algorithm  Although the QL-SIC approach described above is an efﬁcient way to suppress and cancel interference, it does not take into account the risk of error propagation. So far, the algorithm has simply started with the column on the right-hand side of H and continued to the left- hand side. Hence, the corresponding signal-to-noise ratios that each layer experiences are ordered randomly, and it might happen that the ﬁrst layer to be detected is the worst one. Intuitively, we would like to start with the layer having the largest SNR. Assuming perfect interference cancellation, the layer-speciﬁc SNRs should decrease monotonically from the ﬁrst to the last layer.   282  SPACE–TIME CODES  From the last subsection describing the ZF-BLAST algorithm we know that the most reliable estimate is obtained for that layer with the smallest noise ampliﬁcation. Interlayer interference does not matter because it is perfectly suppressed. This layer corresponds to the smallest diagonal element of the error covariance matrix deﬁned in Equation  5.104 . Applying the QL decomposition, the error covariance matrix becomes −1L  ZFWZF = σ 2N · cid:2    cid:3 −1 = σ 2N · L   cid:21 ZF = σ 2N · WH  −H .  HHH  Obviously, the smallest diagonal element of  cid:21 ZF corresponds to the smallest row norm of L−1. Therefore, we have to exchange the rows of L−1 such that their row norms increase from top to bottom. Unfortunately, exchanging rows in L or L−1 destroys its triangular structure. A solution to this dilemma is presented elsewhere  Hassibi, 2000  and is termed the Post-Sorting Algorithm  PSA . After the conventional unsorted QL decomposition of H as described above, the inverse of L−1 has to be determined. According to Figure 5.49, the row of L−1 with the smallest norm is moved to the top of the matrix. This step can be described mathematically by the permutation matrix    0 0 1 0  .  0 1 0 0 1 0 0 0 0 0 0 1  P1 =  Since the ﬁrst row should consist of only one non-zero element, Householder reﬂections or Givens rotations  Golub and van Loan, 1996  can be used to retrieve the triangular shape again. They are brieﬂy described in Appendix B. In this book, we conﬁne ourselves to Householder reﬂections denoted by unitary matrices  cid:22 µ. The multiplication of P1L−1 with  cid:22 1 forces all elements of the ﬁrst rows except the ﬁrst one to zero without changing the row norm. Hence, the norm is now concentrated in a single non-zero element. Now, the ﬁrst row of the intermediate matrix P1L−1 cid:22 1 already has its ﬁnal shape. Assuming a correct decision of the ﬁrst layer in the Successive Interference Cancellation  SIC  procedure, x1 does not inﬂuence other layers any more. Therefore, the next recursion is restricted to a 3 × 3 submatrix corresponding to the remaining three layers. Although the row norms to be compared are only taken from this submatrix, permutation and Householder matrices are constructed for the original size. We obtain the permutation matrix    1 0 0 0  0 0 0 1 0 1 0 0 0 0 1 0   and the Householder matrix  cid:22 2. With the same argumentation as above, the relevant matrix is reduced again to a 2 × 2 matrix for which we obtain  1 0 0 0  P2 =  P3 =  0 1 0 0 0 0 0 1 0 0 1 0  and  cid:22 3. The optimised inverse matrix has the form  −1 opt  L  = PNT−1 ··· P1L  −1 ·  cid:22 1 ···  cid:22 NT−1   5.112    SPACE–TIME CODES  283  Optimum post-sorting algorithm  L−1  P1L−1 cid:22 1  P2P1L−1 cid:22 1 cid:22 2  P1L−1  P2P1L−1 cid:22 1  P3P2P1L−1 cid:22 1 cid:22 2  P1L−1 cid:22 1  P2P1L−1 cid:22 1 cid:22 2  P3P2P1L−1 cid:22 1 cid:22 2 cid:22 3  Figure 5.49: Illustration of the post-sorting algorithm  white squares indicate zeros, light-grey squares indicate non-zero elements, dark-grey squares indicate a row with a minimum norm   K¨uhn, 2006 . Reproduced by permission of John Wiley & Sons, Ltd  Since all permutation and Householder matrices are unitary with PPH = PHP = I, we obtain the ﬁnal triangular matrix  Lopt =  cid:22 H  ···  cid:22 H  · L · PH  ··· PH   5.113  At most NT − 1, permutations and Householder reﬂections have to be carried out. The optimised triangular matrix results in the QL decomposition of a modiﬁed channel matrix  1  1  NT−1  NT−1  H = QL  ⇒  Hopt = H · PH  1  ··· PH  NT−1  = QoptLopt  with  Qopt = Q ·  cid:22 1 ···  cid:22 NT−1  Applying this result to the received vector r delivers  r = H · x + n = Hopt · xopt + n = H · PH  ··· PH  1  NT−1  · xopt + n   5.114    5.115    5.116    284  SPACE–TIME CODES  It follows that the optimised vector xopt equals ··· PH  x = PH  1  · xopt  NT−1   5.117   Hence, we can conclude that a QL decomposition of H with a subsequent post-sorting algo- rithm delivers two matrices Qopt and Lopt with which successive interference cancellation can be performed. The estimated vector ˆxopt has then to be reordered by Equation  5.117  in order to get the estimate ˆx.  MMSE-QL Decomposition  From Subsection 5.5.4 we saw that the linear MMSE detector is obtained by applying the zero-forcing solution to the extended channel matrix   cid:6    cid:7   H =  H  σN σX INT  .  Therefore, it is self-evident to decompose H instead of H in order to obtain a QL decom- position in the MMSE sense  B¨ohnke et al., 2003; W¨ubben et al., 2003 . The basic steps  MMSE extension of QL decomposition    QL decomposition of extended channel matrix   cid:6    cid:7   H =  H  σN  σX · INT  = Q · L =   cid:6    cid:7   Q1 Q2   cid:6    cid:7  · L · L  · L =  Q1 Q2   5.118     Error covariance matrix   cid:21 MMSE =  cid:21 ZF  = σ  HHH  2N · L  −1L  −H   5.119   2N · cid:2    cid:3 −1 = σ    Inverse of L as byproduct obtained    Received vector after ﬁltering with QH  ⇒  cid:6    cid:7   · L  σN σX  · INT = Q2 y = QH · r = cid:2   = L · x − σN σX   cid:3  · QH 2 2 x + QH 1 n  QH 1 · QH  L  −1 = σX σN  · Q2 ,   5.120   r  0NT×1  = QH  · Hx + QH  · n  1  1   5.121a    5.121b   Figure 5.50: Summary of MMSE extension of QL decomposition   SPACE–TIME CODES  285  are summarised in Figure 5.50. The QL decomposition presented in Equation  5.118  now delivers a unitary  NR + NT  × NT matrix Q and a lower triangular NT × NT matrix L. Certainly, Q and L are not identical with Q and L, e.g. Q has NT more rows than the original matrix Q. It can be split into the submatrices Q1 and Q2 such that Q1 has the same size as Q for the ZF approach.  Similarly to the V-BLAST algorithm, the order of detection has to be determined with respect to the error covariance matrix given in Equation  5.119 . Please note that underlined vectors and matrices are associated with the extended system model. Since the only difference between ZF and MMSE is the use of H instead of H, it is not surprising that the row norm of the inverse extended triangular matrix L determines the optimum sorting. However, in contrast to the ZF case, L need not be explicitly inverted. Looking at Equation  5.118 , we recognise that the lower part of the equation delivers the relation given −1 is gained as a byproduct of the initial QL decomposition in Equation  5.120 . Hence, L and the optimum post-sorting algorithm exploits the row norms of Q2. This compensates for the higher computational costs due to QL decomposing a larger matrix H.  Since the MMSE approach represents a compromise between matched and zero-forcing ﬁlters, residual interference remains in its outputs. This effect will now be considered in more detail. Using the extended channel matrix requires modiﬁcation of the received vector r as well. An appropriate way is to append NT zeros. The detection starts by multiplying r with QH, yielding the result in Equation  5.121a . In fact, only a multiplication with QH 1 , is performed, having the same complexity as ﬁltering with QH for the zero-forcing solution. However, in contrast to Q and Q, Q1 does not contain orthogonal columns because it consists of only the ﬁrst NR rows of Q. Hence, the noise term QH 1 n is coloured, i.e. its samples are correlated and a symbol-by-symbol detection as considered here is suboptimum. Furthermore, the product Q1Q does not equal the identity matrix any more. In order to illuminate the consequence, we will take a deeper look at the product of the extended matrices Q and H. Inserting their speciﬁc structures given in Equation  5.118  results in  != L ⇔ QH  1 H = L − σN σX  · QH  QHH = QH  1 H + QH  2  · σN σX  INT   5.122  · H in Equation  5.121a  delivers Equation  5.121b . We observe the Replacing the term QH 1 desired term Lx, the coloured noise contribution QH 1 n and a third term in the middle also depending on x. It is this term that represents the residual interference after ﬁltering with QH 1 . If the noise power σ 2N becomes small, the problem of noise ampliﬁcation becomes less severe and the ﬁlter can concentrate on the interference suppression. For σ 2N → 0, the MMSE solution tends to the zero-forcing solution and no interference remains in the system.  2  Sorted QL Decomposition  We saw from the previous discussion that the order of detection is crucial in successive interference cancellation schemes. However, reordering the layers by the explained post- sorting algorithm requires rather large computational costs owing to several permutations and Householder reﬂections. They can be omitted by realising that the QL decomposition is performed column by column of H. Therefore, it should be possible to change the order   286  SPACE–TIME CODES  of columns during the modiﬁed Gram–Schmidt procedure in order directly to obtain the optimum detection order without post-processing.  Unfortunately, a computational conﬂict arises if we pursue this approach. The QL decomposition starts with the rightmost column of H; the ﬁrst element to be determined is LNT,NT in the lower right corner. By contrast, the detection starts with L1,1 in the upper left corner, since the corresponding layer does not suffer from interference. Remember that the risk of error propagation is minimised if the diagonal elements decrease with each iteration step, i.e. L1,1 ≥ L2,2 ≥ ··· ≥ LNT,NT. Therefore, the QL decomposition should be performed such that L1,1 is the largest among all diagonal elements. However, it is the last element to be determined.  A way out of this conﬂict is found by exploiting the property of triangular matrices that their determinants equal the product of their diagonal elements. Moreover, the determinant is invariant with respect to row or column permutations. Therefore, the following strategy  W¨ubben et al., 2001  can be pursued. If the diagonal elements are determined in ascending order, i.e. starting with the smallest for the lower right corner ﬁrst, the last one for the upper left corner should be the desired largest value because the total product is constant. The corresponding procedure is sketched in Figure 5.51. Equivalently to the modi- ﬁed Gram–Schmidt algorithm in Figure 5.47, the algorithm is initialised with Q = H and L = 0. Next, step 3 determines the column with the smallest norm and exchanges it with the rightmost unprocessed vector. After normalising its length to unity and, therefore, determining the corresponding diagonal element Lµ,µ  steps 5 an 6 , the projections of the remaining columns onto the new vector qµ provide the off-diagonal elements Lµ,ν.  Sorted QL decomposition  Task  Initialisation: L = 0, Q = H for µ = NT, . . . , 1  Step   1   2   3    4   5   6   7   8   9   10   11   end  end   cid:30 qν cid:30 2  search for minimum norm among remaining columns in Q kµ = argmin ν=1, ..., µ exchange columns µ and kµ in Q, and determine Pµ set diagonal element Lµ,µ =  cid:30 qµ cid:30  normalise qµ = qµ Lµ,µ to unit length for ν = 1, . . . , µ − 1 · qν calculate projections Lµ,ν = qH qν = qν − Lµ,ν · qµ  µ  Figure 5.51: Pseudocode for sorted QL decomposition  K¨uhn, 2006    SPACE–TIME CODES  287  At the end of the procedure, we obtain a orthonormal matrix Q, a triangular matrix L as well as a set of permutation matrices Pµ with 1 ≤ µ ≤ NT. It has to be mentioned that the computational overheads compared with the conventional Gram–Schmidt procedure are negligible.  At this point, we have no wish to conceal that the strategy does not always achieve the optimum succession as is the case with the post-sorting algorithm. The algorithm especially fails in situations where two column vectors qµ and qν have large norms but point in similar directions. Owing to their large norms, these vectors are among the latest columns to be processed. When the smaller one has been chosen as the next vector to be processed, its projection to the other vector is rather large on account of their similar directions. Hence, the remaining orthogonal component may become small, leading to a very small diagonal element in the upper left corner of L.  Fortunately, simulations demonstrate that those pathological events occur very rarely, so that the SQLD performs nearly as well as the optimum post-sorting algorithm. Moreover, it is possible to concatenate the sorted QL decomposition and the post-sorting algorithm. This combination always ensures the best detection order. The supplemental costs are rather small because only very few additional permutations are required owing to the presorting of the SQLD. Hence, this algorithm is suited to perform the QL decomposition and to provide a close-to-optimal order of detection.  5.5.6 Performance of Multilayer Detection Schemes  This section analyses the performance of the different detection schemes described above. The comparison is drawn for a multiple-antenna system with NT = 4 transmit and NR = 4 receive antennas. The channel matrix H consists of independent identically distributed complex channel coefﬁcients hµ,ν whose real and imaginary parts are independent and Gaussian distributed with zero mean and variance 1 2. Moreover, the channel is constant during one coded frame and H is assumed to be perfectly known to the receiver, while the transmitter has no channel knowledge at all. QPSK was chosen as a modulation scheme. It has to be emphasised that all derived techniques work as well with other modulation schemes. However, the computational costs increase exponentially with M in the case of the optimal APP detector, while the complexity of the QL decomposition based approach is independent of the modulation alphabet’s size.  Turbo Multilayer Detection  We start with the turbo detection approach from Subsection 5.5.2. For the simulations, we used a simple half-rate convolutional code with memory m = 2 and generator polynomials g1 D  = 1 + D2 and g2 D  = 1 + D + D2. In all cases, a max-log MAP decoder was deployed. Figure 5.52 shows the obtained results, where solid lines correspond to perfectly interleaved channels, i.e. each transmitted vector x[k] experiences a different channel matrix H[k]. Dashed lines indicate a block fading channel with only a single channel matrix for the entire coded frame. In the left-hand diagram, the bit error rates of the max-log MAP solution are depicted versus the signal-to-noise ratio Eb N0. We observe that the performance increases from iteration to iteration. However, the largest gains are achieved in the ﬁrst iterations, while additional runs lead to only minor improvements. This coincides with the observations made in the context of turbo decoding in Chapter 4. Moreover, the   288  SPACE–TIME CODES  Performance of Turbo-BLAST detection   a  Max-log MAP detector   b  5th iteration  MAP Max MAP cid:358 Exp Max cid:358 Exp  → R E B  100  10 cid:358 1  10 cid:358 2  10 cid:358 3  10 cid:358 4  10 cid:358 5 0  it. 1 it. 2 it. 3 it. 4 it. 5  → R E B  100  10 cid:358 1  10 cid:358 2  10 cid:358 3  10 cid:358 4  10 cid:358 5 0  2  6  4 8 Eb N0 in dB →  10  12  2  6  4 8 Eb N0 in dB →  10  12  Figure 5.52: Error rate performance of V-BLAST system with NT = NR = 4, a  convolutional code with g1 D  = 1 + D2 and g2 D  = 1 + D + D2 and turbo detection   solid lines: perfectly interleaved channel; dashed lines: block fading channel   perfectly interleaved channel leads to a much better performance because the FEC decoders exploit temporal diversity and, therefore, provide more reliable estimates of interfering symbols. Hence, the total diversity degree amounts to NTNRdmin instead of NTNR.  The right-hand diagram illustrates the performance of different algorithms for the mul- tilayer detector. Obviously, MAP and max-log MAP detectors show a similar behavior. The performance loss of the max-log MAP approximation compared with the optimal MAP solution is very small and less than 0.2 dB. However, the algorithms incorporating expected interfering symbols  ‘MAP-EXP’ and ‘Max-Exp’  perform worse. The loss com- pared with MAP and max-log MAP amounts to 1 dB for the block fading channel and to nearly 2 dB for the perfectly interleaved channel. It has to be mentioned that the expected interfering symbols ¯sµ have been obtained by applying the optimum MAP or max-log MAP solutions only in the ﬁrst iteration owing to the lack of a-priori information. In all subsequent iteration steps, expected symbols have been used, reducing the computational costs markedly.  QL-Based Successive Interference Cancellation  Next, we will focus on the QL decomposition based approaches. In contrast to the turbo detector discussed before, we consider now an uncoded system. The reason is that we compare QL-SIC strategies with the optimum maximum likelihood detection. Besides the brute-force approach, which considers all possible hypotheses, Maximum Likelihood   SPACE–TIME CODES  289  Decoding  MLD  can also be accomplished by means of sphere detection  Agrell et al., 2002; Fincke and Pohst, 1985; Schnoor and Euchner, 1994  with lower computational costs. Since these detection algorithms originally deliver hard-decision outputs, a subsequent chan- nel decoder would suffer from the missing reliability information and a direct comparison with the turbo detector would be unfair. Although there exist soft-output extensions for sphere detectors, their derivation is outside the scope of this book.  Figure 5.53 shows the results for different detection schemes, diagram  a  for the ZF solution and diagram  b  for the MMSE solution. Performing only a linear detection accord- ing to Section 5.5.3 has obviously the worst performance. As expected, the MMSE ﬁlter performs slightly better than the ZF approach because the background noise is not ampliﬁed. An additional QL-based successive interference cancellation without appropriate sorting  ‘QLD’  already leads to improvements of roughly 3 dB for the ZF ﬁlter and 2 dB for the MMSE ﬁlter. However, the loss compared with the optimum maximum likelihood detector is still very high and amounts to more than 10 dB. The reason is that the average error rate is dominated by the layer detected ﬁrst owing to error propagation. If this layer has incidentally a low SNR, its error rate, and hence the interference in subsequent layers, becomes quite high. Consequently, the average error rate is rather high. This effect will be illustrated in more detail later on and holds for both ZF and MMSE solutions.  Optimal post-sorting with the algorithm proposed in Figure 5.49 leads to remarkable improvements at least for the MMSE ﬁlter. It gains 8 dB compared with the unsorted detection at an error rate of 2 · 10 −3 and shows a loss of only 2 dB to the maximum likelihood detector. This behaviour emphasises the severe inﬂuence of error propagation.  Comparison of Turbo-BLAST detection algorithms   a  ZF detection   b  MMSE detection  → R E B  100  10 cid:358 1  10 cid:358 2  10 cid:358 3  10 cid:358 4  10 cid:358 5 0  100  10 cid:358 1  10 cid:358 2  10 cid:358 3  10 cid:358 4  → R E B  20  10 cid:358 5 0  linear QLD SQLD SQLD+PSA MLD 5 15 Eb N0 in dB →  10  linear QLD SQLD SQLD+PSA MLD 5 15 Eb N0 in dB →  10  20  Figure 5.53: Error rate performance of an uncoded V-BLAST system with  NT = NR = 4, QPSK and different detection methods  K¨uhn, 2006    290  SPACE–TIME CODES  Regarding the ZF ﬁlter, the ampliﬁcation of the background noise is too high to approach the MLD performance. The suboptimum SQLD with its negligible complexity overheads comes very close to the optimum sorting algorithm. Only at high signal-to-noise ratios does the loss increase for the MMSE solution.  Error Propagation and Diversity  The effects of error propagation and layer-speciﬁc diversity gains will be further examined in Figure 5.54 for a system with NT = NR = 4 and QPSK. Diagram  a  showing layer- speciﬁc error rates for the turbo detection scheme shows that all layers have the same error probability and error propagation is not an issue. Moreover, the slope of the curves indicates that the full diversity degree is obtained for all layers. This holds for both the perfectly interleaved and the block fading channel, although the overall performance of the former is much better.  By Contrast, the zero-forcing SQLD with successive interference cancellation shows a diverging behavior as illustrated in diagram  b . The solid lines represent the real world where error propagation from one layer to the others occurs. Hence, all error rates are dominated by the layer detected ﬁrst. If the ﬁrst detection generates many errors, subsequent layers cannot be reliably separated. Error propagation could be avoided by a genie-aided detector which cancels the interference always, perfectly although the layer-wise detection  Per-Layer error rate analysis   a  turbo detection   b  ZF-QL based SIC  → R E B  100  10 cid:358 1  10 cid:358 2  10 cid:358 3  10 cid:358 4  10 cid:358 5 0  → R E B  100  10 cid:358 1  10 cid:358 2  10 cid:358 3  10 cid:358 4  10 cid:358 5 0  layer 1 layer 2 layer 3 layer 4  2  6  4 8 Eb N0 in dB →  10  12  10  5 15 Eb N0 in dB →  20  Figure 5.54: Per-layer error rate analysis of  a  turbo-decoded system  solid lines:  perfectly interleaved channel; dashed lines: block fading channel  and  b   ZF-SQLD-based interference cancellation for NT = NR = 4 antennas  dashed-dotted line:  genie-aided detector    SPACE–TIME CODES  291  may be erroneous. This detector is analysed by the dashed-dotted lines. While the ﬁrst detected layer naturally has the same error rate as in the real-world scenario, substantial improvements are achieved with each perfect cancellation step. Besides the absence of error propagation, the main observation is that the slope of the curves varies. Layer 4 comes very close to the performance of the maximum likelihood detection  bold solid line . Since the slope of the error rate curves depends on the diversity gain, we can conclude that the MLD detector and the last detected layer have the same diversity gain. In our example, the diversity degree amounts to NR = 4.  However, all other layers seem to have a lower diversity degree, and the ﬁrst layer has no diversity at all. This effect is explained by the zero-forcing ﬁlter Q. In the ﬁrst detection step, three interfering layers have to be suppressed using four receive antennas. Hence, the null space of the interference contains only NR −  NT − 1  = 1 ‘dimension’ and there is no degree of freedom left. Assuming perfect interference cancellation as for the genie-aided detector, the linear ﬁlter has to suppress only NT − 2 = 2 interferers for the next layer. With NR = 4 receive antennas, there is one more degree of freedom, and diversity of order NR −  NT − 2  = 2 is obtained. Continuing this thought, we obtain the full diversity degree NR for the last layer because no interference has to be suppressed any more. Distinguishing perfectly interleaved and block fading channels makes no sense for uncoded transmissions because the detection is performed symbol by symbol and temporal diversity cannot be exploited.  5.5.7 Uniﬁed Description by Linear Dispersion Codes  Comparing orthogonal space–time block codes and multilayer transmission schemes such as the BLAST system, we recognise that they have been introduced for totally different purposes. While orthogonal space–time codes have been designed to achieve the highest possible diversity gain, they generally suffer from a rate loss due to the orthogonality constraint. By Contrast, BLAST-like systems aim to multiply the data rate without looking at the diversity degree per layer. Hence, one may receive the impression that both design goals exclude each other.  However, it is possible to achieve a trade-off between diversity and multiplexing gains  Heath and Paulraj, 2002 . In order to reach this target, a uniﬁed description of both techniques would be helpful and can be obtained by Linear Dispersion  LD  codes  Hassibi and Hochwald, 2000, 2001, 2002 . As the name suggests, the information is distributed or spread in several dimensions similarly to the physical phenomenon dispersion. In this context, we consider the dimensions space and time. Obviously, the dimension frequency can be added as well.  Taking into account that space–time code words are generally made up of K symbols ∗ µ, the matrix X describing a linear dispersion  sµ and their conjugate complex counterparts s code word can be constructed  B1,µ · sµ + B2,µ · s  ∗ µ   5.123  The NT × L dispersion matrices B1,µ and B2,µ distribute the information into NT spatial and L temporal directions. Hence, the resulting code word has a length of L time instants. In order to illustrate this general description, we apply Equation  5.123  to Alamouti’s space–time code.  µ=1  X = K cid:1    292  SPACE–TIME CODES  LD Description of Alamouti’s Scheme We remember from Section 5.4.1 that a code word X2 consists of K = 2 symbols s1 and s2 that are transmitted over two antennas within two time slots. The matrix has the form  Comparing the last equation with Equation  5.123 , the four matrices describing the linear dispersion code are   cid:7    cid:6  s1 −s ∗ ∗ 2 1  s2  s  .  ·  X2 = 1√ 2  cid:7   cid:6  , B1,2 = 1√  cid:6   cid:7  1 0 0 0 2 0 −1 , B2,2 = 1√ 0 0 2  ·  ·  ·   cid:6   ·   cid:7   0 0 1 0   cid:6   ,   cid:7   0 0 0 1  .  B1,1 = 1√ 2 B2,1 = 1√ 2  For different space–time block codes, an equivalent description is obtained in the same way. Relaxing the orthogonality constraint, one can design arbitrary codes with speciﬁc properties. However, the beneﬁt of low decoding complexity gets lost in this case. +  LD Description of Multilayer Transmissions  The next step will be to ﬁnd an LD description for multilayer transmission schemes. As we know from the last section, BLAST-like systems transmit NT independent symbols at each time instant. Hence, the code word length equals L = 1 and the dispersion matrices reduce to column vectors. Moreover, no complex-values symbols are used, so that the vectors B2,µ contain only zeros. Finally, each symbol is transmitted over a single antenna, resulting in vectors B1,µ that contain only a single 1 at the µth row. For the special case of NT = 4 transmit antennas, we obtain    1  , B1,2 =   0  , B2,2 =  0 0 0    0  , B1,3 =   0  , B2,3 =  1 0 0   0  0  0 1 0    , B1,4 =   , B2,4 =  0 0 0  0 0 0  B1,1 =  B2,1 =  0 0 0   0  0  0 0 1    ,   .  0 0 0  Detection of Linear Dispersion Codes  Unless linear dispersion codes can be reduced to special cases such as orthogonal space– time block codes, the detection requires the multilayer detection philosophies introduced in this section. In order to be able to apply the discussed algorithms such as QL-based interference cancellation or multilayer turbo detection, we need an appropriate system model. Using the transmitted code word deﬁned in Equation  5.123 , the received word has the form  B1,µ · sµ + B2,µ · s  ∗ µ  + N   5.124   R = HX + N = H · K cid:1   µ=1   SPACE–TIME CODES  293  and consists of NR rows according to the number of receive antennas and L columns denoting the duration of a space–time code word. The operator   cid:4  cid:2   vec{A} = vec  ··· an  a1   cid:3  cid:5  =      a1  ... an  stacks the columns of matrix A on top of each other. Applying it to B1,µ and B2,µ transforms the sum in Equation  5.124  into  K cid:1    cid:4   vec  B1,µ   cid:5  · sµ + vec   cid:4   B2,µ   cid:5  · s  ∗ µ  = B1 · s + B2 · s  ∗   5.125   µ=1 The vectors s and s∗ ∗ µ respect- ively. Equivalently, the matrices Bν are made up of the vectors vec{Bν,µ}. Both terms of the sum can be merged into the expression  comprise all data symbols sµ and their complex conjugates s  ∗ = cid:2    cid:3 · =   cid:7    cid:6  s s∗  B1 · s + B2 · s  B1 B2  = B · s .  Owing to the arrangement of the time axis along a single column, the channel matrix H has to be enlarged by repeating it L times. This can be accomplished by the Kronecker product which is generally deﬁned as     A1,1 · B ··· A1,N · B  . AM,1 · B ··· AM,N · B  ...  ...  A ⊗ B =  Finally, the application of the vec-operator to the matrices R and N results in  r = vec{R} =  IL ⊗ H  · B · s + vec{N} = Hld · s + vec{N}   5.126   We have derived an equivalent system description that resembles the structure of a BLAST system. Hence, the same detection algorithms can be applied for LD codes as well. How- ever, we have to be aware that s may not only contain independent data symbols but conjugate complex versions of them as well. In that case, an appropriate combination of ∗ the estimates for sµ and s µ is required, and an independent detection suboptimum. This effect can be circumvented by using a real-valued description  K¨uhn, 2006 .  Optimising Linear Dispersion Codes  Optimising linear dispersion codes can be done with respect to different goals. A maximi- sation of the ergodic capacity according to Section 5.3.1 would result in  B = argmax˜B  log2 det  ILNR  + σ 2N σ 2X  ·  IL ⊗ H   ˜B˜BH   IL ⊗ H    5.127a    cid:7    cid:6    294  subject to a power constraint, e.g.    K cid:1   µ=1  tr  B1,µ B1,µ   H + B2,µ B2,µ   H   5.127b   SPACE–TIME CODES    = K .  Such an optimisation was performed elsewhere  Hassibi and Hochwald, 2000, 2001, 2002 . A different approach also considering the error rate performance was taken by  Heath and Paulraj, 2002 . Generally, the obtained LD codes do not solely pursue diversity or multiplexing gains but can achieve a trade-off between the two aspects.  5.6 Summary  This chapter introduced some examples for space–time signal processing. Based on the description of the MIMO channel and some evaluation criteria, the principle of orthogonal space–time block codes was explained. All presented coding schemes have achieved the full diversity degree, and a simple linear processing at the receiver was sufﬁcient for data detection. However, Alamouti’s scheme with NT = 2 transmit antennas is the only code with rate R = 1. Keeping the orthogonality constraint for more transmit antennas directly leads to a loss of spectral efﬁciency that has to be compensated for by choosing modulation schemes with M > 2. At high signal-to-noise ratios, the diversity effect is dominating and more transmit antennas are beneﬁcial. By contrast, only two transmit antennas and a more robust modulation scheme is an appropriate choice at medium and low SNRs.  In contrast to diversity achieving space–time codes, spatial multiplexing increases the data rate. In the absence of channel knowledge at the transmitter, the main complexity of this approach has to be spent at the receiver. For coded systems, we discussed turbo detec- tors whose structure is similar to that of the turbo decoder explained in Chapter 4. Since the complexity grows exponentially with the number of layers  transmit antennas  and the modulation alphabet size, it becomes quickly infeasible for a practical implementation. A suitable detection strategy has been proposed that is based on the QL decomposition of the channel matrix H. It consists of a linear interference suppression and a non-linear inter- ference cancellation step. Using the MMSE solution and an appropriate sorting algorithm, this approach performs almost as well to the maximum likelihood solution. Moreover, its complexity grows only polynomially with NT and is independent of the alphabet size S. Finally, we showed that space–time block codes and spatial multiplexing can be uniquely described by linear dispersion codes. They also offer a way to obtain a trade-off between diversity and multiplexing gains.  As the discussed MIMO techniques offer the potential of high spectral efﬁciencies, they are also discussed for the standardisation of UMTS Terrestrial Radio Access  UTRA  exten- sions. In the context of HSDPA, spatial multiplexing and space–time coding concepts, as well as the combination of the two, are considered. A multitude of proposals from different companies is currently being evaluated in actual standardisation bodies of 3GPP  3GPP, 2007 . Furthermore, the upcoming standards Worldwide Interoperability for Microwave Access  WIMAX   IEEE, 2004  and IEEE 802.11n will also incorporate space–time coding concepts.   A  Algebraic Structures  In this appendix we will give a brief overview of those algebraic basics that we need throughout the book. We start with the deﬁnition of some algebraic structures  Lin and Costello, 2004; McEliece, 1987; Neubauer, 2006b .  A.1 Groups, Rings and Finite Fields  The most important algebraic structures in the context of algebraic coding theory are groups, rings and ﬁnite ﬁelds.  A.1.1 Groups A non-empty set G together with a binary operation ‘·’ is called a group if for all elements a, b, c ∈ G the following properties hold:  G1   a · b ∈ G, a ·  b · c  =  a · b  · c, ∃e ∈ G : ∀a ∈ G : a · e = e · a = a, ∀a ∈ G : ∃a   cid:1  ∈ G : a · a   cid:1  = e.   G2    G3    G4   The element e is the identity element, and a the commutativity property   cid:1    G5   a · b = b · a  is the inverse element of a. If, additionally,  holds, then the group is called commutative. The number of elements of a group G is given by the order ord G . If the number of elements is ﬁnite ord G  < ∞, we have a ﬁnite group. A cyclic group is a group where all elements γ ∈ G are obtained from the powers αi of one element α ∈ G. The powers are deﬁned according to  0 = e, α  1 = α, α  2 = α · α, . . .  α  Coding Theory – Algorithms, Architectures, and Applications Andr´e Neubauer, J¨urgen Freudenberger, Volker K¨uhn  cid:148  2007 John Wiley & Sons, Ltd   296  ALGEBRAIC STRUCTURES This particular element α is the primitive element of the group G. All elements γ ∈ G  of a ﬁnite group G of order ord G  fulﬁl the condition  γ ord G  = e.  A.1.2 Rings If we deﬁne two operations – multiplication ‘·’ and addition ‘+’ – over the set S, then S is called a ring if the following properties are fulﬁlled for all elements a, b, c ∈ S:  R1   a + b ∈ S, a +  b + c  =  a + b  + c, ∃0 ∈ S : ∀a ∈ S : a + 0 = 0 + a = a, ∀a ∈ S : ∃ − a ∈ S : a +  −a  = 0, a + b = b + a, a · b ∈ S, a ·  b · c  =  a · b  · c, a ·  b + c  = a · b + a · c.   R2    R3    R4    R5    R6    R7    R8   The element 0 is the zero element. The inverse element of a with respect to addition is given by −a. If, additionally, the following two properties  R9   ∃1 ∈ S : ∀a ∈ S : a · 1 = 1 · a = a, a · b = b · a   R10   are met, then the ring is called a commutative ring with identity. The element 1 denotes the identity element. In the following, we will use the common notation a b for the multipli- cation a · b. The set of integers Z with ordinary addition ‘+’ and multiplication ‘·’ forms a commutative ring with identity. Another example of a commutative ring with identity is the residue class ring deﬁned in Figure A.1. An important operation for rings is the division with remainder. As a well-known example, we will consider the set of integers Z. For two integers a, b ∈ Z there exist numbers q, r ∈ Z such that with quotient q, remainder r and 0 ≤ r < b. Written with congruences, this reads  a = q b + r  a ≡ r mod b.  If the remainder r is zero according to a ≡ 0 modulo b, the number b divides a which we will write as ba. b is called a divisor of a. A prime number p > 1 is deﬁned as a number that is only divisible by 1 and itself. The greatest common divisor gcd a, b  of two numbers a and b can be calculated with the help of Euclid’s algorithm. This algorithm is based on the observation that gcd a, b  = gcd a, b − c a  for each integer c ∈ Z. Therefore, the greatest common divisor gcd a, b    ALGEBRAIC STRUCTURES  Residue class ring  297    As an example we consider the set of integers  Zm = {0, 1, 2, . . . , m − 1}    This set with addition and multiplication modulo m fulﬁls the ring properties. Zm is called the residue class ring. In order to denote the calculation modulo m, we will call this ring Z  m .    The element a ∈ Zm has a multiplicative inverse if the greatest common  divisor gcd a, m  = 1.    If m = p is a prime number p, then Zp yields the ﬁnite ﬁeld Fp.  Figure A.1: Residue class ring Zm  can be obtained from successive subtractions. A faster implementation uses successive divisions according to the following well-known iterative algorithm. After the initialisation r−1 = a and r0 = b in each iteration we divide the remainder  ri−2 by ri−1 which yields the new remainder ri according to  Because 0 ≤ ri < ri−1, the algorithm ﬁnally converges with rn+1 = 0. The greatest common divisor gcd a, b  of a and b is then given by  ri ≡ ri−2 mod ri−1.  gcd a, b  = rn.  In detail, Euclid’s algorithm reads  with r−1 = a and r0 = b  r−1 ≡ r1 mod r0 r0 ≡ r2 mod r1 r1 ≡ r3 mod r2 r2 ≡ r4 mod r3  ...  rn−2 ≡ rn mod rn−1 rn−1 ≡ 0 mod rn  with rn+1 = 0   298  ALGEBRAIC STRUCTURES  Euclid’s algorithm for integers    Initialisation    Iterations until rn+1 = 0  r−1 = a f−1 = 1 g−1 = 0  and  and  and  r0 = b f0 = 0 g0 = 1  ri ≡ ri−2 mod ri−1 fi = fi−2 − qi fi−1 gi = gi−2 − qi gi−1    Greatest common divisor  gcd a, b  = rn = fn a + gn b   A.1   Figure A.2: Euclid’s algorithm for integers  It can be shown that each remainder ri can be written as a linear combination of a and  b according to  The numbers fi and gi can be obtained with the initialisations  and by successively calculating the iterations  ri = fi a + gi b.  f−1 = 1 g−1 = 0  and  and  f0 = 0, g0 = 1  fi = fi−2 − qi fi−1, gi = gi−2 − qi gi−1.  Euclid’s algorithm for calculating the greatest common divisor of two integers is sum- marised in Figure A.2.  A.1.3 Finite Fields  If it is possible to divide two arbitrary non-zero numbers we obtain the algebraic structure of a ﬁeld. A ﬁeld F with addition ‘+’ and multiplication ‘·’ is characterised by the following properties for all elements a, b, c ∈ F:   ALGEBRAIC STRUCTURES  299   F1    F2    F3    F4    F5    F6    F7    F8    F9    F10    F11   a + b ∈ F, a +  b + c  =  a + b  + c, ∃0 ∈ F : ∀a ∈ F : a + 0 = 0 + a = a, ∀a ∈ F : ∃ − a ∈ F : a +  −a  = 0, a + b = b + a, a · b ∈ F, a ·  b · c  =  a · b  · c, ∃1 ∈ F : ∀a ∈ F : a · 1 = 1 · a = a, ∀a ∈ F \ {0} : ∃a a · b = b · a, a ·  b + c  = a · b + a · c.  −1 ∈ F : a · a  −1 = 1,  −1 is called the multiplicative inverse if a  cid:7 = 0. If the number of elements of  The element a F is ﬁnite, i.e.  then F is called a ﬁnite ﬁeld. We will write a ﬁnite ﬁeld of cardinality q as Fq. A deep algebraic result is the ﬁnding that every ﬁnite ﬁeld Fq has a prime power of elements, i.e.  F = q,  q = pl  with the prime number p. Finite ﬁelds are also called Galois ﬁelds.  A.2 Vector Spaces  For linear block codes, which we will discuss in Chapter 2, code words are represented by n-dimensional vectors over the ﬁnite ﬁeld Fq. A vector a is deﬁned as the n-tuple  a =  a0, a1, . . . , an−1   with ai ∈ Fq. The set of all n-dimensional vectors is the n-dimensional space Fn q with q n elements. We deﬁne the vector addition of two vectors a =  a0, a1, . . . , an−1  and b =  b0, b1, . . . , bn−1  according to  a + b =  a0, a1, . . . , an−1  +  b0, b1, . . . , bn−1   =  a0 + b0, a1 + b1, . . . , an−1 + bn−1   as well as the scalar multiplication  β a = β  a0, a1, . . . , an−1   with the scalar β ∈ Fq. The set Fn two vectors a and b in Fn  =  β a0, β a1, . . . , β an−1  q is called the vector space over the ﬁnite ﬁeld Fq if for q and two scalars α and β in Fq the following properties hold:   ALGEBRAIC STRUCTURES  300   V1    V2    V3    V4    V5    V6    V7   q : ∀a ∈ Fn q : ∃ − a ∈ Fn  a +  b + c  =  a + b  + c, a + b = b + a, ∃0 ∈ Fn ∀a ∈ Fn α ·  a + b  = α · a + α · b,  α + β  · a = α · a + β · a,  α · β  · a = α ·  β · a , 1 · a = a.  q : a + 0 = a,  q : a +  −a  = 0,   V8  For a +  −b  we will also write a − b. Because of property V3 the zero vector 0 =  0, 0, . . . , 0  is always an element of the vector space Fn q.  q if the addition and the scalar multiplication of elements from B lead to elements in B, i.e. the set B is closed under addition and scalar multiplication.  q is called a subspace of Fn  A non-empty subset B ⊆ Fn  Another important concept is the concept of linear independency. A ﬁnite number of  vectors a1, a2, . . . , ak is called linearly independent if  implies  β1 · a1 + β2 · a2 + ··· + βk · ak = 0  β1 = β2 = ··· = βk = 0.  Otherwise, the vectors a1, a2, . . . , ak are said to be linearly dependent. With the help of this term we can deﬁne the dimension of a subspace. The dimension of the subspace B ⊆ Fn is equal to dim B = k if there exist k linearly independent vectors in B but k + 1 vectors are always linearly dependent.  A.3 Polynomials and Extension Fields We have already noted that ﬁnite ﬁelds Fq always have a prime power q = pl of elements. In the case of l = 1, the residue class ring Zp in Figure A.1 yields the ﬁnite ﬁeld Fp, i.e.  ∼= Zp.  Fp  We now turn to the question of how ﬁnite ﬁelds F end, we consider the set Fp[z] of all polynomials  pl with l > 1 can be constructed. To this  a z  = a0 + a1 z + a2 z  2 + ··· + an−1 zn−1  over the ﬁnite ﬁeld Zp. The degree deg a z   = n − 1 of the polynomial a z  corresponds to the highest power of z with an−1  cid:7 = 0. Since Fp[z] fulﬁls the ring properties, it is called the polynomial ring over the ﬁnite ﬁeld Fp.   ALGEBRAIC STRUCTURES  301  Euclid’s algorithm for polynomials    Initialisation    Iterations until rn+1 z  = 0  r−1 z  = a z  f−1 z  = 1 g−1 z  = 0  and  and  and  r0 z  = b z  f0 z  = 0 g0 z  = 1  ri  z  ≡ ri−2 z  mod ri−1 z  fi  z  = fi−2 z  − qi  z  fi−1 z  gi  z  = gi−2 z  − qi  z  gi−1 z     Greatest common divisor  gcd a z , b z   = rn z  = fn z  a z  + gn z  b z    A.2   Figure A.3: Euclid’s algorithm for polynomials  a z  = q z  b z  + r z   Similar to the ring Z of integers, two polynomials a z , b z  ∈ Fp[z] can be divided with remainder according to with quotient q z  and remainder r z  = 0 or 0 ≤ deg r z   < deg b z  . With congruences this can be written as If b z  ∈ Fp[z] \ {0} divides a z  according to b z  a z , then there exists a polynomial q z  ∈ Fp[z] such that a z  = q z  · b z . On account of the ring properties, Euclid’s algo- rithm can also be formulated for polynomials in Fp[z], as shown in Figure A.3.  a z  ≡ r z  mod b z .  Similarly to the deﬁnition of the residue class ring Z  m , we deﬁne the so-called factorial ring Fp[z]  m z   by carrying out the calculations on the polynomials modulo m z . This factorial ring fulﬁls the ring properties. If the polynomial  m z  = m0 + m1 z + m2 z  2 + ··· + ml zl  of degree deg m z   = l with coefﬁcients mi ∈ Fp in the ﬁnite ﬁeld Fp is irreducible, i.e. it cannot be written as the product of two polynomials of smaller degree, then  Fp[z]  m z    ∼= F  pl   ALGEBRAIC STRUCTURES  302  Finite ﬁeld F24  ring F2[z].    Let m z  be the irreducible polynomial m z  = 1 + z + z4 in the polynomial    Each element a z  = a0 + a1 z + a2 z2 + a3 z3 of  F2[z]  1 + z + z4  corresponds to one element a of the ﬁnite ﬁeld F24.  the extension ﬁeld    Addition and multiplication are carried out on the polynomials modulo m z .  Index Element  a z  ∈ F  a3  a2  24 a1  a0  a  0000 0001 0010 0011 0100 0101 0110 0111 1000 1001 1010 1011 1100 1101 1110 1111  0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1  0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1  0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1  0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1  Figure A.4: Finite ﬁeld F24. Reproduced by permission of J. Schlembach Fachverlag  yields a ﬁnite ﬁeld with pl elements. Therefore, the ﬁnite ﬁeld Fq with cardinality q = pl can be generated by an irreducible polynomial m z  of degree deg m z   = l over the ﬁnite ﬁeld Fp. This ﬁeld is the so-called extension ﬁeld of Fp. In Figure A.4 the ﬁnite ﬁeld F24 is constructed with the help of the irreducible polynomial m z  = 1 + z + z4.  In order to simplify the multiplication of elements in the ﬁnite ﬁeld F  pl , the modular polynomial m z  can be chosen appropriately. To this end, we deﬁne the root α of the irreducible polynomial m z  according to m α  = 0. Since the polynomial is irreducible over the ﬁnite ﬁeld Fp, this root is an element of the extension ﬁeld Fp[z]  m z   or equivalently α ∈ F  pl .   ALGEBRAIC STRUCTURES  The powers of α are deﬁned by  303  or equivalently in the notation of the ﬁnite ﬁeld F  pl by  α  α  0 ≡ 1 mod m α , 1 ≡ α mod m α , α2 ≡ α · α mod m α , 3 ≡ α · α · α mod m α , ...  α  α  0 = 1, α1 = α, 2 = α · α, 3 = α · α · α, ...  α  α  −∞  If these powers of α run through all pl − 1 non-zero elements of the extension ﬁeld Fp[z]  m z   or the ﬁnite ﬁeld F pl respectively, the element α is called a primitive root. Each irreducible polynomial m z  with a primitive root is itself called a primitive polyno- mial. With the help of the primitive root α, all non-zero elements of the ﬁnite ﬁeld F pl  see Figure A.5 . can be generated. Formally, the element 0 is denoted by the power α The multiplication of two elements of the extension ﬁeld F pl can now be carried out using the respective powers αi and αj . In Figure A.6 some primitive polynomials over the ﬁnite ﬁeld F2 are listed.  The operations of addition and multiplication within the ﬁnite ﬁeld Fq = F  pl can be carried out with the help of the respective polynomials or the primitive root α in the case of a primitive polynomial m z . As an example, the addition table and multiplication table for the ﬁnite ﬁeld F24 are given in Figure A.7 and Figure A.8 using the primitive polynomial m z  = 1 + z + z4. Figure A.9 illustrates the arithmetics in the ﬁnite ﬁeld F24. pl is deﬁned as the smallest number for which γ ord γ   ≡ 1 modulo m γ   or γ ord γ   = 1 in the notation of the ﬁnite pl . Thus, the order of the primitive root α is equal to ord α  = pl − 1. For each ﬁeld F non-zero element γ ∈ F  The order ord γ   of an arbitrary non-zero element γ ∈ F  pl we have  γ pl = γ ⇔ γ pl−1 = 1.  Furthermore, the order ord γ   divides the number of non-zero elements pl − 1 in the ﬁnite ﬁeld F  pl , i.e.  There is a close relationship between the roots of an irreducible polynomial m z  of degree deg m z   = l over the ﬁnite ﬁeld Fp. If α is a root of m z  with m α  = 0, the powers  ord γ   pl − 1.  αp, αp2  , . . . , αpl−1   304  ALGEBRAIC STRUCTURES  Primitive root in the ﬁnite ﬁeld F24    Let m z  be the irreducible polynomial m z  = 1 + z + z4 in the polynomial  ring F2[z].    We calculate the powers of the root α using 1 + α + α4 = 0.   Since α yields all 24 − 1 = 15 non-zero elements of F24, α is a primitive root  of the primitive polynomial m z  = 1 + z + z4.  αj −∞ α α0 α1 α2 α3 α4 α5 α6 α7 α8 α9 α10 α11 α12 α13 α14 α15  0 1  α  α2  α3  α + 1  α2 + α  α3 + α2 α3  + α + 1 + 1 α2 + α α3 α2 + α + 1 α3 + α2 + α α3 + α2 + α + 1 α3 + α2 + 1 + 1 α3 1  a  0000 0001 0010 0100 1000 0011 0110 1100 1011 0101 1010 0111 1110 1111 1101 1001 0001  Figure A.5: Primitive root in the ﬁnite ﬁeld F24. Reproduced by permission of J.  Schlembach Fachverlag  are also roots of m z . These elements are called the conjugate roots of m z . For a primitive root α of a primitive polynomial m z , the powers α, αp, αp2, . . . , αpl−1 are all different. The primitive polynomial m z  can thus be written as a product of linear factors  m z  =  z − α   z − αp   z − αp2    ···  z − αpl−1   .  Correspondingly, the product over all different linear factors z − αi pj for a given power αi yields the so-called minimal polynomial mi  z , which we will encounter in Section 2.3.6.   ALGEBRAIC STRUCTURES  305  Primitive polynomials over the ﬁnite ﬁeld F2    Primitive polynomials m z  of degree l = deg m z   for the construction of  the ﬁnite ﬁeld F2l  l = deg m z   m z  1 + z 1 + z + z2 1 + z + z3 1 + z + z4 1 + z2 + z5 1 + z + z6 1 + z + z7 1 + z4 + z5 + z6 + z8 1 + z4 + z9 1 + z3 + z10 1 + z2 + z11 1 + z3 + z4 + z7 + z12 1 + z + z3 + z4 + z13 1 + z + z6 + z8 + z14 1 + z + z15 1 + z + z3 + z12 + z16  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  Figure A.6: Primitive polynomials over the ﬁnite ﬁeld F2  A.4 Discrete Fourier Transform As is wellknown from signal theory, a ﬁnite complex sequence {x[0], x[1], . . . , x[n − 1]} of samples x[k] ∈ C can be transformed into the spectral sequence {X[0], X[1], . . . , X[n − 1]} with the help of the Discrete Fourier Transform  DFT  according to  X[l] = n−1 cid:1  n−1 cid:1   k=0  x[k] = 1   cid:3  cid:2   cid:2   n  l=0  x[k] wkl n  −kl  X[l] w n  wn = e  −j 2π n.  using the so-called twiddle factor   306  ALGEBRAIC STRUCTURES  Addition table of the ﬁnite ﬁeld F24  + 0000 0001 0010 0011 0100 0101 0110 0111 1000 1001 1010 1011 1100 1101 1110 1111  0000 0000 0001 0010 0011 0100 0101 0110 0111 1000 1001 1010 1011 1100 1101 1110 1111 0001 0001 0000 0011 0010 0101 0100 0111 0110 1001 1000 1011 1010 1101 1100 1111 1110 0010 0010 0011 0000 0001 0110 0111 0100 0101 1010 1011 1000 1001 1110 1111 1100 1101 0011 0011 0010 0001 0000 0111 0110 0101 0100 1011 1010 1001 1000 1111 1110 1101 1100 0100 0100 0101 0110 0111 0000 0001 0010 0011 1100 1101 1110 1111 1000 1001 1010 1011 0101 0101 0100 0111 0110 0001 0000 0011 0010 1101 1100 1111 1110 1001 1000 1011 1010 0110 0110 0111 0100 0101 0010 0011 0000 0001 1110 1111 1100 1101 1010 1011 1000 1001 0111 0111 0110 0101 0100 0011 0010 0001 0000 1111 1110 1101 1100 1011 1010 1001 1000 1000 1000 1001 1010 1011 1100 1101 1110 1111 0000 0001 0010 0011 0100 0101 0110 0111 1001 1001 1000 1011 1010 1101 1100 1111 1110 0001 0000 0011 0010 0101 0100 0111 0110 1010 1010 1011 1000 1001 1110 1111 1100 1101 0010 0011 0000 0001 0110 0111 0100 0101 1011 1011 1010 1001 1000 1111 1110 1101 1100 0011 0010 0001 0000 0111 0110 0101 0100 1100 1100 1101 1110 1111 1000 1001 1010 1011 0100 0101 0110 0111 0000 0001 0010 0011 1101 1101 1100 1111 1110 1001 1000 1011 1010 0101 0100 0111 0110 0001 0000 0011 0010 1110 1110 1111 1100 1101 1010 1011 1000 1001 0110 0111 0100 0101 0010 0011 0000 0001 1111 1111 1110 1101 1100 1011 1010 1001 1000 0111 0110 0101 0100 0011 0010 0001 0000  Figure A.7: Addition table of the ﬁnite ﬁeld F24. Reproduced by permission of J.  Schlembach Fachverlag      numbers, i.e. w n n  follows   X[0]  This twiddle factor corresponds to an nth root of unity in the ﬁeld C of complex = 1. The discrete Fourier transform can be written in matrix form as   =  ··· ··· ··· . . . ··· w n−1 · n−1  Correspondingly, the inverse discrete Fourier transform reads  1 1 1 ... 1 w n−1 ·1    x[0]   1 1· n−1  2· n−1  ...  1 w1·1 w2·1 ...    .  X[1] ...  x[1] ...  w n w n    n  n  n  n     X[n − 1]   = 1    x[0]  x[1] ...  n  x[n − 1]   ·  ·  x[n − 1]   X[0]  X[1] ...  X[n − 1]    .  w n w n  1 −1·1 −2·1 ...  1 1 1 ... 1 w n  − n−1 ·1  ··· ··· ··· . . . ··· w  1  −1· n−1  −2· n−1   w n w n  ...  − n−1 · n−1   n  With the Fast Fourier Transform  FFT  there exist fast algorithms for calculating the DFT. The discrete Fourier transform can also be deﬁned over ﬁnite ﬁelds F ql . To this end, we consider the vector a =  a0, a1, a2, . . . , an−1  over the ﬁnite ﬁeld F ql which can also be represented by the polynomial  a z  = a0 + a1 z + a2 z2 + ··· + an−1 zn−1   ALGEBRAIC STRUCTURES  307  Multiplication table of the ﬁnite ﬁeld F24  ·  0000 0001 0010 0011 0100 0101 0110 0111 1000 1001 1010 1011 1100 1101 1110 1111  0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0001 0000 0001 0010 0011 0100 0101 0110 0111 1000 1001 1010 1011 1100 1101 1110 1111 0010 0000 0010 0100 0110 1000 1010 1100 1110 0011 0001 0111 0101 1011 1001 1111 1101 0011 0000 0011 0110 0101 1100 1111 1010 1001 1011 1000 1101 1110 0111 0100 0001 0010 0100 0000 0100 1000 1100 0011 0111 1011 1111 0110 0010 1110 1010 0101 0001 1101 1001 0101 0000 0101 1010 1111 0111 0010 1101 1000 1110 1011 0100 0001 1001 1100 0011 0110 0110 0000 0110 1100 1010 1011 1101 0111 0001 0101 0011 1001 1111 1110 1000 0010 0100 0111 0000 0111 1110 1001 1111 1000 0001 0110 1101 1010 0011 0100 0010 0101 1100 1011 1000 0000 1000 0011 1011 0110 1110 0101 1101 1100 0100 1111 0111 1010 0010 1001 0001 1001 0000 1001 0001 1000 0010 1011 0011 1010 0100 1101 0101 1100 0110 1111 0111 1110 1010 0000 1010 0111 1101 1110 0100 1001 0011 1111 0101 1000 0010 0001 1011 0110 1100 1011 0000 1011 0101 1110 1010 0001 1111 0100 0111 1100 0010 1001 1101 0110 1000 0011 1100 0000 1100 1011 0111 0101 1001 1110 0010 1010 0110 0001 1101 1111 0011 0100 1000 1101 0000 1101 1001 0100 0001 1100 1000 0101 0010 1111 1011 0110 0011 1110 1010 0111 1110 0000 1110 1111 0001 1101 0011 0010 1100 1001 0111 0110 1000 0100 1010 1011 0101 1111 0000 1111 1101 0010 1001 0110 0100 1011 0001 1110 1100 0011 1000 0111 0101 1010  Figure A.8: Multiplication table of the ﬁnite ﬁeld F24. Reproduced by permission of  J.Schlembach Fachverlag  Arithmetics in the ﬁnite ﬁeld F24    Let a5 = 0101 and a6 = 0110.   Addition    Multiplication  a5 + a6 = 0101 + 0110 = 0011 = a3.  a5 · a6 = 0101 · 0110 = 1101 = a13  corresponding to the product  2 + 1  ·  α  2 + α  = α  8 · α  5 = α  13 = α  3 + α  2 + 1.   α  Figure A.9: Arithmetics in the ﬁnite ﬁeld F24  in the factorial ring F the extension ﬁeld F  ql [z]  zn − 1 . With the help of the nth root of unity α ∈ F in ql with αn = 1, the discrete Fourier transform over the factorial ring  ql   308  ALGEBRAIC STRUCTURES  Discrete Fourier transform over ﬁnite ﬁelds    Let α be an nth root of unity in the extension ﬁeld F  ql with αn = 1.    DFT equations  Aj = a αj   = n−1 cid:1   ai αij  i=0   cid:3  cid:2   cid:2   ai = n  −1 A α  −i   = n −1  −ij  Aj α  n−1 cid:1   j=0   A.3    A.4   Figure A.10: Discrete Fourier transform over the ﬁnite ﬁeld F ql  ql [z]  zn − 1  is deﬁned by  F  A z  = A0 + A1 z + A2 z2 + ··· + An−1 zn−1   cid:3  cid:2   cid:2   a z  = a0 + a1 z + a2 z with the DFT formulas given in Figure A.10. The discrete Fourier transform is a mapping of a polynomial a z  in the factorial ring ql [z]  zn − 1  onto a polynomial A z  in F  2 + ··· + an−1 zn−1  F  DFT : F  ql [z]  zn − 1   cid:3 → F  In matrix form the transform equations read    =    A0 A1 ...   = n  An−1  −1    a0  a1 ... an−1       1 α1·1 α2·1 ...  1 1 1 ... 1 α n−1 ·1  1 1 1 ... 1 α  α α  1 −1·1 −2·1 ...  − n−1 ·1     1  α1· n−1  α2· n−1   ql [z]  zn − 1 , i.e. ql [z]  zn − 1 .     a0     A0  ··· ··· ··· . . . ··· α n−1 · n−1  ··· ··· ··· . . . ··· α  −1· n−1  −2· n−1   a1 ... an−1  − n−1 · n−1   A1 ...  α α  1  ...  ...  An−1    .  and   ALGEBRAIC STRUCTURES  309  The polynomial A z  – possibly in different order – is also called the Mattson–Solomon polynomial. Similarly to the discrete Fourier transform over the ﬁeld C of complex num- bers there also exist fast transforms for calculating the discrete Fourier transform over ﬁnite ﬁelds F ql . The discrete Fourier transform is, for example, used in the context of Reed–Solomon codes, as we will see in Chapter 2.    B  Linear Algebra  This appendix aims to provide some basics of linear algebra as far as it is necessary for the topics of this book. For further information the reader is referred to the rich literature that is available for linear algebra. Generally, we denote an N × N identity matrix by IN , 0N×M is an N × M matrix containing only 0s and 1N×M is a matrix of the same size consisting only of 1s.  Deﬁnition B.0.1  Hermitian Matrix  A Hermitian matrix A is a square matrix whose com- plex conjugate transposed version is identical to itself  A = AH  The real part A cid:1  metric, i.e.  of a Hermitian matrix is symmetric while the imaginary part A cid:1  cid:1   is asym-   cid:1  =  A   cid:1   A  T     and   cid:1  cid:1  = − A   cid:1  cid:1   A  T     holds. Obviously, the properties symmetric and Hermitian are identical for real matrices.  According to strang  Strang, 1988 , Hermitian matrices have the following properties:   B.1    B.2   ● Its diagonal elements Ai,i are real. ● For each element, Ai,j = A ∗ j,i holds. ● For all complex vectors x, the number xHAx is real. ● From  B.1 , it follows directly that AAH = AHA. ● The eigenvalues λi of a Hermitian matrix are real.  ● The eigenvectors xi belonging to different eigenvalues λi of a real symmetric or  Hermitian matrix are mutually orthogonal.  Coding Theory – Algorithms, Architectures, and Applications Andr´e Neubauer, J¨urgen Freudenberger, Volker K¨uhn  cid:148  2007 John Wiley & Sons, Ltd   312  LINEAR ALGEBRA  Deﬁnition B.0.2  Spectral Norm  Following Golub and van Loan,  Golub and van Loan, 1996 , the spectral norm or  cid:6 2 norm of an arbitrary N × M matrix A is deﬁned as   cid:30 A cid:30 2 = sup x cid:7 =0   cid:30 Ax cid:30   cid:30 x cid:30    B.3   The  cid:6 2 norm describes the maximal ampliﬁcation of a vector x that experiences a linear transformation by A. It has the following basic properties:  ● The spectral norm equals its largest singular value σmax   B.4  ● The spectral norm of the inverse A−1 is identical to the reciprocal of the smallest   cid:30 A cid:30 2 = σmax A   singular value σmin of A   cid:30 A  −1 cid:30 2 =  1   B.5  Deﬁnition B.0.3  Frobenius Norm  The Frobenius norm of an arbitrary N × M matrix A resembles the norm of a vector and is deﬁned as the sum over the squared magnitudes of all matrix elements  Golub and van Loan, 1996   σmin A      * N cid:1   M cid:1   i=1  j=1  +   cid:30 A cid:30 F =  Ai,j2 =  tr{AAH}   B.6   Consequently, the squared Frobenius norm is  cid:30 A cid:30 2 Deﬁnition B.0.4  Rank  The rank r = rank A  of an arbitrary matrix A equals the largest number of linear independent columns or rows. According to this deﬁnition, we can conclude that the rank of an N × M matrix is always upper bounded by the minimum of N and M  = tr{AAH}.  F  r = rank A  ≤ min N, M    B.7   The following properties hold:  ● An N × N matrix A is called regular if its determinant is non-zero and, therefore, r = rank A  = N holds. For regular matrices, the inverse A−1 with A−1A = IN×N exists. ● If the determinant is zero, the rank r is smaller than N and the matrix is singular.  The inverse does not exist for singular matrices.  ● For each N × N matrix A of rank r there exist at least one r × r submatrix whose determinant is non-zero. The determinants of all  r + 1  ×  r + 1  submatrices of A are zero.  ● The rank of the product AAH equals  rank AAH  = rank A    B.8    LINEAR ALGEBRA  313  Deﬁnition B.0.5  Orthogonality  An orthogonal real-valued matrix consists of columns that are orthogonal to each other, i.e. the inner product between different columns equals i qj = 0. A matrix is termed orthonormal if its columns are orthogonal and additionally qT have unit length  B.9   i qj = δ i, j   qT  Orthonormal matrices have the properties  QTQ = IN  ⇔ QT = Q −1   B.10  Deﬁnition B.0.6  Unitary Matrix  An N × N matrix U with orthonormal columns and complex elements is called unitary. The Hermitian of a unitary matrix is also its inverse  UHU = UUH = IN  ⇔ UH = U  −1   B.11   The columns of U span an N-dimensional orthonormal vector space.  Unitary matrices U have the following properties:  ● The eigenvalues of U have unit magnitude  λi = 1 . ● The eigenvectors belonging to different eigenvalues are orthogonal to each other.  ● The inner product xHy between two vectors is invariant to multiplications with a  unitary matrix because  Ux H Uy  = xHUHUy = xHy.  ● The norm of a vector is invariant to the multiplication with a unitary matrix,   cid:30 Ux cid:30  =  cid:30 x cid:30 .  ● A random matrix B has the same statistical properties as the matrices BU and UB. ● The determinant of a unitary matrix equals det U  = 1  Blum, 2000 .  A · x = λ · x  Deﬁnition B.0.7  Eigenvalue Problem  The calculation of the eigenvalues λi and the asso- ciated eigenvectors xi of a square N × N matrix A is called the eigenvalue problem. The basic problem is to ﬁnd a vector x proportional to the product Ax. The corresponding equation  B.12  can be rewritten as  A − λ IN   x = 0. For the non-trivial solution x  cid:7 = 0, the matrix  A − λ IN   has to be singular, i.e. its columns are linear dependent. This results in det  A − λ IN   = 0 and the eigenvalues λi represent the zeros of the characteristic poly- nomial pN  λ  = det A − λIN   of rank N. Each N × N matrix has exactly N eigenvalues that need not be different. xi = 0 has to be solved for each eigenvalue λi. Since xi as well as c · xi fulﬁl the above equation, a unique eigenvector is only obtained by normalising it to unit length. The eigenvectors x1, . . . , xk belonging to different eigenvalues λ1, . . . , λk are linear independent of each other  Horn and Johnson, 1985; Strang, 1988 .  In order to determine the associated eigenvectors xi, the equation  A − λi IN   cid:3    cid:2   The following relationships exist between the matrix A and its eigenvalues:   314  LINEAR ALGEBRA  ● The sum over all eigenvalues equals the trace of a square matrix A  ● The product of the eigenvalues of a square matrix A with full rank equals the deter-  minant of A  ● If eigenvalues λi = 0 exist, the matrix is singular, i.e. det A  = 0 holds.  Deﬁnition B.0.8  Eigenvalue Decomposition  An N × N matrix A with N linear inde- pendent eigenvectors xi can be transformed into a diagonal matrix according to  Ai,i = r=N cid:1   λi  i=1  tr A  = r=N cid:1  r=N cid:20   i=1  λi = det A   i=1    λ1     −1AU =  cid:13  =  U  λ2  . . .  λN  −1 = U cid:13 UH.  if U =  x1, x2, . . . , xN   contains the N independent eigenvectors of A  Horn and Johnson, 1985 . The resulting diagonal matrix  cid:13  contains the eigenvalues λi of A. Since U is unitary, it follows from deﬁnition B.0.8 that each matrix A can be decomposed as A = U cid:13 U Deﬁnition B.0.9  Singular Value Decomposition  A generalisation of deﬁnition  B.0.8  for arbitrary N × M matrices A is called singular value decomposition  SVD . A matrix A can be expressed by  B.16  with the unitary N × N matrix U and the unitary M × M matrix V. The columns of U contain the eigenvectors of AAH and the columns of V the eigenvectors of AHA. The matrix  cid:23  is an N × M diagonal matrix with non-negative, real-valued elements σk on its diagonal. Denoting the eigenvalues of AAH and, therefore, also of AHA with λk, the diagonal elements σk are the positive square roots of λk  A = U cid:23 VH  ’  σk =  λk  They are called singular values of A. For a matrix A with rank r, we obtain   B.13    B.14    B.15    B.17    B.18       σ1 0 ... 0 +0 ... 0   cid:13    cid:23  =  0 σ2  0 0  0   cid:14  cid:15   . . .  0 0 ... . . . . . . σr 0 . . . ... . . .  cid:13  0 . . .   cid:16   0 . . . 0 0 0 ... ... 0 . . . 0 0 . . . 0 ... ...  cid:14  cid:15  0 . . . 0   cid:16   r columns  N-r columns        r rows   M-r rows   LINEAR ALGEBRA  315  Graphical illustration of Householder reﬂection  u  x  −2uuH x  plain surface  y =  cid:22 x  Figure B.1: Graphical illustration of Householder reﬂection      a1  ... an  vec A  =  Deﬁnition B.0.10  vec-Operator  The application of the vec-operator onto a matrix A stacks its columns on top of each other. For A = [a1, . . . , an], we obtain  Deﬁnition B.0.11  Householder Reﬂections  A vector x can be reﬂected at a plane or line onto a new vector y of the same length by Householder reﬂections. The reﬂection is performed by a multiplication of x with a unitary matrix  cid:22 . Since we generally consider column vectors, we obtain y =  cid:22  · x with the unitary matrix  The vector u and the scalar w are deﬁned as   cid:22  = IN −  1 + w  · uuH  u = x − y  cid:30 x − y cid:30   and  w = xHu uHx   cid:22  = IN − 2 · uuH  If x contains only real-valued elements, w = 1 holds and Equation  B.20  becomes  The reﬂection is graphically illustrated in Figure B.1 for a real-valued vector x. The plane x has to be reﬂected at the plain surface being represented by the vector u that is perpendicular to the plane. The projection uuHx of x onto u has to be subtracted twice from the vector x in order to obtain a reﬂection at the line perpendicular to u.   B.19    B.20    B.21    B.22    316  LINEAR ALGEBRA  Householder reﬂection based QL decomposition  Step  Task  Initialise with L = A and Q = IM for k = N, . . . , 1  x = L[1 : M − N + k, k]  y =.  0  cid:30 x cid:30  T   1   2   3   4   5   6   7   8   calculate u, w and  cid:22  L[1 : M − N + k, 1 : k] =  cid:22  · L[1 : M − N + k, 1 : k] Q[:, 1 : M − N + k] = Q[:, 1 : M − N + k] ·  cid:22 H  end  Figure B.2: Pseudocode for QL decomposition via Householder reﬂections  If a row vector x instead of a column vector has to be reﬂected, w and  cid:22  have the form   cid:22  = IN −  1 + w  · uHu  and  w = u xH x uH   B.23   The reﬂection is performed by y = x ·  cid:22 .  Householder reﬂections have been used for the Post-Sorting Algorithm  PSA  in Section 5.5 to force certain elements of a matrix to zero and thus restore the triangular structure after permutations. For this special case, the target vector has only one non-zero element and becomes  y =.  0  cid:30 x cid:30  T  .  Similarly, Householder reﬂections can be used to decompose an M × N matrix A with M ≥ N into the matrices Q and at L. The algorithm for this QL decomposition is shown as a pseudocode in Figure B.2. Deﬁnition B.0.12  Givens Rotation  Let G i, k, θ   be an N × N identity matrix except for = Gk,i = sin θ = β, i.e. it has the form the elements G  = Gk,k = cos θ = α and −G  ∗ i,k  ∗ i,i  1      G i, k, θ   =  . . .  ··· −β ... . . . ··· ∗ α  α ... ∗ β      . . .  1   B.24    LINEAR ALGEBRA  317  Hence, G i, k, θ   is unitary and describes a rotation by the angle θ in the N-dimensional vector space. If θ is chosen appropriately, the rotation can force the ith element of a vector to zero.  As an example, the ith element of an arbitrary column vector x = [x1, . . . , xN ]T should  and  β = sin θ =   B.25   we obtain the new vector y = G i, k, θ  x with  equal zero. By choosing  α = cos θ =  xk  xi2 + xk2   1  . . .  ’        =      ’  x1 ... 0 ...  ... xN  xi2 + xk2  xk√ xi  2+xk ... ∗ i√ 2+xk xi  x  2  2  ···  . . . ···  −xi√ xi  2+xk ... ∗ k√ xi 2+xk  x  2  2  ’  xi  xi2 + xk2     ·          x1 ... xi ... xk ... xN  . . .  ’ It can be recognised that only two elements of x and y differ: yi = 0 and yk xi2 + xk2. The Givens rotation can also be used to perform QL decompositions = similarly to the application of Householder reﬂections.  1    C  Acronyms  3GPP Third-Generation Partnership Project  4PSK Four-point Phase Shift Keying  8PSK Eight-point Phase Shift Keying  16QAM 16-Point Quadrature Amplitude Modulation  ACK Acknowledgement  ACSU Add Compare Select Unit  AGC Automatic Gain Control  AMR Adaptive MultiRate  AP Acknowledgement Period  APP A-Posteriori Probability  ARQ Automatic Repeat Request  ASK Amplitude Shift Keying  AWGN Additive White Gaussian Noise  BCJR Bahl, Cocke, Jelinek, Raviv  BCH Bose, Chaudhuri, Hocquenghem  BCS Block Check Sequence  BEC Binary Erasure Channel  BER Bit Error Rate Coding Theory – Algorithms, Architectures, and Applications Andr´e Neubauer, J¨urgen Freudenberger, Volker K¨uhn  cid:148  2007 John Wiley & Sons, Ltd   ACRONYMS  COST European Cooperation in the ﬁeld of Scientiﬁc and Technical Research  320  BLAST Bell Labs Layered Space–Time  BPSK Binary Phase Shift Keying  BSC Binary Symmetric Channel  CDMA Code Division Multiple Access  CRC Cyclic Redundancy Check  CSI Channel State Information  DAB Digital Audio Broadcast  D-BLAST diagonal BLAST  DFT Discrete Fourier Transform  DoA Direction of Arrival  DoD Direction of Departure  DRAM Dynamic Random Access Memory  DS-CDMA direct-sequence CDMA  DSL Digital Subscriber Line  DSP Digital Signal Processor  DVB Digital Video Broadcast  EDGE Enhanced Data rates for GSM Evolution  EFR Enhanced Full Rate  EGPRS Enhanced General Packet Radio Service  EXIT EXtrinsic Information Transfer  FDD Frequency Division Duplex  FDMA Frequency Division Multiple Access  FEC Forward Error Correction  FER Frame Error Rate  FFT Fast Fourier Transform  FHT Fast Hadamard Transform  FR Full-Rate   321  ACRONYMS  GF Galois Field  GMSK Gaussian Minimum Key Shifting  GPRS General Packet Radio Service  GSM Global System for Mobile communications  HCS Header Check Sequence  HSCSD High-Speed Circuit Switched Data  HSDPA High-Speed Downlink Packet Access  IEEE Institute of Electrical and Electronics Engineers  i.i.d. independent identically distributed  IOPEF Input–Output Path Enumerator Function  IOWEF Input–Output Weight Enumerating Function  IR Incremental Redundancy  ISI Inter Symbol Interference  LA Link Adaptation  LAN Local Area Network  LD Linear Dispersion  LDPC Low-Density Parity Check  LFSR Linear Feedback Shift Register  LLL Lenstra, Lenstra and Lov´asz  LLR Log-Likelihood Ratio  LoS Line of Sight  LR lattice reduction  LTI Linear Time-Invariant  MAC Medium Access Control  MAP Maximum A-Posteriori  MDS Maximum Distance Separable  MCS Modulation and Coding Scheme  MED Minimum Error Probability Decoding   ACRONYMS  OFDM Orthogonal Frequency Division Multiplexing  322  MIMO Multiple-Input Multiple-Output  MISO Multiple-Input Single-Output  ML Maximum Likelihood  MLD Maximum Likelihood Decoding  MMSE Minimum Mean-Square Error  MRC Maximum Ratio Combining  NACK Not Acknowledgement  NLoS Non-Line of Sight  PDF Probability Density Function  PEF Path Enumerator Function  PSA Post-Sorting Algorithm  PSK Phase Shift Keying  QAM Quadrature Amplitude Modulation  QLD QL Decomposition  QoS Quality of Service  QPSK Quaternary Phase Shift Keying  RLC Radio Link Control  SCC Serially Concatenated Code  SCM Spatial Channel Model  SDM Space Division Multiplexing  SDMA Space Division Multiple Access  SIC Successive Interference Cancellation  SIMO Single-input Multiple-Output  SINR Signal to Interference plus Noise Ratio  SISO Soft-Input Soft-Output  SMU Survivor Memory Unit  SNR Signal-to-Noise Ratio   323  ACRONYMS  SOVA Soft-Output Viterbi Algorithm  SQLD Sorted QL Decomposition  SR-ARQ Selective Repeat ARQ  STBC Space-Time Block Code  STC Space-Time Code  STTC Space-Time Trellis Code  SVD Singular Value Decomposition  TB Tail Bits  TC Turbo Code  TDD Time Division Duplex  TDMA Time Division Multiple Access  TMU Transition Metric Unit  TFCI Transport Format Combination Indicator  TU Typical Urban  UEP Unequal Error Protection  UMTS Universal Mobile Telecommunications System  UTRA UMTS Terrestrial Radio Access  V-BLAST vertical BLAST  VLSI Very Large-Scale Integration  WCC Woven Convolutional Code  WEF Weight Enumerating Function  WER Word Error Rate  WIMAX Worldwide Interoperability for Microwave Access  WSSUS wide sense stationary uncorrelated scattering  WLAN Wireless Local Area Network  WTC Woven Turbo Code  ZF Zero Forcing    Bibliography  3GPP  1999  Physical Channels and Mapping of Transport Channels onto Physical Channels  FDD . 3rd Generation Partnership Project, Technical Speciﬁcation Group Radio Access Network, TS 25.211, http:  www.3gpp.org ftp Specs html-info 25-series.htm.  3GPP  2003  Spatial Channel Model for Multiple Input Multiple Output  MIMO  Simulations  Release 6 . 3rd Generation Partnership Project, Technical Speciﬁcation Group Radio Access Network, TR25.996, http:  www.3gpp.org ftp Specs html-info 25-series.htm.  3GPP  2007  Multiple-Input Multiple-Output in UTRA. 3rd Generation Partnership Project, Techni- cal Speciﬁcation Group Radio Access Network, TS25.876, http:  www.3gpp.org ftp Specs html- info 25-series.htm.  Agrell, E., Eriksson, T., Vardy, A. and Zeger, K.  2002  Closest point search in lattices. IEEE  Transactions on Information Theory, 48  8 , 2201–2214.  Alamouti, S.  1998  A simple transmit diversity technique for wireless communications. IEEE Journal  on Selected Areas in Communications, 16  8 , 1451–1458.  Bahl, L., Cocke, J., Jelinek, F. and Raviv, J.  1974  Optimal decoding of linear codes for minimum  symbol error rate. IEEE Transactions on Information Theory, 20, 284–287.  Ball, C., Ivanov, K., Bugl, L. and St¨ockl, P.  2004a  Improving GPRS EDGE end-to-end performance by optimization of the RLC protocol and parameters. IEEE Proceedings of Vehicular Technology Conference, Los angeles, CA, USA, pp. 4521–4527.  Ball, C., Ivanov, K., St¨ockl P., Masseroni, C., Parolari, S. and Trivisonno, R.  2004b  Link quality control beneﬁts from a combined incremental redundancy and link adaptation in EDGE networks. IEEE Proceedings of Vehicular Technology Conference, Milan, Italy, pp. 1004–1008.  B¨aro, S., Bauch, G. and Hansmann, A.  2000a  Improved codes for space–time trellis coded modu-  lation. IEEE Communications Letters, 4  1 , 20–22.  B¨aro, S., Bauch, G. and Hansmann, A.  2000b  New trellis codes for Space–time coded modulation.  ITG Conference on Source and Channel Coding, Munich, Germany, pp. 147–150.  Benedetto, S. and Biglieri, E.  1999  Principles of Digital Transmission with Wireless Applications,  Kluwer Academic Plenum Publishers, New York.  Benedetto, S. and Montorsi, G.  1996  Unveiling turbo codes: some results on parallel concatenated  coding schemes. IEEE Transactions on Information Theory, 42, 409–429.  Benedetto, S. and Montorsi, G.  1998  Serial concatenation of interleaved codes: performance anal-  ysis, design, and iterative decoding. IEEE Transactions on Information Theory, 44, 909–926.  Berlekamp, E.  1984  Algebraic Coding Theory, revised 1984, Aegean Park Press, Laguna Hills, CA,  USA.  Berrou, C., Glavieux, A. and Thitimasjshima, P.  1993  Near shannon limit error-correcting coding and decoding: turbo-codes  1 . IEEE Proceedings of International Conference on Communications, Geneva, Switzerland, pp. 1064–1070.  Coding Theory – Algorithms, Architectures, and Applications Andr´e Neubauer, J¨urgen Freudenberger, Volker K¨uhn  cid:148  2007 John Wiley & Sons, Ltd   BIBLIOGRAPHY  326  UK.  Bluetooth  2004  Speciﬁcation of the Bluetooth System, Bluetooth SIG. http:  www.bluetooth.com. Blum, R.  2000  Analytical tools for the design of space–time convolutional codes. Conference on  Information Sciences and Systems, Princeton, NJ, USA.  Bocharova, I., Kudryashov, B., Handlery, M. and Johannesson, R.  2002  Convolutional codes with large slopes yield better tailbiting codes. IEEE Proceedings of International Symposium on Infor- mation Theory, Lausanne, Switzerland.  Bose, R., Ray-Chaudhuri, D. and Hocquenghem, A.  1960  On a class of error-correcting binary  group codes. Information and Control 3, 68–79, 279–290.  Bossert, M.  1999  Channel Coding for Telecommunications, John Wiley & Sons, Ltd, Chichester,  Bossert, M., Gabidulin, E. and Lusina, P.  2000  Space–time codes based on Hadamard matrices.  IEEE Proceedings of International Symposium on Information Theory, Sorrento, Italy, p. 283.  Bossert, M., Gabidulin, E. and Lusina, P.  2002  Space–time codes based on Gaussian integers IEEE Proceedings of International Symposium on Information Theory, Lausanne, Switzerland, p. 273. B¨ohnke, R., K¨uhn, V. and Kammeyer, K.D.  2004a  Efﬁcient near maximum-likelihood decoding of multistratum space-time codes. IEEE Semiannual Vehicular Technology Conference  VTC2004- Fall , Los Angeles, CA, USA.  B¨ohnke, R., K¨uhn, V. and Kammeyer, K.D.  2004b  Multistratum space-time codes for the asyn- chronous uplink of MIMO-CDMA systems. International Symposium on Spread Spectrum Tech- niques and Applications  ISSSTA’04 , Sydney, Australia.  B¨ohnke R., K¨uhn, V. and Kammeyer, K.D.  2004c  Quasi-orthogonal multistratum space-time codes.  IEEE Global Conference on Communications  Globecom’04 , Dallas, TX, USA.  B¨ohnke R., W¨ubben, D., K¨uhn, V. and Kammeyer, K.D.  2003  Reduced complexity MMSE detection for BLAST architectures. IEEE Proceedings of Global Conference on Telecommunications, San Francisco, CA, USA.  Bronstein, I., Semendjajew, K., Musiol, G. and M¨uhlig, H.  2000  Taschenbuch der Mathematik, 5th  edn, Verlag Harri Deutsch, Frankfurt, Germany.  Clark, G.C. and Cain, J.B.  1988  Error-correcting Coding for Digital Communications, Plenum Press,  Costello, D., Hagenauer, J., Imai, H. and Wicker, S.  1998  Applications of error-control coding.  IEEE Transactions on Information Theory, 44  6 , 2531–2560.  Cover, T. and Thomas, J.  1991  Elements of Information Theory, John Wiley & Sons, Inc., New  New York, NY, USA.  York, NY, USA.  Di, C., Proietti, D., Telatar, E., Richardson, T. and Urbanke, R.  2002  Finite-length analysis of low-density parity-check codes on the binary erasure channel. IEEE Transactions on Information Theory, 48, 1570–1579.  Divsalar, D. and McEliece, R.J.  1998  On the Design of Generalized Concatenated Coding Systems with Interleavers. TMO Progress Report 42-134, Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA, USA.  Divsalar, D. and Pollara, F.  1995  Multiple Turbo Codes for Deep-Space Communications. TDA Progress Report 42-121, Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA, USA.  Dornstetter, J.  1987  On the equivalence between Berlekamp’s and Euclid’s algorithms. IEEE Trans-  actions on Information Theory, 33  3 , 428–431.  Elias, P.  1954  Error-free coding. IEEE Transactions on Information Theory, 4, 29–37. Elias, P.  1955  Coding for noisy channels. IRE Convention Record 4, pp. 37–46. ETSI  2001  Broadband Radio Access Networks  BRAN ; HIPERLAN type 2; Physical  PHY  Layer.  Norme ETSI, Document RTS0023003-R2, Sophia-Antipolis, France.   BIBLIOGRAPHY  327  ETSI  2006  Digital Video Broadcasting  DVB , Framing Structure, Channel Coding and Modu- lation for Digital Terrestrial Television, European Telecommunications Standards Institute, Sophia Antipolis, France, http:  www.etsi.org about etsi 5 minutes home.htm.  Fano, R.M.  1963  A heuristic discussion of probabilistic decoding. IEEE Transactions on Information  Fincke, U. and Pohst, M.  1985  Improved methods for calculating vectors of short length in a lattice,  including a complexity analysis. Mathematics of Computation, 44, 463–471.  Fischer, R.  2002  Precoding and Signal Shaping for Digital Transmission, John Wiley & Sons, Inc.,  Theory, 9, 64–73.  New York, NY, USA.  Fischer, R. and Huber, J.  1996  A New loading algorithm for discrete multitone transmission. IEEE  Global Conference on Telecommunications  Globecom’96 , London, UK, pp. 724, 728.  Forney, Jr, G.  1966  Concatenated Codes, MIT Press, Cambridge, MA, USA. Forney, Jr, G.D.  1970  Convolutional codes I: algebraic structure. IEEE Transactions on Information  Forney, Jr, G.D.  1973a  Structural analyses of convolutional codes via dual codes. IEEE Transactions  Theory, 16, 720–738.  on Information Theory, 19, 512–518.  Forney, Jr, G.D.  1973b  The Viterbi algorithm. Proceedings of the IEEE, 61, 268–278. Forney, Jr, G.D.  1974  Convolutional codes II: maximum likelihood decoding. Information Control,  25, 222–266.  Forney, Jr, G.  1991  Algebraic structure of convolutional codes, and algebraic system the- ory, In Mathematical System Theory  ed. A. C. Antoulas , Springer-Verlag, Berlin, Germany, pp. 527–558.  Forney, Jr, G., Johannesson, R. and Wan, Z.X.  1996  Minimal and canonical rational generator  matrices for convolutional codes. IEEE Transactions, on Information Theory, 42, 1865–1880.  Foschini, G.  1996  Layered space–time architecture for wireless communication in a fading envi-  ronment when using multiple antennas. Bell Labs Technical Journal, 1  2 , 41–59.  Foschini, G. and Gans, M.  1998  On limits of wireless communications in a fading environment  when using multiple antennas. Wireless Personal Communications, 6  3 , 311–335.  Foschini, G., Golden, G., Valencuela, A. and Wolniansky, P.  1999  Simpliﬁed processing for high spectral efﬁciency wireless communications emplying multi-element arrays. IEEE Journal on Selected Areas in Communications, 17  11 , 1841–1852.  Freudenberger, J. and Stender, B.  2004  An algorithm for detecting unreliable code sequence seg-  ments and its applications. IEEE Transactions on Communications, COM-52, 1–7.  Freudenberger, J., Bossert, M. and Shavgulidze, S.  2004  Partially concatenated convolutional codes.  IEEE Transactions on Communications, COM-52, 1–5.  Freudenberger, J., Bossert, M., Shavgulidze, S. and Zyablov, V.  2000a  Woven turbo codes. Pro- ceedings of 7th International Workshop on Algebraic and Combinatorial Coding Theory, Bansko, Bulgaria, pp. 145–150.  Freudenberger, J., Bossert, M., Shavgulidze, S. and Zyablov, V.  2001  Woven codes with outer warp: variations, design, and distance properties. IEEE Journal on Selected Areas in Communications, 19, 813–824.  Freudenberger, J., Jordan, R., Bossert, M. and Shavgulidze, S.  2000b  Serially concatenated convo- lutional codes with product distance Proceedings of 2nd International Symposium on Turbo Codes and Related Topics, Brest, France, pp. 81–84.  Gabidulin, E., Bossert, M. and Lusina, P.  2000  Space–time codes based on rank codes. IEEE  Proceedings of International Symposium on Information Theory, Sorrento, Italy, p. 284. Gallager, R.  1963  Low-Density Parity-Check Codes, MIT Press, Cambridge, MA, USA. Gesbert, D., Shaﬁ, M., Shiu, D., Smith, P. and Naguib, A.  2003  From theory to practice: an overview  of MIMO Space–time coded wireless systems. IJSAC, 21  3 , 281–302.   328  BIBLIOGRAPHY  Gibson, D., Berger, T., Lookabaugh, T., Lindbergh, D. and Baker, R.  1998  Digital Compression  for Multimedia–Principles and Standards, Morgan Kaufmann, San Franciso, CA, USA.  Golden, G., Foschini, G., Wolniansky, P. and Valenzuela, R.  1998  V-BLAST: a high capacity space–time architecture for the rich-scattering wireless channel. Proceedings of International Sym- posium on Advanced Radio Technologies, Boulder, CO, USA.  Golub, G. and van Loan, C.  1996  Matrix Computations, 3rd edn, The John Hopkins University  Press, London, UK.  Gorenstein, D. and Zierler, N.  1961  A class of error-correcting codes in pm symbols. Journal of  the Society for Industrial and Applied Mathematics, 9  3 , 207–214.  Hagenauer, J.  1988  Rate-compatible punctured convolutional codes  RCPC codes  and their appli-  cations. IEEE Transactions on Communications, COM-36, 389–400.  Hagenauer, J. and Hoeher, P.  1989  A Viterbi algorithm with soft-decision outputs and its appli- cations. IEEE Proceedings of International Conference on Communications, Dallas, TX, USA, pp. 1680–1686.  Hagenauer, J. Offer, E. and Papke, L.  1996  Iterative decoding of binary block and convolutional  codes. IEEE Transactions on Information Theory, 42, 429–445.  Hamming, R.  1950  Error detecting and error correcting codes. The Bell System Technical Journal,  29, 147–160.  Hamming, R.  1986  Information und Codierung. Hanzo, L., Webb, W. and Keller, T.  2000  Single- and Multi-carrier Quadrature Amplitude Modu- lation–Principles and Applications for Personal Communications WLANs Broadcasting 2d edn, IEEE-Press, John Wiley & Sons, Ltd, Chichester, UK.  Hanzo, L., Wong, C. and Yee, M.  2002  Adaptive Wireless Transceivers: Turbo-Coded, Space–Time Coded TDMA, CDMA and OFDM Systems, IEEE-Press, John Wiley & Sons, Ltd, Chichester, UK. Hassibi, B.  2000  An efﬁcient square-root algorithm for BLAST. IEEE Proceedings of International  Conference on Acoustics, Speech and Signal Processing, Istanbul, Jurkey, pp. 5–9.  Hassibi, B. and Hochwald, B.  2000  High rate codes that are linear in space and time. Allerton Con- ference on Communication, Control and Computing, University of Jllinois at Urbana-Champaign, Urbana, IL, USA.  Hassibi, B. and Hochwald, B.  2001  Linear dispersion codes. IEEE Proceedings of International  Symposium on Information Theory, Washington, DC, USA.  Hassibi, B. and Hochwald, B.  2002  High rate codes that are linear in space and time. IEEE Trans-  actions on Information Theory, 48  7 , 1804–1824.  Heath, R. and Paulraj, A.  2002  Linear dispersion codes for MIMO systems based on frame theory.  IEEE Transactions on Signal Processing, 50  10 , 2429–2441.  Hochwald, B. and Marzetta, T.  2000  Unitary space–time modulation for multiple-antenna commu-  nications in Rayleigh flat fading. IEEE Transactions on Information Theory, 46  2 , 543–564.  Hochwald, B. and Sweldens, W.  2000  Differential unitary space–time modulation. IEEE Transac-  tions on Communications Technology, 48  12 , 2041–2052.  Hochwald, B., Marzetta, T., Richardson, T., Sweldens, W. and Urbanke, R.  2000  Systematic design of unitary Space–time constellations. IEEE Transactions on Information Theory, 46  6 , 1962–1973.  Hoeve, H., Timmermans, J. and Vries, L.  1982  Error correction and concealment in the compact  disc system. Philips Technical Review, 40  6 , 166–172.  Holma, H. and Toskala, A.  2004  WCDMA for UMTS, 3rd edn, John Wiley & Sons, Ltd, Chichester,  Honig, M. and Tsatsanis, M.  2000  Multiuser CDMA receivers. IEEE Signal Processing Magazine,  UK.   3 , 49–61.  Horn, R. and Johnson, C.  1985  Matrix Analysis, Cambridge University Press, Cambridge, UK.   BIBLIOGRAPHY  329  H¨ost, S.  1999  On Woven Convolutional Codes. PhD thesis, Lund University, ISBN 91-7167-016-5. H¨ost, S., Johannesson, R. and Zyablov, V.  1997  A ﬁrst encounter with binary woven convolu- tional codes Proceedings of International Symposium on Communication Theory and Applications, Ambleside, Lake District, UK.  H¨ost, S., Johannesson, R. and Zyablov, V.  2002  Woven convolutional codes I: encoder properties.  IEEE Transactions on Information Theory, 48, 149–161.  H¨ost, S., Johannesson, R., Zigangirov, K. and Zyablov, V.  1999  Active distances for convolutional  codes. IEEE Transactions on Information Theory, 45, 658–669.  H¨ost, S., Johannesson, R., Sidorenko, V., Zigangirov, K. and Zyablov, V.  1998  Cascaded convo- lutional codes, in Communication and Coding  Eds M. Darnell and B. Honary , Research Studies Press Ltd and John Wiley & Sons, Ltd, Chichester, UK, pp. 10–29.  Hughes, B.  2000  Differential space–time modulation. IEEE Transactions on Information Theory,  46  7 , 2567–2578.  Hughes-Hartogs, D.  1989  Ensemble modem structure for imperfect transmission media. US Patents  4,679,227 and 4,731,816 and 4,833,706, Orlando, FL, USA.  H¨ubner, A. and Jordan, R.  2006  On higher order permutors for serially concatenated convolutional  codes. IEEE Transactions on Information Theory, 52, 1238–1248.  H¨ubner, A. and Richter, G.  2006  On the design of woven convolutional encoders with outer warp  row permutors. IEEE Transactions on Communications, COM-54, 438–444.  H¨ubner, A., Truhachev, D.V. and Zigangirov, K.S.  2004  On permutor designs based on cycles for serially concatenated convolutional codes. IEEE Transactions on Communications, COM-52, 1494–1503.  IEEE  2004  Part 16: Air Interface for Fixed Broadband Wireless Access Systems, IEEE, New York,  Jelinek, F.  1969  A fast sequential decoding algorithm using a stack. IBM Journal of Research and  Johannesson, R. and Zigangirov, K.S.  1999  Fundamentals of Convolutional Coding, IEEE Press,  Jordan, R., Pavlushkov, V., and Zyablov, V.  2004b  Maximum slope convolutional codes. IEEE  Transactions on Information Theory, 50, 2511–2521.  Jordan, R., Freudenberger, J., Dieterich, H., Bossert, M. and Shavgulidze, S.  1999  Simulation results for woven codes with outer warp. Proceedings of 4th. ITG Conference on Mobile Communication, Munich, Germany, pp. 439–444.  Jordan, R., Freudenberger, J., Pavlouchkov, V., Bossert, M. and Zyablov, V.  2000  Optimum slope convolutional codes. IEEE Proceedings of International Symposium on Information Theory, Sor- rento, Italy.  Jordan, R., H¨ost, S., Johannesson, R., Bossert, M. and Zyablov, V.  2004a  Woven convolutional  codes II: decoding aspects. IEEE Transactions on Information Theory, 50, 2522–2531.  Jungnickel, D.  1995  Codierungstheorie, Spektrum Akademischer Verlag, Heidelberg, Germany. Justesen. J., Thommensen, C. and Zyablov, V.  1988  Concatenated codes with convolutional inner  codes. IEEE Transactions on Information Theory, 34, 1217–1225.  Kallel, S.  1992  Sequential decoding with an efﬁcient incremental redundancy ARQ strategy. IEEE  Transactions on Communications, 40, 1588–1593.  Kammeyer, K.D.  2004  Nachrichten¨ubertragung, 3rd edn, Teubner, Stuttgart, Germany. Krongold, B., Ramchandran, K. and Jones, D.  1999  An efﬁcient algorithm for optimum margin maximization in multicarrier communication systems. IEEE Proceedings of Global Conference on Telecommunications, Rio de Janeiro, Brazil, pp. 899–903.  K¨uhn, V.  2006  Wireless Communications over MIMO Channels – Applications to CDMA and Mul-  tiple Antenna Systems, John Wiley & Sons, Ltd, Chichester, UK.  NY, USA.  Development, 13, 675–685.  Piscataway, NJ, USA.   330  BIBLIOGRAPHY  K¨uhn, V. and Kammeyer, K.D.  2004  Multiple antennas in mobile communications: concepts and  algorithms. International Symposium on Electromagnetic Theory  URSI 2004 , Pisa, Italy.  Lee, H.  2003  An area-efﬁcient Euclidean algorithm block for Reed–Solomon decoders. IEEE Com-  puter Society Annual Symposium on VLSI, Tampa, FL, USA.  Lee, H.  2005  A high speed low-complexity Reed-Solomon decoder for optical communications.  IEEE Transactions on Circuits and Systems II, 52  8 , 461–465.  Lee, L.H.C.  1997  Convolutional Coding: Fundamentals and Applications, Artech House, Boston,  Liew, T. and Hanzo, L.  2002  Space–time codes and concatenated channel codes for wireless  communications. IEEE Proceedings, 90  2 , 187–219.  Lin, S. and Costello, D.  2004  Error Control Coding–Fundamentals and Applications, Pearson  Prentice Hall, Upper Saddle River, NJ. USA.  Ling, S. and Xing, C.  2004  Coding Theory–A First Course, Cambridge University Press, Cambridge,  MA, USA.  UK.  Luby, M., Mitzenmacher, M. and Shokrollahi, A.  1998  Analysis of random processes via and-or tree evaluation. Proceedings of the 9th Annual ACM-SIAM Symposium on Discrete Algorithms, San Francisco, CA, USA, pp. 364–373.  Luby, M., Mitzenmacher, M., Shokrollahi, A. and Spielman, D.  2001  Analysis of low density codes and improved designs using irregular graphs. IEEE Transactions on Information Theory, 47, 585–598.  Lusina, P., Gabidulin, E. and Bossert, M.  2001  Efﬁcient decoding of space–time Hadamard codes using the Hadamard transform IEEE Proceedings of International Symposium on Information Theory, Washington, DC, USA, p. 243.  Lusina, P., Gabidulin, E. and Bossert, M.  2003  Maximum rank distance codes as space–time codes.  IEEE Transactions on Information Theory, 49  10 , 2757–2760.  Lusina, P., Shavgulidze, S. and Bossert, M.  2002  Space time block code construction based on cyclo- tomic coset factorization. IEEE Proceedings of International Symposium on Information Theory, Lausanne, Switzerland, p. 135.  MacKay, D.  1999  Good error correcting codes based on very sparse matrices. IEEE Transactions  on Information Theory, 45, 399–431.  MacWilliams, F. and Sloane, N.  1998  The Theory of Error-Correcting Codes, North-Holland, Ams-  terdam, The Netherlands.  ory, 15, 122–127.  Massey, J.  1969  Shift register synthesis and bch decoding. IEEE Transactions on Information The-  Massey, J.  1974  Coding and modulation in digital communications Proceedings of International  Z¨urich Seminar on Digital Communications, Z¨urich Switzerland. pp. E2 1 –E2 4 .  Massey, J. and Sain, M.  1967  Codes, automata, and continuous systems: explicit interconnections.  IEEE Transactions on Automatic Control, 12, 644–650.  Massey, J. and Sain, M.  1968  Inverses of linear sequential circuits. IEEE Transactions on Computers,  McAdam, P.L., Welch, L.R. and Weber, C.L.  1972  MAP bit decoding of convolutional codes. IEEE  Proceedings of International Symposium on Information Theory, p. 91.  McEliece, R.  1987  Finite Fields for Computer Scientists and Engineers, Kluwer Academic, Boston,  McEliece, R.J.  1998  The algebraic theory of convolutional codes, in Handbook of Coding The- ory  eds V. Pless and W. Huffman , Elsevier Science, Amsterdam, The Netherlands, vol. 1, pp. 1065–1138.  McEliece, R.  2002  The Theory of Information and Coding, 2nd edn, Cambridge University Press,  17, 330–337.  MA, USA.  Cambridge, UK.   331  BIBLIOGRAPHY  Magazine, 34  10 , 124–136.  Moshavi, S.  1996  Multi-user detection for DS-CDMA communications. IEEE Communications  Muller, D.  1954  Applications of boolean algebra to switching circuits design and to error detection.  IRE Transactions on Electronic Computation, EC-3, 6–12.  Mutti, C. and Dahlhaus, D.  2004  Adaptive loading procedures for multiple-input multiple-output OFDM systems with perfect channel state information. Joint COST Workshop on Antennas and Related System Aspects in Wireless Communications, Chalmers University of Technology, Gothen- burg, Sweden.  Naguib, A., Tarokh, V., Seshadri, N. and Calderbank, A.  1997  Space–time coded modulation for high data rate wireless communications. IEEE Proceedings of Global Conference on Telecommu- nications, Phoenix, AZ, USA, vol. 1, pp. 102–109.  Naguib, A., Tarokh, V., Seshadri, N. and Calderbank, A.  1998  A space-time coding modem for high-data-rate wireless communications. IEEE Journal on Selected Areas in Communications, 16  8 , 1459–1478.  Neubauer, A.  2006a  Informationstheorie und Quellencodierung–Eine Einf¨uhrung f¨ur Ingenieure,  Informatiker und Naturwissenschaftler, J. Schlembach Fachverlag, Wilburgstetten, Germany.  Neubauer, A.  2006b  Kanalcodierung–Eine Einf¨uhrung f¨ur Ingenieure, Informatiker und Naturwis-  senschaftler, J. Schlembach Fachverlag, Wilburgstetten, Germany.  Neubauer, A.  2007  Digitale Signal¨ubertragung–Eine Einf¨uhrung in die Signal- und Systemtheorie,  J. Schlembach Fachverlag, Wilburgstetten, Germany.  Olofsson, H. and Furusk¨ar, A.  1998  Aspects of introducing EDGE in existing GSM networks. IEEE Proceedings of International Conference on Universal Personal Communications, Florence, Italy, pp. 421–426.  Orten, P.  1999  Sequential decoding of tailbiting convolutional codes for hybrid ARQ on wire- less channels. IEEE Proceedings of Vehicular Technology Conference, Houston, TX, USA, pp. 279–284.  Paulraj. A., Nabar, R. and Gore, D.  2003  Introduction to Space–Time Wireless Communications,  Pearl, J.  1988  Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, Mor-  Cambridge University Press, Cambridge, UK.  gan Kaufmann, San Francisco, CA, USA.  Perez, L., Seghers, J. and Costello, Jr, D.J.  1996  A distance spectrum interpretation of turbo codes.  IEEE Transactions on Information Theory, 42, 1698–1709.  Peterson, W.  1960  Encoding and error-correcting procedures for Bose–Chaudhuri codes. IRE Trans-  actions on Information Theory, IT-6, 459–470.  Proakis, J.  2001  Digital Communications, 4th edn, McGraw-Hill, New York, NY, USA. Reed, I. and Solomon, G.  1960  Polynomial codes over certain ﬁnite ﬁelds. Journal of the Society  for Industrial and Applied Mathematics, 8, 300–304.  Robertson, P., Hoeher, P. and Villebrun, E.  1997  Optimal and sub-optimal maximum a poste- riori algorithms suitable for turbo decoding. European Transactions on Telecommunications, 8, 119–125.  Richardson, T., Shokrollahi, A. and Urbanke, R.  2001  Design of capacity-approaching irregular  low-density parity-check codes. IEEE Transactions on Information Theory, 47, 619–637.  Richardson, T. and Urbanke, R.  2001  The capacity of low-density parity-check codes under message-  passing decoding. IEEE Transactions on Information Theory, 47, 599–618.  Sayood, K.  2000  Introduction to Data Compression, 2nd edn, Morgan Kaufmann, San Francisco,  CA, USA.  Sayood, K.  2003  Lossless Compression Handbook, Academic Press, Amsterdam, The Netherlands. Schnoor, C. and Euchner, M.  1994  Lattice basis reduction: improved practical algorithms and  solving subset sum problems. Mathematical Programming, 66, 181–191.   332  BIBLIOGRAPHY  Schnug, W.  2002  On Generalized Woven Codes, VDI-Verlag, D¨usseldorf, Germany. Schober, R. and Lampe, L.  2002  Noncoherent Receivers for differential space–time modulation.  ITCT, 50  5 , 768–777.  Schramm, P., Andreasson, H., Edholm, C., Edvardsson, N., H¨o¨ok, M., J¨averbring, S., M¨uller, F. and Sk¨old, J.  1998  radio interface performance of EDGE, a proposal for enhanced data rates in existing digital cellular systems. IEEE Proceedings of Vehicular Technology Conference, Ottawa, Canada, pp. 1064–1068.  Schulze, H. and L¨uders, C.  2005  Theory and Applications of OFDM and CDMA, John Wiley &  Sons, Ltd, Chichester, UK.  Seshadri, N. and Winters, J.  1994  Two signaling schemes for improving the error performance of frequency division  FDD  transmission systems using transmitter antenna diversity. International Journal of Wireless Networks, 1  1 , 49–60.  Seshadri, N., Tarokh, V. and Calderbank, A.  1997  Space–time codes for wireless communication: code construction. IEEE Proceedings of Vehicular Technology Conference, Phoenix, AZ, USA, vol. 2-A, pp. 637–641.  Sezgin, A., W¨ubben, D. and K¨uhn, V.  2003  Analysis of mapping strategies for turbo coded  space–time block codes. IEEE Information Theory Workshop  ITW03 , Paris, France.  Shannon, C.  1948  A mathematical theory of communication. The Bell System Technical Journal,  27, 379–424, 623–656.  Shung, C., Siegel, P., Ungerb¨ock, G. and Thapar, H.  1990  Vlsi architectures for metric normaliza- tion in the Viterbi algorithm. IEEE Proceedings of International Conference on Communications, Atlanta, GA, USA. pp. 1723–1728.  Simon, M. and Alouini, M.S.  2000  Digital Communication over Fading Channels, John Wiley &  Sons, Inc., New York, NY, USA.  Saddle River, NJ, USA.  Publishers, Orlando, FL, USA.  Theory, 27, 533–547.  Sklar, B.  2003  Digital Communications: Fundamentals and Applications. Prentice Hall PTR, Upper  Strang, G.  1988  Linear Algebra and its Applications, 3rd edn, Harcout Brace Jovanovich College  Tanner, M.  1981  A recursive approach to low complexity codes. IEEE Transactions on Information  Tarokh, V., Jafarkhani, H. and Calderbank, A.  1999a  Space–time block codes from orthogonal  designs. IEEE Transactions on Information Theory, 45  5 , 1456–1467.  Tarokh, V., Jafarkhani, H. and Calderbank, A.  1999b  Space–time block coding for wireless com- munications: performance results. IEEE Journal on Selected Areas in Communications, 17  3 , 451–460.  Tarokh, V., Seshadri, N. and Calderbank, A.  1997  Space–time codes for high data rate wireless communication: performance criteria. IEEE Proceedings of International Conference on Commu- nications, Montreal, Canada, vol. 1, pp. 299–303.  Tarokh, V., Seshadri, N. and Calderbank, A.  1998  Space–time codes for high data rate wireless communication: performance criterion and code construction. IEEE Transactions on Information Theory, 44  2 , 744–765.  Telatar, E.  1995  Capacity of multi-antenna Gaussian channels. ATT-Bell Labs Internal Tech. Memo. ten Brink, S.  2000  Design of serially concatenated codes based on iterative decoding convergence. Proceedings 2nd International Symposium on Turbo Codes and Related Topics, Brest, France, pp. 319–322.  ten Brink, S.  2001  Convergence behavior of iteratively decoded parallel concatenated codes. IEEE  Transactions on Communications, COM-49, 1727–1737.  van Lint, J.  1999  Introduction to Coding Theory, 3rd edn, Springer Verlag, Berlin, Germany.   BIBLIOGRAPHY  333  Viterbi, A.J.  1967  Error bounds for convolutional codes and an asymptotically optimum decoding  algorithm. IEEE Transactions on Information Theory, 13, 260–269.  Viterbi, A.J.  1971  Convolutional codes and their performance in communication systems. IEEE  Transactions on Communication Technology, 19, 751–772.  Weiss, C., Bettstetter, C., Riedel, S. and Costello, D.J.  1998  Turbo decoding with tail-biting trellises. Proceedings of URSI International Symposium on Signals, Systems and Electronics, Pisa, Italy, pp. 343–348.  Windpassinger, C. and Fischer, R.  2003a  Low-complexity near-maximum-likelihood detection and precoding for MIMO systems using lattice reduction. IEEE Information Theory Workshop  ITW 2003 , Paris, France.  Windpassinger, C. and Fischer, R.  2003b  Optimum and sub-optimum lattice-reduction-aided detec- tion and precoding for MIMO communications. Canadian Workshop on Information Theory, Waterloo, Ontario, Canada, pp. 88–91.  Wittneben, A.  1991  Basestation modulation diversity for digital SIMUL-CAST. IEEE Proceedings  of Vehicular Technology Conference, pp. 848–853, St Louis, MO, USA.  Wolniansky, P., Foschini, G., Golden, G. and Valenzuela, R.  1998  V-BLAST: an architecture for realizing very high data rates over the rich-scattering wireless channel. Invited paper, Proceedings of International Symposium on Signals, Systems and Electronics, Pisa, Italy.  Wozencraft, J.M.  1957  Sequential decoding for reliable communications. IRE National Convention  Record, 5, 2–11.  Wozencraft, J.M. and Reiffen, B.  1961  Sequential Decoding, MIT Press, Cambridge, MA, USA. W¨ubben, D.  2006  Efﬁziente Detektionsverfahren f¨ur Multilayer-MIMO-Systeme. PhD thesis, Uni-  versit¨at Bremen, Bremen, Germany.  W¨ubben, D, B¨ohnke, R, K¨uhn, V. and Kammeyer, K.D.  2003  MMSE extension of V-BLAST based on sorted QR decomposition. IEEE Semiannual Vehicular Technology Conference  VTC2003-Fall , Orlando, FL, USA.  W¨ubben, D., B¨ohnke, R, K¨uhn, V. and Kammeyer, K.D.  2004a  Near-maximum-likelihood detec- tion of MIMO systems using MMSE-based lattice reduction. IEEE International Conference on Communications  ICC’2004 , Paris, France.  W¨ubben, D., B¨ohnke, R., Rinas, J., K¨uhn, V. and Kammeyer, K.D.  2001  Efﬁcient algorithm for  decoding layered space–time codes. IEE Electronic Letters, 37  22 , 1348–1350.  W¨ubben, D., K¨uhn, V. and Kammeyer, K.D.  2004b  On the robustness of lattice-reduction aided detectors in correlated MIMO systems. IEEE Semiannual Vehicular Technology Conference  VTC2004-Fall , Los Angeles, CA, USA.  Zheng, L. and Tse, D.  2003  Diversity and multiplexing: a fundamental tradeoff in multiple antenna  channels. IEEE Transactions on Information Theory, 49  5 , 1073–1096.  Zigangirov, K.S.  1966  Some sequential decoding procedures. Problemy Peredachi Informatsii, 2,  13–25.  Zyablov, V., Shavgulidze, S., Skopintsev, O., H¨ost, S. and Johannesson, R.  1999b  On the error expo- nent for woven convolutional codes with outer warp. IEEE Transactions on Information Theory, 45, 1649–1653.  Zyablov, V., Shavgulidze, S. and Johannesson, R.  2001  On the error exponent for woven convolu-  tional codes with inner warp. IEEE Transactions on Information Theory, 47, 1195–1199.  Zyablov. V., Johannesson, R., Skopintsev, O. and H¨ost, S.  1999a  Asymptotic distance capabilities  of binary woven convolutional codes. Problemy Peredachi Informatsii, 35, 29–46.    Index   u,v  code construction, 46  a-posteriori probability, 22 acknowledgement, 150 active distances, 122, 202  active burst distance, 123, 202 active column distance, 124 active reverse column distance, 124 active segment distance, 124  addition table, 303 Alamouti, 257 algebraic decoding algorithm, 84 Amplitude Shift Keying, see ASK angular spread, 231 APP decoding, 140, 145,  186  ARQ, 68 ASK, 218 automatic repeat request, 68 AWGN channel, 6, 57  Bahl, 140 basis, 27 Bayes’ rule, 22 BCH bound, 78 BCH code, 80 BCJR algorithm, 140, 181, 186 beamforming, 257 BEC, binary erasure channel, 168 belief propagation algorithm, 168, 174 Benedetto, 184, 196, 212 Berger’s channel diagram, 4 Berlekamp–Massey algorithm, 89 Berrou, 163 Bhattacharyya bound, 132 Bhattacharyya parameter, 132  binary symmetric channel, 5 binomial coefﬁcient, 18 bit energy, 7 bit error probability, 5, 134 bit interleaving, 149 BLAST detection, 275 block check sequence, 153 block code, 13  n, k , 14  Bluetooth, 47, 51 Boolean functions, 58 bottleneck region, 194 bound, 37  asymptotic, 40 Bhattacharyya, 132, 134 Gilbert–Varshamov, 40 Griesmer, 40 Hamming, 37 Plotkin, 40 Singleton, 37 sphere packing, 37 Viterbi, 134, 135  boxplus operation, 172, 173 BSC, 5, 114 burst, 202 burst error probability, 134  cardinality, 299 catastrophic generator matrix, 110 channel  capacity, 4, 248 code, 2 coding, 1 decoder, 2 decoding, 2 encoder, 2  Coding Theory – Algorithms, Architectures, and Applications Andr´e Neubauer, J¨urgen Freudenberger, Volker K¨uhn  cid:148  2007 John Wiley & Sons, Ltd   336  check nodes, 165 CIRC, 83 Cocke, 140 code  block, 13 extension, 43 interleaving, 44 parameter, 16 polynomial, 63 puncturing, 43 rate, 17 shortening, 42 space, 18 space–time, 215 termination, 104, 148, 153 word, 14  commutative group, 295 commutative ring with identity, 296 commutativity, 295 compact disc, 83 complementary error function, 7 concatenated codes, 177, 182 concatenated convolutional codes, 182 conditional entropy, 4 conditional probability, 4 conjugate root, 76, 304 constraint length, 100 controller canonical form, 100 convolutional codes, 98–101, 121, 147,  182, 198  106  convolutional encoder, 98–101, 103,  correction ball, 25 correlated channels, 236 correlation matrix, 238 coset, 34 coset leader, 35 CRC, 66 cross-interleaved Reed–Solomon code,  83  cyclic code, 62 cyclic group, 295 cyclic redundancy check, 66 cyclotomic coset, 77  decision region, 20 decoding strategy, 19  INDEX  demodulation, 2 demodulator, 2 design distance, 80 DFT, 82, 305 differential entropy, 6 digital video broadcasting, 83 dimension, 300 discrete convolution, 69 discrete Fourier transform, 82, 305 distance, 17 diversity, 256  frequency diversity, 223 polarization diversity, 223 space diversity, 223 time diversity, 223 diversity combining, 158 division with remainder, 296 divisor, 296 DoA, direction of arrival, 230 DoD, direction of departure, 230, 237 Doppler delay-angle scattering function,  230  DRAM, 29 dual code, 36 dual cyclic code, 71 DVB, 83 dynamic programming, 117  ECSD, 152 EDGE, 152 effective length, 204 EGPRS, 152 eigenvalue decomposition, 314 elementary symmetric polynomial, 89 Elias, 160 encoder state, 103 encoder state space, 103 entropy, 4 equidistant code, 40 equivalent encoders, 110 equivalent generator matrices, 110 erasure, 93 error  burst, 44, 66 covariance matrix, 275 detection, 34 evaluator polynomial, 89   INDEX  locator, 86 locator polynomial, 88 polynomial, 66, 86 position, 86 value, 86  error event, 121, 134 Euclid’s algorithm, 89, 296 Euclidean metric, 136 EXIT charts, 188, 190, 192–196 EXIT, extrinsic information transfer, 188 expected weight distribution, 196 extended code, 43 extended Hamming code, 51 extension ﬁeld, 75, 302 extrinsic log-likelihood ratio, 173  factorial ring, 63, 301 Fano, 160 fast Fourier transform, 82 fast Hadamard transform, 57 FDD, 239 FFT, 82 FHT, 57 ﬁeld, 298 ﬁnite ﬁeld, 299 ﬁnite geometric series, 91 ﬁnite group, 295 ﬁrst event error probability, 134 Forney, 160 Forney’s formula, 93 fractional rate loss, 105 free distance, 121, 122 frequency division duplex, see FDD frequency hopping, 149 Frobenius norm, 312  Gallager, 163, 165, 167, 168 Galois ﬁeld, 299 generating length, 203 generating tuples, 203 generator matrix, 29 generator polynomial, 64 girth, 177 Glavieux, 163 GMSK, 152 Golay code, 50 GPRS, 152  337  group, 295 GSM, 147, 152, 163  Hadamard matrix, 53 Hagenauer, 140, 169 Hamming code, 29, 49 Hamming distance, 17 hard output, 140 hard-decision decoding, 7 header check sequence, 153 H¨oher, 140 H¨ost, 212 HSCSD, 152 hybrid ARQ, 150  type-I, 151, 156 type-II, 151, 157  identity element, 295 incremental redundancy, 152, 158 independence assumption, 176 information polynomial, 65 information source, 3 information word, 14 inner code, 163 input–output path enumerator, 136 interleaved code, 44 interleaving, 153 interleaving depth, 44 inverse element, 295 IOPEF, 131 IOWEF, 128 irregular code, 167  Jacobian logarithm, 146, 172 Jelinek, 140, 160 Johannesson, 212  key equation, 89, 91  L-value, 169 latency, 46 LDPC codes, 163, 165, 168,  174  linear block code, 27 linear dispersion codes, 265 linear feedback shift register, 72 linear independency, 300   338  linear multilayer detection, 272 linearity, 27 link adaptation, 152, 156 log-likelihood algebra, 169 log-likelihood ratio, 141, 169, 171, 188 LoS, line-of-sight, 230  macrocell, 231 MacWilliams identity, 37 majority decision, 10 majority decoding, 47 majority logic decoding, 56 MAP, 22 MAP decoding, 112 Mariner, 58 Massey, 161 matched ﬁlter, 6 Mattson–Solomon polynomial, 309 max-log approximation, 147, 172 maximum a-posteriori, 22 maximum distance separable, 37 maximum likelihood decoding, 23, 112,  113, 116, 134, 196  maximum ratio combining, 159, 224 McAdam, 140 McEliece, 161 MCS, 152–155 MDS code, 37, 81 MED, 22 memory, 100 memoryless channel, 5 message nodes, 165 message passing, 174 message-passing algorithms, 168 MIMO channel  frequency-selective, 234 modelling, 237  minimal polynomial, 76, 304 minimum constraint length, 100 minimum distance decoding, 24,  113–115, 136  minimum error probability decoding, 22 minimum Hamming distance, 17 minimum length, 210, 211 minimum weight, 17 MISO channel, Multiple-Input  Single-Output channel, 235  INDEX  MLD, 23 modulation, 1 modulator, 2 Montorsi, 196, 212 Moore–Penrose inverse, 276 multilayer transmission, 266 multiplication table, 303 multiplicative inverse, 299 mutual information, 4  narrow-sense BCH code, 80, 84 NLoS, 230 node metric, 117 noise power, 6 noise vector, 57 normal burst, 149 not acknowledgement, 150  octal notation, 109 odd even interleaver, 210 OFD codes, 122 optimal code, 37 optimum free distance codes, 122 order, 295, 303 orthogonal  matrix, 313 space–time block codes, 257  orthogonal code, 36 orthogonality, 30, 56 outer code, 163 overall constraint length, 100  pairwise error probability, 131, 134 parallel concatenation, 182, 185 parity  bit, 28 check code, 27, 48 check condition, 31 check equations, 31 check matrix, 30 check polynomial, 67 check symbol, 29, 48 frequency, 83  partial concatenation, 185 partial distance, 205 partial rate, 185 partial weights, 196   INDEX  path enumerator, 129 path enumerator function, 134 PEF, 131 perfect code, 39 Phase shift keying, see PSK pinch-off region, 194 pit, 85 polling, 155 polynomial, 63, 300 irreducible, 301  polynomial channel model, 72, 86 polynomial generator matrix,  110  polynomial ring, 300 power spectral density, 7 prime number, 296 primitive BCH code, 80 primitive element, 296 primitive polynomial, 303 primitive root, 303 primitive root of unity, 75 product code, 163, 177–179 PSK, 219 punctured code, 43 puncturing, 106, 137, 153–155, 159 pure code combining, 159  QAM, 218 QL decomposition, 278 Quadrature Amplitude Modulation, see  QAM  rank criterion, 256 rank of a matrix, 312 rate-compatible punctured convolutional  codes, 160  Raviv, 140 received polynomial, 66 received word, 14 Reed–Solomon code, 81 Reed–Muller code, 55 regular code, 167 repetition code, 47 residue class ring, 297 ring, 296 RLC, 152, 153 root, 302  339  SCCC, 184 SDMA, 216 selective repeat ARQ, 155 self-dual code, 36 sequence estimation, 112,  116  serial concatenation, 184,  185 Shannon, 163 Shannon function, 4 shortened code, 42 signal, 2 signal power, 6 signal-to-noise ratio, 6 SIMO channel, Single-Input  Multiple-Output channel, 235  simplex code, 51 singular value decomposition, 314 SISO, 192 SISO decoder, 181 SISO decoding, 186 SISO, soft-in soft-out, 181 soft output, 140 soft-decision decoding, 7, 57 soft-input, 136 soft-output decoding, 140 sorted QL decomposition, see SQLD source coding, 1 source decoder, 2 source decoding, 2 source encoder, 2 space–time code, 215 sparse graphs, 165 spatial multiplexing, 265 spectral encoding, 82 spectral norm, 312 spectral polynomial, 82 spectral sequence, 305 sphere detection, 289 SQLD, 285 squared Euclidean distance, 137 state diagram, 103 subspace, 27, 300 superposition, 27 survivor, 118 symbol error probability, 19   340  symbol-by-symbol decoding, 112 syndrome, 33, 86 syndrome polynomial, 72, 89 systematic block code, 29 systematic encoding, 70, 111 systematic generator matrix, 111  tail-biting, 104, 153 tail-biting code, 104 Tanner graph, 164, 165 TDD, 239 termination of a convolutional code,  TFCI, 58 Thitimasjshima, 163 time division duplex, see TDD transmission, 1 trellis diagram, 95, 112, 115 triple repetition code, 9 truncation of a convolutional code,  turbo codes, 163, 182, 183 turbo decoding, 163, 186, 188 turbo decoding algorithm,  104  104  180  twiddle factor, 305  UART, 49 UMTS, 58, 147, 183 unitary matrix, 313  INDEX  V-BLAST, 266 Vandermonde matrix, 79 vector, 299 vector space, 18, 299 Viterbi, 97, 160 Viterbi algorithm, 112, 116, 136 Viterbi bound, 134 Voyager, 50  waterfall region, 193 Weber, 140 WEF, 126 weight, 17 weight distribution, 17, 126, 196 weight enumerator, 126 Welch, 140 wide-open region, 195 Wiener solution, 274 word error probability, 19 woven convolutional codes, 177, 198–200, 202, 203, 205  woven turbo codes, 201 Wozencraft, 160  zero, 75 zero element, 296 zero padding, 46 zero-forcing, 273 Zigangirov, 160 Zyablov, 212
